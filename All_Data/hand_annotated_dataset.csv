Unnamed: 0,repo,filename,Commit,Comment,link,MLTD
0,Erotemic/netharn,netharn/api.py,ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,"TODO: allow for \""discriminative fine-tuning\""",https://github.com/Erotemic/netharn/commit/ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,No
1,Erotemic/netharn,netharn/fit_harn.py,ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,"TODO: allow for \""discriminative fine-tuning\""",https://github.com/Erotemic/netharn/commit/ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,No
2,roberthangu/snn_object_recognition,classify-images-one-shot.py,f77a229eda9deca2562e9098098a1794c5f204c0,# TODO: uncomment these lines after the completion of the STDP tuning,https://github.com/roberthangu/snn_object_recognition/commit/f77a229eda9deca2562e9098098a1794c5f204c0,Yes
3,jason718/game-feature-learning,eval-3rd-party/voc_cls/phikr_caffe/python/detect.py,c1844d400fe3d191c3e012f8c6eb83dfc46ff7e9,"\""\""\"" || detector.py is an out-of-the-box windowed detector || callable from the command line. ||  || By default it configures and runs the Caffe reference ImageNet model. || Note that this model was trained for image classification and not detection; || and finetuning for detection can be expected to improve results. ||  || The selective_search_ijcv_with_python code required for the selective search || proposal mode is available at ||     https:\/\/github.com\/sergeyk\/selective_search_ijcv_with_python ||  || TODO: || - batch up image filenames as well: don't want to load all of them into memory || - come up with a batching scheme that preserved order \/ keeps a unique ID || \""\""\""",https://github.com/jason718/game-feature-learning/commit/c1844d400fe3d191c3e012f8c6eb83dfc46ff7e9,Yes
4,simonfqy/PADME,dcCustom/models/tensor_graph.py,4f3610b7ee05d2bd755fa73ffdd2afdb154f1421,TODO: the following two variables are temporary; for hyperparameter tuning purpose.,https://github.com/simonfqy/PADME/commit/4f3610b7ee05d2bd755fa73ffdd2afdb154f1421,No
5,simonfqy/PADME,dcCustom/models/tensorgraph/tensor_graph.py,2cb677a6ee205673b6e17454b979c3a2cce991a5,TODO: the following two variables are temporary; for hyperparameter tuning purpose.,https://github.com/simonfqy/PADME/commit/2cb677a6ee205673b6e17454b979c3a2cce991a5,No
6,simonfqy/PADME,dcCustom/models/tensorgraph/tensor_graph.py,90a881f4afc0e536b8fc242c9f6646c6cce09b0a,TODO: the following two variables are temporary; for hyperparameter tuning purpose.,https://github.com/simonfqy/PADME/commit/90a881f4afc0e536b8fc242c9f6646c6cce09b0a,No
7,davidhaohanli/Abnormal-Behaviour-Detection,code/split.py,2bcfb1b25d15a402761f41ef866388ded80c50cd,TODO Default tuning,https://github.com/davidhaohanli/Abnormal-Behaviour-Detection/commit/2bcfb1b25d15a402761f41ef866388ded80c50cd,No
8,davidhaohanli/Abnormal-Behaviour-Detection,code/split.py,6a9fd7464ec95dbb8812a557e3f17cb384f5bd01,TODO Default tuning,https://github.com/davidhaohanli/Abnormal-Behaviour-Detection/commit/6a9fd7464ec95dbb8812a557e3f17cb384f5bd01,No
9,HewlettPackard/dlcookbook-dlbs,python/pytorch_benchmarks/benchmarks.py,8f953918efc41de684507734351b7b226066cbb6,TODO: Is it good to enable cuDNN autotuning (batch size is fixed)?,https://github.com/HewlettPackard/dlcookbook-dlbs/commit/8f953918efc41de684507734351b7b226066cbb6,No
10,ShuangLI59/person_search,caffe-fast-rcnn/python/detect.py,52350f294541ceee9cb5b3c04ab728a7babf0bed,"\""\""\"" || detector.py is an out-of-the-box windowed detector || callable from the command line. ||  || By default it configures and runs the Caffe reference ImageNet model. || Note that this model was trained for image classification and not detection; || and finetuning for detection can be expected to improve results. ||  || The selective_search_ijcv_with_python code required for the selective search || proposal mode is available at ||     https:\/\/github.com\/sergeyk\/selective_search_ijcv_with_python ||  || TODO: || - batch up image filenames as well: don't want to load all of them into memory || - come up with a batching scheme that preserved order \/ keeps a unique ID || \""\""\""",https://github.com/ShuangLI59/person_search/commit/52350f294541ceee9cb5b3c04ab728a7babf0bed,Yes
11,explosion/thinc,thinc/neural/_classes/window_encode.py,0e2df2a6d000ed5c7a81f19a710338fb3adaab59,TODO: Implement fine-tuning,https://github.com/explosion/thinc/commit/0e2df2a6d000ed5c7a81f19a710338fb3adaab59,Yes
12,chrislit/abydos,abydos/phones.py,6da092970e952341c60ab024a4e35c36a820f239,TODO: finish implementation\/testing\/tuning,https://github.com/chrislit/abydos/commit/6da092970e952341c60ab024a4e35c36a820f239,No
13,chrislit/abydos,abydos/phones/_phones.py,0186998d684753e070edf1fdc124aaa56133db27,TODO: finish implementation\/testing\/tuning,https://github.com/chrislit/abydos/commit/0186998d684753e070edf1fdc124aaa56133db27,No
14,biolab/orange3,Orange/__init__.py,5cf6b70c5115524f48b8f3eb9a2ce2632ebe4933,"\""\""\"" || _import(\""data.sample\"") || _import(\""data.outliers\"") || _import(\""data.preprocess\"") || _import(\""data.preprocess.scaling\"") || _import(\""data.utils\"") || _import(\""data.discretization\"") || _import(\""data.continuization\"") || _import(\""data.filter\"") || _import(\""data.imputation\"") ||  || _import(\""feature\"") || _import(\""feature.construction\"") || _import(\""feature.construction.functionDecomposition\"") || _import(\""feature.construction.univariate\"") || _import(\""feature.discretization\"") || _import(\""feature.imputation\"") || _import(\""feature.scoring\"") || _import(\""feature.selection\"") ||  || _import(\""network\"") ||  || _import(\""stat\"") ||  || _import(\""statistics\"") || _import(\""statistics.estimate\"") || _import(\""statistics.contingency\"") || _import(\""statistics.distribution\"") || _import(\""statistics.basic\"") || _import(\""statistics.evd\"") ||  || _import(\""classification\"") || _import(\""classification.tree\"") ||  || _import(\""classification.rules\"") ||  || _import(\""classification.lookup\"") || _import(\""classification.bayes\"") || _import(\""classification.svm\"") || _import(\""classification.logreg\"") || _import(\""classification.knn\"") || _import(\""classification.majority\"") ||  || _import(\""tuning\"") ||  || _import(\""projection\"") || _import(\""projection.linear\"") || _import(\""projection.mds\"") || _import(\""projection.som\"") ||  || _import(\""ensemble\"") || _import(\""ensemble.bagging\"") || _import(\""ensemble.boosting\"") || _import(\""ensemble.forest\"") || _import(\""ensemble.stacking\"") ||  || _import(\""regression\"") || _import(\""regression.base\"") || _import(\""regression.earth\"") || _import(\""regression.lasso\"") || _import(\""regression.linear\"") || _import(\""regression.mean\"") || _import(\""regression.pls\"") || _import(\""regression.tree\"") ||  || _import(\""multitarget\"") || _import(\""multitarget.tree\"") ||  || _import(\""multilabel\"") || _import(\""multilabel.multibase\"") || _import(\""multilabel.br\"") || _import(\""multilabel.lp\"") || _import(\""multilabel.mlknn\"") || _import(\""multilabel.brknn\"") || _import(\""multilabel.mulan\"") ||  || _import(\""associate\"") ||  || _import(\""distance\"") ||  || _import(\""wrappers\"") ||  || _import(\""featureConstruction\"") || _import(\""featureConstruction.univariate\"") || _import(\""featureConstruction.functionDecomposition\"") ||  || _import(\""evaluation\"") || _import(\""evaluation.scoring\"") || _import(\""evaluation.testing\"") ||  || _import(\""clustering\"") || _import(\""clustering.kmeans\"") || _import(\""clustering.hierarchical\"") || _import(\""clustering.consensus\"") ||  || _import(\""misc\"") ||  || _import(\""utils\"") #TODO hide utils from the user || _import(\""utils.environ\"") || _import(\""utils.counters\"") || _import(\""utils.addons\"") || _import(\""utils.render\"") || _import(\""utils.serverfiles\"") ||  || _import_addons() || \""\""\""",https://github.com/biolab/orange3/commit/5cf6b70c5115524f48b8f3eb9a2ce2632ebe4933,No
15,Tencent/PocketFlow,learners/nonuniform_quantization/learner.py,82244d9db61f0cf6f28c3d2e31f94b057ce6e623,TODO: add layerwise finetuning. working not very weill,https://github.com/Tencent/PocketFlow/commit/82244d9db61f0cf6f28c3d2e31f94b057ce6e623,Yes
16,Tencent/PocketFlow,learners/uniform_quantization/learner.py,82244d9db61f0cf6f28c3d2e31f94b057ce6e623,add layerwise finetuning. TODO: working not very well,https://github.com/Tencent/PocketFlow/commit/82244d9db61f0cf6f28c3d2e31f94b057ce6e623,Yes
17,chainer/chainer,chainer/links/normalization/batch_normalization.py,d518feb1ee563c9ec5dc22d6fb5a1481fe3561b1,(TODO: mkusumoto) Test finetuning in recomputation.,https://github.com/chainer/chainer/commit/d518feb1ee563c9ec5dc22d6fb5a1481fe3561b1,Yes
18,NervanaSystems/neon,neon/backends/nervanagpu.py,3497dcc8bb6200f2ce073b61bd6e5ad599f23141,TODO: Perhaps I'll add an autotuning mode.,https://github.com/NervanaSystems/neon/commit/3497dcc8bb6200f2ce073b61bd6e5ad599f23141,Yes
19,alvations/pywsd,lesk.py,970ed2b288d3d82f880ab8d4ae63c233211d8b3d,"'' || #TODO: various stem \/ lemmatizers. || from nltk.stem import WordNetLemmatizer || wnl = WordNetLemmatizer() || def stem(word; option=\""wnlemma\"") ||   if option == \""wnlemma\"": ||     return wnl.lemmatize(word) ||   if option == \""porter\"": ||     return porter.stem(word) || '''",https://github.com/alvations/pywsd/commit/970ed2b288d3d82f880ab8d4ae63c233211d8b3d,Yes
20,alvations/pywsd,pywsd/utils.py,802c094aa3b0b97f9fa8d32d8f9379d5cb0329e3,"'' || #TODO: various stem \/ lemmatizers. || from nltk.stem import WordNetLemmatizer || wnl = WordNetLemmatizer() || def stem(word; option=\""wnlemma\"") ||   if option == \""wnlemma\"": ||     return wnl.lemmatize(word) ||   if option == \""porter\"": ||     return porter.stem(word) || '''",https://github.com/alvations/pywsd/commit/802c094aa3b0b97f9fa8d32d8f9379d5cb0329e3,Yes
21,Rostlab/nalaf,nalaf/download_data.py,13bf86d5973aa24fc75f93b73571308700bc94c3,TODO download non-packaged [biolemmatizer-core-1.2-jar-with-dependencies.jar](https:\/\/github.com\/Rostlab\/nalaf\/blob\/develop\/nalaf\/data\/biolemmatizer-core-1.2-jar-with-dependencies.jar),https://github.com/Rostlab/nalaf/commit/13bf86d5973aa24fc75f93b73571308700bc94c3,No
22,explosion/spaCy,spacy/language.py,e5d9eaf79c5c935b4553c5ede31de383571fe0dc,TODO: Will be replaced when the lemmatizer becomes a pipeline component,https://github.com/explosion/spaCy/commit/e5d9eaf79c5c935b4553c5ede31de383571fe0dc,Yes
23,explosion/spaCy,spacy/language.py,0cddb0dbe9b9af85bb017393ec017a16f2acdaec,TODO: Will be replaced when the lemmatizer becomes a pipeline component,https://github.com/explosion/spaCy/commit/0cddb0dbe9b9af85bb017393ec017a16f2acdaec,Yes
24,explosion/spaCy,spacy/language.py,e257e66ab908e289b64976558f843956376923d5,TODO: Will be replaced when the lemmatizer becomes a pipeline component,https://github.com/explosion/spaCy/commit/e257e66ab908e289b64976558f843956376923d5,Yes
25,GilesStrong/lumin,lumin/nn/models/helpers.py,af0cbd5bb81072384b592230616fd97f8f8f1734,TODO: load pretrained embeddings to check sizes,https://github.com/GilesStrong/lumin/commit/af0cbd5bb81072384b592230616fd97f8f8f1734,No
26,noxouille/rt-mrcnn,model.py,2368d206f079adf1bfe7ccb23824d5d18c22e031,TODO: support training mode?,https://github.com/noxouille/rt-mrcnn/commit/2368d206f079adf1bfe7ccb23824d5d18c22e031,Yes
27,shahrukhqasim/TIES-2.0,python/models/basic_model.py,8daa8c380f58759ebd59daf1304bf40eb40371dd,TODO: Fix training parameter,https://github.com/shahrukhqasim/TIES-2.0/commit/8daa8c380f58759ebd59daf1304bf40eb40371dd,Yes
28,Seanforfun/GMAN_Net_Haze_Removal,DehazeNet/dehazenet_eval.py,ab7d09ca84b7bba44924e1db3bdd5071aa693393,TODO Restore our CNN from trained data,https://github.com/Seanforfun/GMAN_Net_Haze_Removal/commit/ab7d09ca84b7bba44924e1db3bdd5071aa693393,Yes
29,sshleifer/object_detection_kitti,slim/models/model_deploy.py,a5c4fd06d21e85a231ec05cc5305478a6c2d6a73,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = slim.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = slim.deploy(config; model_fn; [inputs_queue]; optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/sshleifer/object_detection_kitti/commit/a5c4fd06d21e85a231ec05cc5305478a6c2d6a73,Yes
30,roscisz/TensorHive,tensorhive/core/task_nursery.py,e1d17035b7a07025eabb4154f4c7885c95b21cfa,"\""\""\""TODO Implement controller ||  || Example request with tasks to the API: || { ||     'userId': 20; ||     'commandTemplate': 'CUDA_VISIBLE_DEVICES={CVD} train.py --task-id {TID}'; ||     'values': [ ||         {'hostname': 'galileo'; 'CVD': 0; 'TID': 'ps'}; ||         {'hostname': 'galileo'; 'CVD': 1; 'TID': 'worker'}; ||         {'hostname': 'galileo'; 'CVD': 1; 'TID': 'worker'}; ||         {'hostname': 'galileo'; 'CVD': 1; 'TID': 'worker'}; ||     ] || } || \""\""\""",https://github.com/roscisz/TensorHive/commit/e1d17035b7a07025eabb4154f4c7885c95b21cfa,Yes
31,IlyaGusev/summarus,summarus/seq2seq.py,62af391e8d72618d376d07e85258724c54b18bdf,TODO: Run beam search whenever self.training is False so that we can get,https://github.com/IlyaGusev/summarus/commit/62af391e8d72618d376d07e85258724c54b18bdf,Yes
32,spallas/time-attention,data_loader.py,7801b7353a4aa12f424b959009e9cd47946bb5c9,TODO(xhebraj) implement return of train\/test\/validation,https://github.com/spallas/time-attention/commit/7801b7353a4aa12f424b959009e9cd47946bb5c9,Yes
33,wouterdewinter/deep-hive,server/server.py,ad644dd2d7fc0c2cfca0f0bb80ac45e5f3006090,@todo only pick training images!!!,https://github.com/wouterdewinter/deep-hive/commit/ad644dd2d7fc0c2cfca0f0bb80ac45e5f3006090,Yes
34,gmontamat/gentun,gentun/models/keras_models.py,9df97b50e01a7207fc0d08f2175fa5099654f45f,TODO: cross-validations or at least train\/test split,https://github.com/gmontamat/gentun/commit/9df97b50e01a7207fc0d08f2175fa5099654f45f,Yes
35,mycrazycracy/tf-kaldi-speaker,egs/voxceleb/v1/nnet/lib/train_mt.py,c5ccc12ea32927933eec7169a3b9fe5a51405b68,TODO: Change the model name to train different models,https://github.com/mycrazycracy/tf-kaldi-speaker/commit/c5ccc12ea32927933eec7169a3b9fe5a51405b68,Yes
36,PINTO0309/MobileNetV2-PoseEstimation,tf_pose/network_dsconv.py,5c406756e2850e6b4a626888aa8d0650e0109fd1,.conv(3; 3; 64; 1; 1; name='conv1_2'; trainable=True)     # TODO,https://github.com/PINTO0309/MobileNetV2-PoseEstimation/commit/5c406756e2850e6b4a626888aa8d0650e0109fd1,No
37,wanjinchang/SSH-TensorFlow,lib/model/train_val.py,961b5a3dc1aedd3d70b44552cbdab89a27ef5883,Needs to restore the other hyper-parameters\/states for training; (TODO xinlei) I have,https://github.com/wanjinchang/SSH-TensorFlow/commit/961b5a3dc1aedd3d70b44552cbdab89a27ef5883,No
38,oroszgy/spacy-hungarian-models,src/model_builder/ner.py,82b6286d1e731bd5dd50ce6e00b9055f4b30af6b,FIXME: pre-train the model,https://github.com/oroszgy/spacy-hungarian-models/commit/82b6286d1e731bd5dd50ce6e00b9055f4b30af6b,Yes
39,phohenecker/pytorch-transformer,examples/overfitting_test.py,e80dad100f507b1dcb056e719391408ca433ec32,"\""\""\""An implementation of the overfitting test for the Transformer model. ||  || A simple test; which often signifies bugs in the implementation of a model; is the overfitting test. To that end; the || considered model is trained and evaluated on the same tiny dataset; which it should be able to overfit easily. || Therefore; the final model should yield very high probabilities for the desired target values. If this is not the case; || however; then there is probably something wrong with the tested model and\/or its implementation. ||  || TODO: explain a bit more || \""\""\""",https://github.com/phohenecker/pytorch-transformer/commit/e80dad100f507b1dcb056e719391408ca433ec32,No
40,andreasvc/disco-dop,discodop/treebank.py,b7841627d884f5e6536f6f4aabd63b181ce83c1a,FIXME: for 'rest' to be able to contain parens; would need constraint,https://github.com/andreasvc/disco-dop/commit/b7841627d884f5e6536f6f4aabd63b181ce83c1a,Yes
41,Angzz/panoptic-fpn-gluon,tests/unittests/test_utils_segmentation.py,7caee8e1cb994730efa849cd10603c1294e2b759,TODO FIXME: change it to ADE20K dataset and pretrained model,https://github.com/Angzz/panoptic-fpn-gluon/commit/7caee8e1cb994730efa849cd10603c1294e2b759,Yes
42,Erotemic/netharn,netharn/fit_harn.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,"\""\""\"" || CommandLine: ||     python ~\/code\/netharn\/netharn\/fit_harn.py __doc__ ||  || Notes: ||     when profiling ensure CUDA_LAUNCH_BLOCKING=1 ||  || Notes: ||     to use; your training session must have the concept of: ||         * epochs ||         * batch_size ||         * xpu ||         * train \/ validation datasets ||  ||     or better yet: ||         * a model ||         * a criterion ||         * an optimizer ||  || TODO: ||     [ ] - output \""glance\"" curves to disk ||     [x] - move logs to a logs folder. Keep a single master log in the root ||     [ ] - Why didnt the best_snapshot.pt get saved in the most recent yolo run? ||  || Example: ||     >>> import netharn as nh ||     >>> size = 3 ||     >>> max_epoch = 10 ||     >>> datasets = { ||     >>>     'train': nh.data.ToyData2d(size=size; border=1; n=256; rng=0); ||     >>>     'vali': nh.data.ToyData2d(size=size; border=1; n=128; rng=1); ||     >>> } ||     >>> hyper = { ||     >>>     # --- Data First ||     >>>     'datasets'    : datasets; ||     >>>     'nice'        : 'demo'; ||     >>>     'workdir'     : ub.ensure_app_cache_dir('netharn\/demo'); ||     >>>     'loaders'     : {'batch_size': 64}; ||     >>>     'xpu'         : nh.XPU.cast('auto'); ||     >>>     # --- Algorithm Second ||     >>>     'model'       : (nh.models.ToyNet2d; {}); ||     >>>     'optimizer'   : (nh.optimizers.SGD; { ||     >>>         'lr': 0.0001 ||     >>>     }); ||     >>>     'criterion'   : (nh.criterions.CrossEntropyLoss; {}); ||     >>>     #'criterion'   : (nh.criterions.FocalLoss; {}); ||     >>>     'initializer' : (nh.initializers.KaimingNormal; { ||     >>>         'param': 0; ||     >>>     }); ||     >>>     'scheduler'   : (nh.schedulers.ListedLR; { ||     >>>         'points': {0: .0001; 2: .01; 5: .015; 6: .005; 9: .001}; ||     >>>         'interpolate': True; ||     >>>     }); ||     >>>     'dynamics'   : {'batch_step': 4}; ||     >>>     'monitor'     : (nh.Monitor; { ||     >>>         'max_epoch': max_epoch; ||     >>>     }); ||     >>> } ||     >>> harn = FitHarn(hyper) ||     >>> harn.config['use_tqdm'] = 1 ||     >>> harn.initialize(reset='delete') ||     >>> harn.run() || \""\""\""",https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,No
43,Erotemic/netharn,netharn/fit_harn.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,"\""\""\"" || Notes: ||     when profiling ensure CUDA_LAUNCH_BLOCKING=1 ||  || Notes: ||     to use; your training session must have the concept of: ||         * epochs ||         * batch_size ||         * xpu ||         * train \/ validation datasets ||  ||     or better yet: ||         * a model ||         * a criterion ||         * an optimizer ||  || TODO: ||     [ ] - output \""glance\"" curves to disk ||     [x] - move logs to a logs folder. Keep a single master log in the root ||     [ ] - Why didnt the best_snapshot.pt get saved in the most recent yolo run? ||  || Notes: ||     In the following example we demonstrate how to use netharn to train a model ||     to solve a toy problem. ||  ||     In this toy problem; we do not extend the nh.FitHarn object; so we are ||     using the default behavior of ``run_batch``. The default ``on_batch``; and ||     ``on_epoch`` do nothing; so only loss will be the only measurement of ||     performance. ||  ||     For further examples please see the examples directory. These example show ||     how to extend nh.FitHarn to measure performance wrt a particular problem. ||     The MNIST and CIFAR examples are the most simple. The YOLO example is more ||     complex.  The IBEIS example depends on non-public data \/ software; but can ||     still be useful to look at.  Its complexity is more than CIFAR but less ||     than YOLO. ||  || CommandLine: ||     xdoctest netharn.fit_harn __doc__:0 ||     xdoctest netharn.fit_harn __doc__:0 --progiter ||  || Example: ||     >>> import netharn as nh ||     >>> hyper = nh.HyperParams(**{ ||     >>>     # ================ ||     >>>     # Environment Components ||     >>>     'workdir'     : ub.ensure_app_cache_dir('netharn\/tests\/demo'); ||     >>>     'nice'        : 'demo'; ||     >>>     'xpu'         : nh.XPU.cast('auto'); ||     >>>     # workdir is a directory where intermediate results can be saved ||     >>>     # nice symlinks <workdir>\/fit\/nice\/<nice> -> ..\/runs\/<hashid> ||     >>>     # XPU auto select a gpu if idle and VRAM>6GB else a cpu ||     >>>     # ================ ||     >>>     # Data Components ||     >>>     'datasets'    : {  # dict of plain ol torch.data.Dataset instances ||     >>>         'train': nh.data.ToyData2d(size=3; border=1; n=256; rng=0); ||     >>>         'vali': nh.data.ToyData2d(size=3; border=1; n=128; rng=1); ||     >>>         'test': nh.data.ToyData2d(size=3; border=1; n=128; rng=1); ||     >>>     }; ||     >>>     'loaders'     : {'batch_size': 64}; # DataLoader instances or kw ||     >>>     # ================ ||     >>>     # Algorithm Components ||     >>>     # Note the (cls; kw) tuple formatting ||     >>>     'model'       : (nh.models.ToyNet2d; {}); ||     >>>     'optimizer'   : (nh.optimizers.SGD; { ||     >>>         'lr': 0.0001 ||     >>>     }); ||     >>>     # focal loss is usually better than nh.criterions.CrossEntropyLoss ||     >>>     'criterion'   : (nh.criterions.FocalLoss; {}); ||     >>>     'initializer' : (nh.initializers.KaimingNormal; { ||     >>>         'param': 0; ||     >>>     }); ||     >>>     # these may receive an overhaul soon ||     >>>     'scheduler'   : (nh.schedulers.ListedLR; { ||     >>>         'points': {0: .0001; 2: .01; 5: .015; 6: .005; 9: .001}; ||     >>>         'interpolate': True; ||     >>>     }); ||     >>>     'monitor'     : (nh.Monitor; { ||     >>>         'max_epoch': 10; ||     >>>     }); ||     >>>     # dynamics are a config option that modify the behavior of the main ||     >>>     # training loop. These parameters effect the learned model. ||     >>>     'dynamics'   : {'batch_step': 4}; ||     >>> }) ||     >>> harn = nh.FitHarn(hyper) ||     >>> # non-algorithmic behavior configs (do not change learned models) ||     >>> harn.config['prog_backend'] = 'tqdm' ||     >>> if ub.argflag('--progiter'):  # I prefer progiter (I may be biased) ||     ...     harn.config['prog_backend'] = 'progiter' ||     >>> # start training. ||     >>> harn.initialize(reset='delete') ||     >>> harn.run()  # note: run calls initialize it hasn't already been called. ||     >>> # xdoc: +IGNORE_WANT ||     RESET HARNESS BY DELETING EVERYTHING IN TRAINING DIR ||     Symlink: ...tests\/demo\/fit\/runs\/demo\/keyeewlr -> ...tests\/demo\/fit\/nice\/demo ||     .... already exists ||     .... and points to the right place ||     Initializing tensorboard (dont forget to start the tensorboard server) ||     Model has 824 parameters ||     Mounting ToyNet2d model on CPU ||     Initializing model weights ||      * harn.train_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr' ||      * harn.nice_dpath  = '...tests\/demo\/fit\/nice\/demo' ||     Snapshots will save to harn.snapshot_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr\/torch_snapshots' ||     dont forget to start: ||         tensorboard --logdir ...tests\/demo\/fit\/nice ||     === begin training === ||     epoch lr:0.001 \u2502 vloss: 0.1409 (n_bad_epochs=00; best=0.1409): 100%|\u2588| 10\/10 [00:01<00:00;  9.95it\/s]  0:00<?; ?it\/s] ||     train x64 \u2502 loss:0.147 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8\/8 [00:00<00:00; 130.56it\/s] ||     vali x64 \u2502 loss:0.140 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4\/4 [00:00<00:00; 342.04it\/s] ||     test x64 \u2502 loss:0.140 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4\/4 [00:00<00:00; 342.92it\/s] ||     <BLANKLINE> ||     Maximum harn.epoch reached; terminating ... ||     <BLANKLINE> ||     training completed ||     current lrs: [0.001] ||     harn.train_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr' ||     harn.nice_dpath  = '...tests\/demo\/fit\/nice\/demo' ||     view tensorboard results for this run via: ||         tensorboard --logdir ...tests\/demo\/fit\/nice ||     exiting fit harness. || \""\""\""",https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,No
44,Erotemic/netharn,netharn/examples/cifar.py,5f79282c043a0d58be110ce7719c72f3402fbd58,"\""\""\"" || The examples\/cifar.py is probably the most clear example of what netharn is and || what it's trying to do \/ not do. ||  || The basic idea is make an object that inherits from nh.FitHarn. This is our || harness object. It will contain the hyperparameters as well as the learning || state. All the training loop boilerplate has already been written in the parent || class; so all our child class needs to do is: define `prepare_batch` (not || usually needed) and `run_batch`. Code to measure and record performance should || be placed in `on_batch` and `on_epoch`. ||  || The `train` function is our main entry point. It reads parameters from the || command line to override defaults. It then consructs the `HyperParams` object || and constructs an instance of `CIFAR_FitHarn` and calls `harn.run()`. ||  || This begins the training process. At a high level the harness will load the || data using torch DataLoaders; and call `run_batch` when it needs to compute the || model outputs and loss based on the input data. The returned loss is used to || update the model weights if `harn.tag === 'train'`; for validation; test; and || calibration (todo) datasets the loss is simply recorded. ||  || After `run_batch` finishes the `on_batch` function is called; where you can || optionally return a dict of scalars to log as measurements for this batch (note || loss is always recorded; so we need not return it here; but loss components may || be useful). A similar thing happens in `on_epoch`; where you should return || metrics about the entire dataset. ||  || The training harness manages the fit directory structure based on a hash of the || hyperparameters; the creation of algorithm component instance (e.g. model; || optimizer); initializing model weights; restarting from the most recent epoch; || updating the learning rates; various training loop boilerplate details; || checking divergence; reporting progress; handling differences between train; || validation; and test sets. In short; netharn handles the necessary parts and || let the developer focus on the important parts. || \""\""\""",https://github.com/Erotemic/netharn/commit/5f79282c043a0d58be110ce7719c72f3402fbd58,No
45,Erotemic/netharn,netharn/hyperparams.py,3c00f88b302692511944e4778cc5b93e68c2bcde,TODO: if pretrained is another netharn model; then we should read that,https://github.com/Erotemic/netharn/commit/3c00f88b302692511944e4778cc5b93e68c2bcde,Yes
46,Erotemic/netharn,netharn/initializers/functional.py,e16c3da37b8806b1dbd0f936a2392bb496de76d5,TODO: need to put all trainable layer types here,https://github.com/Erotemic/netharn/commit/e16c3da37b8806b1dbd0f936a2392bb496de76d5,Yes
47,Erotemic/netharn,netharn/initializers/pretrained.py,e16c3da37b8806b1dbd0f936a2392bb496de76d5,TODO: check for train_info.json in a few different places,https://github.com/Erotemic/netharn/commit/e16c3da37b8806b1dbd0f936a2392bb496de76d5,Yes
48,Erotemic/netharn,examples/object_detection.py,ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,TODO: Use sliding windows so detection can be run and trained on,https://github.com/Erotemic/netharn/commit/ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,Yes
49,Erotemic/netharn,netharn/fit_harn.py,a6562c56cb90a795f50be3ac5dc3989bac913183,TODO: find a better\/general way of handling training dynamics,https://github.com/Erotemic/netharn/commit/a6562c56cb90a795f50be3ac5dc3989bac913183,Yes
50,aldro61/kover,src/kover/core/learning/set_covering_machine/rules.py,3ab9ac99fac0a050b91f3c4290e23af9e12fd3ab,TODO: Remove the constraint that forces columns to be sorted,https://github.com/aldro61/kover/commit/3ab9ac99fac0a050b91f3c4290e23af9e12fd3ab,Yes
51,felixchenfy/Realtime-Action-Recognition,src/s5_test.py,4e61c338786efb12c9593bfc9f7f0bc45b40481b,''' || Test action recognition on || (1) a video; (2) a folder of images; (3) or web camera. ||  || Input: ||     classes: data_proc\/classes.csv # TODO: change this to a config file ||     model: model\/trained_classifier.pickle ||  || Output: ||     result video:    output\/${video_name}\/video.avi ||     result skeleton: output\/${video_name}\/skeleton_res\/XXXXX.txt ||     visualization by cv2.imshow() in img_displayer || ''',https://github.com/felixchenfy/Realtime-Action-Recognition/commit/4e61c338786efb12c9593bfc9f7f0bc45b40481b,No
52,andrew-houghton/moon-board-climbing,moon/models/keras_gan_gen/gan.py,d935124d9c4090f0d50801b0725619c5fe70c51d,X_train = X_train \/ 127.5 - 1.  #TODO,https://github.com/andrew-houghton/moon-board-climbing/commit/d935124d9c4090f0d50801b0725619c5fe70c51d,No
53,eigenfoo/modo-de-ambar,game.py,d2d7d159176e267ed3a1c410f3e90a5080fb9146,FIXME stop and start training!!,https://github.com/eigenfoo/modo-de-ambar/commit/d2d7d159176e267ed3a1c410f3e90a5080fb9146,Yes
54,fpaupier/tensorflow-serving_sidecar,slim/deployment/model_deploy.py,e78e2946749dd804dc3868eee8973161222c3d68,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/fpaupier/tensorflow-serving_sidecar/commit/e78e2946749dd804dc3868eee8973161222c3d68,No
55,richemslie/galvanise_zero,src/ggpzero/nn/datacache.py,3e59b1bdd0f6299bdf4190840c261e69330b6bd2,''' || * todo || * bcolz.set_nthreads(nthreads) || * bucket sampling ||  ||  || --- ||  || same bucket algorithm; with indexing: ||  || figure out the sizes required from each generation based on buckets (easy; any rounding issues; || drop from oldest generation) ||  || create a range(n) where n is the size of a generation.  shuffle.  remove head or tail until size. || [old version removed tail; but it doesn't matter] ||  || combine all (need to offset start index of each generation data] ||  || shuffle. ||  || --- ||  || old algo for validation set: || * no shuffling done on generation.  ie always the same last n of the generation data. || * any buckets < 1.0; threw away tail || * was shuffled before training ||  || --- ||  ||  || # XXX call samples -> observations (from self play). ||  ||  || # steps: ||  || init || ---- || * read summary file (gendata_summary.json); and validate against cache || * if no cache \/ summary file.  delete any spurious files.  Create. || * Validate existing files (md5sum). ||  ||  || sync || ---- || * check directory for any recent files. || * read new data; and preprocessed into numpy arrays (as generator) || * append numpy data to cache || * update the summary ||  ||  || create an indexer || ----------------- || * specify from buckets || * init: how to do this?  Should be a bunch of ranges; I guess. || * resample_data\/validation_data() both create a chunk indexer || * what about weightings?  Future step. ||  ||  || generator || --------- || * bcolz generator.  yields batches. ||  || * inputs: ||   * cache & chunk indexer ||  || * evaluation speed tests ||  ||  || callbacks || --------- || * before each epoch.  Idea is to keep epochs small (1 million) ||  ||  ||  ||  ||  || missing from train.py: ||  ||  ||     def verify_samples(self; sm): ||         # create a basestate ||         basestate = sm.new_base_state() ||  ||         counters = [Counter(); Counter()] ||         max_values = [{}; {}] ||         min_values = [{}; {}] ||         for s in self.samples: ||             basestate.from_list(decode_state(s.state)) ||             sm.update_bases(basestate) ||  ||             # get legals... ||             for ri in range(2): ||                 ls = sm.get_legal_state(ri) ||                 policy = s.policies[ri] ||                 for legal in ls.to_list(): ||                     found = False ||                     for ll; pp in policy: ||                         if ll == legal: ||                             max_values[ri][legal] = max(max_values[ri].get(legal; -1); pp) ||                             min_values[ri][legal] = min(max_values[ri].get(legal; 2); pp) ||                             found = True ||                             break ||                     assert found ||                     counters[ri][legal] += 1 ||  ||  ||  ||  ||  ||     def debug(self): ||         # good to see some outputs ||         for x in (10; 420; 42): ||             log.info('train input; shape: %s.  Example: %s' % (self.inputs.shape; self.inputs[x])) ||             for o in self.outputs: ||                 log.info('train output; shape: %s.  Example: %s' % (o.shape; o[x])) ||  ||  || # XXX add tests || class Buckets(object): ||     def __init__(self; bucket_def): ||         self.bucket_def = bucket_def ||  ||     def get(self; depth; max_depth): ||         if not self.bucket_def: ||             return 1.0 ||  ||         for idx; (cut_off; pct) in enumerate(self.bucket_def): ||             if cut_off <= 0: ||                 return self.get2(depth; max_depth; self.bucket_def[idx:]) ||  ||             if depth < cut_off: ||                 return pct ||  ||     def get2(self; depth; max_depth; stripped_def): ||         assert len(stripped_def) == 1 ||         return stripped_def[0][1] ||  ||  || ''',https://github.com/richemslie/galvanise_zero/commit/3e59b1bdd0f6299bdf4190840c261e69330b6bd2,No
56,Sanqiang/text_simplification,tensor2tensor/data_generators/allen_brain.py,aa326601488d19ed8b6209d53c1179632b99a25e,"\""\""\""Problem definitions for Allen Brain Atlas problems. ||  || Notes: ||  ||   * TODO(cwbeitel): Want to be able to increase up-sampling ratio and\/or ||     in-paint fraction over the course of training. This could be done by ||     defining a range of problems or perhaps more aptly with an hparam ||     that is dialed up depending on training performance. ||  || \""\""\""",https://github.com/Sanqiang/text_simplification/commit/aa326601488d19ed8b6209d53c1179632b99a25e,No
57,jason718/game-feature-learning,eval-3rd-party/voc_cls/phikr_caffe/python/detect.py,c1844d400fe3d191c3e012f8c6eb83dfc46ff7e9,"\""\""\"" || detector.py is an out-of-the-box windowed detector || callable from the command line. ||  || By default it configures and runs the Caffe reference ImageNet model. || Note that this model was trained for image classification and not detection; || and finetuning for detection can be expected to improve results. ||  || The selective_search_ijcv_with_python code required for the selective search || proposal mode is available at ||     https:\/\/github.com\/sergeyk\/selective_search_ijcv_with_python ||  || TODO: || - batch up image filenames as well: don't want to load all of them into memory || - come up with a batching scheme that preserved order \/ keeps a unique ID || \""\""\""",https://github.com/jason718/game-feature-learning/commit/c1844d400fe3d191c3e012f8c6eb83dfc46ff7e9,No
58,ziqizhang/chase,python/src/exp/classifier_main.py,3c60baaf6b510847d6fe30af24e144bf7deb9cff,todo: this is not completed. feature dimension must be the same as training data,https://github.com/ziqizhang/chase/commit/3c60baaf6b510847d6fe30af24e144bf7deb9cff,Yes
59,csvance/keras-mobile-detectnet,generator.py,3b6ecaaf8f84e1f44c822445cb27894fed9f2889,"TODO: Do stuff with \""remainder\"" training data",https://github.com/csvance/keras-mobile-detectnet/commit/3b6ecaaf8f84e1f44c822445cb27894fed9f2889,Yes
60,JingqingZ/KG4ZeroShotText,src_key/train.py,3c9f2399b93acfde652bb203915506f9a8775b3c,TODO: uncomment to train on all data,https://github.com/JingqingZ/KG4ZeroShotText/commit/3c9f2399b93acfde652bb203915506f9a8775b3c,Yes
61,JingqingZ/KG4ZeroShotText,src_key/model.py,8d20b1b164d93375d2a8d9011128857e09f7efd8,TODO: kg_vector for training,https://github.com/JingqingZ/KG4ZeroShotText/commit/8d20b1b164d93375d2a8d9011128857e09f7efd8,No
62,JingqingZ/KG4ZeroShotText,src_key/train.py,8d20b1b164d93375d2a8d9011128857e09f7efd8,TODO: remove to add kg_vector for training,https://github.com/JingqingZ/KG4ZeroShotText/commit/8d20b1b164d93375d2a8d9011128857e09f7efd8,Yes
63,JingqingZ/KG4ZeroShotText,src_key/train.py,3d941e3e9bef9b750dff49fcb03815618b036663,TODO: apply constrain on training set size,https://github.com/JingqingZ/KG4ZeroShotText/commit/3d941e3e9bef9b750dff49fcb03815618b036663,Yes
64,JingqingZ/KG4ZeroShotText,src_reject/train_unseen.py,1e995e7efd0fc16ccbde83b0145da241f0442f54,TODO: remove to add kg_vector for training,https://github.com/JingqingZ/KG4ZeroShotText/commit/1e995e7efd0fc16ccbde83b0145da241f0442f54,Yes
65,JingqingZ/KG4ZeroShotText,src_reject/train_unseen.py,1e995e7efd0fc16ccbde83b0145da241f0442f54,TODO: apply constrain on training set size,https://github.com/JingqingZ/KG4ZeroShotText/commit/1e995e7efd0fc16ccbde83b0145da241f0442f54,Yes
66,zhouyanasd/DL-NC,Brian2_scripts/sim_brian_paper/sim_brian_paper_CoE/src/optimizer/bayesian.py,7b69cd47e24e6bce02d7060098ec19db5de2cd21,TODO: support integer; category; and basic scipy.optimize constraints,https://github.com/zhouyanasd/DL-NC/commit/7b69cd47e24e6bce02d7060098ec19db5de2cd21,Yes
67,zhouyanasd/DL-NC,Brian2_scripts/sim_brian_paper/sim_brian_paper_CoE/src/optimizer/surrogate.py,b60d1a75bc2f586ecfd22b4f9d0d0a87180916da,TODO: support integer; category; and basic scipy.optimize constraints,https://github.com/zhouyanasd/DL-NC/commit/b60d1a75bc2f586ecfd22b4f9d0d0a87180916da,Yes
68,CoderPat/OpenGNN,opengnn/utils/copying_wrapper.py,5a4dc2862dbc68e41e5168fca4c0beb34b7defbc,TODO AND WARNING: this is a hotfix so that copying works for trained models.,https://github.com/CoderPat/OpenGNN/commit/5a4dc2862dbc68e41e5168fca4c0beb34b7defbc,Yes
69,diningphil/CGMM,DGI_example_PPI.py,e9c8987e4c5474cadc0929571cac5496241fe003,todo training on the first graph only!,https://github.com/diningphil/CGMM/commit/e9c8987e4c5474cadc0929571cac5496241fe003,No
70,jinnovation/derain-net,train.py,8c0c612f4e677bf338735236382a4a6905bdb921,TODO: handle each of ModeKeys.{EVAL;TRAIN;PREDICT},https://github.com/jinnovation/derain-net/commit/8c0c612f4e677bf338735236382a4a6905bdb921,Yes
71,jinnovation/derain-net,train.py,644414bb2941bed809ea305a3a5949ed2293134f,TODO: handle each of ModeKeys.{EVAL;TRAIN;PREDICT},https://github.com/jinnovation/derain-net/commit/644414bb2941bed809ea305a3a5949ed2293134f,Yes
72,rmalav15/Super-SloMo,main.py,baeee54f3b0a8fbb4dc7366a65edf82ea2730bc9,TODO: Deprecated; Update with tf.train.MonitoredTrainingSession,https://github.com/rmalav15/Super-SloMo/commit/baeee54f3b0a8fbb4dc7366a65edf82ea2730bc9,Yes
73,xingchensong/Speech-Transformer-tf2.0,train_transformer.py,c3b7bc84dee9ddf18463ce6018f42ee7586d9bca,TODO: build model or load pre-trained model,https://github.com/xingchensong/Speech-Transformer-tf2.0/commit/c3b7bc84dee9ddf18463ce6018f42ee7586d9bca,Yes
74,dabasajay/YOLO-Object-Detection,YOLO Algorithm for Object Detection/yad2k/models/keras_yolo.py,dc9779a3bf11d3fe70e2b9d808d725a129da9cb7,TODO: Darknet region training includes extra coordinate loss for early,https://github.com/dabasajay/YOLO-Object-Detection/commit/dc9779a3bf11d3fe70e2b9d808d725a129da9cb7,Yes
75,dlarsen5/AdaptiveSFM,TensorFlow_SFM.py,b92d7e8f97194344146f7af1fa5f094111001135,TODO: train step makes cross_entropy NAN,https://github.com/dlarsen5/AdaptiveSFM/commit/b92d7e8f97194344146f7af1fa5f094111001135,No
76,dlarsen5/AdaptiveSFM,TensorFlow_SFM.py,5b7b0a829d006e31ff23a6df6426182a9eb8bdd3,TODO: train step makes cross_entropy NAN,https://github.com/dlarsen5/AdaptiveSFM/commit/5b7b0a829d006e31ff23a6df6426182a9eb8bdd3,Yes
77,Spground/DLToy,YOLO/yad2k/models/keras_yolo.py,59c4caba09600aad54753f7cf26bfa032405fa84,TODO: Darknet region training includes extra coordinate loss for early,https://github.com/Spground/DLToy/commit/59c4caba09600aad54753f7cf26bfa032405fa84,Yes
78,VSainteuf/izitorch,izitorch/trainRack.py,64d94452c00857131bd49ad85f07cd5123ea997e,TODO implement resuming a training,https://github.com/VSainteuf/izitorch/commit/64d94452c00857131bd49ad85f07cd5123ea997e,No
79,921kiyo/3d-dl,kerasmodels/optimization.py,33dab7d5a21abf10309a87e4cfc4bb939a2357d4,TODO does the imported retrain function take this path?,https://github.com/921kiyo/3d-dl/commit/33dab7d5a21abf10309a87e4cfc4bb939a2357d4,No
80,andreasgrv/johnny,johnny/models.py,9b5ccda43e8b52e89205e48ae53735214e3307f5,TODO: maybe we should use arc_pred sometimes in training??,https://github.com/andreasgrv/johnny/commit/9b5ccda43e8b52e89205e48ae53735214e3307f5,Yes
81,andreasgrv/johnny,johnny/models.py,107448299eb5e36893a35cceecc72d477033a657,TODO Check constraint algorithms + optimise,https://github.com/andreasgrv/johnny/commit/107448299eb5e36893a35cceecc72d477033a657,Yes
82,andreasgrv/johnny,johnny/models.py,107448299eb5e36893a35cceecc72d477033a657,TODO Maybe switch to using predicted arcs towards end of training,https://github.com/andreasgrv/johnny/commit/107448299eb5e36893a35cceecc72d477033a657,Yes
83,Ryan-Amaral/PyTPG,tpg/trainer.py,cf34d341be10e8a91d5e4b0150eb08264b306c19,TODO: I think this checking should be removed (already checked in trainer init),https://github.com/Ryan-Amaral/PyTPG/commit/cf34d341be10e8a91d5e4b0150eb08264b306c19,Yes
84,annomator/annomator_1.0,tf_slim_obj_det/deployment/model_deploy.py,042cc36cba515855a4cbc963df4328123d624c9f,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/annomator/annomator_1.0/commit/042cc36cba515855a4cbc963df4328123d624c9f,No
85,bareblackfoot/lddp-tf-faster-rcnn,lib/model/train_val.py,ee512c4533be6d7c26a1f8ebbc1c373114b5bc9f,Needs to restore the other hyperparameters\/states for training; (TODO xinlei) I have,https://github.com/bareblackfoot/lddp-tf-faster-rcnn/commit/ee512c4533be6d7c26a1f8ebbc1c373114b5bc9f,No
86,bareblackfoot/lddp-tf-faster-rcnn,lib/model/train_val_vgg16.py,f230e29fe94ccffa151a4a4406005c5be280192e,Needs to restore the other hyperparameters\/states for training; (TODO xinlei) I have,https://github.com/bareblackfoot/lddp-tf-faster-rcnn/commit/f230e29fe94ccffa151a4a4406005c5be280192e,No
87,bareblackfoot/lddp-tf-faster-rcnn,lib/model/train_val.py,5ed25ab9bf38c1bc0505dfe83f1a6031a29a5d2a,Needs to restore the other hyper-parameters\/states for training; (TODO xinlei) I have,https://github.com/bareblackfoot/lddp-tf-faster-rcnn/commit/5ed25ab9bf38c1bc0505dfe83f1a6031a29a5d2a,No
88,mpkuse/cartwheel_train,train_netvlad.py,b4ceb6962dfede1926a341156223161c3259f6d3,TODO: codeup a while loop - dry iterations (non training). or may be have a separate,https://github.com/mpkuse/cartwheel_train/commit/b4ceb6962dfede1926a341156223161c3259f6d3,No
89,comtravo/ctparse,ctparse/ctparse.py,0b70e9b1bfa3730613304649bdcbe1d8acde0c8a,TODO: for compatibility reason; remove once we retrain,https://github.com/comtravo/ctparse/commit/0b70e9b1bfa3730613304649bdcbe1d8acde0c8a,No
90,pyro-ppl/funsor,funsor/jax/distributions.py,e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,"TODO fix Delta.arg_constraints[\""v\""] to be a",https://github.com/pyro-ppl/funsor/commit/e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,Yes
91,pyro-ppl/funsor,funsor/jax/distributions.py,e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,"TODO fix Dirichlet.arg_constraints[\""concentration\""] to be a",https://github.com/pyro-ppl/funsor/commit/e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,Yes
92,pyro-ppl/funsor,funsor/torch/distributions.py,e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,"TODO fix Delta.arg_constraints[\""v\""] to be a",https://github.com/pyro-ppl/funsor/commit/e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,Yes
93,pyro-ppl/funsor,funsor/torch/distributions.py,e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,"TODO fix Dirichlet.arg_constraints[\""concentration\""] to be a",https://github.com/pyro-ppl/funsor/commit/e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,Yes
94,pyro-ppl/funsor,funsor/torch/distributions.py,e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,"TODO fix DirichletMultinomial.arg_constraints[\""concentration\""] to be a",https://github.com/pyro-ppl/funsor/commit/e3eba15e2ad7d9db5dd5622c247e0aaa7e81e7dd,Yes
95,pyro-ppl/funsor,funsor/jax/distributions.py,6575ac2c3f7ac25a6a1e5f1107b1b7c072edd992,"TODO fix DirichletMultinomial.arg_constraints[\""concentration\""] to be a",https://github.com/pyro-ppl/funsor/commit/6575ac2c3f7ac25a6a1e5f1107b1b7c072edd992,Yes
96,pyro-ppl/funsor,funsor/jax/distributions.py,84286d17a7af1af9d6721d1a5cb92b6c858dcb37,TODO fix LowRankMultivariateNormal.arg_constraints upstream,https://github.com/pyro-ppl/funsor/commit/84286d17a7af1af9d6721d1a5cb92b6c858dcb37,Yes
97,pyro-ppl/funsor,funsor/torch/distributions.py,84286d17a7af1af9d6721d1a5cb92b6c858dcb37,TODO fix LowRankMultivariateNormal.arg_constraints upstream,https://github.com/pyro-ppl/funsor/commit/84286d17a7af1af9d6721d1a5cb92b6c858dcb37,Yes
98,pyro-ppl/funsor,funsor/torch/distributions.py,84286d17a7af1af9d6721d1a5cb92b6c858dcb37,TODO add temperature to RelaxedBernoulli.arg_constraints upstream,https://github.com/pyro-ppl/funsor/commit/84286d17a7af1af9d6721d1a5cb92b6c858dcb37,Yes
99,aws-samples/deep-learning-models,models/nlp/common/arguments.py,f6f74fb705d38ba94bf0e1ecf815b264fa328c9b,TODO: Change this to per_gpu_train_batch_size,https://github.com/aws-samples/deep-learning-models/commit/f6f74fb705d38ba94bf0e1ecf815b264fa328c9b,Yes
100,aws-samples/deep-learning-models,models/nlp/electra/run_pretraining.py,f768c761dc7d248e52f408f765f87ddd81a39ded,"\""\""\"" || What's the best way to develop a new pretraining script? ||  || Dynamic masking straight from text. || Abtract out the gradient accumulation functionality. Tracking loss; acc variables within the accumulator rather than outside. || Incorporate the new transformers version. Be willing to lose my current work. ||  || # TODO: Should we include special tokens? <BOS>; <EOS>. || # TODO: Weight sharing between generator and discriminator; only token embeddings. ||  || \""\""\""",https://github.com/aws-samples/deep-learning-models/commit/f768c761dc7d248e52f408f765f87ddd81a39ded,No
101,aws-samples/deep-learning-models,models/nlp/common/arguments.py,bda976272e0db701032048f99a378bb897603dc8,TODO: Change this to per_gpu_train_batch_size,https://github.com/aws-samples/deep-learning-models/commit/bda976272e0db701032048f99a378bb897603dc8,Yes
102,aws-samples/deep-learning-models,models/nlp/electra/run_pretraining.py,bda976272e0db701032048f99a378bb897603dc8,"\""\""\"" || Batch sizes: 32 = 5GB memory; 128 = 17GB ||  || The \""read -1 expected ...\"" errors are harmless and come from Docker. See https:\/\/github.com\/horovod\/horovod\/issues\/503 || Running Docker in privileged mode (docker run --privileged) solves the issue. ||  || Dataset handling: Lots of empty lines; use dataset.filter() to eliminate those. || For now; just grab one sentence. || TODO: Combine two segments into a single example. https:\/\/github.com\/google-research\/electra\/blob\/master\/build_pretraining_dataset.py || TODO: Add zero-padding for shorter sequences ||  || nlp feature request: Select from dataset with arbitrary slices || `nlp` package tutorial: https:\/\/colab.research.google.com\/github\/huggingface\/nlp\/blob\/master\/notebooks\/Overview.ipynb || \""\""\""",https://github.com/aws-samples/deep-learning-models/commit/bda976272e0db701032048f99a378bb897603dc8,No
103,aws-samples/deep-learning-models,models/nlp/electra/run_pretraining.py,bda976272e0db701032048f99a378bb897603dc8,TODO: Limit code duplication between train_step and val_step.,https://github.com/aws-samples/deep-learning-models/commit/bda976272e0db701032048f99a378bb897603dc8,Yes
104,estnltk/estnltk,estnltk/ner.py,077bb990814fa1fbf408fc646d5875a066667961,todo: make this script capable of training and annotating corpora,https://github.com/estnltk/estnltk/commit/077bb990814fa1fbf408fc646d5875a066667961,Yes
105,estnltk/estnltk,estnltk/tools/train_default_ner_model.py,0df8df91a57b1ca72687ba70f5934867fa6bee3f,todo: make this script capable of training and annotating corpora,https://github.com/estnltk/estnltk/commit/0df8df91a57b1ca72687ba70f5934867fa6bee3f,Yes
106,Pinafore/qb,generate_makefile.py,507a03a2f01c33ef93828598dfc8fe26e1b99e68,(TODO): Perhaps compress the training files after pasting them together?,https://github.com/Pinafore/qb/commit/507a03a2f01c33ef93828598dfc8fe26e1b99e68,Yes
107,BerkeleyAutomation/gqcnn,gqcnn/search/utils.py,a0d976fd7dd37fee594c1528310d1990efa401af,"\""\""\"" || class GQCNNTrainingProgress(object): ||     def __init__(self; total_epochs): ||         self._training_status = GQCNNTrainingStatus.NOT_STARTED ||         self._epoch = np.nan ||         self._total_epochs = total_epochs || #        self._train_error = np.nan || #        self._val_error = np.nan || #        self._train_loss = np.nan || #        self._val_loss = np.nan ||  ||     @property ||     def training_status(self): ||         return self._training_status ||  ||     @property ||     def epoch(self): ||         return self._epoch ||  ||     @property ||     def total_epochs(self): ||         return self._total_epochs ||  ||     @training_status.setter ||     def training_status(self; status): ||         assert status in GQCNNTrainingStatus.__dict__.keys(); 'Invalid training status \""{}\""'.format(status) #TODO: @Vishal this is kind of jank but still works ||         self._training_status = training_status ||  ||     @epoch.setter ||     def epoch(self; epoch): ||         self._epoch = epoch || \""\""\""",https://github.com/BerkeleyAutomation/gqcnn/commit/a0d976fd7dd37fee594c1528310d1990efa401af,No
108,blue-oil/blueoil,lmnet/executor/tune_ray.py,ef0bdb2707295a1211cc3554106967735157e1b6,TODO (Neil): Enable both train and validation,https://github.com/blue-oil/blueoil/commit/ef0bdb2707295a1211cc3554106967735157e1b6,Yes
109,IntelAI/models,models/image_segmentation/tensorflow/maskrcnn/model.py,370048aa73e610e5f81d082f4651e91e74dceb80,TODO: support training mode?,https://github.com/IntelAI/models/commit/370048aa73e610e5f81d082f4651e91e74dceb80,No
110,IntelAI/models,benchmarks/language_translation/tensorflow/transformer_mlperf/training/bfloat16/model_init.py,77b413820977815fe7d0cfe1217b604acb93e244,TODO: need more arguments for full training,https://github.com/IntelAI/models/commit/77b413820977815fe7d0cfe1217b604acb93e244,Yes
111,IntelAI/models,benchmarks/language_translation/tensorflow/transformer_mlperf/training/fp32/model_init.py,77b413820977815fe7d0cfe1217b604acb93e244,TODO: need more arguments for full training,https://github.com/IntelAI/models/commit/77b413820977815fe7d0cfe1217b604acb93e244,Yes
112,mozilla/bugbug,http_service/models.py,1bae5834abd49f70c69da84d36b66f5f5cf503b0,TODO: Do not crash when the asked model is not one of the trained models,https://github.com/mozilla/bugbug/commit/1bae5834abd49f70c69da84d36b66f5f5cf503b0,Yes
113,mozilla/bugbug,http_service/models.py,8fc95985191ad28b76d4329562133a1d7ea585de,TODO: Do not crash when the asked model is not one of the trained models,https://github.com/mozilla/bugbug/commit/8fc95985191ad28b76d4329562133a1d7ea585de,Yes
114,persephone-tools/persephone,src/datasets/chatino.py,b9528a88a5b73823bf3b1f72e4ff858be18fa10a,TODO Reconsider the place of these splits. Perhaps train\/dev\/test,https://github.com/persephone-tools/persephone/commit/b9528a88a5b73823bf3b1f72e4ff858be18fa10a,Yes
115,persephone-tools/persephone,src/datasets/na.py,d0b0e2be7dd864810568be4004b94fba68d59d7a,TODO Probably should be hardcoding the list of train\/dev\/test utterances,https://github.com/persephone-tools/persephone/commit/d0b0e2be7dd864810568be4004b94fba68d59d7a,Yes
116,persephone-tools/persephone,src/datasets/na.py,588fe2472c158f240e1268426e796e689d178d86,TODO Probably should be hardcoding the list of train\/dev\/test utterances,https://github.com/persephone-tools/persephone/commit/588fe2472c158f240e1268426e796e689d178d86,Yes
117,persephone-tools/persephone,persephone/corpus.py,f4b39f41a07628792e7b99fe4287bc92455478a9,TODO Perhaps the ReadyCorpus train_prefixes variable should be a,https://github.com/persephone-tools/persephone/commit/f4b39f41a07628792e7b99fe4287bc92455478a9,Yes
118,persephone-tools/persephone,persephone/corpus_reader.py,cd8f27586fdaca1a812b8f6d3bc7be956f1dd4a1,TODO This logic should be changed. The number of training instances,https://github.com/persephone-tools/persephone/commit/cd8f27586fdaca1a812b8f6d3bc7be956f1dd4a1,Yes
119,Calysto/conx,conx/network.py,f8a217037617ac60c75fc5fa28bae0d8b4e7ae86,# FIXME: need standard format for train\/test\/inputs\/targets,https://github.com/Calysto/conx/commit/f8a217037617ac60c75fc5fa28bae0d8b4e7ae86,Yes
120,Calysto/conx,conx/network.py,714b30e25858a6cb8586814dcf36bc5e003be0d6,# FIXME: combine results from train and test,https://github.com/Calysto/conx/commit/714b30e25858a6cb8586814dcf36bc5e003be0d6,Yes
121,SMTorg/smt,smt/sm.py,1ec61a2ddabed66564bd0b407c53a617fe772fac,TODO: Extend to multifidelity problems by adding training_pts = {'approx': {}},https://github.com/SMTorg/smt/commit/1ec61a2ddabed66564bd0b407c53a617fe772fac,Yes
122,SMTorg/smt,smt/methods/gekpls.py,37d0ef2c4084a4e9fd3ee2a2ce71cd12e3fc0751,"\""\""\"" || Author: Dr. Mohamed Amine Bouhlel <mbouhlel@umich.edu> ||  || Some functions are copied from gaussian_process submodule (Scikit-learn 0.14) ||  || TODO: || - Add additional points GEKPLS1; GEKPLS2 and so on ||  || - define outputs['sol'] = self.sol ||  || - debug _train: self_pkl = pickle.dumps(obj) ||                            cPickle.PicklingError: Can't pickle <type 'function'>: attribute lookup __builtin__.function failed ||  || \""\""\""",https://github.com/SMTorg/smt/commit/37d0ef2c4084a4e9fd3ee2a2ce71cd12e3fc0751,Yes
123,SMTorg/smt,smt/methods/kplsk.py,37d0ef2c4084a4e9fd3ee2a2ce71cd12e3fc0751,"\""\""\"" || Author: Dr. Mohamed Amine Bouhlel <mbouhlel@umich.edu> ||  || Some functions are copied from gaussian_process submodule (Scikit-learn 0.14) ||  || TODO: || - define outputs['sol'] = self.sol ||  || - debug _train: self_pkl = pickle.dumps(obj) ||                            cPickle.PicklingError: Can't pickle <type 'function'>: attribute lookup __builtin__.function failed ||  || \""\""\""",https://github.com/SMTorg/smt/commit/37d0ef2c4084a4e9fd3ee2a2ce71cd12e3fc0751,Yes
124,SMTorg/smt,smt/methods/krg.py,37d0ef2c4084a4e9fd3ee2a2ce71cd12e3fc0751,"\""\""\"" || Author: Dr. Mohamed Amine Bouhlel <mbouhlel@umich.edu> ||  || Some functions are copied from gaussian_process submodule (Scikit-learn 0.14) ||  || TODO: || - define outputs['sol'] = self.sol ||  || - debug _train: self_pkl = pickle.dumps(obj) ||                            cPickle.PicklingError: Can't pickle <type 'function'>: attribute lookup __builtin__.function failed ||  || \""\""\""",https://github.com/SMTorg/smt/commit/37d0ef2c4084a4e9fd3ee2a2ce71cd12e3fc0751,Yes
125,SMTorg/smt,smt/methods/krg_based.py,dd0a9eaf5737cdb9c34f5597fc0585ccc2ead38f,"\""\""\"" || Author: Dr. Mohamed Amine Bouhlel <mbouhlel@umich.edu> ||  || Some functions are copied from gaussian_process submodule (Scikit-learn 0.14) ||  || TODO: || - fail_iteration and nb_iter_max to remove from options || - define outputs['sol'] = self.sol ||  || - debug _train: self_pkl = pickle.dumps(obj) ||                            cPickle.PicklingError: Can't pickle <type 'function'>: attribute lookup __builtin__.function failed ||  || \""\""\""",https://github.com/SMTorg/smt/commit/dd0a9eaf5737cdb9c34f5597fc0585ccc2ead38f,Yes
126,SMTorg/smt,smt/surrogate_models/surrogate_model.py,8cfc6467dd328c21681c3a559bf17bb628694cf9,TODO: Extend to multifidelity problems by adding training_points = {'approx': {}},https://github.com/SMTorg/smt/commit/8cfc6467dd328c21681c3a559bf17bb628694cf9,Yes
127,AcutronicRobotics/ros2learn,optimization/Spearmint/spearmint/choosers/default_chooser.py,ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,TODO -- could use BFGS for this (unconstrained) optimization as well -- everytime for min of mean,https://github.com/AcutronicRobotics/ros2learn/commit/ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,Yes
128,alan-turing-institute/sktime,sktime/experiments/orchestrator.py,bc775748d83316091efb1d2231d44ce2af042f4a,TODO: save trained strategy on disk,https://github.com/alan-turing-institute/sktime/commit/bc775748d83316091efb1d2231d44ce2af042f4a,Yes
129,IntelAI/models,benchmarks/object_detection/tensorflow/ssd-mobilenet/inference/fp32/model_init.py,396dc9636451a9d6ebce11f9bf0d3283564b2918,TODO: Add training commands,https://github.com/IntelAI/models/commit/396dc9636451a9d6ebce11f9bf0d3283564b2918,Yes
130,IntelAI/models,models/image_recognition/tensorflow/squeezenet/fp32/deployment/model_deploy.py,5f0f82d5b1303d17522789a0132c7aa222348420,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/IntelAI/models/commit/5f0f82d5b1303d17522789a0132c7aa222348420,Yes
131,IntelAI/models,benchmarks/object_detection/tensorflow/fastrcnn/inference/fp32/model_init.py,952bf6983c3c66c48d4ba2356486c5c7cde022e3,TODO: Add training commands,https://github.com/IntelAI/models/commit/952bf6983c3c66c48d4ba2356486c5c7cde022e3,Yes
132,IntelAI/models,benchmarks/classification/tensorflow/wide_deep/inference/fp32/model_init.py,65970463728dbe3cbc761a2688f6f7f16df6f11d,TODO: Add support for training,https://github.com/IntelAI/models/commit/65970463728dbe3cbc761a2688f6f7f16df6f11d,Yes
133,IntelAI/models,benchmarks/object_detection/tensorflow/rfcn/inference/fp32/model_init.py,08455579ddd3c1c3ef1d95e4ea904caad788100d,TODO: Add training commands,https://github.com/IntelAI/models/commit/08455579ddd3c1c3ef1d95e4ea904caad788100d,Yes
134,IntelAI/models,benchmarks/speech_recognition/tensorflow/deep-speech/inference/fp32/model_init.py,481ea9e1c8d12ed74df1b558e587cc5c9cf5befa,TODO: Add training commands,https://github.com/IntelAI/models/commit/481ea9e1c8d12ed74df1b558e587cc5c9cf5befa,Yes
135,spotty-cloud/spotty,spotty/commands/create_ami.py,2feae7f6760bc1e99d429251cf6cdbc7f2b03ea0,TODO: Constraints: 3-128 alphanumeric characters; parentheses (()); square brackets ([]); spaces ( ); periods (.); slashes (\/); dashes (-); single quotes ('); at-signs (@); or underscores(_),https://github.com/spotty-cloud/spotty/commit/2feae7f6760bc1e99d429251cf6cdbc7f2b03ea0,No
136,JayYip/bert-multitask-learning,bert_multitask_learning/top.py,35dac78bd0ed1af4274b54ee3a1d316aa6774d66,TODO: add mlm back to pretrain,https://github.com/JayYip/bert-multitask-learning/commit/35dac78bd0ed1af4274b54ee3a1d316aa6774d66,Yes
137,mondejar/ecg-classification,my_dnn_mitdb.py,8ceb7d449a146e9b66db9e35b6975f612377a74d,TODO weights = tf.mul(train_labels; class_weight),https://github.com/mondejar/ecg-classification/commit/8ceb7d449a146e9b66db9e35b6975f612377a74d,No
138,ThibaultGROUEIX/3D-CODED,training/train_sup.py,383bcbc4e0cf02a644fec8f1c5badc69b762dda1,TODO Build trainer class,https://github.com/ThibaultGROUEIX/3D-CODED/commit/383bcbc4e0cf02a644fec8f1c5badc69b762dda1,No
139,ThibaultGROUEIX/3D-CODED,training/train_sup_2.py,383bcbc4e0cf02a644fec8f1c5badc69b762dda1,TODO Build trainer class,https://github.com/ThibaultGROUEIX/3D-CODED/commit/383bcbc4e0cf02a644fec8f1c5badc69b762dda1,No
140,ThibaultGROUEIX/3D-CODED,training/train_sup_2.py,e066c76f8255a77558d32b12cd3b2e5e7cfc2154,TODO implement multi-gpu training and compare performance,https://github.com/ThibaultGROUEIX/3D-CODED/commit/e066c76f8255a77558d32b12cd3b2e5e7cfc2154,Yes
141,medipixel/rl_algorithms,tests/integration/test_run_distillation_agent.py,8b90ba85162ed0d69cf060770bfbaed92ee0f8bc,TODO: Add student training test code.,https://github.com/medipixel/rl_algorithms/commit/8b90ba85162ed0d69cf060770bfbaed92ee0f8bc,Yes
142,brightmart/ai_law,HAN_train.py,1ddc6853194f18da0b2ada4afbdaf08662abc786,assign_pretrained_word_embedding(sess; vocabulary_index2word; vocab_size; model;FLAGS.word2vec_model_path2;model.Embedding2) #TODO,https://github.com/brightmart/ai_law/commit/1ddc6853194f18da0b2ada4afbdaf08662abc786,No
143,brightmart/ai_law,data_util_hdf5.py,2e0cdd872df063b637f52e3cc7cfe0ddc12c9e8d,3. transform to train\/valid data to standardized format #TODO change to multi-processing version for train,https://github.com/brightmart/ai_law/commit/2e0cdd872df063b637f52e3cc7cfe0ddc12c9e8d,Yes
144,MillionIntegrals/vel,waterboy/rl/commands/rl_train.py,9c69dd532416e0ae9f7677aac41fbfee0f9f2a2a,TODO(jerry): Implement training resume,https://github.com/MillionIntegrals/vel/commit/9c69dd532416e0ae9f7677aac41fbfee0f9f2a2a,Yes
145,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/environment.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Allow providing separate train_input; train_target dataframes; or the full df,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
146,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/recorders.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Record the column features in train df,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
147,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/data/data_core.py,aaea5bb6c0452f694e9da3206836d0152b490bb9,"\""\""\""This module defines mechanisms for managing an experiment's various datasets; and each datasets's || inputs; targets; and predictions. ||  || **Important Contents** ||  || In order to maintain the states of different datasets across all divisions of an experiment and || amid transformations that may be applied to the data via || :mod:`~hyperparameter_hunter.feature_engineering`; two main classes are defined herein: ||  || 1. :class:`BaseDataChunk`: ||  ||     * Logical separations between \""columns\"" of data for a given :class:`BaseDataset` ||     * Held and maintained by :class:`BaseDataset` and its descendants ||     * Three primary descendants of :class:`BaseDataChunk`: ||  ||         1. :class:`InputChunk`: Maintains a dataset's input data (and transformations) ||         2. :class:`TargetChunk`: Maintains a dataset's target data (and transformations) ||         3. :class:`PredictionChunk`: Maintains a dataset's predictions (and transformations) ||  ||     * Descendants of :class:`BaseDataChunk` should implement the eight \""on_<division>_<point>\"" ||       callback methods defined by :class:`~hyperparameter_hunter.callbacks.bases.BaseCallback` ||  ||         * Because :class:`BaseDataChunk` subclasses are isolated from the experiment; these methods ||           need not invoke their `super` methods; although they are allowed to if necessary ||  ||     * :class:`NullDataChunk` does nothing but mimic the normal :class:`BaseDataChunk` child structure ||  ||         * Used for :class:`BaseDataset` subclasses lacking a particular data chunk; such as: ||  ||             1) `TestDataset`'s `TargetChunk`; because the targets for a test dataset are unknown; or ||             2) `TrainDataset`'s `PredictionChunk`; because predictions are not made on training data ||  || 2. :class:`BaseDataset`: ||  ||     # TODO: ... ||  || **Dataset Attribute Syntax** ||  || The intricate subclass network bolstering the module's predominant :class:`BaseDataset` subclasses || may be intimidating at first; but don't worry; there's a shortcut. Follow these steps to ensure || proper syntax and a valid result when accessing data from a || :class:`~hyperparameter_hunter.experiments.CVExperiment`: ||  || 1. {`data_train`; `data_oof`; `data_holdout`; `data_test`} - Dataset attribute || 2. {`input`; `target`; `prediction`} - Data chunk || 3. [`T`] - Optional transformation || 4. {`d`; `run`; `fold`; `rep`; `final`} - Division; initial (`d`) or `final` data ||  || By stacking three values (four if following optional step \""3\"") from the above formula; you can || access all of the interesting stuff stored in the datasets from the comfort of your experiment or || :func:`~hyperparameter_hunter.callbacks.bases.lambda_callback`. ||  || Related || ------- || :mod:`hyperparameter_hunter.callbacks.bases` ||     This module defines the core callback method structure mirrored by :class:`BaseDataCore`. ||     Despite the strong logical connection to this module; it is important to remember that the only ||     actual connection between the two modules is in :mod:`hyperparameter_hunter.callbacks.wranglers` || :mod:`hyperparameter_hunter.callbacks.wranglers` ||     # TODO: ... Handlers for the `Dataset`s to invoke callback methods with required parameters ||     This module defines the callback classes that act as handlers for the descendants of ||     :class:`BaseDataset` || :mod:`hyperparameter_hunter.experiments` ||     # TODO: ... || \""\""\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/aaea5bb6c0452f694e9da3206836d0152b490bb9,No
148,f90/Wave-U-Net,Predict.py,86661874f4a302bdb1b4834641055e19832f9395,TODO add a pretrained model?,https://github.com/f90/Wave-U-Net/commit/86661874f4a302bdb1b4834641055e19832f9395,Yes
149,weiaicunzai/pytorch-cifar100,assignment1/cs231n/classifiers/neural_net.py,2656b874397342bda0ea0abb9374eb8b601ada7e,TODO: Create a random minibatch of training data and labels; storing  #,https://github.com/weiaicunzai/pytorch-cifar100/commit/2656b874397342bda0ea0abb9374eb8b601ada7e,Yes
150,voxelmorph/voxelmorph,src/train_img_template.py,ea4d9e0b063bdd3c142af278a2ae5a3bf92b6f31,TODO: note this is the opposite of train_miccai and it might be confusing.,https://github.com/voxelmorph/voxelmorph/commit/ea4d9e0b063bdd3c142af278a2ae5a3bf92b6f31,Yes
151,pavlin-policar/openTSNE,tsne/tsne.py,7aaa295c13987c7711fa3f15f58998c5cce13774,TODO: This is wrong. We'd want to use the PCA model fit on training data,https://github.com/pavlin-policar/openTSNE/commit/7aaa295c13987c7711fa3f15f58998c5cce13774,Yes
152,ShuangLI59/person_search,caffe-fast-rcnn/python/detect.py,52350f294541ceee9cb5b3c04ab728a7babf0bed,"\""\""\"" || detector.py is an out-of-the-box windowed detector || callable from the command line. ||  || By default it configures and runs the Caffe reference ImageNet model. || Note that this model was trained for image classification and not detection; || and finetuning for detection can be expected to improve results. ||  || The selective_search_ijcv_with_python code required for the selective search || proposal mode is available at ||     https:\/\/github.com\/sergeyk\/selective_search_ijcv_with_python ||  || TODO: || - batch up image filenames as well: don't want to load all of them into memory || - come up with a batching scheme that preserved order \/ keeps a unique ID || \""\""\""",https://github.com/ShuangLI59/person_search/commit/52350f294541ceee9cb5b3c04ab728a7babf0bed,Yes
153,ShuangLI59/person_search,lib/datasets/psdb.py,ac78c9a6bab90dc6962a1cd0b2c19f67cc4e1659,TODO: support evaluation on training split,https://github.com/ShuangLI59/person_search/commit/ac78c9a6bab90dc6962a1cd0b2c19f67cc4e1659,Yes
154,xvjiarui/GCNet,mmdet/models/detectors/mask_scoring_rcnn.py,466926eb499f4b5c93ce03bd7670ce766bb28e18,TODO: refactor forward_train in two stage to reduce code redundancy,https://github.com/xvjiarui/GCNet/commit/466926eb499f4b5c93ce03bd7670ce766bb28e18,Yes
155,astorfi/3D-convolutional-speaker-recognition,1-development/deployment/model_deploy.py,af8b21a2ac3a04cbe60a749846a63ce1584112b8,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/astorfi/3D-convolutional-speaker-recognition/commit/af8b21a2ac3a04cbe60a749846a63ce1584112b8,No
156,astorfi/3D-convolutional-speaker-recognition,2-enrollment/deployment/model_deploy.py,af8b21a2ac3a04cbe60a749846a63ce1584112b8,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/astorfi/3D-convolutional-speaker-recognition/commit/af8b21a2ac3a04cbe60a749846a63ce1584112b8,No
157,astorfi/3D-convolutional-speaker-recognition,3-evaluation/deployment/model_deploy.py,af8b21a2ac3a04cbe60a749846a63ce1584112b8,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/astorfi/3D-convolutional-speaker-recognition/commit/af8b21a2ac3a04cbe60a749846a63ce1584112b8,No
158,deepmedic/deepmedic,deepmedic/dataManagement/sampling.py,902382cdd2f095a89cb8b3bff81f4503dcc5be75,"TODO: I think this should be a \""sampler\"" class and moved to training.py. To keep this file generic-sampling.",https://github.com/deepmedic/deepmedic/commit/902382cdd2f095a89cb8b3bff81f4503dcc5be75,Yes
159,deepmedic/deepmedic,deepmedic/frontEnd/configParsing/modelParams.py,07663bf1b5a9ae61cc93533a8f2f0c586832a1a9,TODO: Move config in train\/test cfg.,https://github.com/deepmedic/deepmedic/commit/07663bf1b5a9ae61cc93533a8f2f0c586832a1a9,Yes
160,deepmedic/deepmedic,deepmedic/frontEnd/configParsing/testSessionParams.py,a84f61744c77e8310dddab69f473211ffd0ced5f,TODO: Merge with same in trainSessionParams,https://github.com/deepmedic/deepmedic/commit/a84f61744c77e8310dddab69f473211ffd0ced5f,Yes
161,brightmart/bert_language_understanding,train_bert_fine_tuning.py,685d3525a55c63200a9a67da549fdb46d432da44,todo need load vocabulary of tokens from pretrain; but labels from real task.,https://github.com/brightmart/bert_language_understanding/commit/685d3525a55c63200a9a67da549fdb46d432da44,Yes
162,brightmart/bert_language_understanding,train_bert_fine_tuning.py,29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,from model.bert_model import BertModel # TODO TODO TODO test whether pretrain can boost perofrmance with other model,https://github.com/brightmart/bert_language_understanding/commit/29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,No
163,brightmart/bert_language_understanding,train_bert_lm.py,29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,from model.bert_model import BertModel # TODO TODO TODO test whether pretrain can boost perofrmance with other model,https://github.com/brightmart/bert_language_understanding/commit/29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,No
164,amdegroot/ssd.pytorch,data_augmentor.py,5fc58c374fcda355749df75e57c4c24dc4ed5b9a,"\""\""\""Data augmentation functionality. Passed as callable transformations to || Dataset classes. ||  || The data augmentation procedures were interpreted from @weiliu89's SSD paper || http:\/\/arxiv.org\/abs\/1512.02325 ||  || TODO: explore https:\/\/github.com\/ncullen93\/torchsample\/blob\/master\/torchsample\/transforms ||     for any useful tranformations || TODO: implement data_augment for training ||  || Ellis Brown || \""\""\""",https://github.com/amdegroot/ssd.pytorch/commit/5fc58c374fcda355749df75e57c4c24dc4ed5b9a,Yes
165,amdegroot/ssd.pytorch,data/data_augmentor.py,041fe47b7ee94960aae0b57c6b979bb733807df1,"\""\""\""Data augmentation functionality. Passed as callable transformations to || Dataset classes. ||  || The data augmentation procedures were interpreted from @weiliu89's SSD paper || http:\/\/arxiv.org\/abs\/1512.02325 ||  || TODO: explore https:\/\/github.com\/ncullen93\/torchsample\/blob\/master\/torchsample\/transforms ||     for any useful tranformations || TODO: implement data_augment for training ||  || Ellis Brown || \""\""\""",https://github.com/amdegroot/ssd.pytorch/commit/041fe47b7ee94960aae0b57c6b979bb733807df1,Yes
166,AKSHAYUBHAT/DeepVideoAnalytics,yolo/train_overfit.py,c83ca25a8ff3691f5d55968969e3e38e6b80b5b4,TODO: For full training; put preprocessing inside training loop.,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/c83ca25a8ff3691f5d55968969e3e38e6b80b5b4,No
167,AKSHAYUBHAT/DeepVideoAnalytics,yolo/yad2k/models/keras_yolo.py,c83ca25a8ff3691f5d55968969e3e38e6b80b5b4,TODO: Darknet region training includes extra coordinate loss for early,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/c83ca25a8ff3691f5d55968969e3e38e6b80b5b4,Yes
168,AKSHAYUBHAT/DeepVideoAnalytics,repos/slim/deployment/model_deploy.py,19b2103de8a502a6569c6a1dce0d75c5465d66dd,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/19b2103de8a502a6569c6a1dce0d75c5465d66dd,No
169,sloria/TextBlob,nltk-3.0a0/nltk/classify/svm.py,d539a164ed1cac8239b632a778946248a00c0c42,TODO: implement passing of SVMlight parameters from train() to learn(),https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
170,sloria/TextBlob,nltk-3.0a0/nltk/tokenize/punkt.py,d539a164ed1cac8239b632a778946248a00c0c42,TODO: Make orthographic heuristic less susceptible to overtraining,https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
171,deepfakes/faceswap,plugins/Model_GAN/Trainer.py,810bd0bce7e63c3bbcb4e3ba3cd3804287f55795,"TODO check \""Tips for mask refinement (optional after >15k iters)\"" => https:\/\/render.githubusercontent.com\/view\/ipynb?commit=87d6e7a28ce754acd38d885367b6ceb0be92ec54&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7368616f616e6c752f66616365737761702d47414e2f383764366537613238636537353461636433386438383533363762366365623062653932656335342f46616365537761705f47414e5f76325f737a3132385f747261696e2e6970796e62&nwo=shaoanlu%2Ffaceswap-GAN&path=FaceSwap_GAN_v2_sz128_train.ipynb&repository_id=115182783&repository_type=Repository#Tips-for-mask-refinement-(optional-after-%3E15k-iters)",https://github.com/deepfakes/faceswap/commit/810bd0bce7e63c3bbcb4e3ba3cd3804287f55795,No
172,deepfakes/faceswap,plugins/Model_GAN128/Trainer.py,810bd0bce7e63c3bbcb4e3ba3cd3804287f55795,"TODO check \""Tips for mask refinement (optional after >15k iters)\"" => https:\/\/render.githubusercontent.com\/view\/ipynb?commit=87d6e7a28ce754acd38d885367b6ceb0be92ec54&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7368616f616e6c752f66616365737761702d47414e2f383764366537613238636537353461636433386438383533363762366365623062653932656335342f46616365537761705f47414e5f76325f737a3132385f747261696e2e6970796e62&nwo=shaoanlu%2Ffaceswap-GAN&path=FaceSwap_GAN_v2_sz128_train.ipynb&repository_id=115182783&repository_type=Repository#Tips-for-mask-refinement-(optional-after-%3E15k-iters)",https://github.com/deepfakes/faceswap/commit/810bd0bce7e63c3bbcb4e3ba3cd3804287f55795,No
173,dsgissin/DiscriminativeActiveLearning,models.py,1cc684947a890d5ee44a88e1546d3a805c2af70d,train the model:  TODO - fix the ugly if statements and put this in the arguments of the script,https://github.com/dsgissin/DiscriminativeActiveLearning/commit/1cc684947a890d5ee44a88e1546d3a805c2af70d,Yes
174,jasonwu0731/trade-dst,models/TRADE.py,bba6c529ec5166fe65aee77fedede4de13c88da4,TODO: Parallel this part to make it train faster,https://github.com/jasonwu0731/trade-dst/commit/bba6c529ec5166fe65aee77fedede4de13c88da4,Yes
175,themightyoarfish/deepVO,model.py,a602fb018c97ca4ee397ca112690d9013847c3fc,TODO: Try different dropout schemes. On the small training set; every kind of dropout,https://github.com/themightyoarfish/deepVO/commit/a602fb018c97ca4ee397ca112690d9013847c3fc,Yes
176,nikonikolov/rltf,rltf/models/bstrap_dqn.py,6e728b4f063567ca5ed81e14a36e434a0b431b47,TODO: If plotting; self.plot_train will be empty,https://github.com/nikonikolov/rltf/commit/6e728b4f063567ca5ed81e14a36e434a0b431b47,Yes
177,nikonikolov/rltf,rltf/models/bstrap_dqn.py,32be7d072eb63f2bde3ce12980688bb920258e4f,TODO: If plotting; self.plot_train will be empty,https://github.com/nikonikolov/rltf/commit/32be7d072eb63f2bde3ce12980688bb920258e4f,Yes
178,nikonikolov/rltf,rltf/models/bstrap_dqn.py,fa81f348bed8e03af8d9b3e28950bd6325a39631,TODO: If plotting; self.plot_train will be empty,https://github.com/nikonikolov/rltf/commit/fa81f348bed8e03af8d9b3e28950bd6325a39631,Yes
179,nikonikolov/rltf,rltf/models/bstrap_dqn.py,2fd6df63d130605a1de3649aa5ff3c6fc4647ce1,TODO: If plotting; self.plot_train will be empty,https://github.com/nikonikolov/rltf/commit/2fd6df63d130605a1de3649aa5ff3c6fc4647ce1,Yes
180,lopuhin/transformer-lm,lm/gpt_2_tf2/main2.py,af7b8a1e48615938cd187c90c4a0b85a63cac493,FIXME this does not train for some reason,https://github.com/lopuhin/transformer-lm/commit/af7b8a1e48615938cd187c90c4a0b85a63cac493,Yes
181,Shun14/TextBoxes_plusplus_Tensorflow,deployment/model_deploy.py,dc5d7b7c5bc9df657bf66122c54685af158fc3a6,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||     g = tf.Graph() ||  ||     # Set up DeploymentConfig ||     config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||     # Create the global step on the device storing the variables. ||     with tf.device(config.variables_device()): ||         global_step = slim.create_global_step() ||  ||     # Define the inputs ||     with tf.device(config.inputs_device()): ||         images; labels = LoadData(...) ||         inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||     # Define the optimizer. ||     with tf.device(config.optimizer_device()): ||         optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||     # Define the model including the loss. ||     def model_fn(inputs_queue): ||         images; labels = inputs_queue.dequeue() ||         predictions = CreateNetwork(images) ||         slim.losses.log_loss(predictions; labels) ||  ||     model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                    optimizer=optimizer) ||  ||     # Run training. ||     slim.learning.train(model_dp.train_op; my_log_dir; ||                                             summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||     * outputs: The return values of the calls to `model_fn()`. ||     * scope: The scope used to create the clone. ||     * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||     * train_op: An operation that run the optimizer training op and include ||         all the update ops created by `model_fn`. Present only if an optimizer ||         was specified. ||     * summary_op: An operation that run the summaries created by `model_fn` ||         and process_gradients. ||     * total_loss: A `Tensor` that contains the sum of all losses created by ||         `model_fn` plus the regularization losses. ||     * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||     * num_clones: Number of model clones to deploy in each replica. ||     * clone_on_cpu: True if clones should be placed on CPU. ||     * replica_id: Integer.  Index of the replica for which the model is ||             deployed.  Usually 0 for the chief replica. ||     * num_replicas: Number of replicas to use. ||     * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||     * worker_job_name: A name for the worker job. ||     * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||     - describe side effect to the graph. ||     - what happens to summaries and update_ops. ||     - which graph collections are altered. ||     - write a tutorial on how to use this. ||     - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/Shun14/TextBoxes_plusplus_Tensorflow/commit/dc5d7b7c5bc9df657bf66122c54685af158fc3a6,No
182,SketchyScene/SketchyScene,Instance_Segmentation/libs/model.py,5728ae33732e425cfde01d358748964ffeb21061,TODO: support training mode?,https://github.com/SketchyScene/SketchyScene/commit/5728ae33732e425cfde01d358748964ffeb21061,No
183,sarnthil/unify-emotion-datasets,create_unified_dataset.py,89d65cf01f32745b0a623ffec1e1d384a2707fa8,TODO add a field _train; _test ; _dev in the unified dataset,https://github.com/sarnthil/unify-emotion-datasets/commit/89d65cf01f32745b0a623ffec1e1d384a2707fa8,No
184,mdangschat/ctc-asr,python/s_input.py,5ea981ef60a3ae6e99fc88b8f259d4d0f7ec8b89,TODO: Rewrite this function to match inputs_train().,https://github.com/mdangschat/ctc-asr/commit/5ea981ef60a3ae6e99fc88b8f259d4d0f7ec8b89,Yes
185,mdangschat/ctc-asr,python/s_input.py,87707ea66dfee298e28cf003254345e48f49e089,TODO train.txt,https://github.com/mdangschat/ctc-asr/commit/87707ea66dfee298e28cf003254345e48f49e089,No
186,mdangschat/ctc-asr,asr/dataset/download_if_necessary.py,93457113aeb7feb766a33131f666c85363b98cbc,TODO Wrapper for all used dataset wrappers; that creates the train.txt; test.txt; dev.txt,https://github.com/mdangschat/ctc-asr/commit/93457113aeb7feb766a33131f666c85363b98cbc,No
187,mdangschat/ctc-asr,asr/dataset/download.py,17e93a3aaf005e4b2adcf2b819a32f1d8a883636,TODO Wrapper for all used dataset wrappers; that creates the train.txt; test.txt; dev.txt,https://github.com/mdangschat/ctc-asr/commit/17e93a3aaf005e4b2adcf2b819a32f1d8a883636,No
188,mdangschat/ctc-asr,python/s_input.py,73ed7bb582d73cc32aa6c77173e82dc04fc3dbae,TODO Acquire train data.,https://github.com/mdangschat/ctc-asr/commit/73ed7bb582d73cc32aa6c77173e82dc04fc3dbae,Yes
189,mdangschat/ctc-asr,python/train.py,a2ea955a2690e465779a2cf12b1d2c396c5f3420,tf.train.NanTensorHook(loss); TODO fix,https://github.com/mdangschat/ctc-asr/commit/a2ea955a2690e465779a2cf12b1d2c396c5f3420,Yes
190,mdangschat/ctc-asr,python/train.py,a2ea955a2690e465779a2cf12b1d2c396c5f3420,TODO: Removed for now. Complete training first.,https://github.com/mdangschat/ctc-asr/commit/a2ea955a2690e465779a2cf12b1d2c396c5f3420,Yes
191,mdangschat/ctc-asr,python/train.py,63f0039ad70757a5545345130604b8d55182b9ac,TODO: Removed for now. Complete training first.,https://github.com/mdangschat/ctc-asr/commit/63f0039ad70757a5545345130604b8d55182b9ac,Yes
192,mdangschat/ctc-asr,python/train.py,03b4112d801477030feea79b29f2a9dc1968308e,TODO: Removed for now. Complete training first.,https://github.com/mdangschat/ctc-asr/commit/03b4112d801477030feea79b29f2a9dc1968308e,Yes
193,mdangschat/ctc-asr,python/train.py,d14b4246be09237d5c370f4fc48f7abd58fc83aa,tf.train.NanTensorHook(loss); TODO fix,https://github.com/mdangschat/ctc-asr/commit/d14b4246be09237d5c370f4fc48f7abd58fc83aa,No
194,mdangschat/ctc-asr,python/train.py,d14b4246be09237d5c370f4fc48f7abd58fc83aa,TODO: Removed for now. Complete training first.,https://github.com/mdangschat/ctc-asr/commit/d14b4246be09237d5c370f4fc48f7abd58fc83aa,Yes
195,gabrielspmoreira/chameleon_recsys,nar_module/nar/benchmarks/sr-gnn/run_sr_gnn.py,e949d94623be7a908f7b71600af5e5d66f0f6b29,TODO: Known issue: Item coverage here is not considering as recommendable items those who were not in the train set,https://github.com/gabrielspmoreira/chameleon_recsys/commit/e949d94623be7a908f7b71600af5e5d66f0f6b29,Yes
196,dsindex/etagger,embvec.py,5981669425f61aba81bef88d9bbea387d83933b2,FIXME for fast training. when it comes to service; comment out,https://github.com/dsindex/etagger/commit/5981669425f61aba81bef88d9bbea387d83933b2,Yes
197,tasoc/starclass,run_training.py,fd0d9ba0035e75aa1e483a7018794b8fbb20662d,TODO: Save results for this classifier\/trainingset in database,https://github.com/tasoc/starclass/commit/fd0d9ba0035e75aa1e483a7018794b8fbb20662d,Yes
198,tasoc/starclass,run_training.py,695cb85e29b5aa21b1045071bd42407eebf90ae7,TODO: Make sure this doesn't load a previous trained model!,https://github.com/tasoc/starclass/commit/695cb85e29b5aa21b1045071bd42407eebf90ae7,Yes
199,tasoc/starclass,starclass/BaseClassifier.py,695cb85e29b5aa21b1045071bd42407eebf90ae7,TODO: Save results for this classifier\/trainingset in database,https://github.com/tasoc/starclass/commit/695cb85e29b5aa21b1045071bd42407eebf90ae7,Yes
200,tasoc/starclass,starclass/BaseClassifier.py,41d344231d2f625a786d544365e577799af2b82a,TODO: Save results for this classifier\/trainingset in database,https://github.com/tasoc/starclass/commit/41d344231d2f625a786d544365e577799af2b82a,Yes
201,tasoc/starclass,starclass/BaseClassifier.py,7f5ef45645ceebb2b509ee282798fe7fc5b1279c,TODO: Save results for this classifier\/trainingset in database,https://github.com/tasoc/starclass/commit/7f5ef45645ceebb2b509ee282798fe7fc5b1279c,Yes
202,tasoc/starclass,starclass/taskmanager.py,fa0cb97c8a8d0db12c12a22a5262206014ed0e9f,FIXME: Enforce this for META only. The problem is the TrainingSet class; which doesn't know about which classifier is running it,https://github.com/tasoc/starclass/commit/fa0cb97c8a8d0db12c12a22a5262206014ed0e9f,Yes
203,praekelt/feersum-nlu-api-wrappers,examples/faq_matcher.py,89bc7c11ab4b266b49822958581e8b8ae193e349,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/89bc7c11ab4b266b49822958581e8b8ae193e349,Yes
204,praekelt/feersum-nlu-api-wrappers,examples/faq_matcher.py,2cb9afe4141f131228d3aeeecb65fc22b201fe07,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/2cb9afe4141f131228d3aeeecb65fc22b201fe07,Yes
205,praekelt/feersum-nlu-api-wrappers,examples/faq_matcher_load_training_samples_from_csv.py,e3f6c1e77f10e0143aa709160271b7b803a6be96,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/e3f6c1e77f10e0143aa709160271b7b803a6be96,Yes
206,praekelt/feersum-nlu-api-wrappers,test/test_intent_classifier.py,9168e2a8571e204dad26dc926cc7fec1883e8ced,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/9168e2a8571e204dad26dc926cc7fec1883e8ced,Yes
207,praekelt/feersum-nlu-api-wrappers,examples/intent_classifier_load_training_samples_from_csv.py,69283eae26bb2af3666175c1f92e1bf311c7aef1,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/69283eae26bb2af3666175c1f92e1bf311c7aef1,Yes
208,praekelt/feersum-nlu-api-wrappers,test/test_faq_matcher_perf.py,69283eae26bb2af3666175c1f92e1bf311c7aef1,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/69283eae26bb2af3666175c1f92e1bf311c7aef1,Yes
209,praekelt/feersum-nlu-api-wrappers,test/test_text_classifier.py,69283eae26bb2af3666175c1f92e1bf311c7aef1,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/69283eae26bb2af3666175c1f92e1bf311c7aef1,Yes
210,praekelt/feersum-nlu-api-wrappers,examples/image_classifier.py,39573ef876b31a2832b1a745dca121de8b306e8d,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/39573ef876b31a2832b1a745dca121de8b306e8d,Yes
211,praekelt/feersum-nlu-api-wrappers,test/test_text_classifier_usent.py,1dee9d8fd1d35cbf1d646c6c3476bb2e02eb7a05,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/1dee9d8fd1d35cbf1d646c6c3476bb2e02eb7a05,Yes
212,praekelt/feersum-nlu-api-wrappers,test/test_text_classifier_word_manifold.py,1dee9d8fd1d35cbf1d646c6c3476bb2e02eb7a05,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/1dee9d8fd1d35cbf1d646c6c3476bb2e02eb7a05,Yes
213,praekelt/feersum-nlu-api-wrappers,examples/image_classifier_from_dataset.py,6babc34e51bc9ac1b4e8dd21ad665395ecb9ac32,ToDo: Stop if details indicate that training failed.,https://github.com/praekelt/feersum-nlu-api-wrappers/commit/6babc34e51bc9ac1b4e8dd21ad665395ecb9ac32,Yes
214,sintefneodroid/agent,agent/training/experimental/grid_train_agent.py,b47c940b9ef87998133c861a6d1e0b4a657bbe71,TODO: Implement grid search training,https://github.com/sintefneodroid/agent/commit/b47c940b9ef87998133c861a6d1e0b4a657bbe71,Yes
215,IXarea/LittleGAN,eager_trainer.py,5cb04c899036f3106ef51e07d3e72fef811efff6,Todo: combine training,https://github.com/IXarea/LittleGAN/commit/5cb04c899036f3106ef51e07d3e72fef811efff6,Yes
216,dlt-rilmta/emtsv,config.py,a2504a712e3636f761852ce7b69635e225dee73b,TODO: Train new model!,https://github.com/dlt-rilmta/emtsv/commit/a2504a712e3636f761852ce7b69635e225dee73b,Yes
217,trungnt13/odin-ai,odin/networks/util_layers.py,f97ed6633426b4b4cb8e2b4c86101f0d12f5a1f9,TODO: this is bad solution to resume the training,https://github.com/trungnt13/odin-ai/commit/f97ed6633426b4b4cb8e2b4c86101f0d12f5a1f9,Yes
218,trungnt13/odin-ai,odin/networks/advance_model.py,b9712f837003d9b32852c0a162afc61f99992842,TODO: this is bad solution to resume the training,https://github.com/trungnt13/odin-ai/commit/b9712f837003d9b32852c0a162afc61f99992842,Yes
219,Neuraxio/Neuraxle,neuraxle/metaopt/random.py,b7b6a1aa3216bb4cdba76c92efe3dcacc46e1771,TODO: Verify that we need to split into training and validation or into inputs and expected_outputs.,https://github.com/Neuraxio/Neuraxle/commit/b7b6a1aa3216bb4cdba76c92efe3dcacc46e1771,Yes
220,Neuraxio/Neuraxle,neuraxle/metaopt/random.py,a8465c8189026255d21a030db1b657199228b93d,TODO: Verify that we need to split into training and validation or into inputs and expected_outputs.,https://github.com/Neuraxio/Neuraxle/commit/a8465c8189026255d21a030db1b657199228b93d,Yes
221,lvzhaoyang/DeeperInverseCompositionalAlgorithm,code/models/geometry.py,dc5f38f3edefba0def7fc310c11682cc86874e2a,@todo: not sure whether it is absoluately necessary in training.,https://github.com/lvzhaoyang/DeeperInverseCompositionalAlgorithm/commit/dc5f38f3edefba0def7fc310c11682cc86874e2a,Yes
222,bxshi/ConMask,ndkgc/models/dkrl.py,8e2d4c09c6ba665195b290a8fd32eea6da25358c,TODO: change n_entity to n_train_entity and n_unseen_entity;,https://github.com/bxshi/ConMask/commit/8e2d4c09c6ba665195b290a8fd32eea6da25358c,Yes
223,bxshi/ConMask,ndkgc/models/dkrl.py,b5a585403b667fa43c4a580ffc420e4a4ae23bbc,TODO: Change to tf.contrib.training.bucket_by_sequence_length,https://github.com/bxshi/ConMask/commit/b5a585403b667fa43c4a580ffc420e4a4ae23bbc,Yes
224,josedolz/HyperDenseNet,src/HyperDenseNet/generateNetwork.py,18c35dd81927707d8e0b17b357b38540a266b33e,TODO: Specify also the weights if pre-trained,https://github.com/josedolz/HyperDenseNet/commit/18c35dd81927707d8e0b17b357b38540a266b33e,Yes
225,josedolz/HyperDenseNet,src/HyperDenseNet/startTraining.py,18c35dd81927707d8e0b17b357b38540a266b33e,TODO: Make a line that adds a point at each trained batch (Or percentage being updated),https://github.com/josedolz/HyperDenseNet/commit/18c35dd81927707d8e0b17b357b38540a266b33e,Yes
226,simonkamronn/kvae,kvae/filter.py,817701004151798ac6957ddd30bdf56bb20c36cf,TODO: better to use trainable=False?,https://github.com/simonkamronn/kvae/commit/817701004151798ac6957ddd30bdf56bb20c36cf,Yes
227,mnicnc404/CartoonGan-tensorflow,train.py,1811b401848f4349568f693124074f5203feb98a,g.load(sess; self.save_dir; ckpt_name)  # TODO: use previously trained gan,https://github.com/mnicnc404/CartoonGan-tensorflow/commit/1811b401848f4349568f693124074f5203feb98a,Yes
228,mnicnc404/CartoonGan-tensorflow,train.py,d8f35de54195dd3c9f92eef375c4f105c3a95a9b,g.load(sess; self.save_dir; ckpt_name)  # TODO: use previously trained gan,https://github.com/mnicnc404/CartoonGan-tensorflow/commit/d8f35de54195dd3c9f92eef375c4f105c3a95a9b,Yes
229,mnicnc404/CartoonGan-tensorflow,tf2/cartoonizer.py,43536744dff57c2f145e0f13aaeff8bc47d24376,TODO: add self-trained cartoongan: MODE,https://github.com/mnicnc404/CartoonGan-tensorflow/commit/43536744dff57c2f145e0f13aaeff8bc47d24376,Yes
230,mnicnc404/CartoonGan-tensorflow,tf2/train.py,1e1b1f8e45565f067aded204cf2a4ab405439c4f,TODO: adversarial training code,https://github.com/mnicnc404/CartoonGan-tensorflow/commit/1e1b1f8e45565f067aded204cf2a4ab405439c4f,Yes
231,drckf/paysage,paysage/models/hidden.py,a6316e764c26589184c846a9c96c18fce8fb935a,TODO: implement parameter constraints,https://github.com/drckf/paysage/commit/a6316e764c26589184c846a9c96c18fce8fb935a,Yes
232,drckf/paysage,paysage/fit.py,990c540af3d7e35ca190ff237170c1374b1d8ecd,tmp.set_state(abatch.get('train')) # TODO: fix,https://github.com/drckf/paysage/commit/990c540af3d7e35ca190ff237170c1374b1d8ecd,No
233,drckf/paysage,paysage/models/tap_machine.py,78078ae23df5f07853cb06180c443ed953f2daa1,TODO: remove special constraint for Bernoulli case,https://github.com/drckf/paysage/commit/78078ae23df5f07853cb06180c443ed953f2daa1,Yes
234,drckf/paysage,paysage/models/model.py,698ccb20b3ef9c99aa3085bda0440331b652697a,TODO: re-implement support for constraint satisfaction method,https://github.com/drckf/paysage/commit/698ccb20b3ef9c99aa3085bda0440331b652697a,Yes
235,drckf/paysage,paysage/models/model.py,708e3583a4d3edda59779f74d2c387bed2130f0b,TODO: re-implement support for constraint satisfaction method as needed,https://github.com/drckf/paysage/commit/708e3583a4d3edda59779f74d2c387bed2130f0b,Yes
236,drckf/paysage,paysage/models/model.py,a28501f94342560209e174b0f8ecd2ea67accd6a,TODO: re-implement support for constraint satisfaction method as needed,https://github.com/drckf/paysage/commit/a28501f94342560209e174b0f8ecd2ea67accd6a,Yes
237,drckf/paysage,paysage/models/model.py,2ac00cb4f671ea4e17b9d47e1270989e1e5b8433,TODO: re-implement support for constraint satisfaction method as needed,https://github.com/drckf/paysage/commit/2ac00cb4f671ea4e17b9d47e1270989e1e5b8433,Yes
238,drckf/paysage,paysage/models/model.py,f92a17b542bfaf833fdb1dbf633642f5417f6909,TODO: re-implement support for constraint satisfaction method as needed,https://github.com/drckf/paysage/commit/f92a17b542bfaf833fdb1dbf633642f5417f6909,Yes
239,kaka-lin/object-detection,yad2k/models/keras_yolo.py,3e16939d14e657cf9a556c68225224e784f598e1,TODO: Darknet region training includes extra coordinate loss for early,https://github.com/kaka-lin/object-detection/commit/3e16939d14e657cf9a556c68225224e784f598e1,No
240,mirzaevinom/data_science_bowl_2018,codes/model.py,3e5e9a7b0e9d6a3a6444f7ab4baefc1e9f6073a4,TODO: support training mode?,https://github.com/mirzaevinom/data_science_bowl_2018/commit/3e5e9a7b0e9d6a3a6444f7ab4baefc1e9f6073a4,No
241,PennyLaneAI/pennylane-pq,tests/test_compare_with_default_qubit.py,a5e75a047c98470845796bc4888aded80e5cb8c5,todo: For these operations it is impossible to guess the size and all constrains on the matrix,https://github.com/PennyLaneAI/pennylane-pq/commit/a5e75a047c98470845796bc4888aded80e5cb8c5,No
242,PennyLaneAI/pennylane-pq,tests/test_compare_with_default_qubit.py,a5e75a047c98470845796bc4888aded80e5cb8c5,todo: For these expectations it is impossible to guess the size and all constrains on the matrix,https://github.com/PennyLaneAI/pennylane-pq/commit/a5e75a047c98470845796bc4888aded80e5cb8c5,No
243,OFAI/hub-toolbox-python3,MutualProximity.py,f8b960447491df3945b621bdf896e9c3b3edac8a,TODO implement train_set_mask! Currently BROKEN.,https://github.com/OFAI/hub-toolbox-python3/commit/f8b960447491df3945b621bdf896e9c3b3edac8a,Yes
244,OFAI/hub-toolbox-python3,MutualProximity.py,f8b960447491df3945b621bdf896e9c3b3edac8a,TODO implement train_set_mask!,https://github.com/OFAI/hub-toolbox-python3/commit/f8b960447491df3945b621bdf896e9c3b3edac8a,Yes
245,OFAI/hub-toolbox-python3,MutualProximity.py,a00fff1b924513f14141cbb111831ca8e6c868a2,TODO implement train_set_mask! Currently BROKEN.,https://github.com/OFAI/hub-toolbox-python3/commit/a00fff1b924513f14141cbb111831ca8e6c868a2,Yes
246,OFAI/hub-toolbox-python3,hub_toolbox/MutualProximity_parallel.py,0b474700a33614bfdac09be06409890e43ec1379,TODO implement train_set_mask!,https://github.com/OFAI/hub-toolbox-python3/commit/0b474700a33614bfdac09be06409890e43ec1379,Yes
247,OFAI/hub-toolbox-python3,hub_toolbox/MutualProximity.py,315a33d0945801355c4395981cc63f1c27d5c1d7,TODO implement train_test split,https://github.com/OFAI/hub-toolbox-python3/commit/315a33d0945801355c4395981cc63f1c27d5c1d7,Yes
248,OFAI/hub-toolbox-python3,hub_toolbox/MutualProximity_parallel.py,8b3627e29ab9ccd99fd4f028c1eced855d05989e,TODO implement train_test split,https://github.com/OFAI/hub-toolbox-python3/commit/8b3627e29ab9ccd99fd4f028c1eced855d05989e,Yes
249,OFAI/hub-toolbox-python3,hub_toolbox/MutualProximity_parallel.py,e9ae86af153db3ad89d78b7e297b9fa5871eb9c9,TODO implement train_test split,https://github.com/OFAI/hub-toolbox-python3/commit/e9ae86af153db3ad89d78b7e297b9fa5871eb9c9,Yes
250,OFAI/hub-toolbox-python3,hub_toolbox/MutualProximity.py,ffdaf604a6a6f129f2da68e7c6e240fbba97cb70,TODO implement train_test split,https://github.com/OFAI/hub-toolbox-python3/commit/ffdaf604a6a6f129f2da68e7c6e240fbba97cb70,Yes
251,OFAI/hub-toolbox-python3,hub_toolbox/approximate.py,a614f5aae2fb8e5115f10655605c0aa38741bb5a,TODO use self.X_train_norm_squared_,https://github.com/OFAI/hub-toolbox-python3/commit/a614f5aae2fb8e5115f10655605c0aa38741bb5a,Yes
252,sigmaai/semantic-segmentation,train.py,6889ee3c806b40220e3efbc8c822bbcd87fc7c96,TODO: Fix the train generator code,https://github.com/sigmaai/semantic-segmentation/commit/6889ee3c806b40220e3efbc8c822bbcd87fc7c96,Yes
253,WHUIR/RAGE,contrib/seq2seq/decoder.py,58ddd33cb9f8473df5bc021eefc3c7b6406dec23,TODO dynamic decode defined scope here! train has to do this too,https://github.com/WHUIR/RAGE/commit/58ddd33cb9f8473df5bc021eefc3c7b6406dec23,No
254,JetsGame/GroomRL,src/groomer/models.py,84398ef18e0ac7201dfcdde874e1cfd760ff7cc2,compute a metric for training set (TODO: change to validation),https://github.com/JetsGame/GroomRL/commit/84398ef18e0ac7201dfcdde874e1cfd760ff7cc2,No
255,drivendataorg/zamba,zamba/models/cnnensemble_model.py,8fcb6d64f83be76f9f61282721ff4f28271c68ad,TODO: for all models types; train a single model on the whole dataset,https://github.com/drivendataorg/zamba/commit/8fcb6d64f83be76f9f61282721ff4f28271c68ad,Yes
256,drivendataorg/zamba,zamba/models/cnnensemble_model.py,8fcb6d64f83be76f9f61282721ff4f28271c68ad,TODO: retrain for -1 fold,https://github.com/drivendataorg/zamba/commit/8fcb6d64f83be76f9f61282721ff4f28271c68ad,Yes
257,drivendataorg/zamba,zamba/models/cnnensemble/src/single_frame_cnn.py,8f27ebfe85bbd6cb99a2ab047c9377a65054d023,TODO: it's worth to switch back to the correct preprocess_input when InceptionResNetV2 model is re-trained,https://github.com/drivendataorg/zamba/commit/8f27ebfe85bbd6cb99a2ab047c9377a65054d023,Yes
258,pailabteam/pailab,repo/repo.py,42cdd23047e6249b2f7d1db6ce953e097f696d6d,todo include training data into measures,https://github.com/pailabteam/pailab/commit/42cdd23047e6249b2f7d1db6ce953e097f696d6d,Yes
259,pailabteam/pailab,pailab/repo.py,fb1e770d54555a69108fef6273b299d0108fd8c6,TODO: replace get_trainign data by get using the training data name,https://github.com/pailabteam/pailab/commit/fb1e770d54555a69108fef6273b299d0108fd8c6,Yes
260,pailabteam/pailab,pailab/repo.py,1772fac536dd295a0796fb73ed39152afab2c360,TODO: replace get_trainign data by get using the training data name,https://github.com/pailabteam/pailab/commit/1772fac536dd295a0796fb73ed39152afab2c360,Yes
261,ucloud/uai-sdk,examples/tensorflow/train/slim/code/deployment/model_deploy.py,33018523cc2b4efe7b9872aebb7d2211a9d68482,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/ucloud/uai-sdk/commit/33018523cc2b4efe7b9872aebb7d2211a9d68482,No
262,bioidiap/bob,src/python/face_verification/test/test_bancasmall.py,bd51d8bb893f104fcb434da9e954c4334d872a4f,TODO: compare trained and enrolled values?,https://github.com/bioidiap/bob/commit/bd51d8bb893f104fcb434da9e954c4334d872a4f,Yes
263,nengo/nengo-dl,nengo_lasagne/nengo_lasagne/layers.py,75c58f100d2b5b7bcadc1b6661d53f860bf1c9cd,# TODO: do we want gains\/biases to be trainable?,https://github.com/nengo/nengo-dl/commit/75c58f100d2b5b7bcadc1b6661d53f860bf1c9cd,Yes
264,nengo/nengo-dl,nengo_deeplearning/signals.py,d86d0addba1cdd82b36bcdd947ae70495d743ee4,TODO: should we disable training on connections to learning,https://github.com/nengo/nengo-dl/commit/d86d0addba1cdd82b36bcdd947ae70495d743ee4,No
265,nengo/nengo-dl,nengo_deeplearning/tests/test_simulator.py,28f2e87497ee78d3a89be08efbef56c6164f58d0,TODO: why does training fail if we probe out instead of out.neurons?,https://github.com/nengo/nengo-dl/commit/28f2e87497ee78d3a89be08efbef56c6164f58d0,No
266,nengo/nengo-dl,nengo_dl/tensor_graph.py,aa1a83622a4973643ad6969d7beb8dff5d04a559,TODO: should we disable training on connections to learning,https://github.com/nengo/nengo-dl/commit/aa1a83622a4973643ad6969d7beb8dff5d04a559,No
267,vecto-ai/vecto,vecto/embeddings/__init__.py,b4a4bc4ed0bab02c909b30243756303b3bef7a19,TODO: remove this hack after we re-train w2v without OOV rows,https://github.com/vecto-ai/vecto/commit/b4a4bc4ed0bab02c909b30243756303b3bef7a19,Yes
268,vecto-ai/vecto,vecto/embeddings/__init__.py,88260bca37e45bd00f407f3ce6d75f2509948594,TODO: remove this hack after we re-train w2v without OOV rows,https://github.com/vecto-ai/vecto/commit/88260bca37e45bd00f407f3ce6d75f2509948594,Yes
269,IBM/mi-prometheus,problems/seq_to_seq/text2text/translation.py,28a866da7a713df7603261aba7afd51d131ca554,TODO: is it useful? How to delimitate train & test dataset?,https://github.com/IBM/mi-prometheus/commit/28a866da7a713df7603261aba7afd51d131ca554,Yes
270,IBM/mi-prometheus,trainer.py,1cdfe57ea74f885f241ef8298d744d5a146ce9d2,"\""\""\"" || trainer.py: Contains the code implementation of the main worker of mi-prometheus. || This worker in particular is called the `episodic trainer` and will take care of training || a specified model on a specified problem for a given number of episodes (among other adjustable || parameters). ||  || #TODO: Enhance this description and documentation. ||  || \""\""\""",https://github.com/IBM/mi-prometheus/commit/1cdfe57ea74f885f241ef8298d744d5a146ce9d2,No
271,IBM/mi-prometheus,workers/grid_tester_cpu.py,c81197f2d557bc9891107facac19ed6e6481ba36,Keep only the folders that contain validation.csv and training.csv TODO: Why?,https://github.com/IBM/mi-prometheus/commit/c81197f2d557bc9891107facac19ed6e6481ba36,No
272,NRCan/geo-deep-learning,inference.py,799154d53596eb8ecea1bdf5dc918400bb6fc452,FIXME: color mapping scheme is hardcoded for now because of memory constraint; To be fixed.,https://github.com/NRCan/geo-deep-learning/commit/799154d53596eb8ecea1bdf5dc918400bb6fc452,Yes
273,NRCan/geo-deep-learning,inference.py,01a112b06ddc151769c62daabda400fe2478bf92,FIXME: color mapping scheme is hardcoded for now because of memory constraint; To be fixed.,https://github.com/NRCan/geo-deep-learning/commit/01a112b06ddc151769c62daabda400fe2478bf92,Yes
274,criteo/tf-yarn,tf_skein/cluster.py,746b8a9a0cd846542182b20827e7d377a1eca70e,TODO: return the result of train_and_evaluate.,https://github.com/criteo/tf-yarn/commit/746b8a9a0cd846542182b20827e7d377a1eca70e,Yes
275,IBM/MAX-Object-Detector,training/training_code/object_detection/slim/deployment/model_deploy.py,e7aef82865713ee7bd109156a2eb2fadd57f2387,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/IBM/MAX-Object-Detector/commit/e7aef82865713ee7bd109156a2eb2fadd57f2387,No
276,ucam-smt/sgnmt,cam/sgnmt/blocks/pruning.py,3bc3f8320fd950aa9a19a7f68044b5b2073cb98d,TODO: Decoder training stream mask!,https://github.com/ucam-smt/sgnmt/commit/3bc3f8320fd950aa9a19a7f68044b5b2073cb98d,No
277,MaestroGraph/sparse-hyper,experiments/sparsity.py,bc40787d01ffe0915cc1527dca8ff8225513b9b2,TODO: make sparse layer respond to train,https://github.com/MaestroGraph/sparse-hyper/commit/bc40787d01ffe0915cc1527dca8ff8225513b9b2,Yes
278,BaderLab/saber,kari/models/multi_task_lstm_crf.py,ae1f5b49942c664ba1d7fe4d5c4cfa6b936fcfaf,TODO (johngiorgi): read up on train_test_split; do I want to shuffle?,https://github.com/BaderLab/saber/commit/ae1f5b49942c664ba1d7fe4d5c4cfa6b936fcfaf,No
279,BaderLab/saber,kari/models/multi_task_lstm_crf.py,59fb29cdbc7d3f197b65e08562d9be547f03e990,TODO (johngiorgi): the way I get train\/test partitions is likely copying,https://github.com/BaderLab/saber/commit/59fb29cdbc7d3f197b65e08562d9be547f03e990,No
280,BaderLab/saber,saber/tests/test_trainer.py,97623f68836314834a6869dc912eeb5842367722,TODO (johngiorgi): begin writing tests; start with _split_train_valid,https://github.com/BaderLab/saber/commit/97623f68836314834a6869dc912eeb5842367722,No
281,jhu-lcsr/costar_plan,costar_models/python/costar_models/husky_sampler.py,421d27cf0727979b428e8a41340754f4c13d91ca,assert train_size == (64*64*3) + self.num_pose_vars + 1 + self.num_options; #KDK TODO,https://github.com/jhu-lcsr/costar_plan/commit/421d27cf0727979b428e8a41340754f4c13d91ca,No
282,jhu-lcsr/costar_plan,costar_google_brainrobotdata/costar_block_stacking_train_regression.py,e7239f2f2ee168cb389bfb95af4a8c9e0b2971ce,TODO(ahundt) it seems set_trainable_layers in grasp_model.py has a bug?,https://github.com/jhu-lcsr/costar_plan/commit/e7239f2f2ee168cb389bfb95af4a8c9e0b2971ce,No
283,jhu-lcsr/costar_plan,costar_hyper/costar_block_stacking_train_regression.py,d98af11625416a37a1125306e8ecac7b94996dc2,TODO(ahundt) it seems set_trainable_layers in grasp_model.py has a bug?,https://github.com/jhu-lcsr/costar_plan/commit/d98af11625416a37a1125306e8ecac7b94996dc2,No
284,google-research/google-research,meta_reward_learning/semantic_parsing/nsm/graph_factory.py,d0c80b240d279fbe2420adf30a837689f5530746,TODO(rishabhagarwal): Hack for loading a model trained on cloud machine.,https://github.com/google-research/google-research/commit/d0c80b240d279fbe2420adf30a837689f5530746,No
285,google-research/google-research,m_theory/dim4/so8_supergravity_extrema/code/distillation.py,b1605a7f036e18d58f8c5f2c5b719a0b522835b5,TODO(tfish): Improve handling of known linear constraints between,https://github.com/google-research/google-research/commit/b1605a7f036e18d58f8c5f2c5b719a0b522835b5,Yes
286,google-research/google-research,constrained_language_typology/sigtyp_reader_main.py,07c42b122d364de5f29b18b195e0d5bc779d9af2,"r\""\""\""Reader for the format provided by SIGTYP 2020 Shared Task. ||  || More information on the format is available here: ||   https:\/\/sigtyp.github.io\/st2020.html ||  || Example: || -------- ||  Clone the GitHub data to ST2020_DIR. Then run: ||  ||  > ST2020_DIR=... ||  > python3 sigtyp_reader_main.py --sigtyp_dir ${ST2020_DIR}\/data \\ ||     --output_dir ${OUTPUT_DIR} ||  ||  The above will create \""train.csv\""; \""dev.csv\"" and \""test_blinded.csv\"" files ||  converted from the format provided by SIGTYP. Our models should be able to ||  injest these csv files. Along each of the above files; an accompanying ||  \""data_train_*.json.gz\"" file is generated that contains metainformation on ||  various features and their values. ||  || TODO: || ----- || Following needs to be done: ||   - Latitude and longitude need to be on a point on a unit sphere? Keep as is ||     and add three further columns for (x;y;z)? ||   - Country codes are *several*. ||   - Other types of SOMs. ||   - Use BaseMap for visualizations? || \""\""\""",https://github.com/google-research/google-research/commit/07c42b122d364de5f29b18b195e0d5bc779d9af2,No
287,OpenNMT/OpenNMT-py,train.py,3b8dccc260327f7602a978c87ee62c9dc3dc1f75,TODO: this creates a problem for pretrained vecs; as Guillaume noted,https://github.com/OpenNMT/OpenNMT-py/commit/3b8dccc260327f7602a978c87ee62c9dc3dc1f75,Yes
288,OpenNMT/OpenNMT-py,train.py,b77a5880c9907d01ad101d31666c2fb95fe4dd34,TODO: this creates a problem for pretrained vecs; as Guillaume noted,https://github.com/OpenNMT/OpenNMT-py/commit/b77a5880c9907d01ad101d31666c2fb95fe4dd34,Yes
289,OpenNMT/OpenNMT-py,onmt/modules/SRU.py,ce28c7355c753ce398ab36298e9ac7992fe1989b,"\""\""\"" || Implementation of \""Training RNNs as Fast as CNNs\"". || TODO: turn to pytorch's implementation when it is available. ||  || This implementation is adpoted from the author of the paper: || https:\/\/github.com\/taolei87\/sru\/blob\/master\/cuda_functional.py. || \""\""\""",https://github.com/OpenNMT/OpenNMT-py/commit/ce28c7355c753ce398ab36298e9ac7992fe1989b,No
290,GalDude33/Fetal-MRI-Segmentation,DataGenerator.py,07b293da3611beea1479eb3e810c5f7ed7dc5ff3,TODO: Edge case? Currently this is handled by flooring the number of training\/testing samples,https://github.com/GalDude33/Fetal-MRI-Segmentation/commit/07b293da3611beea1479eb3e810c5f7ed7dc5ff3,Yes
291,ludwig-ai/ludwig,ludwig/models/model.py,760c1b5a7812432e61c3fb4fbb815b63db6e0e19,train_writer.add_summary(summary; step)  # todo tf2: delete following to clean up after TF2,https://github.com/ludwig-ai/ludwig/commit/760c1b5a7812432e61c3fb4fbb815b63db6e0e19,No
292,ludwig-ai/ludwig,ludwig/models/modules/recurrent_modules.py,3994e2c67e8a274f285e513b700483e605c8f4ce,todo: tf2 to be removed #train_helper;,https://github.com/ludwig-ai/ludwig/commit/3994e2c67e8a274f285e513b700483e605c8f4ce,No
293,ludwig-ai/ludwig,ludwig/models/modules/optimization_modules.py,82ee8bf74e06b92a6b7e2499e3d827cd0542baba,tf.train.AdagradDAOptimizer  todo appears tf.keras.optimizers does not support,https://github.com/ludwig-ai/ludwig/commit/82ee8bf74e06b92a6b7e2499e3d827cd0542baba,No
294,ludwig-ai/ludwig,ludwig/models/modules/optimization_modules.py,82ee8bf74e06b92a6b7e2499e3d827cd0542baba,tf.train.MomentumOptimizer  todo appears tf.keras.optimizers does not support,https://github.com/ludwig-ai/ludwig/commit/82ee8bf74e06b92a6b7e2499e3d827cd0542baba,No
295,ludwig-ai/ludwig,ludwig/models/modules/optimization_modules.py,82ee8bf74e06b92a6b7e2499e3d827cd0542baba,tf.train.ProximalGradientDescentOptimizer todo appears tf.keras.optimizers does not support,https://github.com/ludwig-ai/ludwig/commit/82ee8bf74e06b92a6b7e2499e3d827cd0542baba,No
296,ludwig-ai/ludwig,ludwig/models/modules/optimization_modules.py,82ee8bf74e06b92a6b7e2499e3d827cd0542baba,tf.train.ProximalAdagradOptimizer todo appears tf.keras.optimizers does not support,https://github.com/ludwig-ai/ludwig/commit/82ee8bf74e06b92a6b7e2499e3d827cd0542baba,No
297,ludwig-ai/ludwig,ludwig/models/modules/recurrent_modules.py,904049eabc5ad507078dd8e791bd3469549d8412,train_sampler;  # todo: tf2 to be removed #train_helper;,https://github.com/ludwig-ai/ludwig/commit/904049eabc5ad507078dd8e791bd3469549d8412,Yes
298,pyro-ppl/pyro,pyro/contrib/gp/__init__.py,168517d164aff0909f3798a8eff43bd2f60ca7e6,TODO: use `constraint_to` inside `pyro.param(...)` when available,https://github.com/pyro-ppl/pyro/commit/168517d164aff0909f3798a8eff43bd2f60ca7e6,Yes
299,pyro-ppl/pyro,pyro/contrib/gp/util.py,5e42018b4694388b9f245ca82eb6ed4c65797882,TODO: use `constraint_to` inside `pyro.param(...)` when available,https://github.com/pyro-ppl/pyro/commit/5e42018b4694388b9f245ca82eb6ed4c65797882,Yes
300,pyro-ppl/pyro,pyro/params/param_store.py,0f84a26492fbdb3851e2c042f5391f53fba89cca,TODO consider returing constrained,https://github.com/pyro-ppl/pyro/commit/0f84a26492fbdb3851e2c042f5391f53fba89cca,Yes
301,pyro-ppl/pyro,pyro/distributions/von_mises_3d.py,d289d02d9a816d7adf81d5e0ecba607121a7c01e,TODO implement constraints.sphere or similar,https://github.com/pyro-ppl/pyro/commit/d289d02d9a816d7adf81d5e0ecba607121a7c01e,Yes
302,pyro-ppl/pyro,pyro/distributions/transforms/cholesky.py,fcd56f986264c7d4aac75839940e157fa80ea2f1,TODO: change corr_cholesky_constraint to corr_cholesky when the latter is availabler,https://github.com/pyro-ppl/pyro/commit/fcd56f986264c7d4aac75839940e157fa80ea2f1,Yes
303,Aayush-Ankit/puma-simulator,src/instrn_proto.py,b150f4f5ee53f7a222ce77048773425250d275ea,TODO: just a hack for now; but eventually opcode will be different in i_mvm and i_train,https://github.com/Aayush-Ankit/puma-simulator/commit/b150f4f5ee53f7a222ce77048773425250d275ea,Yes
304,astier/model-free-episodic-control,mfec/utils.py,1b67ee82ec8c88f12178c6608f796fe97cb3289d,TODO output training time,https://github.com/astier/model-free-episodic-control/commit/1b67ee82ec8c88f12178c6608f796fe97cb3289d,Yes
305,brendanhasz/probflow,bk/variables.py,b304acd881ad9f08b3ed75370bd5f46c1c4000ba,TODO: do we have to re-compute batch size here so it'll work w\/ both validation and training?,https://github.com/brendanhasz/probflow/commit/b304acd881ad9f08b3ed75370bd5f46c1c4000ba,Yes
306,brendanhasz/probflow,src/probflow/core.py,ac404e2bb4f1e6ba154f81083b60b0c79233aca4,TODO: this'll throw an error when using training data tho...,https://github.com/brendanhasz/probflow/commit/ac404e2bb4f1e6ba154f81083b60b0c79233aca4,Yes
307,brendanhasz/probflow,src/probflow/core.py,2aa86ddfcef4c03c302a290925bdbf8f38e8ec25,if not isinstance(optimizer; tf.train.Optimizer): #TODO uh this fails,https://github.com/brendanhasz/probflow/commit/2aa86ddfcef4c03c302a290925bdbf8f38e8ec25,No
308,Cartus/AMR-Parser,data/align/scripts/feat2tree_v9.py,13085d32760add0a9e3fdd2c3db42f9bfa88914b,TODO: ygao20130130 relax constraint by comment out following; see if it is ok,https://github.com/Cartus/AMR-Parser/commit/13085d32760add0a9e3fdd2c3db42f9bfa88914b,No
309,cyschneck/Hydra,NN_gender_class.py,141390e8a63aedf9bdfe4b05a791cb81e72cb51d,TODO: update with better model for testing (currently ~85% on testing; ~99% on training),https://github.com/cyschneck/Hydra/commit/141390e8a63aedf9bdfe4b05a791cb81e72cb51d,Yes
310,cyschneck/Hydra,raw_text_processing.py,33493e402bc0e0a7b7f71422b0f4d47db8a12432,# TODO: train and benchmark against Parsey,https://github.com/cyschneck/Hydra/commit/33493e402bc0e0a7b7f71422b0f4d47db8a12432,No
311,Ekim-Yurtsever/DeepTL-Lane-Change-Classification,Mask_RCNN/mask_rcnn/model.py,91d4e598ca4bfba984ee5058f040aeac2f6071cb,TODO: support training mode?,https://github.com/Ekim-Yurtsever/DeepTL-Lane-Change-Classification/commit/91d4e598ca4bfba984ee5058f040aeac2f6071cb,Yes
312,hollygrimm/art-composition-cnn,trainers/resnet_50_trainer.py,2057af6f672a90115de204a736115c52cfeda4a0,TODO: Split training data into validation,https://github.com/hollygrimm/art-composition-cnn/commit/2057af6f672a90115de204a736115c52cfeda4a0,Yes
313,hollygrimm/art-composition-cnn,models/resnet50_attr_model.py,2103f469ca78b48475518a81a30ef27d6e540e50,TODO: Train on harmony and style,https://github.com/hollygrimm/art-composition-cnn/commit/2103f469ca78b48475518a81a30ef27d6e540e50,Yes
314,hoya012/shake-shake-tensorflow,train.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Training hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,Yes
315,jonasrauber/linear-region-attack,staxmod.py,ee13a3e861ebd93d36fc94c8e2be264c4dfd2431,TODO: MaxPool constraints,https://github.com/jonasrauber/linear-region-attack/commit/ee13a3e861ebd93d36fc94c8e2be264c4dfd2431,Yes
316,vkuznet/MLaaS4HEP,src/python/models.py,94f103c85a1578c1ef5d1dbd5df59cd766b96a9f,TODO: the y_train should be given us externally; so far we create it as random values,https://github.com/vkuznet/MLaaS4HEP/commit/94f103c85a1578c1ef5d1dbd5df59cd766b96a9f,Yes
317,VITA-Group/ALISTA,models/ALISTA_conv.py,9419a1e35ed8e042961f682481ca3875d0178e4f,"\""\""\""TODO: Docstring for setup_training.",https://github.com/VITA-Group/ALISTA/commit/9419a1e35ed8e042961f682481ca3875d0178e4f,No
318,VITA-Group/ALISTA,models/LISTA_cp_conv.py,9419a1e35ed8e042961f682481ca3875d0178e4f,"\""\""\""TODO: Docstring for setup_training.",https://github.com/VITA-Group/ALISTA/commit/9419a1e35ed8e042961f682481ca3875d0178e4f,No
319,VITA-Group/ALISTA,utils/models/ALISTA_conv.py,07a3052cecea37dc47678b3fbfb6c38e5dd7c848,"\""\""\""TODO: Docstring for setup_training.",https://github.com/VITA-Group/ALISTA/commit/07a3052cecea37dc47678b3fbfb6c38e5dd7c848,No
320,VITA-Group/ALISTA,utils/models/LISTA_cp_conv.py,07a3052cecea37dc47678b3fbfb6c38e5dd7c848,"\""\""\""TODO: Docstring for setup_training.",https://github.com/VITA-Group/ALISTA/commit/07a3052cecea37dc47678b3fbfb6c38e5dd7c848,No
321,apacha/Mensural-Detector,slim/deployment/model_deploy.py,f54532c1ddedb69c7531d13ad793c251a87957d3,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/apacha/Mensural-Detector/commit/f54532c1ddedb69c7531d13ad793c251a87957d3,No
322,flairNLP/flair,flair/embeddings.py,eb2d601c11dc19cb192f43e8695d11bd275bfbfa,by default; use_cache is false (for older pre-trained models TODO: remove in version 0.4),https://github.com/flairNLP/flair/commit/eb2d601c11dc19cb192f43e8695d11bd275bfbfa,Yes
323,flairNLP/flair,flair/embeddings.py,c9a75e887c1517cbea603a527406ff9dd05ca97c,token_ids = [torch.LongTensor(self.spm.SampleEncodeAsIds(sentence.to_original_text(); self.length; self.alpha[self.training])) for sentence in sentences]  # TODO: expose these params; we want this to be random in training; but fixed in inference,https://github.com/flairNLP/flair/commit/c9a75e887c1517cbea603a527406ff9dd05ca97c,Yes
324,HealthCatalyst/healthcareai-py,healthcareai/deploy_supervised_model.py,9ad3830c409d28759b31f38a0cf452f8d3ba60d5,TODO This might change as deploy no longer trains a model,https://github.com/HealthCatalyst/healthcareai-py/commit/9ad3830c409d28759b31f38a0cf452f8d3ba60d5,Yes
325,HealthCatalyst/healthcareai-py,healthcareai/deploy_supervised_model.py,89db9bc2c485326b41965b01f0071e80ba423553,TODO This might change as deploy no longer trains a model,https://github.com/HealthCatalyst/healthcareai-py/commit/89db9bc2c485326b41965b01f0071e80ba423553,Yes
326,HealthCatalyst/healthcareai-py,healthcareai/simple_mode.py,fa122381d514c7f2c050c7331bc811bcb0d5a4b4,TODO put TrainedSupervisedModel into advanced class and compare how it feels with the linear_regression(),https://github.com/HealthCatalyst/healthcareai-py/commit/fa122381d514c7f2c050c7331bc811bcb0d5a4b4,Yes
327,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,a36fc5f5aa194f0775860b87d2e06cfce8534026,TODO Could these be trained separately then after the best is found; train the factor model and add to TSM?,https://github.com/HealthCatalyst/healthcareai-py/commit/a36fc5f5aa194f0775860b87d2e06cfce8534026,Yes
328,HealthCatalyst/healthcareai-py,healthcareai/common/transformers.py,6cf4c8650416cac84ecb6091466cd9cced3ccf4c,TODO how do we validate this happens before train\/test split? Or do we need to? Can we implement it in the,https://github.com/HealthCatalyst/healthcareai-py/commit/6cf4c8650416cac84ecb6091466cd9cced3ccf4c,Yes
329,HealthCatalyst/healthcareai-py,healthcareai/common/transformers.py,6cf4c8650416cac84ecb6091466cd9cced3ccf4c,TODO      simple trainer in the correct order and leave this to advanced users?,https://github.com/HealthCatalyst/healthcareai-py/commit/6cf4c8650416cac84ecb6091466cd9cced3ccf4c,Yes
330,HealthCatalyst/healthcareai-py,healthcareai/common/transformers.py,d28f470b0c7267e4a9cdcb77ae55c384d17a24de,TODO how do we validate this happens before train\/test split? Or do we need to? Can we implement it in the,https://github.com/HealthCatalyst/healthcareai-py/commit/d28f470b0c7267e4a9cdcb77ae55c384d17a24de,No
331,HealthCatalyst/healthcareai-py,healthcareai/common/transformers.py,d28f470b0c7267e4a9cdcb77ae55c384d17a24de,TODO      simple trainer in the correct order and leave this to advanced users?,https://github.com/HealthCatalyst/healthcareai-py/commit/d28f470b0c7267e4a9cdcb77ae55c384d17a24de,No
332,RobotLocomotion/pytorch-dense-correspondence,dataset/labelfusion.py,a4690a73c829be630f8c3a2cf2d10d1cc689b28f,Pete Todo: separate train\/val?,https://github.com/RobotLocomotion/pytorch-dense-correspondence/commit/a4690a73c829be630f8c3a2cf2d10d1cc689b28f,No
333,mila-iqia/babyai,training.py,99c11b13d454059ca7e3c4dbd84d3890a8b17ce6,TODO: code for training; experience replay; storing data to a database,https://github.com/mila-iqia/babyai/commit/99c11b13d454059ca7e3c4dbd84d3890a8b17ce6,No
334,mila-iqia/babyai,rl/scripts/train_il.py,93b42ca0d43ea10a15ce009f582493528fe1d775,TODO merge it with train_wd.py,https://github.com/mila-iqia/babyai/commit/93b42ca0d43ea10a15ce009f582493528fe1d775,Yes
335,mila-iqia/babyai,scripts/intelligent_expert.py,ae44ceb2de92081c18c294de4006d9666e6e5b22,TODO: maybe we should log the indices of episodes used to train ?,https://github.com/mila-iqia/babyai/commit/ae44ceb2de92081c18c294de4006d9666e6e5b22,No
336,scikit-learn-contrib/lightning,lightning/primal.py,7fc44ee933817c4fafc75adb053d0192b86e1ad2,FIXME: don't retrain if there was only one fold,https://github.com/scikit-learn-contrib/lightning/commit/7fc44ee933817c4fafc75adb053d0192b86e1ad2,Yes
337,deepchem/deepchem,deepchem/models/__init__.py,a4ab24f33a57d0ea9a807cf5b5d9248aae1c04fe,TODO(rbharath): This training is currently broken w.r.t minibatches! Fix.,https://github.com/deepchem/deepchem/commit/a4ab24f33a57d0ea9a807cf5b5d9248aae1c04fe,Yes
338,deepchem/deepchem,deepchem/splits/splitters.py,c1d13d23aa6fb2e19e584eb376ceb4f99bde39dc,"FIXME: Signature of \""train_valid_test_split\"" incompatible with supertype \""Splitter\""",https://github.com/deepchem/deepchem/commit/c1d13d23aa6fb2e19e584eb376ceb4f99bde39dc,Yes
339,deepchem/deepchem,deepchem/splits/splitters.py,a3f1fd671baab33c4284cb51d7df5ec38a3f5051,"FIXME: Signature of \""train_valid_test_split\"" incompatible with supertype \""Splitter\""",https://github.com/deepchem/deepchem/commit/a3f1fd671baab33c4284cb51d7df5ec38a3f5051,Yes
340,dmlc/gluon-nlp,scripts/language_model/transformer_xl.py,b1b61d3f90cf795c7b48b6d109db7b7b96fa21ff,TODO: training not yet supported,https://github.com/dmlc/gluon-nlp/commit/b1b61d3f90cf795c7b48b6d109db7b7b96fa21ff,Yes
341,DistrictDataLabs/yellowbrick,yellowbrick/regressor.py,ae2601fdbe8e08d54ab013b45c637aef83c23d4c,TODO Is there a better way to differentiate between train and test points?,https://github.com/DistrictDataLabs/yellowbrick/commit/ae2601fdbe8e08d54ab013b45c637aef83c23d4c,Yes
342,DistrictDataLabs/yellowbrick,yellowbrick/classifier/confusion_matrix.py,870022ab7abce46f9bff47c8a9c787808a630e59,TODO: determine how to use quick methods that require train and test data.,https://github.com/DistrictDataLabs/yellowbrick/commit/870022ab7abce46f9bff47c8a9c787808a630e59,Yes
343,pyannote/pyannote-audio,pyannote/audio/applications/base.py,a6cd8771f4ea35240253da9d306296900fc07a2d,TODO: move this into Trainer class,https://github.com/pyannote/pyannote-audio/commit/a6cd8771f4ea35240253da9d306296900fc07a2d,Yes
344,pyannote/pyannote-audio,pyannote/audio/labeling/tasks/base.py,98e7c298a4b9f35e6ac7e993e241a5f009e51e6b,TODO: in pyannote.audio.train.model,https://github.com/pyannote/pyannote-audio/commit/98e7c298a4b9f35e6ac7e993e241a5f009e51e6b,No
345,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/center_loss.py,3eb7e89b9f149128c7eed1d8b95348da746213e5,FIXME add support for pretrained model with different specs,https://github.com/pyannote/pyannote-audio/commit/3eb7e89b9f149128c7eed1d8b95348da746213e5,Yes
346,catalyst-team/catalyst,catalyst/dl/experiments/runner.py,aae22d5a17c0d4382eb2463a7eb3b7153f95c012,@TODO: better solution with train\/inference handling ?,https://github.com/catalyst-team/catalyst/commit/aae22d5a17c0d4382eb2463a7eb3b7153f95c012,No
347,catalyst-team/catalyst,catalyst/dl/experiments/core.py,71f148bd4485e668af3096a977dc815e7ab929d5,@TODO: better solution with train\/inference handling ?,https://github.com/catalyst-team/catalyst/commit/71f148bd4485e668af3096a977dc815e7ab929d5,No
348,catalyst-team/catalyst,catalyst/dl/experiments/runner.py,b99c6649c48bb9574fd2d29f3116e28445cee783,@TODO: better solution with train\/inference handling ?,https://github.com/catalyst-team/catalyst/commit/b99c6649c48bb9574fd2d29f3116e28445cee783,No
349,catalyst-team/catalyst,catalyst/rl/scripts/run_samplers.py,861d85b48f3cf29632d6d6ecd5cdc9d0da9b1aea,@TODO: add registry for algorithms; trainers; samplers,https://github.com/catalyst-team/catalyst/commit/861d85b48f3cf29632d6d6ecd5cdc9d0da9b1aea,Yes
350,catalyst-team/catalyst,catalyst/rl/scripts/run_trainer.py,861d85b48f3cf29632d6d6ecd5cdc9d0da9b1aea,@TODO: add registry for algorithms; trainers; samplers,https://github.com/catalyst-team/catalyst/commit/861d85b48f3cf29632d6d6ecd5cdc9d0da9b1aea,Yes
351,catalyst-team/catalyst,catalyst/rl/scripts/run_trainer.py,81c7ae8468d224c70d09cf3a5ec3a0dfa05aa72e,@TODO: add registry for algorithms; trainers; samplers,https://github.com/catalyst-team/catalyst/commit/81c7ae8468d224c70d09cf3a5ec3a0dfa05aa72e,Yes
352,catalyst-team/catalyst,catalyst/rl/scripts/run_samplers.py,edb1e1abab5d428202095cfc793d99c37ed4cf72,@TODO: add registry for algorithms; trainers; samplers,https://github.com/catalyst-team/catalyst/commit/edb1e1abab5d428202095cfc793d99c37ed4cf72,Yes
353,catalyst-team/catalyst,catalyst/core/runner.py,f909e5b44eb4c2c26039e201bcbe67001529a515,@TODO: better solution with train\/inference handling ?,https://github.com/catalyst-team/catalyst/commit/f909e5b44eb4c2c26039e201bcbe67001529a515,Yes
354,catalyst-team/catalyst,catalyst/rl2/core/runner.py,f909e5b44eb4c2c26039e201bcbe67001529a515,todo: should implement different training steps,https://github.com/catalyst-team/catalyst/commit/f909e5b44eb4c2c26039e201bcbe67001529a515,Yes
355,probcomp/bayeslite,src/crosscat.py,ba24c1fbda59b79b43ac5d3ac1133f8cada156dd,dep_constraints = bdb.sql TODO,https://github.com/probcomp/bayeslite/commit/ba24c1fbda59b79b43ac5d3ac1133f8cada156dd,No
356,probcomp/bayeslite,src/crosscat.py,41d513124021eca6760013c9c20342566153348d,dep_constraints = bdb.sql TODO,https://github.com/probcomp/bayeslite/commit/41d513124021eca6760013c9c20342566153348d,No
357,DragonComputer/Dragonfire,dragonfire/conversational/__init__.py,1b000c167a7d08dafa0673308ded2f494c5a12fa,TODO: For now; the model are trained for a specific dataset (because of the maxLength which define the,https://github.com/DragonComputer/Dragonfire/commit/1b000c167a7d08dafa0673308ded2f494c5a12fa,Yes
358,DragonComputer/Dragonfire,dragonfire/conversational/trainer.py,1b000c167a7d08dafa0673308ded2f494c5a12fa,TODO: Save params\/results ? or already inside training args ?,https://github.com/DragonComputer/Dragonfire/commit/1b000c167a7d08dafa0673308ded2f494c5a12fa,Yes
359,mne-tools/mne-python,mne/decoding/tests/test_time_gen.py,dabf0984963905bb1dcd09a2df766b39655a0510,TODO JRK: test GAT with non-exhaustive CV (eg. train on 80%; test on 10%),https://github.com/mne-tools/mne-python/commit/dabf0984963905bb1dcd09a2df766b39655a0510,Yes
360,plasticityai/magnitude,pymagnitude/third_party/allennlp/models/semantic_parsing/nlvr/nlvr_decoder_step.py,96f0fc6a8839cf1c824febcb170ab75ce8a6f854,TODO (pradeep): Make the distinction between the two kinds of trainers in the way they,https://github.com/plasticityai/magnitude/commit/96f0fc6a8839cf1c824febcb170ab75ce8a6f854,Yes
361,naver/claf,claf/learn/experiment.py,dd573c51f3f8d2885e78d16ce1879bb22ea3c0cf,TODO: distributed training and 16-bits training (FP16),https://github.com/naver/claf/commit/dd573c51f3f8d2885e78d16ce1879bb22ea3c0cf,No
362,scikit-multiflow/scikit-multiflow,src/skmultiflow/evaluation/evaluate_holdout.py,f56388a32146e1eb704d26eea782ce320773acae,logging.info('Pre-training on 1 sample.')   # TODO Confirm if needed,https://github.com/scikit-multiflow/scikit-multiflow/commit/f56388a32146e1eb704d26eea782ce320773acae,No
363,inferno-pytorch/inferno,inferno/trainers/callbacks/base.py,97266eeceabfe848d75127d6f8254168bf1510c8,FIXME This makes bind_trainer in register_callback reduntant;,https://github.com/inferno-pytorch/inferno/commit/97266eeceabfe848d75127d6f8254168bf1510c8,Yes
364,inferno-pytorch/inferno,inferno/extensions/model/unet.py,2c6d464fdc7d5821bf808bfcc10cae8a60424080,TODO implement function to load a pretrained unet,https://github.com/inferno-pytorch/inferno/commit/2c6d464fdc7d5821bf808bfcc10cae8a60424080,Yes
365,menpo/menpo,pybug/activeappearancemodel/functions.py,3f5518a07c91255c3a4a6a69dbfc69e4be3af2bf,TODO: revise kwarg trilist in method constrain_mask_to_landmarks;,https://github.com/menpo/menpo/commit/3f5518a07c91255c3a4a6a69dbfc69e4be3af2bf,Yes
366,menpo/menpo,menpo/fitmultilevel/aam/builder.py,86b28ca75f40b1f9acffaafa156885cfaabf8535,TODO: revise kwarg trilist in method constrain_mask_to_landmarks;,https://github.com/menpo/menpo/commit/86b28ca75f40b1f9acffaafa156885cfaabf8535,Yes
367,menpo/menpo,menpo/fitmultilevel/sdm/trainer.py,457f0c142cc983a40cb8e2f5b1e13e046418c354,TODO: repeated code from Builder. Should builder and Trainer have a,https://github.com/menpo/menpo/commit/457f0c142cc983a40cb8e2f5b1e13e046418c354,Yes
368,menpo/menpo,menpo/fitmultilevel/sdm/trainer.py,8ba23be95e8368051bc587349226fa7a5353d688,TODO: repeated code from Builder. Should builder and Trainer have a,https://github.com/menpo/menpo/commit/8ba23be95e8368051bc587349226fa7a5353d688,Yes
369,feedly/transfer-nlp,transfer_nlp/runners/runner.py,459944bedcc8c5e18237123570bf8b377a1cd81d,TODO: include this into the NMT training part,https://github.com/feedly/transfer-nlp/commit/459944bedcc8c5e18237123570bf8b377a1cd81d,Yes
370,feedly/transfer-nlp,transfer_nlp/runners/runnersABC.py,f5c28a8d1d295f0674897b05d24fb0f63ac32c57,sample_probability = (20 + self.epoch_index) \/ self.config_args['num_epochs']  # TODO: include this into the NMT training part,https://github.com/feedly/transfer-nlp/commit/f5c28a8d1d295f0674897b05d24fb0f63ac32c57,Yes
371,online-ml/river,src/skmultiflow/evaluation/evaluate_holdout.py,f56388a32146e1eb704d26eea782ce320773acae,logging.info('Pre-training on 1 sample.')   # TODO Confirm if needed,https://github.com/online-ml/river/commit/f56388a32146e1eb704d26eea782ce320773acae,Yes
372,CPJKU/madmom,bin/OnsetDetector.py,7c547b70e51b9b150dd965b4fb74408e0f4661a8,TODO: make sure newer models are trained with mul=1,https://github.com/CPJKU/madmom/commit/7c547b70e51b9b150dd965b4fb74408e0f4661a8,Yes
373,CPJKU/madmom,bin/OnsetDetectorLL.py,7c547b70e51b9b150dd965b4fb74408e0f4661a8,TODO: make sure newer models are trained with mul=1,https://github.com/CPJKU/madmom/commit/7c547b70e51b9b150dd965b4fb74408e0f4661a8,Yes
374,CPJKU/madmom,bin/OnsetDetectorLL.py,7c547b70e51b9b150dd965b4fb74408e0f4661a8,TODO: make sure newer models are trained with diff_ratio=0.5,https://github.com/CPJKU/madmom/commit/7c547b70e51b9b150dd965b4fb74408e0f4661a8,Yes
375,CPJKU/madmom,bin/OnsetDetector.py,e9b06b18f947f1aeec678e7ece01c65b797cc874,TODO: make sure newer models are trained with diff_ratio=0.5,https://github.com/CPJKU/madmom/commit/e9b06b18f947f1aeec678e7ece01c65b797cc874,Yes
376,nyu-mll/jiant,main.py,ddf678de93e9256469e7e9094f819f656ea57c8c,TODO(Yada): Move logic for checkpointing finetuned vs frozen pretrained tasks,https://github.com/nyu-mll/jiant/commit/ddf678de93e9256469e7e9094f819f656ea57c8c,Yes
377,nyu-mll/jiant,src/preprocess.py,fd52653b41fbcc60ecde569e75e8bfec4ecfcbdb,TODO: We don't want diagnostic tasks in train_task_names,https://github.com/nyu-mll/jiant/commit/fd52653b41fbcc60ecde569e75e8bfec4ecfcbdb,Yes
378,EducationalTestingService/skll,classifier.py,c8ccbf28f5c0381b005a7d9a3c570745ddbc3c31,TODO there is probably some elegant way to combine ConstrainedRidge and ConstrainedSVR,https://github.com/EducationalTestingService/skll/commit/c8ccbf28f5c0381b005a7d9a3c570745ddbc3c31,Yes
379,scikit-learn-contrib/metric-learn,metric_learn/rca.py,3c57c64e139ed25454901df2c9611cb14fb40cdd,@TODO: remove seed from param. See @TODO in constraints\/chunks,https://github.com/scikit-learn-contrib/metric-learn/commit/3c57c64e139ed25454901df2c9611cb14fb40cdd,Yes
380,Accenture/AmpliGraph,ampligraph/latent_features/models.py,c6c7ca5a66c1e709cf0770212b7e50f3e7952e3f,TODO: Check should trainable=True really be set here,https://github.com/Accenture/AmpliGraph/commit/c6c7ca5a66c1e709cf0770212b7e50f3e7952e3f,Yes
381,awslabs/sockeye,sockeye/image_captioning/train.py,09a90021453e8d8254201d92a830cd8da0c6e2c6,TODO: make training compatible with full net,https://github.com/awslabs/sockeye/commit/09a90021453e8d8254201d92a830cd8da0c6e2c6,Yes
382,awslabs/sockeye,test/integration/test_seq_copy_int.py,ce5b1b65550f7852f2e26ab6343a4a1c5028c13a,TODO: Refactor the run_train_translate function!,https://github.com/awslabs/sockeye/commit/ce5b1b65550f7852f2e26ab6343a4a1c5028c13a,Yes
383,google/uis-rnn,demo.py,f4b67e7d3de68580d857167d842d508bf237282a,TODO: support using pretrained model.,https://github.com/google/uis-rnn/commit/f4b67e7d3de68580d857167d842d508bf237282a,Yes
384,tensorflow/ranking,tensorflow_ranking/python/head.py,c0ca2bae4937b035b4beae7371c4edd62abacbcf,TODO: Figure out a better way to set train_op_fn and optimizer,https://github.com/tensorflow/ranking/commit/c0ca2bae4937b035b4beae7371c4edd62abacbcf,Yes
385,tensorflow/ranking,tensorflow_ranking/extension/pipeline.py,2037fe23b708a93db9a2cebd86c34748a2d22d0d,TODO: supports for distributed training and evaluation.,https://github.com/tensorflow/ranking/commit/2037fe23b708a93db9a2cebd86c34748a2d22d0d,Yes
386,pytorch/translate,pytorch_translate/dual_learning/dual_learning_criterion.py,62500acec498d02179b8efacaf9a06d0a3082eae,TODO (T36875783): load pretrained lm to score,https://github.com/pytorch/translate/commit/62500acec498d02179b8efacaf9a06d0a3082eae,No
387,cltk/cltk,cltk/corpus/readers.py,320e810184204d9171e683241adea4c5c1c73a04,TODO and add:  ['latin_text_perseus'; 'latin_treebank_perseus'; 'latin_text_latin_library'; 'phi5'; 'phi7'; 'latin_proper_names_cltk'; 'latin_models_cltk'; 'latin_pos_lemmata_cltk'; 'latin_treebank_index_thomisticus'; 'latin_lexica_perseus'; 'latin_training_set_sentence_cltk'; 'latin_word2vec_cltk'; 'latin_text_antique_digiliblt'; 'latin_text_corpus_grammaticorum_latinorum'; 'latin_text_poeti_ditalia'],https://github.com/cltk/cltk/commit/320e810184204d9171e683241adea4c5c1c73a04,Yes
388,cltk/cltk,src/cltkv1/tokenizers/latin/params.py,ce6ba260f38c522d1642e2be98d56e38a73f1933,"\""\""\"" Params: Latin ||  || TODO: Some of these are only used for training. PRAENOMINA for training punkt tokenizer (als ABBREVIATIONS; CALENDAR; MISC) || TODO: The enclitic exceptions (que_exceptions and below) can all be deleted ||  || \""\""\""",https://github.com/cltk/cltk/commit/ce6ba260f38c522d1642e2be98d56e38a73f1933,Yes
389,fastnlp/fastNLP,reproduction/chinese_word_segment/train_context.py,38aa207ea21a24361ff089984d257010ba8cefe6,TODO pretrain\u7684embedding\u662F\u600E\u4E48\u89E3\u51B3\u7684\uFF1F,https://github.com/fastnlp/fastNLP/commit/38aa207ea21a24361ff089984d257010ba8cefe6,No
390,fastnlp/fastNLP,fastNLP/api/api.py,e4c1ab60a633b47933bb7dca081308bb144380c5,TODO add pretrain urls,https://github.com/fastnlp/fastNLP/commit/e4c1ab60a633b47933bb7dca081308bb144380c5,Yes
391,DIVA-DIA/DeepDIVA,template/CIFAR_CNN_classifier.py,7875d579a8aeffd60b5ab70ded47cc9d74353145,TODO why in the training we have this and here is flat without the if ?,https://github.com/DIVA-DIA/DeepDIVA/commit/7875d579a8aeffd60b5ab70ded47cc9d74353145,Yes
392,DIVA-DIA/DeepDIVA,template/CIFAR_CNN_classifier.py,fb5793a66a4a05b8408328f705e3ab58d8d2584e,TODO why in the training we have this and here is flat without the if ?,https://github.com/DIVA-DIA/DeepDIVA/commit/fb5793a66a4a05b8408328f705e3ab58d8d2584e,Yes
393,neuronets/nobrainer,train.py,90bb6e58f42b75b08669f9953e24fd184b0aa6f4,"\""\""\""Script to train highres3dnet model. ||  || The input CSV must have two columns: ||     1. filepaths of features ||     2. filepaths of corresponding labels ||  || TODO || ---- || - Make this script more general. Ideally; one could drop in their model and ||     loss function. || - Move some common methods (eg; i\/o) to dedicated modules. || - Dice coefficient for class 1 (brainmask) is sometimes NaN. || - Input of 1 * 128**3 is too large for 1080ti. This seems to be related to the ||     `input_fn` used. || - Remove pandas as a dependency. Make pure python reader that accepts CSV or ||     TSV as input. || \""\""\""",https://github.com/neuronets/nobrainer/commit/90bb6e58f42b75b08669f9953e24fd184b0aa6f4,No
394,neuronets/nobrainer,train.py,54200bc13c0b6e63cea50d143a2f83037ad04024,"\""\""\""Example script to train model. ||  || The input CSV must have two columns: ||     1. filepaths of features ||     2. filepaths of corresponding labels ||  || TODO || ---- || - Dice coefficient for class 1 (brainmask) is sometimes NaN. This occurs when ||     Dice should be zero. || - Input of 1 * 128**3 is too large for 1080ti to train HighRes3DNet. It is OK ||     for MeshNet. This issue seems to be related to the `input_fn` used. || \""\""\""",https://github.com/neuronets/nobrainer/commit/54200bc13c0b6e63cea50d143a2f83037ad04024,No
395,neuronets/nobrainer,nobrainer/layers/dropout.py,21a9b3dd5863b0c0c5bef7e872b629429ace11f0,TODO: add `K.in_train_phase`.,https://github.com/neuronets/nobrainer/commit/21a9b3dd5863b0c0c5bef7e872b629429ace11f0,Yes
396,ai-med/quickNAT_pytorch,quickNat_pytorch/data_utils.py,043257abf9eab2049a7d5f1fb396665b19dbc616,TODO: Presets for training; prediction and evaluation,https://github.com/ai-med/quickNAT_pytorch/commit/043257abf9eab2049a7d5f1fb396665b19dbc616,No
397,openml/automlbenchmark,automl/openml.py,47ef33cfa63c3e8a9cc9a0a75e9064dae5f21d9d,todo: make auto split 80% train; 20% test (make this configurable; also random vs sequential) and save it to disk,https://github.com/openml/automlbenchmark/commit/47ef33cfa63c3e8a9cc9a0a75e9064dae5f21d9d,Yes
398,graknlabs/kglib,kglib/kgcn/examples/animal_trade/main.py,40e2a2234ad26dc8189f32a7f232dc9ffc1b7e94,"raise ValueError(\""Model is not persisted; so training must be performed\"") # TODO is this true?",https://github.com/graknlabs/kglib/commit/40e2a2234ad26dc8189f32a7f232dc9ffc1b7e94,Yes
399,DependableSystemsLab/TensorFI,Tests/NotWorking/mnist_deep.py,ed9ea0db1adb3ba5e8751139b42d3f11e68f32fa,FIXME: Code to save and restore the model if Training is skipped,https://github.com/DependableSystemsLab/TensorFI/commit/ed9ea0db1adb3ba5e8751139b42d3f11e68f32fa,Yes
400,a2i2/surround,surround/split_data.py,7eba40075de5cf111568f2b4cf48b6a44fa2ab53,"\""\""\""A script to randomly move files in a directory to a test; train and || validate folder. ||  || This script is intended to be used on projects where the data is made || up of multiple files e.g. images or email files. ||  || Usage: python3 split_data.py <directory> <file extension> ||  || TODO: Add a flag to sort data into sub-folders based on a prefix for file names || TODO: Make sure reset works for all flags || TODO: Modify script to work on a CSV file. In this case split the CSV file into multiple files under each folder. ||  || \""\""\""",https://github.com/a2i2/surround/commit/7eba40075de5cf111568f2b4cf48b6a44fa2ab53,No
401,a2i2/surround,surround/visualise.py,7eba40075de5cf111568f2b4cf48b6a44fa2ab53,"\""\""\"" visualise.py ||  || Visualises the output from training a classifier. ||  || Supports both binary and multi class classifiers. ||  || Use cases: ||  - Visualising the output from training a model ||  - Viewing the output from running batch predictions on a dataset ||  || TODO: Order confusion matrix by most popular class to least popular class || TODO: Output file format in HTML. Always print to the screen. || TODO: Visualisation function should be different from function the output metrics || TODO: Wrap in a Visualiser interface for use in Surround || TODO: Support multiple ground truth and prediction columns || TODO: Add flag to output file with incorrect records. True by default. || TODO: Rename module to visualise_classifier.py ||  || TODO: Add a flag to set probability thresholds || TODO: Add a flag that describes each aspect of the generated report in human readable terminology ||  || \""\""\""",https://github.com/a2i2/surround/commit/7eba40075de5cf111568f2b4cf48b6a44fa2ab53,No
402,Aifred-Health/Vulcan,vulcanai2/models/metrics.py,448d32b3bbfa2f047b2f00874a420e2b98e63557,TODO:  Needs to be updated to use train loader,https://github.com/Aifred-Health/Vulcan/commit/448d32b3bbfa2f047b2f00874a420e2b98e63557,Yes
403,Aifred-Health/Vulcan,vulcanai2/models/cnn.py,f6e0e9e9ab12962084906405d8c4d673fdd333c6,TODO: add additional constraints in the future,https://github.com/Aifred-Health/Vulcan/commit/f6e0e9e9ab12962084906405d8c4d673fdd333c6,Yes
404,Aifred-Health/Vulcan,vulcanai2/models/cnn.py,0a0202dbc4f970ea8fc84bad679c045f8e575df1,TODO: add additional constraints in the future,https://github.com/Aifred-Health/Vulcan/commit/0a0202dbc4f970ea8fc84bad679c045f8e575df1,Yes
405,Aifred-Health/Vulcan,examples/fashion_conv_dense_test.py,1d28faf9c44c547c85dbe169a843a30fbd0bb56b,TODO: need to revisit this to be able to plot after training; interactive plotting is messing up,https://github.com/Aifred-Health/Vulcan/commit/1d28faf9c44c547c85dbe169a843a30fbd0bb56b,Yes
406,Aifred-Health/Vulcan,vulcanai2/datasets/tabulardataset.py,21441d5ae0fc999e4571e7ae6a34fbc4f69a6b03,TODO: give option to mirror train\/target,https://github.com/Aifred-Health/Vulcan/commit/21441d5ae0fc999e4571e7ae6a34fbc4f69a6b03,Yes
407,Aifred-Health/Vulcan,vulcanai2/datasets/tabulardataset.py,74a8dadffbdb46f3abb0dbea2cd374460927ec08,TODO: give option to mirror train\/target,https://github.com/Aifred-Health/Vulcan/commit/74a8dadffbdb46f3abb0dbea2cd374460927ec08,Yes
408,Aifred-Health/Vulcan,examples/fashion_conv_dense_test.py,fa1524e56970b4451958edc5cef79216c92b0ac2,TODO: to train parts of the model in different device,https://github.com/Aifred-Health/Vulcan/commit/fa1524e56970b4451958edc5cef79216c92b0ac2,Yes
409,Aifred-Health/Vulcan,examples/fashion_conv_dense_test.py,b7c890a59c21f87475c019153f17a6e1cb26f06a,# TODO: to train parts of the model in different device,https://github.com/Aifred-Health/Vulcan/commit/b7c890a59c21f87475c019153f17a6e1cb26f06a,Yes
410,Aifred-Health/Vulcan,vulcanai/models/cnn.py,39b5ad778d0f672b2c763b878b0ed159e8d69479,TODO: add additional constraints in the future,https://github.com/Aifred-Health/Vulcan/commit/39b5ad778d0f672b2c763b878b0ed159e8d69479,Yes
411,ResponsiblyAI/responsibly,ethically/tests/test_we.py,7ebb8666d1ea8cf2e2529ef35a68f36807f6b7b2,TODO maybe it was trained on the whole w2v?,https://github.com/ResponsiblyAI/responsibly/commit/7ebb8666d1ea8cf2e2529ef35a68f36807f6b7b2,Yes
412,nltk/nltk,nltk/tokenize/punkt.py,296df31d08af3f5bb05cf8496c5057ebbc9c12eb,TODO: Make orthographic heuristic less susceptible to overtraining,https://github.com/nltk/nltk/commit/296df31d08af3f5bb05cf8496c5057ebbc9c12eb,Yes
413,nltk/nltk,nltk/classify/svm.py,8793ed7e0cdccf4c319a7270bb05f04e8e6dbd8f,TODO: implement passing of SVMlight parameters from train() to learn(),https://github.com/nltk/nltk/commit/8793ed7e0cdccf4c319a7270bb05f04e8e6dbd8f,Yes
414,nltk/nltk,nltk/tag/brill/trainer/fast.py,9acaec8c7aa4e8bee59c30f2e873db3d116e1dbc,!! FIXME: tests in trainer.fast and trainer.brillorig are exact duplicates,https://github.com/nltk/nltk/commit/9acaec8c7aa4e8bee59c30f2e873db3d116e1dbc,Yes
415,tensorflow/tensor2tensor,tensor2tensor/data_generators/allen_brain.py,7967b446c5a0f7e7ea2e6b39150abf850abec463,"\""\""\""Problem definitions for Allen Brain Atlas problems. ||  || Notes: ||  ||   * TODO(cwbeitel): Want to be able to increase up-sampling ratio and\/or ||     in-paint fraction over the course of training. This could be done by ||     defining a range of problems or perhaps more aptly with an hparam ||     that is dialed up depending on training performance. ||  || \""\""\""",https://github.com/tensorflow/tensor2tensor/commit/7967b446c5a0f7e7ea2e6b39150abf850abec463,No
416,tensorflow/tensor2tensor,tensor2tensor/trax/trax.py,77673a0cd2cee3a1568a69d7ff0108e6501d9ffb,TODO(trax): Move to trainer.py. Only here because of t2t_trainer usage.,https://github.com/tensorflow/tensor2tensor/commit/77673a0cd2cee3a1568a69d7ff0108e6501d9ffb,Yes
417,tensorflow/tensor2tensor,tensor2tensor/rl/batch_dqn_agent_test.py,150aad3be93c68f9424c0769e9e710c754ebaaea,TODO: maybe add testStepTrain (and possibly some other tests) from dopamine,https://github.com/tensorflow/tensor2tensor/commit/150aad3be93c68f9424c0769e9e710c754ebaaea,No
418,tensorflow/tensor2tensor,tensor2tensor/rl/batch_dqn_agent_test.py,f28a5e9cec63fb2e0af575fdca754318b0cbdbca,TODO: maybe add testStepTrain (and possibly some other tests) from dopamine,https://github.com/tensorflow/tensor2tensor/commit/f28a5e9cec63fb2e0af575fdca754318b0cbdbca,No
419,RaRe-Technologies/gensim,gensim/models/ldamodelmulticore.py,fc0d6811a61c34b852a46b68523df7aa81d84081,"\""\""\"" || Latent Dirichlet Allocation (LDA) in Python; using all cores to parallelize and || speed up model training. ||  || The parallelization uses multiprocessing; in case this doesn't work for you for || some reason; try `LdaModel` which is an equivalent; but more straightforward and || single-core implementation. ||  || FIXME wiki timings ||  || This module allows both LDA model estimation from a training corpus and inference of topic || distribution on new; unseen documents. The model can also be updated with new documents || for online training. ||  || The core estimation code is based on the `onlineldavb.py` script by M. Hoffman [1]_; see || **Hoffman; Blei; Bach: Online Learning for Latent Dirichlet Allocation; NIPS 2010.** ||  || The algorithm: ||  || * is **streamed**: training documents may come in sequentially; no random access required; || * runs in **constant memory** w.r.t. the number of documents: size of the ||   training corpus does not affect memory footprint; can process corpora larger than RAM; and || * is **distributed**: makes use of a cluster of machines; if available; to ||   speed up model estimation. ||  || .. [1] http:\/\/www.cs.princeton.edu\/~mdhoffma || \""\""\""",https://github.com/RaRe-Technologies/gensim/commit/fc0d6811a61c34b852a46b68523df7aa81d84081,No
420,RaRe-Technologies/gensim,gensim/models/word2vec.py,e6bd0be0085015e7ba098bf79e1fabc808753154,TODO: batching; send a set of sentences to the train_sentence_sg Cython method. Sum of sentence lengths should not exceed MAX_SENTENCE_LENGTH (probably just import this constant directly from word2vec_inner.pyx).,https://github.com/RaRe-Technologies/gensim/commit/e6bd0be0085015e7ba098bf79e1fabc808753154,Yes
421,explosion/spaCy,spacy/cli/train.py,5211645af3f5308b5cd43314997659fdb87d13cf,TODO: Get spaCy using Thinc's trainer and optimizer,https://github.com/explosion/spaCy/commit/5211645af3f5308b5cd43314997659fdb87d13cf,Yes
422,explosion/spaCy,examples/training/train_textcat.py,f1b86dff8cef8802f5493bad4331d23e016dca7c,TODO: Remove this once we're not supporting models trained with thinc <6.9.0,https://github.com/explosion/spaCy/commit/f1b86dff8cef8802f5493bad4331d23e016dca7c,Yes
423,yzhao062/pyod,pyod/models/abod.py,798882c255701943d530d7024d0da2db22e33cc0,TODO: verify if n_train is needed,https://github.com/yzhao062/pyod/commit/798882c255701943d530d7024d0da2db22e33cc0,Yes
424,raamana/neuropredict,neuropredict/rhst.py,fc93ff003c943076eeaea91d364a02ad9e5e3401,TODO perhaps k-fold is a better inner CV; which guarantees full use of training set with fewer repeats?,https://github.com/raamana/neuropredict/commit/fc93ff003c943076eeaea91d364a02ad9e5e3401,Yes
425,williamSYSU/TextGAN-PyTorch,instructor/oracle_data/evocatgan_instructor.py,d30235e7b9f8029ad3a739ac707a740817ed061c,self.update_temperature(adv_epoch; cfg.ADV_train_epoch)   # TODO: update parents temperature,https://github.com/williamSYSU/TextGAN-PyTorch/commit/d30235e7b9f8029ad3a739ac707a740817ed061c,Yes
426,williamSYSU/TextGAN-PyTorch,instructor/oracle_data/evogan_instructor.py,d30235e7b9f8029ad3a739ac707a740817ed061c,self.update_temperature(adv_epoch; cfg.ADV_train_epoch)   # TODO: update parents temperature,https://github.com/williamSYSU/TextGAN-PyTorch/commit/d30235e7b9f8029ad3a739ac707a740817ed061c,Yes
427,williamSYSU/TextGAN-PyTorch,instructor/oracle_data/evogan_instructor.py,02cada9ce2f974590568db33b1bc3b0729c9f2b3,self.update_temperature(adv_epoch; cfg.ADV_train_epoch)   # TODO: update parents temperature,https://github.com/williamSYSU/TextGAN-PyTorch/commit/02cada9ce2f974590568db33b1bc3b0729c9f2b3,Yes
428,williamSYSU/TextGAN-PyTorch,instructor/real_data/evocatgan_instructor.py,8db96d5200f3c1b429729a8da0d8af965752ff98,self.clas_data.reset(train_s)  # TODO: bug: have to reset,https://github.com/williamSYSU/TextGAN-PyTorch/commit/8db96d5200f3c1b429729a8da0d8af965752ff98,Yes
429,csxeba/brainforge,model/backpropagation.py,c149e3a3a76e7692da4cdb6e60253981bfed5d8f,TODO: optimize this; skip untrainable architecture at the beginning,https://github.com/csxeba/brainforge/commit/c149e3a3a76e7692da4cdb6e60253981bfed5d8f,Yes
430,nschaetti/EchoTorch,test/test_narma10_prediction.py,6159b46ed280df738b10e0aa3d290dc4314b883f,TODO: Check why the CUDA version does 3.7291 (train_nrmse32); 0.16 (test_mse32) and 3.42 (test_nrmse32),https://github.com/nschaetti/EchoTorch/commit/6159b46ed280df738b10e0aa3d290dc4314b883f,Yes
431,kermitt2/delft,sequenceLabelling/trainer.py,263dfdb9b188a53ba79d1f1e52e6853b6d2ea037,todo: if valid set if None; create it as random segment of the shuffled train set,https://github.com/kermitt2/delft/commit/263dfdb9b188a53ba79d1f1e52e6853b6d2ea037,Yes
432,kermitt2/delft,delft/sequenceLabelling/trainer.py,c83fdf6a3480b45eb92e654e03210f9f83a3e915,todo: if valid set is None; create it as random segment of the shuffled train set,https://github.com/kermitt2/delft/commit/c83fdf6a3480b45eb92e654e03210f9f83a3e915,Yes
433,PyTorchLightning/pytorch-lightning,pytorch_lightning/models/trainer.py,1b497ac69a07ad0caa11a860a5e46125fd41a771,TODO: fix training part...,https://github.com/PyTorchLightning/pytorch-lightning/commit/1b497ac69a07ad0caa11a860a5e46125fd41a771,Yes
434,PyTorchLightning/pytorch-lightning,tests/trainer/test_trainer.py,6d58fb1353b04b85beb4c71b61e8b317bc456276,todo: check duplicated tests against trainer_checks,https://github.com/PyTorchLightning/pytorch-lightning/commit/6d58fb1353b04b85beb4c71b61e8b317bc456276,Yes
435,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/trainer.py,04c794ca72f45dd492e49baff773c2b8e274b9ed,TODO: convert train_percent_check to limit_train_batches,https://github.com/PyTorchLightning/pytorch-lightning/commit/04c794ca72f45dd492e49baff773c2b8e274b9ed,Yes
436,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/connectors/logger_connector.py,96009266192905093d9c2347f8f1ab08bf1e3df3,todo: IDE is complaining; these shall be initialized in the Trainer init at leas as placeholders,https://github.com/PyTorchLightning/pytorch-lightning/commit/96009266192905093d9c2347f8f1ab08bf1e3df3,Yes
437,PyTorchLightning/pytorch-lightning,pytorch_lightning/accelerators/tpu_backend.py,69833dad5b2a0e7e68ed60a91a5a8c32ae22f707,TODO: Move this check to Trainer __init__ or device parser,https://github.com/PyTorchLightning/pytorch-lightning/commit/69833dad5b2a0e7e68ed60a91a5a8c32ae22f707,Yes
438,PyTorchLightning/pytorch-lightning,tests/test_deprecated.py,c50c225f05f9a27687d5d5244c8d46a43900c635,TODO: remove bool from Trainer.profiler param in v1.3.0; update profiler_connector.py,https://github.com/PyTorchLightning/pytorch-lightning/commit/c50c225f05f9a27687d5d5244c8d46a43900c635,Yes
439,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py,ac3f7393fd090484c8230ec375db503819710227,TODO: How to start training in middle of epoch,https://github.com/PyTorchLightning/pytorch-lightning/commit/ac3f7393fd090484c8230ec375db503819710227,No
440,PyTorchLightning/pytorch-lightning,tests/trainer/test_dataloaders.py,c72880f109b0fcf1de57c5e2a67b7f352b28a7d7,todo: add also `train_dataloader__multiple_sequence`,https://github.com/PyTorchLightning/pytorch-lightning/commit/c72880f109b0fcf1de57c5e2a67b7f352b28a7d7,Yes
441,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/deprecated_api.py,7449ce216de8786b4370e1a39faa84b96cd6de42,todo: consider rename as `is_training`,https://github.com/PyTorchLightning/pytorch-lightning/commit/7449ce216de8786b4370e1a39faa84b96cd6de42,Yes
442,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/trainer.py,cb67e1d0b29e1da251c064c78220aa78b44e0620,"TODO: the old setup is now called \""pre_training\""; where should this hook be called now?",https://github.com/PyTorchLightning/pytorch-lightning/commit/cb67e1d0b29e1da251c064c78220aa78b44e0620,No
443,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/trainer.py,da6dbc8d1d128cf783d7151b012a5502bbd52bf5,"TODO: the old setup is now called \""pre_training\""; where should this hook be called now?",https://github.com/PyTorchLightning/pytorch-lightning/commit/da6dbc8d1d128cf783d7151b012a5502bbd52bf5,No
444,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/connectors/callback_connector.py,b8619a695f4e1f9a91894badb903ceaa61ea7201,TODO: connectors refactor: move callbacks list to connector and do not write Trainer state,https://github.com/PyTorchLightning/pytorch-lightning/commit/b8619a695f4e1f9a91894badb903ceaa61ea7201,Yes
445,PyTorchLightning/pytorch-lightning,pytorch_lightning/callbacks/model_checkpoint.py,826375effe30e9d9be0c59a40be3453c12f00977,TODO: the rpc plugin should wrap trainer.save_checkpoint,https://github.com/PyTorchLightning/pytorch-lightning/commit/826375effe30e9d9be0c59a40be3453c12f00977,Yes
446,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/training_type/ddp_spawn.py,c81b2a81891cc39e454b841b1ed221b734e39cee,TODO: is there a better way than accessing trainer through model -> trainer?,https://github.com/PyTorchLightning/pytorch-lightning/commit/c81b2a81891cc39e454b841b1ed221b734e39cee,No
447,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/training_type/tpu_spawn.py,bb92754119934c79f97de38ce59470f0059f3a0d,TODO: is there a better way than accessing trainer through model -> trainer?,https://github.com/PyTorchLightning/pytorch-lightning/commit/bb92754119934c79f97de38ce59470f0059f3a0d,No
448,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/training_type/tpu_spawn.py,bb92754119934c79f97de38ce59470f0059f3a0d,TODO: check for trainer reference,https://github.com/PyTorchLightning/pytorch-lightning/commit/bb92754119934c79f97de38ce59470f0059f3a0d,Yes
449,shawnwun/RNNLG,nn/basic.py,49e8e66eadaabbaecd75b5436cb7957512631b5a,TODO: unrolling function in theano; for training,https://github.com/shawnwun/RNNLG/commit/49e8e66eadaabbaecd75b5436cb7957512631b5a,Yes
450,shawnwun/RNNLG,nn/basic.py,49e8e66eadaabbaecd75b5436cb7957512631b5a,TODO: per step recurrence function in theano; for training,https://github.com/shawnwun/RNNLG/commit/49e8e66eadaabbaecd75b5436cb7957512631b5a,Yes
451,flennerhag/mlens,mlens/model_selection/ensemble_transfomer.py,9b84a3b144f8301f7065eb2b3e8b38eda2429411,TODO: Should id_train be used here?,https://github.com/flennerhag/mlens/commit/9b84a3b144f8301f7065eb2b3e8b38eda2429411,Yes
452,kengz/SLM-Lab,slm_lab/experiment/control.py,8f5f62f14e777cf2f88fe71360581be95d643163,TODO also missing: index in AEB space; train_mode,https://github.com/kengz/SLM-Lab/commit/8f5f62f14e777cf2f88fe71360581be95d643163,Yes
453,kengz/SLM-Lab,slm_lab/agent/algorithm/dqn.py,425f3a1ae83c6a5d76d73a3750ffc0e0262c5fed,TODO Fix for training iters; docstring,https://github.com/kengz/SLM-Lab/commit/425f3a1ae83c6a5d76d73a3750ffc0e0262c5fed,Yes
454,kengz/SLM-Lab,slm_lab/agent/algorithm/reinforce.py,499af60a872dd87d78b5eda6ee5f0419d8ae65cd,TODO Fix for multi-episode training. Won't know where to delete after the first episode.,https://github.com/kengz/SLM-Lab/commit/499af60a872dd87d78b5eda6ee5f0419d8ae65cd,Yes
455,kengz/SLM-Lab,slm_lab/agent/memory/replay.py,e057512b1dd0ba6ca1b776e2e758c28e8ef937d8,TODO set to_train here,https://github.com/kengz/SLM-Lab/commit/e057512b1dd0ba6ca1b776e2e758c28e8ef937d8,Yes
456,ryfeus/gcf-packs,pandas_numpy/sources/numpy/linalg/linalg.py,255a05a5980efb8b096c283d79872d0695886161,TODO: relax this constraint,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
457,cschoeller/pycandle,torch/callbacks.py,2a4897d80ee3cdb38214ba5ca5d611af05541462,#TODO stop training,https://github.com/cschoeller/pycandle/commit/2a4897d80ee3cdb38214ba5ca5d611af05541462,Yes
458,jefkine/zeta-learn,ztlearn/optimizers.py,9f62b9c3deb0dc4397899d8aab00ec6c84139057,@@TODO: this should come from the trainer class as the current epoch number,https://github.com/jefkine/zeta-learn/commit/9f62b9c3deb0dc4397899d8aab00ec6c84139057,Yes
459,jefkine/zeta-learn,ztlearn/utils/data_utils.py,b70f3ec614f831b250af418e4aa82036c39bd568,@@TODO: split the sample_folds and label_folds into sizes K-1 and 1 for test and training sets,https://github.com/jefkine/zeta-learn/commit/b70f3ec614f831b250af418e4aa82036c39bd568,Yes
460,explosion/spaCy,spacy/cli/debug_data.py,37c7c85a86bb6472d6918dd87c99ea6a2170037f,TODO: validate_json(train_data; schema),https://github.com/explosion/spaCy/commit/37c7c85a86bb6472d6918dd87c99ea6a2170037f,Yes
461,explosion/spaCy,spacy/cli/pretrain.py,4ed6278663c9482e14b549b2079f02cc186bc078,Load in pretrained weights - TODO test,https://github.com/explosion/spaCy/commit/4ed6278663c9482e14b549b2079f02cc186bc078,Yes
462,explosion/spaCy,spacy/cli/pretrain.py,43b960c01b0c64e56859ad5eb304a5422af46516,TODO: validate that [pretraining] block exists,https://github.com/explosion/spaCy/commit/43b960c01b0c64e56859ad5eb304a5422af46516,Yes
463,explosion/spaCy,spacy/pipeline/textcat.py,43b960c01b0c64e56859ad5eb304a5422af46516,TODO: begin_training is not guaranteed to see all data \/ labels ?,https://github.com/explosion/spaCy/commit/43b960c01b0c64e56859ad5eb304a5422af46516,Yes
464,explosion/spaCy,spacy/cli/pretrain.py,4c055f0aa703974ff3d14fb4ea5966c226013a1d,TODO: validate that [pretraining] block exists,https://github.com/explosion/spaCy/commit/4c055f0aa703974ff3d14fb4ea5966c226013a1d,Yes
465,scikit-learn/scikit-learn,scikits/learn/hmm.py,c716c75641f08340da8ad6e1a512d120aa81b2bf,FIXME: replace  with GaussianHMMMAPTrainer code?,https://github.com/scikit-learn/scikit-learn/commit/c716c75641f08340da8ad6e1a512d120aa81b2bf,Yes
466,scikit-learn/scikit-learn,scikits/learn/sgd/sparse/sgd.py,dad8deb08e17aa7577792d0e8781ac030d620c86,TODO: parallel training using joblib.,https://github.com/scikit-learn/scikit-learn/commit/dad8deb08e17aa7577792d0e8781ac030d620c86,Yes
467,scikit-learn/scikit-learn,scikits/learn/linear_model/ridge.py,fedd5499e161c731402cd1ba0bed01c4df6bbef4,FIXME: sample_weight must be split into training\/validation data,https://github.com/scikit-learn/scikit-learn/commit/fedd5499e161c731402cd1ba0bed01c4df6bbef4,Yes
468,scikit-learn/scikit-learn,skeletons/exercise_01_language_train_model.py,d859f402eb8b1eb0a03b961280e50b50144e15e1,TODO: define variables 'filenames_train' and 'filenames_test',https://github.com/scikit-learn/scikit-learn/commit/d859f402eb8b1eb0a03b961280e50b50144e15e1,Yes
469,scikit-learn/scikit-learn,skeletons/exercise_01_language_train_model.py,d859f402eb8b1eb0a03b961280e50b50144e15e1,TODO: define variables 'y_train' and 'y_test',https://github.com/scikit-learn/scikit-learn/commit/d859f402eb8b1eb0a03b961280e50b50144e15e1,Yes
470,scikit-learn/scikit-learn,skeletons/exercise_01_language_train_model.py,d859f402eb8b1eb0a03b961280e50b50144e15e1,TODO: define a variable named 'X_train',https://github.com/scikit-learn/scikit-learn/commit/d859f402eb8b1eb0a03b961280e50b50144e15e1,Yes
471,scikit-learn/scikit-learn,skeletons/exercise_04_face_recognition.py,40beda3eb74bb5a4a89b2304d160c2785c4595dc,TODO: define variables X_train; X_test; y_train; y_test by splitting the data,https://github.com/scikit-learn/scikit-learn/commit/40beda3eb74bb5a4a89b2304d160c2785c4595dc,Yes
472,scikit-learn/scikit-learn,scikits/learn/boosting/adaboost.py,5a71b609e12655f5d331084e19916f466a3abd00,TODO request that classifiers return classification of training sets when fitting,https://github.com/scikit-learn/scikit-learn/commit/5a71b609e12655f5d331084e19916f466a3abd00,Yes
473,scikit-learn/scikit-learn,sklearn/cross_validation.py,34c85ed995907f018279decb36be41fe2d29fba6,todo this won't work correctly when test_size > train_size,https://github.com/scikit-learn/scikit-learn/commit/34c85ed995907f018279decb36be41fe2d29fba6,Yes
474,scikit-learn/scikit-learn,sklearn/linear_model/ridge.py,2f69574393d822326a68d3238536d60b50a9e2fe,FIXME: sample_weight must be split into training\/validation data,https://github.com/scikit-learn/scikit-learn/commit/2f69574393d822326a68d3238536d60b50a9e2fe,Yes
475,plstcharles/thelper,src/thelper/train.py,aac5d8b089b18e5ae70d53795017364934a3dd79,todo: run eval in parallel (i.e. at the same time as training?),https://github.com/plstcharles/thelper/commit/aac5d8b089b18e5ae70d53795017364934a3dd79,Yes
476,plstcharles/thelper,thelper/optim/utils.py,06c4873f82272ec58c4e9ccc800c7527614b2747,todo: add flag to toggle loss comp in validation? (add to trainer config maybe?),https://github.com/plstcharles/thelper/commit/06c4873f82272ec58c4e9ccc800c7527614b2747,Yes
477,plstcharles/thelper,thelper/nn/resnet.py,2c9a8deb64e34458d6e8d3e73d8d6b4e5c770822,TODO: add pretrained param to toggle loading weights from imagenet before applying task?,https://github.com/plstcharles/thelper/commit/2c9a8deb64e34458d6e8d3e73d8d6b4e5c770822,Yes
478,plstcharles/thelper,thelper/nn/resnet.py,808e40487254553e3d0c5176faaf458e6c1d91ed,TODO: add pretrained param to toggle loading weights from imagenet before applying task?,https://github.com/plstcharles/thelper/commit/808e40487254553e3d0c5176faaf458e6c1d91ed,Yes
479,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/autoencoders.py,77d63fbc95f2db520e1aecec56faa57b873e96a9,TODO: 1. use better training parameters. 2. use consistant activation functions; 3. consider how to do this for hierarchical case,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/77d63fbc95f2db520e1aecec56faa57b873e96a9,Yes
480,delzac/cntkx,layers/blocks.py,fd0a374042fec8b79ab8d56ed866993b51531c6e,TODO: change activation default to relu and implement weight constraint,https://github.com/delzac/cntkx/commit/fd0a374042fec8b79ab8d56ed866993b51531c6e,Yes
481,DreamingRaven/nemesyst,ravenRecSyst.py,f9c2ba78e6210d7a4b2bcec131732504d3f61c46,test #TODO: implement complementary testing to training set selection,https://github.com/DreamingRaven/nemesyst/commit/f9c2ba78e6210d7a4b2bcec131732504d3f61c46,Yes
482,DreamingRaven/nemesyst,examples/gan.py,71cf9f535174f0345344082e1a2e72281065e439,TODO: make sure we arent falling for any https:\/\/medium.com\/@utk.is.here\/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9,https://github.com/DreamingRaven/nemesyst/commit/71cf9f535174f0345344082e1a2e72281065e439,Yes
483,manuwhs/Trapyng,libs/BBBLSTM/BBB_LSTM_Model.py,020f88a525e1718a5cfbbccde8410c321ff08a4b,TODO: maybe do not execute this line in the training model to save computation ? Maybe it wouldnt be executed anyway ?,https://github.com/manuwhs/Trapyng/commit/020f88a525e1718a5cfbbccde8410c321ff08a4b,Yes
484,biolab/orange3,Orange/data/variable.py,1c65f8552ef4a590cffa50cc43e5777708cc2509,"\""\""\"" || ======================== || Variables (``variable``) || ======================== ||  || Data instances in Orange can contain several types of variables: || :ref:`discrete <discrete>`; :ref:`continuous <continuous>`; || :ref:`strings <string>`; and :ref:`Python <Python>` and types derived from it. || The latter represent arbitrary Python objects. || The names; types; values (where applicable); functions for computing the || variable value from values of other variables; and other properties of the || variables are stored in descriptor classes defined in this module. ||  || Variable descriptors || -------------------- ||  || Variable descriptors can be constructed either directly; using  || constructors and passing attributes as parameters; or by a  || factory function :func:`Orange.data.variable.make`; which either  || retrieves an existing descriptor or constructs a new one. ||  || .. class:: Variable ||  ||     An abstract base class for variable descriptors. ||  ||     .. attribute:: name ||  ||         The name of the variable. Variable names do not need to be unique since two ||         variables are considered the same only if they have the same descriptor ||         (e.g. even multiple variables in the same table can have the same name). ||         This should; however; be avoided since it may result in unpredictable ||         behavior. ||      ||     .. attribute:: var_type ||         ||         Variable type; it can be Orange.data.Type.Discrete; ||         Orange.data.Type.Continuous; Orange.data.Type.String or ||         Orange.data.Type.Other.   ||  ||     .. attribute:: get_value_from ||  ||         A function (an instance of :obj:`Orange.classification.Classifier`) which computes ||         a value of the variable from values of one or more other variables. This ||         is used; for instance; in discretization where the variables describing ||         the discretized variable are computed from the original variable.  ||  ||     .. attribute:: ordered ||      ||         A flag telling whether the values of a discrete variable are ordered. At ||         the moment; no built-in method treats ordinal variables differently than ||         nominal ones. ||      ||     .. attribute:: distributed ||      ||         A flag telling whether the values of the variables are distributions. ||         As for the flag ordered; no methods treat such variables in any special ||         manner. ||      ||     .. attribute:: random_generator ||      ||         A local random number generator used by method ||         :obj:`Variable.random_value`. ||      ||     .. attribute:: default_meta_id ||      ||         A proposed (but not guaranteed) meta id to be used for that variable. ||         This is used; for instance; by the data loader for tab-delimited file ||         format instead of assigning an arbitrary new value; or by ||         :obj:`Orange.data.new_meta_id` if the variable is passed as an argument.  ||          ||     .. attribute:: attributes ||          ||         A dictionary which allows the user to store additional information ||         about the variable. All values should be strings. See the section  ||         about :ref:`storing additional information <attributes>`. ||  ||     .. method:: __call__(obj) ||      ||            Convert a string; number; or other suitable object into a variable ||            value. ||             ||            :param obj: An object to be converted into a variable value ||            :type o: any suitable ||            :rtype: :class:`Orange.data.Value` ||         ||     .. method:: randomvalue() ||  ||            Return a random value for the variable. ||         ||            :rtype: :class:`Orange.data.Value` ||         ||     .. method:: compute_value(inst) ||  ||            Compute the value of the variable given the instance by calling ||            obj:`~Variable.get_value_from` through a mechanism that prevents deadlocks by ||            circular calls. ||  ||            :rtype: :class:`Orange.data.Value` ||  || .. _discrete: || .. class:: Discrete ||  ||     Bases: :class:`Variable` ||     ||     Descriptor for discrete variables. ||      ||     .. attribute:: values ||      ||         A list with symbolic names for variables' values. Values are stored as ||         indices referring to this list. Therefore; modifying this list  ||         instantly changes the (symbolic) names of values as they are printed out or ||         referred to by user. ||      ||         .. note:: ||          ||             The size of the list is also used to indicate the number of ||             possible values for this variable. Changing the size - especially ||             shrinking the list - can have disastrous effects and is therefore not ||             really recommended. Also; do not add values to the list by ||             calling its append or extend method: call the :obj:`add_value` ||             method instead. ||  ||             It is also assumed that this attribute is always defined (but can ||             be empty); so never set it to None. ||      ||     .. attribute:: base_value ||  ||             Stores the base value for the variable as an index in `values`. ||             This can be; for instance; a \""normal\"" value; such as \""no ||             complications\"" as opposed to abnormal \""low blood pressure\"". The ||             base value is used by certain statistics; continuization etc. ||             potentially; learning algorithms. The default is -1 which means that ||             there is no base value. ||      ||     .. method:: add_value ||      ||             Add a value to values. Always call this function instead of ||             appending to values. ||  || .. _continuous: || .. class:: Continuous ||  ||     Bases: :class:`Variable` ||  ||     Descriptor for continuous variables. ||      ||     .. attribute:: number_of_decimals ||      ||         The number of decimals used when the value is printed out; converted to ||         a string or saved to a file. ||      ||     .. attribute:: scientific_format ||      ||         If ``True``; the value is printed in scientific format whenever it ||         would have more than 5 digits. In this case; :obj:`number_of_decimals` is ||         ignored. ||  ||     .. attribute:: adjust_decimals ||      ||         Tells Orange to monitor the number of decimals when the value is ||         converted from a string (when the values are read from a file or ||         converted by; e.g. ``inst[0]=\""3.14\""``):  ||         0: the number of decimals is not adjusted automatically; ||         1: the number of decimals is (and has already) been adjusted; ||         2: automatic adjustment is enabled; but no values have been converted yet. ||  ||         By default; adjustment of the number of decimals goes as follows: ||      ||         If the variable was constructed when data was read from a file; it will  ||         be printed with the same number of decimals as the largest number of  ||         decimals encountered in the file. If scientific notation occurs in the  ||         file; :obj:`scientific_format` will be set to ``True`` and scientific format  ||         will be used for values too large or too small.  ||      ||         If the variable is created in a script; it will have; by default; three ||         decimal places. This can be changed either by setting the value ||         from a string (e.g. ``inst[0]=\""3.14\""``; but not ``inst[0]=3.14``) or by ||         manually setting the :obj:`number_of_decimals`. ||  ||     .. attribute:: start_value; end_value; step_value ||      ||         The range used for :obj:`randomvalue`. ||  || .. _String: || .. class:: String ||  ||     Bases: :class:`Variable` ||  ||     Descriptor for variables that contain strings. No method can use them for  ||     learning; some will complain and others will silently ignore them when they  ||     encounter them. They can be; however; useful for meta-attributes; if  ||     instances in a dataset have unique IDs; the most efficient way to store them  ||     is to read them as meta-attributes. In general; never use discrete  ||     attributes with many (say; more than 50) values. Such attributes are  ||     probably not of any use for learning and should be stored as string ||     attributes. ||  ||     When converting strings into values and back; empty strings are treated  ||     differently than usual. For other types; an empty string can be used to ||     denote undefined values; while :obj:`String` will take empty strings ||     as empty strings -- except when loading or saving into file. ||     Empty strings in files are interpreted as undefined; to specify an empty ||     string; enclose the string in double quotes; these are removed when the ||     string is loaded. ||  || .. _Python: || .. class:: Python ||  ||     Bases: :class:`Variable` ||  ||     Base class for descriptors defined in Python. It is fully functional ||     and can be used as a descriptor for attributes that contain arbitrary Python ||     values. Since this is an advanced topic; PythonVariables are described on a  ||     separate page. !!TODO!! ||      ||      || Variables computed from other variables || --------------------------------------- ||  || Values of variables are often computed from other variables; such as in || discretization. The mechanism described below usually functions behind the scenes; || so understanding it is required only for implementing specific transformations. ||  || Monk 1 is a well-known dataset with target concept ``y := a==b or e==1``. || It can help the learning algorithm if the four-valued attribute ``e`` is || replaced with a binary attribute having values `\""1\""` and `\""not 1\""`. The || new variable will be computed from the old one on the fly.  ||  || .. literalinclude:: code\/variable-get_value_from.py ||     :lines: 7-17 ||      || The new variable is named ``e2``; we define it with a descriptor of type  || :obj:`Discrete`; with appropriate name and values ``\""not 1\""`` and ``1`` (we  || chose this order so that the ``not 1``'s index is ``0``; which can be; if  || needed; interpreted as ``False``). Finally; we tell e2 to use  || ``checkE`` to compute its value when needed; by assigning ``checkE`` to  || ``e2.get_value_from``.  ||  || ``checkE`` is a function that is passed an instance and another argument we  || do not care about here. If the instance's ``e`` equals ``1``; the function  || returns value ``1``; otherwise it returns ``not 1``. Both are returned as  || values; not plain strings. ||  || In most circumstances the value of ``e2`` can be computed on the fly - we can  || pretend that the variable exists in the data; although it does not (but  || can be computed from it). For instance; we can compute the information gain of || variable ``e2`` or its distribution without actually constructing data containing || the new variable. ||  || .. literalinclude:: code\/variable-get_value_from.py ||     :lines: 19-22 ||  || There are methods which cannot compute values on the fly because it would be || too complex or time consuming. In such cases; the data need to be converted || to a new :obj:`Orange.data.Table`:: ||  ||     new_domain = Orange.data.Domain([data.domain[\""a\""]; data.domain[\""b\""]; e2; data.domain.class_var]) ||     new_data = Orange.data.Table(new_domain; data)  ||  || Automatic computation is useful when the data is split into training and  || testing examples. Training instances can be modified by adding; removing  || and transforming variables (in a typical setup; continuous variables  || are discretized prior to learning; therefore the original variables are  || replaced by new ones). Test instances; on the other hand; are left as they  || are. When they are classified; the classifier automatically converts the  || testing instances into the new domain; which includes recomputation of  || transformed variables.  ||  || .. literalinclude:: code\/variable-get_value_from.py ||     :lines: 24- ||  || .. _attributes: ||  || Storing additional variables || ----------------------------- ||  || All variables have a field :obj:`~Variable.attributes`; a dictionary || which can contain strings. Although the current implementation allows all || types of value we strongly advise to use only strings. An example: ||  || .. literalinclude:: code\/attributes.py ||  || These attributes can only be saved to a .tab file. They are listed in the || third line in <name>=<value> format; after other attribute specifications || (such as \""meta\"" or \""class\""); and are separated by spaces.  ||  || .. _variable_descriptor_reuse: ||  || Reuse of descriptors || -------------------- ||  || There are situations when variable descriptors need to be reused. Typically; the  || user loads some training examples; trains a classifier; and then loads a separate || test set. For the classifier to recognize the variables in the second data set; || the descriptors; not just the names; need to be the same.  ||  || When constructing new descriptors for data read from a file or during unpickling; || Orange checks whether an appropriate descriptor (with the same name and; in case || of discrete variables; also values) already exists and reuses it. When new || descriptors are constructed by explicitly calling the above constructors; this || always creates new descriptors and thus new variables; although a variable with || the same name may already exist. ||  || The search for an existing variable is based on four attributes: the variable's name; || type; ordered values; and unordered values. As for the latter two; the values can  || be explicitly ordered by the user; e.g. in the second line of the tab-delimited  || file. For instance; sizes can be ordered as small; medium; or big. ||  || The search for existing variables can end with one of the following statuses. ||  || .. data:: Orange.data.variable.MakeStatus.NotFound (4) ||  ||     The variable with that name and type does not exist.  ||  || .. data:: Orange.data.variable.MakeStatus.Incompatible (3) ||  ||     There are variables with matching name and type; but their ||     values are incompatible with the prescribed ordered values. For example; ||     if the existing variable already has values [\""a\""; \""b\""] and the new one ||     wants [\""b\""; \""a\""]; the old variable cannot be reused. The existing list can; ||     however be appended with the new values; so searching for [\""a\""; \""b\""; \""c\""] would ||     succeed. Likewise a search for [\""a\""] would be successful; since the extra existing value ||     does not matter. The formal rule is thus that the values are compatible iff ``existing_values[:len(ordered_values)] == ordered_values[:len(existing_values)]``. ||  || .. data:: Orange.data.variable.MakeStatus.NoRecognizedValues (2) ||  ||     There is a matching variable; yet it has none of the values that the new ||     variable will have (this is obviously possible only if the new variable has ||     no prescribed ordered values). For instance; we search for a variable ||     \""sex\"" with values \""male\"" and \""female\""; while there is a variable of the same  ||     name with values \""M\"" and \""F\"" (or; well; \""no\"" and \""yes\"" :). Reuse of this  ||     variable is possible; though this should probably be a new variable since it  ||     obviously comes from a different data set. If we do decide to reuse the variable; the  ||     old variable will get some unneeded new values and the new one will inherit  ||     some from the old. ||  || .. data:: Orange.data.variable.MakeStatus.MissingValues (1) ||  ||     There is a matching variable with some of the values that the new one  ||     requires; but some values are missing. This situation is neither uncommon  ||     nor suspicious: in case of separate training and testing data sets there may ||     be values which occur in one set but not in the other. ||  || .. data:: Orange.data.variable.MakeStatus.OK (0) ||  ||     There is a perfect match which contains all the prescribed values in the ||     correct order. The existing variable may have some extra values; though. ||  || Continuous variables can obviously have only two statuses;  || :obj:`~Orange.data.variable.MakeStatus.NotFound` or :obj:`~Orange.data.variable.MakeStatus.OK`. ||  || When loading the data using :obj:`Orange.data.Table`; Orange takes the safest  || approach and; by default; reuses everything that is compatible up to  || and including :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`. Unintended reuse would be obvious from the || variable having too many values; which the user can notice and fix. More on that  || in the page on `loading data`. !!TODO!! ||  || There are two functions for reusing the variables instead of creating new ones. ||  || .. function:: Orange.data.variable.make(name; type; ordered_values; unordered_values[; create_new_on]) ||  ||     Find and return an existing variable or create a new one if none of the existing ||     variables matches the given name; type and values. ||      ||     The optional `create_new_on` specifies the status at which a new variable is ||     created. The status must be at most :obj:`~Orange.data.variable.MakeStatus.Incompatible` since incompatible (or ||     non-existing) variables cannot be reused. If it is set lower; for instance  ||     to :obj:`~Orange.data.variable.MakeStatus.MissingValues`; a new variable is created even if there exists ||     a variable which is only missing the same values. If set to :obj:`~Orange.data.variable.MakeStatus.OK`; the function ||     always creates a new variable. ||      ||     The function returns a tuple containing a variable descriptor and the ||     status of the best matching variable. So; if ``create_new_on`` is set to ||     :obj:`~Orange.data.variable.MakeStatus.MissingValues`; and there exists a variable whose status is; say; ||     :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`; a variable would be created; while the second  ||     element of the tuple would contain :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`. If; on the other ||     hand; there exists a variable which is perfectly OK; its descriptor is  ||     returned and the returned status is :obj:`~Orange.data.variable.MakeStatus.OK`. The function returns no  ||     indicator whether the returned variable is reused or not. This can be; ||     however; read from the status code: if it is smaller than the specified ||     ``create_new_on``; the variable is reused; otherwise a new descriptor has been constructed. ||  ||     The exception to the rule is when ``create_new_on`` is OK. In this case; the  ||     function does not search through the existing variables and cannot know the  ||     status; so the returned status in this case is always :obj:`~Orange.data.variable.MakeStatus.OK`. ||  ||     :param name: Variable name ||     :param type: Variable type ||     :type type: Orange.data.variable.Type ||     :param ordered_values: a list of ordered values ||     :param unordered_values: a list of values; for which the order does not ||         matter ||     :param create_new_on: gives the condition for constructing a new variable instead ||         of using the new one ||      ||     :return_type: a tuple (:class:`Orange.data.variable.Variable`; int) ||      || .. function:: Orange.data.variable.retrieve(name; type; ordered_values; onordered_values[; create_new_on]) ||  ||     Find and return an existing variable; or :obj:`None` if no match is found. ||      ||     :param name: variable name. ||     :param type: variable type. ||     :type type: Orange.data.variable.Type ||     :param ordered_values: a list of ordered values ||     :param unordered_values: a list of values; for which the order does not ||         matter ||     :param create_new_on: gives the condition for constructing a new variable instead ||         of using the new one ||  ||     :return_type: :class:`Orange.data.variable.Variable` ||      || These following examples (from :download:`variable-reuse.py <code\/variable-reuse.py>`) give the shown results if || executed only once (in a Python session) and in this order. ||  || :func:`Orange.data.variable.make` can be used for the construction of new variables. :: ||      ||     >>> v1; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""a\""; \""b\""]) ||     >>> print s; v1.values ||     4 <a; b> ||  || No surprises here: a new variable is created and the status is :obj:`~Orange.data.variable.MakeStatus.NotFound`. :: ||  ||     >>> v2; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""a\""]; [\""c\""]) ||     >>> print s; v2 is v1; v1.values ||     1 True <a; b; c> ||  || The status is 1 (:obj:`~Orange.data.variable.MakeStatus.MissingValues`); yet the variable is reused (``v2 is v1``). || ``v1`` gets a new value; ``\""c\""``; which was given as an unordered value. It does || not matter that the new variable does not need the value ``b``. :: ||  ||     >>> v3; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""a\""; \""b\""; \""c\""; \""d\""]) ||     >>> print s; v3 is v1; v1.values ||     1 True <a; b; c; d> ||  || This is like before; except that the new value; ``d`` is not among the || ordered values. :: ||  ||     >>> v4; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""b\""]) ||     >>> print s; v4 is v1; v1.values; v4.values ||     3; False; <b>; <a; b; c; d> ||  || The new variable needs to have ``b`` as the first value; so it is incompatible  || with the existing variables. The status is thus 3 (:obj:`~Orange.data.variable.MakeStatus.Incompatible`); the two  || variables are not equal and have different lists of values. :: ||  ||     >>> v5; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; None; [\""c\""; \""a\""]) ||     >>> print s; v5 is v1; v1.values; v5.values ||     0 True <a; b; c; d> <a; b; c; d> ||  || The new variable has values ``c`` and ``a``; but the order is not important;  || so the existing attribute is :obj:`~Orange.data.variable.MakeStatus.OK`. :: ||  ||     >>> v6; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; None; [\""e\""]) \""a\""]) ||     >>> print s; v6 is v1; v1.values; v6.values ||     2 True <a; b; c; d; e> <a; b; c; d; e> ||  || The new variable has different values than the existing variable (status is 2; || :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`); but the existing one is nonetheless reused. Note that we || gave ``e`` in the list of unordered values. If it was among the ordered; the || reuse would fail. :: ||  ||     >>> v7; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; None; ||             [\""f\""]; Orange.data.variable.MakeStatus.NoRecognizedValues))) ||     >>> print s; v7 is v1; v1.values; v7.values ||     2 False <a; b; c; d; e> <f> ||  || This is the same as before; except that we prohibited reuse when there are no || recognized values. Hence a new variable is created; though the returned status is  || the same as before:: ||  ||     >>> v8; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; ||             [\""a\""; \""b\""; \""c\""; \""d\""; \""e\""]; None; Orange.data.variable.MakeStatus.OK) ||     >>> print s; v8 is v1; v1.values; v8.values ||     0 False <a; b; c; d; e> <a; b; c; d; e> ||  || Finally; this is a perfect match; but any reuse is prohibited; so a new  || variable is created. ||  || \""\""\""",https://github.com/biolab/orange3/commit/1c65f8552ef4a590cffa50cc43e5777708cc2509,No
485,biolab/orange3,Orange/canvas/canvas/items/controlpoints.py,cae097749e098191b6789e0bc4ca68b85851c749,TODO: keyboard modifiers and constraints.,https://github.com/biolab/orange3/commit/cae097749e098191b6789e0bc4ca68b85851c749,No
486,pytorch/ignite,tests/ignite/trainer/test_trainer.py,78beb636670bf810134bc43feece960bdc6dffa7,TODO add test to assure history is written to from trainer,https://github.com/pytorch/ignite/commit/78beb636670bf810134bc43feece960bdc6dffa7,Yes
487,skorch-dev/skorch,examples/word_language_model/train.py,f4969e2ec63686911b6697099e9cf55bfbe1d441,FIXME: but we need validation data during training (LR annealing),https://github.com/skorch-dev/skorch/commit/f4969e2ec63686911b6697099e9cf55bfbe1d441,Yes
488,skorch-dev/skorch,examples/word_language_model/train.py,f4969e2ec63686911b6697099e9cf55bfbe1d441,FIXME: currently we have iterators for training and validation. Both of those,https://github.com/skorch-dev/skorch/commit/f4969e2ec63686911b6697099e9cf55bfbe1d441,Yes
489,RasaHQ/rasa_core,tests/test_domain.py,97980c7277924880e75def490624aa813efc1886,TODO creation of training data changed,https://github.com/RasaHQ/rasa_core/commit/97980c7277924880e75def490624aa813efc1886,Yes
490,RasaHQ/rasa_core,tests/test_dsl.py,6b908071f0c4dc51034583f9847eaba52779626e,TODO creation of training data changed,https://github.com/RasaHQ/rasa_core/commit/6b908071f0c4dc51034583f9847eaba52779626e,Yes
491,RasaHQ/rasa_core,rasa_core/policies/embedding_policy.py,e5c266edef1d4948b918c28a7ee37d3d22da748d,TODO we reload params again in train do we need it here?,https://github.com/RasaHQ/rasa_core/commit/e5c266edef1d4948b918c28a7ee37d3d22da748d,Yes
492,RasaHQ/rasa_core,rasa_core/server.py,5c969d88a45f2ff872a3c5a37a50819dadbafd31,TODO local: except specific training exception that can occur,https://github.com/RasaHQ/rasa_core/commit/5c969d88a45f2ff872a3c5a37a50819dadbafd31,Yes
493,RasaHQ/rasa_core,rasa_core/server.py,5124609e65844f3733fb0c04815833b75cc9520a,TODO local: except specific training exception that can occur,https://github.com/RasaHQ/rasa_core/commit/5124609e65844f3733fb0c04815833b75cc9520a,Yes
494,RasaHQ/rasa_core,rasa_core/server.py,bca29e139ba895206050ad6eb146d645c363f9c8,TODO local: except specific training exception that can occur,https://github.com/RasaHQ/rasa_core/commit/bca29e139ba895206050ad6eb146d645c363f9c8,Yes
495,dmlc/dgl,mx.py,62a39e5b37144c06eb11c5282b7844c71baf8dd3,TODO: loss functions and training,https://github.com/dmlc/dgl/commit/62a39e5b37144c06eb11c5282b7844c71baf8dd3,Yes
496,dmlc/dgl,mx_scalar.py,05547b37117ef164d2226e8fff136b018c2eb38d,TODO: loss functions and training,https://github.com/dmlc/dgl/commit/05547b37117ef164d2226e8fff136b018c2eb38d,Yes
497,dmlc/dgl,tutorials/models/gcnTutorialNew.py,68ec624782bb4e4fb9f1adf1088cf39c1826533e,This is a simple implementation of Kipf & Welling's Semi-Supervised Classificaton with Graph Convolutional Networks in ICLR 2017; which propose a simple yet efficient model that extends convolutional neual network from the grid structured data we all familiar and like to graphs; like social network and knowledge graph. It starts from the framework of spectral graph convolutions and makes reasonable simplifications to achieve both faster training and higher prediction accuracy. It also achieves start-of-the-art classification results on a number of graph datasets like CORA; etc. \/TODO: elaborate.,https://github.com/dmlc/dgl/commit/68ec624782bb4e4fb9f1adf1088cf39c1826533e,No
498,dmlc/gluon-cv,tests/unittests/test_segmentation.py,dfac51fcd0fbceb3f394a1d8f1ab7c8652e9cf78,TODO FIXME: change it to ADE20K dataset and pretrained model,https://github.com/dmlc/gluon-cv/commit/dfac51fcd0fbceb3f394a1d8f1ab7c8652e9cf78,Yes
499,dmlc/gluon-cv,docs/tutorials/depth/demo_monodepth2.py,1c41c89c414cc9e447bbc9d2f96f3b6ac519c07c,"\""\""\""01. Predict depth from a single image with pre-trained Monodepth2 models || =========================================================================== ||  || TODO || \""\""\""",https://github.com/dmlc/gluon-cv/commit/1c41c89c414cc9e447bbc9d2f96f3b6ac519c07c,No
500,dmlc/gluon-cv,docs/tutorials/depth/train_monodepth2.py,1c41c89c414cc9e447bbc9d2f96f3b6ac519c07c,"\""\""\""02. Monodepth2 training on KITTI dataset || ================================================== ||  || TODO\""\""\""",https://github.com/dmlc/gluon-cv/commit/1c41c89c414cc9e447bbc9d2f96f3b6ac519c07c,No
501,microsoft/nni,nni/retiarii/graph.py,fddc8adc872cea6c2662e178ca24254e92c22642,TODO this may be a problem when training config is large,https://github.com/microsoft/nni/commit/fddc8adc872cea6c2662e178ca24254e92c22642,Yes
502,allenai/allennlp,allennlp/models/semantic_parsing/nlvr/nlvr_decoder_step.py,7dfc34247cedfe16c9d35a557855977fbd3df15c,TODO (pradeep): Make the distinction between the two kinds of trainers in the way they,https://github.com/allenai/allennlp/commit/7dfc34247cedfe16c9d35a557855977fbd3df15c,Yes
503,allenai/allennlp,allennlp/models/encoder_decoders/simple_seq2seq.py,b529f6df91ac13992171f3e73d6eb232e8e6110c,TODO: Run beam search whenever self.training is False so that we can get,https://github.com/allenai/allennlp/commit/b529f6df91ac13992171f3e73d6eb232e8e6110c,Yes
504,allenai/allennlp,allennlp/tests/commands/train_test.py,73bc922ad7a1d2265f84e4f15e15d9fc3345ecee,TODO: This is somewhat brittle. Make these constants in trainer.py.,https://github.com/allenai/allennlp/commit/73bc922ad7a1d2265f84e4f15e15d9fc3345ecee,Yes
505,ray-project/ray,python/ray/tune/track/session.py,1ef9c0729d104dd101b71323a6aec6c0ad502e03,TODO: Move Trainable autopopulation to a util function,https://github.com/ray-project/ray/commit/1ef9c0729d104dd101b71323a6aec6c0ad502e03,Yes
506,ray-project/ray,python/ray/util/sgd/torch/torch_trainer.py,86cff17e7ef1b355e1ab4862ccd664ed4c856227,TODO: Implement autoscaling. If num_workers=-1; the trainer will use as,https://github.com/ray-project/ray/commit/86cff17e7ef1b355e1ab4862ccd664ed4c856227,Yes
507,ray-project/ray,rllib/agents/dqn/tests/test_apex.py,2298f6fb40cb6d348b5b48593c93cdd58ddd1f29,TODO(ekl) fix iterator metrics bugs w\/multiple trainers.,https://github.com/ray-project/ray/commit/2298f6fb40cb6d348b5b48593c93cdd58ddd1f29,Yes
508,ray-project/ray,rllib/utils/exploration/curiosity.py,2cbe29a7fa8bc63d57f97b03ba726676425e160b,TODO: (sven) if sub_exploration is None; use Trainer's default,https://github.com/ray-project/ray/commit/2cbe29a7fa8bc63d57f97b03ba726676425e160b,No
509,NervanaSystems/neon,neon/layers/container.py,8d1fb75b434a942f58291d11252c8064c0dbecfb,TODO: implement loopy training case,https://github.com/NervanaSystems/neon/commit/8d1fb75b434a942f58291d11252c8064c0dbecfb,Yes
510,NervanaSystems/neon,neon/layers/container.py,8d1b86ceb6e34fde6819b6a7d36899cb57f61f2f,TODO: implement loopy training case,https://github.com/NervanaSystems/neon/commit/8d1b86ceb6e34fde6819b6a7d36899cb57f61f2f,Yes
511,NervanaSystems/neon,examples/ssd/datasets/ingest_pascalvoc.py,175bd3c94b9cea0f92424e5c4277206ce1a76a3d,"\""emit_constraint_type\"": \""center\""; TODO: enable when adds support for no gt boxes",https://github.com/NervanaSystems/neon/commit/175bd3c94b9cea0f92424e5c4277206ce1a76a3d,No
512,cleverhans-lab/cleverhans,examples/multigpu_advtrain/model.py,a5c03a32f9e51dade38c18418357f7517db40f7d,TODO: training and bn_training,https://github.com/cleverhans-lab/cleverhans/commit/a5c03a32f9e51dade38c18418357f7517db40f7d,No
513,cleverhans-lab/cleverhans,cleverhans/future/tf2/attacks/carlini_wagner_l2.py,69367795e2bdb84d77c424f191f7b7cc0df41c08,TODO: find a way to reset the state of Adam so we only need to compile the training function ones,https://github.com/cleverhans-lab/cleverhans/commit/69367795e2bdb84d77c424f191f7b7cc0df41c08,Yes
514,chainer/chainer,chainer/graph_optimimzations/static_graph.py,ed93a50e8f37bba1cc3fccf0f37d33654e85b0c0,fixme: check if `configuration.config.train` flag has changed. If so; run define-by-run code.,https://github.com/chainer/chainer/commit/ed93a50e8f37bba1cc3fccf0f37d33654e85b0c0,Yes
516,Seanforfun/GMAN_Net_Haze_Removal,DehazeNet/dehazenet_input.py,070c04a0abc4f94786a8de26c6ecd41634ac284a,TODO Put all graphs into queue,https://github.com/Seanforfun/GMAN_Net_Haze_Removal/commit/070c04a0abc4f94786a8de26c6ecd41634ac284a,Yes
517,Seanforfun/GMAN_Net_Haze_Removal,DehazeNet/dehazenet_input.py,23c6bcf63a401a449afa0aa0b6a88961e52545bd,TODO Put all graphs into queue,https://github.com/Seanforfun/GMAN_Net_Haze_Removal/commit/23c6bcf63a401a449afa0aa0b6a88961e52545bd,Yes
568,kabkabm/defensegan,cleverhans/examples/multigpu_advtrain/attacks_multigpu.py,f111360536fe6902d357a1e575a1f403b185782a,TODO: Break the graph only nGPU times instead of nb_iter times.,https://github.com/kabkabm/defensegan/commit/f111360536fe6902d357a1e575a1f403b185782a,Yes
625,nengo/nengo-dl,nengo_dl/tensor_graph.py,5da45781a5cea40efb89b79b2cddc61842f03478,TODO: enable autograph,https://github.com/nengo/nengo-dl/commit/5da45781a5cea40efb89b79b2cddc61842f03478,Yes
663,tensorflow/addons,tensorflow_addons/image/dense_image_warp_test.py,d921dfac15e40a2eb9c74f65df9af698b6f0a31d,TODO: run in both graph and eager modes,https://github.com/tensorflow/addons/commit/d921dfac15e40a2eb9c74f65df9af698b6f0a31d,Yes
664,tensorflow/addons,tensorflow_addons/image/dense_image_warp_test.py,83195c98032409aeb6ae9a27b4047597a5b04a2b,TODO: run in both graph and eager modes,https://github.com/tensorflow/addons/commit/83195c98032409aeb6ae9a27b4047597a5b04a2b,Yes
735,dmlc/dgl,examples/mxnet/sampling/gcn_cv_sc.py,3a1392e6417ce966318b0ee1033249a2cfe06665,TODO we could use DGLGraph.pull to implement this; but the current,https://github.com/dmlc/dgl/commit/3a1392e6417ce966318b0ee1033249a2cfe06665,Yes
736,dmlc/dgl,examples/mxnet/sampling/graphsage_cv.py,3a1392e6417ce966318b0ee1033249a2cfe06665,TODO we could use DGLGraph.pull to implement this; but the current,https://github.com/dmlc/dgl/commit/3a1392e6417ce966318b0ee1033249a2cfe06665,Yes
737,dmlc/dgl,examples/mxnet/sampling/dis_sampling/dis_gcn_cv_sc.py,28379f927f6f1dfb5d627e56df49c25ba9b3f35c,TODO we could use DGLGraph.pull to implement this; but the current,https://github.com/dmlc/dgl/commit/28379f927f6f1dfb5d627e56df49c25ba9b3f35c,Yes
738,dmlc/dgl,examples/mxnet/sampling/dis_sampling/dis_graphsage_cv.py,28379f927f6f1dfb5d627e56df49c25ba9b3f35c,TODO we could use DGLGraph.pull to implement this; but the current,https://github.com/dmlc/dgl/commit/28379f927f6f1dfb5d627e56df49c25ba9b3f35c,Yes
744,microsoft/nni,src/sdk/pynni/nni/nas/tensorflow/mutator.py,76c819c00acb2952436da611c574030aa0beedd4,TODO: graph,https://github.com/microsoft/nni/commit/76c819c00acb2952436da611c574030aa0beedd4,Yes
756,chainer/chainer,chainer/variable.py,a4baca2de43ed2b220be84b57e3ecbcd1895e23d,todo (vogel): support this accumulation in static graphs.,https://github.com/chainer/chainer/commit/a4baca2de43ed2b220be84b57e3ecbcd1895e23d,Yes
757,chainer/chainer,chainer/variable.py,def5bc916a17f50b45a622818ad7455d1e3126bb,todo (vogel): support this accumulation in static graphs.,https://github.com/chainer/chainer/commit/def5bc916a17f50b45a622818ad7455d1e3126bb,Yes
758,chainer/chainer,chainer/variable.py,3b8b5a03e41b1620d30488a467b0156ee06b2f91,todo (vogel): support this accumulation in static graphs.,https://github.com/chainer/chainer/commit/3b8b5a03e41b1620d30488a467b0156ee06b2f91,Yes
759,chainer/chainer,chainer/variable.py,bda68c362732c313dc8f6d3df9ee2caa6f4145c8,todo (vogel): support this accumulation in static graphs.,https://github.com/chainer/chainer/commit/bda68c362732c313dc8f6d3df9ee2caa6f4145c8,Yes
760,chainer/chainer,chainer/variable.py,e153581892220b98cbd792dd8ed16d0201f46d9e,fixme (vogel): support this in static sub-graphs,https://github.com/chainer/chainer/commit/e153581892220b98cbd792dd8ed16d0201f46d9e,Yes
761,chainer/chainer,chainer/variable.py,e153581892220b98cbd792dd8ed16d0201f46d9e,todo (vogel): support this accumulation in static graphs.,https://github.com/chainer/chainer/commit/e153581892220b98cbd792dd8ed16d0201f46d9e,Yes
766,NickleDave/hybrid-vocal-classifier,hvc/featureextract.py,c03b461dc2c606e394ecdb1109c78e38ef8b9801,if seg params not defined in todo; revert to default,https://github.com/NickleDave/hybrid-vocal-classifier/commit/c03b461dc2c606e394ecdb1109c78e38ef8b9801,No
767,tommytracey/DeepRL-P3-Collaboration-Competition,python/unityagents/environment.py,0fca20fdd859be224d0c2ca9ba5f3baeb206c445,TODO : think of a better way to expose the academyParameters,https://github.com/tommytracey/DeepRL-P3-Collaboration-Competition/commit/0fca20fdd859be224d0c2ca9ba5f3baeb206c445,Yes
768,rstemmer/musicdb,mdbapi/extern.py,b88a22c4d2ecbe4c09517384815a8831fbbfca9a,2.: Start the copy-process TODO: Make it in parallel,https://github.com/rstemmer/musicdb/commit/b88a22c4d2ecbe4c09517384815a8831fbbfca9a,Yes
769,shahrukhqasim/TIES-2.0,python/models/basic_model.py,8daa8c380f58759ebd59daf1304bf40eb40371dd,TODO: Fix training parameter,https://github.com/shahrukhqasim/TIES-2.0/commit/8daa8c380f58759ebd59daf1304bf40eb40371dd,Yes
770,thuijskens/scikit-hyperband,hyperband/tests/test_hyperband.py,89398bae46fe4c5607ab63c87c1ccfd4c2b50fd4,"\""\""\"" || TODO: This test fails due to the random state not being properly fixed ||  || def test_hyperband(): ||     model; param_dist; X; y; rng = setup() ||     search = HyperbandSearchCV(model; param_dist; random_state=rng) ||     search.fit(X; y) ||  ||     # results = pd.DataFrame(search.cv_results_) ||     expected_params = { ||         'bootstrap': False; ||         'criterion': 'entropy'; ||         'max_depth': None; ||         'max_features': 7; ||         'min_samples_leaf': 2; ||         'min_samples_split': 2; ||         'n_estimators': 81 ||     } ||  ||     # assert(results.shape[0] == 186) TODO: sort out what the expected n_i and r_i values are ||     assert(search.best_params_ == expected_params) || \""\""\""",https://github.com/thuijskens/scikit-hyperband/commit/89398bae46fe4c5607ab63c87c1ccfd4c2b50fd4,No
771,sshleifer/object_detection_kitti,slim/models/model_deploy.py,a5c4fd06d21e85a231ec05cc5305478a6c2d6a73,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = slim.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = slim.deploy(config; model_fn; [inputs_queue]; optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/sshleifer/object_detection_kitti/commit/a5c4fd06d21e85a231ec05cc5305478a6c2d6a73,No
772,roscisz/TensorHive,tensorhive/core_anew/connections/SSHConnector.py,2092028378e996696794f08a9f86707f78b59e05,TODO replace hardcoded parameters,https://github.com/roscisz/TensorHive/commit/2092028378e996696794f08a9f86707f78b59e05,Yes
773,roscisz/TensorHive,tensorhive/core_anew/connectors/SSHConnector.py,b2c0406aa5242857be47741e465f5c86400eb8d3,TODO add more parameters to SshShell call,https://github.com/roscisz/TensorHive/commit/b2c0406aa5242857be47741e465f5c86400eb8d3,Yes
774,roscisz/TensorHive,tensorhive/core_anew/monitors/GPUMonitor.py,10a3c308c708c4d8d2667433c17fb0777d9295e2,TODO Right now these commands receive output containing info about all gpus present... it is intended to query them by uuid and store under separate dict keys,https://github.com/roscisz/TensorHive/commit/10a3c308c708c4d8d2667433c17fb0777d9295e2,Yes
775,roscisz/TensorHive,tensorhive/core_anew/monitors/GPUMonitor.py,7a1cf227192523349a0fe33960d0e36de88db6b8,TODO Make separate class for it,https://github.com/roscisz/TensorHive/commit/7a1cf227192523349a0fe33960d0e36de88db6b8,Yes
776,roscisz/TensorHive,tensorhive/models/resources/user/UserRegistration.py,5bf25c805e05ba5d21395e1eaffd708b9621d90c,TODO Add more user parameters,https://github.com/roscisz/TensorHive/commit/5bf25c805e05ba5d21395e1eaffd708b9621d90c,Yes
777,roscisz/TensorHive,tensorhive/core/monitors/GPUMonitor.py,df2a450668e111855b8cd2299146b040711a186c,TODO Make separate class for it,https://github.com/roscisz/TensorHive/commit/df2a450668e111855b8cd2299146b040711a186c,Yes
778,roscisz/TensorHive,tensorhive/config.py,4d63640595c763a28277cba0929ccd6e86fc36fa,TODO May want use dictConfig (must import separately: logging.config),https://github.com/roscisz/TensorHive/commit/4d63640595c763a28277cba0929ccd6e86fc36fa,No
779,roscisz/TensorHive,tensorhive/config.py,11a2ae39a53f7a1a0b6d5c0893ff63371ce83f0d,TODO Handle more options (https:\/\/github.com\/ParallelSSH\/parallel-ssh\/blob\/2e9668cf4b58b38316b1d515810d7e6c595c76f3\/pssh\/clients\/base_pssh.py#L119),https://github.com/roscisz/TensorHive/commit/11a2ae39a53f7a1a0b6d5c0893ff63371ce83f0d,Yes
780,roscisz/TensorHive,tensorhive/cli.py,39eacc8c4f0b025d8bc4e89c72a923f6bc4d6557,TODO May want use dictConfig instead of basicConfig (must import separately: logging.config),https://github.com/roscisz/TensorHive/commit/39eacc8c4f0b025d8bc4e89c72a923f6bc4d6557,No
781,roscisz/TensorHive,tensorhive/config.py,53d87158f05d55dae51265fedb68a5de29f0bd2a,TODO May want use dictConfig instead of basicConfig (must import separately: logging.config),https://github.com/roscisz/TensorHive/commit/53d87158f05d55dae51265fedb68a5de29f0bd2a,No
782,roscisz/TensorHive,tensorhive/config.py,b10feacb5e2c9ce716672fd581261138bdea4faa,TODO Handle more options (https:\/\/github.com\/ParallelSSH\/parallel-ssh\/blob\/2e9668cf4b58b38316b1d515810d7e6c595c76f3\/pssh\/clients\/base_pssh.py#L119),https://github.com/roscisz/TensorHive/commit/b10feacb5e2c9ce716672fd581261138bdea4faa,Yes
783,roscisz/TensorHive,tensorhive/config.py,b248f1d8b3df3ecdda56f4145d182de28b961d94,TODO May want use dictConfig instead of basicConfig (must import separately: logging.config),https://github.com/roscisz/TensorHive/commit/b248f1d8b3df3ecdda56f4145d182de28b961d94,No
784,roscisz/TensorHive,tensorhive/controllers/user/UserLoginController.py,b21cbd3554a2f90e48a41f7a32fd5f71aea63516,TODO Add more user parameters,https://github.com/roscisz/TensorHive/commit/b21cbd3554a2f90e48a41f7a32fd5f71aea63516,Yes
785,roscisz/TensorHive,tensorhive/config.py,6973c97927d6595b22d5e7b17b8a117ac350258b,TODO Handle more options (https:\/\/github.com\/ParallelSSH\/parallel-ssh\/blob\/2e9668cf4b58b38316b1d515810d7e6c595c76f3\/pssh\/clients\/base_pssh.py#L119),https://github.com/roscisz/TensorHive/commit/6973c97927d6595b22d5e7b17b8a117ac350258b,No
786,roscisz/TensorHive,tensorhive/controllers/user/DeleteUserController.py,3c6e439d0d4b76b65d992996af7d60f9dab59d6f,TODO Add more user parameters,https://github.com/roscisz/TensorHive/commit/3c6e439d0d4b76b65d992996af7d60f9dab59d6f,Yes
787,roscisz/TensorHive,tensorhive/config.py,d3c91ca2e81fb8ddb68690ef85e797d02fbb045b,TODO Handle more options (https:\/\/github.com\/ParallelSSH\/parallel-ssh\/blob\/2e9668cf4b58b38316b1d515810d7e6c595c76f3\/pssh\/clients\/base_pssh.py#L119),https://github.com/roscisz/TensorHive/commit/d3c91ca2e81fb8ddb68690ef85e797d02fbb045b,No
788,roscisz/TensorHive,tensorhive/controllers/task.py,ea42ca17cc28c6d2c8234efe71a9732ba97f716d,"\""\""\"" || TODO Group business functions into one place || TODO May want to have 1 function per endpoint.  || My goal was to separate authorization from controllers' logic; so that || manual and automatic testing don't require patching Flask context (@jwt_required breaks things) ||  || \""\""\""",https://github.com/roscisz/TensorHive/commit/ea42ca17cc28c6d2c8234efe71a9732ba97f716d,Yes
789,telmomenezes/synthetic,netgens/progs/node.py,8b65c70f4129d7e841a7af535a466d81e242ff3a,TODO: check both have same number of params!,https://github.com/telmomenezes/synthetic/commit/8b65c70f4129d7e841a7af535a466d81e242ff3a,Yes
790,telmomenezes/synthetic,netgens/prog.py,21b4be699cdc3719bbaca88475e1d4fa47b8c762,TODO: check both have same number of params!,https://github.com/telmomenezes/synthetic/commit/21b4be699cdc3719bbaca88475e1d4fa47b8c762,Yes
791,telmomenezes/synthetic,synthetic/prog.py,8850c3229d75d621a46bf9e5661f2c803992356b,TODO: check both have same number of params!,https://github.com/telmomenezes/synthetic/commit/8850c3229d75d621a46bf9e5661f2c803992356b,Yes
792,wanjinchang/SSH-TensorFlow,lib/model/train_val.py,961b5a3dc1aedd3d70b44552cbdab89a27ef5883,Needs to restore the other hyper-parameters\/states for training; (TODO xinlei) I have,https://github.com/wanjinchang/SSH-TensorFlow/commit/961b5a3dc1aedd3d70b44552cbdab89a27ef5883,No
793,erp12/pyshgp,pyshgp/gp/params.py,b33fb1b1eeca25b30c156a09e0bc78644e900cd2,"\""\""\"" || The :mod:`params` module defines the default hyper-parameters for genetic || programming runs and utility functions regarding updating these parameters. || This module also is responsible for the initialization and configuration of || the process pool used to parallelize parts of evolution. ||  || .. todo:: ||     Much of the logic in the file can be handled in a more robust way using ||     argparse. This is a fairly high priority change that will improve usability. ||  || \""\""\""",https://github.com/erp12/pyshgp/commit/b33fb1b1eeca25b30c156a09e0bc78644e900cd2,Yes
794,Hyperparticle/udify,udify/dataset_readers/sigmorphon_2019_task_2.py,b6a1173e7e5fc1e4c63f4a7cf1563b469268a3b8,TODO: parameter to turn this off,https://github.com/Hyperparticle/udify/commit/b6a1173e7e5fc1e4c63f4a7cf1563b469268a3b8,Yes
795,sinhrks/daskperiment,daskperiment/board/component.py,286537e6544d14144f81adef9e1ee3aded3f21c2,TODO: handle parameter colides,https://github.com/sinhrks/daskperiment/commit/286537e6544d14144f81adef9e1ee3aded3f21c2,Yes
796,yuvalpinter/m3gm,predict_wn18.py,6328dcfea7be74f67b6b40d34b20ac1daa7f68af,TODO this part is parallelizable,https://github.com/yuvalpinter/m3gm/commit/6328dcfea7be74f67b6b40d34b20ac1daa7f68af,Yes
797,yuvalpinter/m3gm,pretrain_assoc.py,6328dcfea7be74f67b6b40d34b20ac1daa7f68af,TODO consider using no_update param for embeddings,https://github.com/yuvalpinter/m3gm/commit/6328dcfea7be74f67b6b40d34b20ac1daa7f68af,Yes
798,andreasvc/disco-dop,treebank.py,64206afe9c58aefd375b5858e89fc662298151be,fixme: treebank specific parameters for detecting punctuation.,https://github.com/andreasvc/disco-dop/commit/64206afe9c58aefd375b5858e89fc662298151be,Yes
799,andreasvc/disco-dop,web/parse.py,43afc5ac17292a2b9cb85f12d769e9f54d9cf8d5,FIXME: return other parameters,https://github.com/andreasvc/disco-dop/commit/43afc5ac17292a2b9cb85f12d769e9f54d9cf8d5,Yes
800,andreasvc/disco-dop,discodop/treebanktransforms.py,04c3799444eda76947506b6f331d34ee74d6369b,fixme: treebank specific parameters for detecting punctuation.,https://github.com/andreasvc/disco-dop/commit/04c3799444eda76947506b6f331d34ee74d6369b,Yes
801,andreasvc/disco-dop,discodop/punctuation.py,8dd015f5601374f7ac242d166c148156035a70c7,fixme: treebank specific parameters for detecting punctuation.,https://github.com/andreasvc/disco-dop/commit/8dd015f5601374f7ac242d166c148156035a70c7,Yes
802,andreasvc/disco-dop,discodop/treesearch.py,132b31be7cdee22a56ca9853ce78ffdd10b61283,FIXME: this function could be parallelized.,https://github.com/andreasvc/disco-dop/commit/132b31be7cdee22a56ca9853ce78ffdd10b61283,Yes
803,varunagrawal/tiny-faces-pytorch,trainer.py,7f6d472f5dd0cd672eaa0f4dc8a426d4ea92683d,TODO Peiyun's code deals with scores and sigmoid probs separately. Check if that matters.,https://github.com/varunagrawal/tiny-faces-pytorch/commit/7f6d472f5dd0cd672eaa0f4dc8a426d4ea92683d,Yes
804,Erotemic/netharn,netharn/examples/cifar.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,TODO: type of augmentation as a parameter dependency,https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,Yes
805,Erotemic/netharn,netharn/examples/tests/expt_cifar.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,TODO: type of augmentation as a parameter dependency,https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,Yes
806,Erotemic/netharn,netharn/fit_harn.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,"\""\""\"" || CommandLine: ||     python ~\/code\/netharn\/netharn\/fit_harn.py __doc__ ||  || Notes: ||     when profiling ensure CUDA_LAUNCH_BLOCKING=1 ||  || Notes: ||     to use; your training session must have the concept of: ||         * epochs ||         * batch_size ||         * xpu ||         * train \/ validation datasets ||  ||     or better yet: ||         * a model ||         * a criterion ||         * an optimizer ||  || TODO: ||     [ ] - output \""glance\"" curves to disk ||     [x] - move logs to a logs folder. Keep a single master log in the root ||     [ ] - Why didnt the best_snapshot.pt get saved in the most recent yolo run? ||  || Example: ||     >>> import netharn as nh ||     >>> size = 3 ||     >>> max_epoch = 10 ||     >>> datasets = { ||     >>>     'train': nh.data.ToyData2d(size=size; border=1; n=256; rng=0); ||     >>>     'vali': nh.data.ToyData2d(size=size; border=1; n=128; rng=1); ||     >>> } ||     >>> hyper = { ||     >>>     # --- Data First ||     >>>     'datasets'    : datasets; ||     >>>     'nice'        : 'demo'; ||     >>>     'workdir'     : ub.ensure_app_cache_dir('netharn\/demo'); ||     >>>     'loaders'     : {'batch_size': 64}; ||     >>>     'xpu'         : nh.XPU.cast('auto'); ||     >>>     # --- Algorithm Second ||     >>>     'model'       : (nh.models.ToyNet2d; {}); ||     >>>     'optimizer'   : (nh.optimizers.SGD; { ||     >>>         'lr': 0.0001 ||     >>>     }); ||     >>>     'criterion'   : (nh.criterions.CrossEntropyLoss; {}); ||     >>>     #'criterion'   : (nh.criterions.FocalLoss; {}); ||     >>>     'initializer' : (nh.initializers.KaimingNormal; { ||     >>>         'param': 0; ||     >>>     }); ||     >>>     'scheduler'   : (nh.schedulers.ListedLR; { ||     >>>         'points': {0: .0001; 2: .01; 5: .015; 6: .005; 9: .001}; ||     >>>         'interpolate': True; ||     >>>     }); ||     >>>     'dynamics'   : {'batch_step': 4}; ||     >>>     'monitor'     : (nh.Monitor; { ||     >>>         'max_epoch': max_epoch; ||     >>>     }); ||     >>> } ||     >>> harn = FitHarn(hyper) ||     >>> harn.config['use_tqdm'] = 1 ||     >>> harn.initialize(reset='delete') ||     >>> harn.run() || \""\""\""",https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,Yes
807,Erotemic/netharn,netharn/fit_harn.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,"\""\""\"" || Notes: ||     when profiling ensure CUDA_LAUNCH_BLOCKING=1 ||  || Notes: ||     to use; your training session must have the concept of: ||         * epochs ||         * batch_size ||         * xpu ||         * train \/ validation datasets ||  ||     or better yet: ||         * a model ||         * a criterion ||         * an optimizer ||  || TODO: ||     [ ] - output \""glance\"" curves to disk ||     [x] - move logs to a logs folder. Keep a single master log in the root ||     [ ] - Why didnt the best_snapshot.pt get saved in the most recent yolo run? ||  || Notes: ||     In the following example we demonstrate how to use netharn to train a model ||     to solve a toy problem. ||  ||     In this toy problem; we do not extend the nh.FitHarn object; so we are ||     using the default behavior of ``run_batch``. The default ``on_batch``; and ||     ``on_epoch`` do nothing; so only loss will be the only measurement of ||     performance. ||  ||     For further examples please see the examples directory. These example show ||     how to extend nh.FitHarn to measure performance wrt a particular problem. ||     The MNIST and CIFAR examples are the most simple. The YOLO example is more ||     complex.  The IBEIS example depends on non-public data \/ software; but can ||     still be useful to look at.  Its complexity is more than CIFAR but less ||     than YOLO. ||  || CommandLine: ||     xdoctest netharn.fit_harn __doc__:0 ||     xdoctest netharn.fit_harn __doc__:0 --progiter ||  || Example: ||     >>> import netharn as nh ||     >>> hyper = nh.HyperParams(**{ ||     >>>     # ================ ||     >>>     # Environment Components ||     >>>     'workdir'     : ub.ensure_app_cache_dir('netharn\/tests\/demo'); ||     >>>     'nice'        : 'demo'; ||     >>>     'xpu'         : nh.XPU.cast('auto'); ||     >>>     # workdir is a directory where intermediate results can be saved ||     >>>     # nice symlinks <workdir>\/fit\/nice\/<nice> -> ..\/runs\/<hashid> ||     >>>     # XPU auto select a gpu if idle and VRAM>6GB else a cpu ||     >>>     # ================ ||     >>>     # Data Components ||     >>>     'datasets'    : {  # dict of plain ol torch.data.Dataset instances ||     >>>         'train': nh.data.ToyData2d(size=3; border=1; n=256; rng=0); ||     >>>         'vali': nh.data.ToyData2d(size=3; border=1; n=128; rng=1); ||     >>>         'test': nh.data.ToyData2d(size=3; border=1; n=128; rng=1); ||     >>>     }; ||     >>>     'loaders'     : {'batch_size': 64}; # DataLoader instances or kw ||     >>>     # ================ ||     >>>     # Algorithm Components ||     >>>     # Note the (cls; kw) tuple formatting ||     >>>     'model'       : (nh.models.ToyNet2d; {}); ||     >>>     'optimizer'   : (nh.optimizers.SGD; { ||     >>>         'lr': 0.0001 ||     >>>     }); ||     >>>     # focal loss is usually better than nh.criterions.CrossEntropyLoss ||     >>>     'criterion'   : (nh.criterions.FocalLoss; {}); ||     >>>     'initializer' : (nh.initializers.KaimingNormal; { ||     >>>         'param': 0; ||     >>>     }); ||     >>>     # these may receive an overhaul soon ||     >>>     'scheduler'   : (nh.schedulers.ListedLR; { ||     >>>         'points': {0: .0001; 2: .01; 5: .015; 6: .005; 9: .001}; ||     >>>         'interpolate': True; ||     >>>     }); ||     >>>     'monitor'     : (nh.Monitor; { ||     >>>         'max_epoch': 10; ||     >>>     }); ||     >>>     # dynamics are a config option that modify the behavior of the main ||     >>>     # training loop. These parameters effect the learned model. ||     >>>     'dynamics'   : {'batch_step': 4}; ||     >>> }) ||     >>> harn = nh.FitHarn(hyper) ||     >>> # non-algorithmic behavior configs (do not change learned models) ||     >>> harn.config['prog_backend'] = 'tqdm' ||     >>> if ub.argflag('--progiter'):  # I prefer progiter (I may be biased) ||     ...     harn.config['prog_backend'] = 'progiter' ||     >>> # start training. ||     >>> harn.initialize(reset='delete') ||     >>> harn.run()  # note: run calls initialize it hasn't already been called. ||     >>> # xdoc: +IGNORE_WANT ||     RESET HARNESS BY DELETING EVERYTHING IN TRAINING DIR ||     Symlink: ...tests\/demo\/fit\/runs\/demo\/keyeewlr -> ...tests\/demo\/fit\/nice\/demo ||     .... already exists ||     .... and points to the right place ||     Initializing tensorboard (dont forget to start the tensorboard server) ||     Model has 824 parameters ||     Mounting ToyNet2d model on CPU ||     Initializing model weights ||      * harn.train_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr' ||      * harn.nice_dpath  = '...tests\/demo\/fit\/nice\/demo' ||     Snapshots will save to harn.snapshot_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr\/torch_snapshots' ||     dont forget to start: ||         tensorboard --logdir ...tests\/demo\/fit\/nice ||     === begin training === ||     epoch lr:0.001 \u2502 vloss: 0.1409 (n_bad_epochs=00; best=0.1409): 100%|\u2588| 10\/10 [00:01<00:00;  9.95it\/s]  0:00<?; ?it\/s] ||     train x64 \u2502 loss:0.147 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8\/8 [00:00<00:00; 130.56it\/s] ||     vali x64 \u2502 loss:0.140 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4\/4 [00:00<00:00; 342.04it\/s] ||     test x64 \u2502 loss:0.140 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4\/4 [00:00<00:00; 342.92it\/s] ||     <BLANKLINE> ||     Maximum harn.epoch reached; terminating ... ||     <BLANKLINE> ||     training completed ||     current lrs: [0.001] ||     harn.train_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr' ||     harn.nice_dpath  = '...tests\/demo\/fit\/nice\/demo' ||     view tensorboard results for this run via: ||         tensorboard --logdir ...tests\/demo\/fit\/nice ||     exiting fit harness. || \""\""\""",https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,Yes
808,Erotemic/netharn,netharn/hyperparams.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,TODO: submit a PR to imgaug that registers parameters,https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,Yes
809,Erotemic/netharn,netharn/examples/cifar.py,5f79282c043a0d58be110ce7719c72f3402fbd58,"\""\""\"" || The examples\/cifar.py is probably the most clear example of what netharn is and || what it's trying to do \/ not do. ||  || The basic idea is make an object that inherits from nh.FitHarn. This is our || harness object. It will contain the hyperparameters as well as the learning || state. All the training loop boilerplate has already been written in the parent || class; so all our child class needs to do is: define `prepare_batch` (not || usually needed) and `run_batch`. Code to measure and record performance should || be placed in `on_batch` and `on_epoch`. ||  || The `train` function is our main entry point. It reads parameters from the || command line to override defaults. It then consructs the `HyperParams` object || and constructs an instance of `CIFAR_FitHarn` and calls `harn.run()`. ||  || This begins the training process. At a high level the harness will load the || data using torch DataLoaders; and call `run_batch` when it needs to compute the || model outputs and loss based on the input data. The returned loss is used to || update the model weights if `harn.tag === 'train'`; for validation; test; and || calibration (todo) datasets the loss is simply recorded. ||  || After `run_batch` finishes the `on_batch` function is called; where you can || optionally return a dict of scalars to log as measurements for this batch (note || loss is always recorded; so we need not return it here; but loss components may || be useful). A similar thing happens in `on_epoch`; where you should return || metrics about the entire dataset. ||  || The training harness manages the fit directory structure based on a hash of the || hyperparameters; the creation of algorithm component instance (e.g. model; || optimizer); initializing model weights; restarting from the most recent epoch; || updating the learning rates; various training loop boilerplate details; || checking divergence; reporting progress; handling differences between train; || validation; and test sets. In short; netharn handles the necessary parts and || let the developer focus on the important parts. || \""\""\""",https://github.com/Erotemic/netharn/commit/5f79282c043a0d58be110ce7719c72f3402fbd58,No
810,Erotemic/netharn,netharn/fit_harn.py,3c00f88b302692511944e4778cc5b93e68c2bcde,TODO: we may fold the functionality of Folders into Hyperparams,https://github.com/Erotemic/netharn/commit/3c00f88b302692511944e4778cc5b93e68c2bcde,Yes
811,Erotemic/netharn,examples/cifar.py,11dd3d227cbb397304522932872aba936f0c216d,TODO: Fast AI params,https://github.com/Erotemic/netharn/commit/11dd3d227cbb397304522932872aba936f0c216d,Yes
812,Erotemic/netharn,netharn/fit_harn.py,e16c3da37b8806b1dbd0f936a2392bb496de76d5,TODO: export in a separate process,https://github.com/Erotemic/netharn/commit/e16c3da37b8806b1dbd0f936a2392bb496de76d5,Yes
813,Erotemic/netharn,netharn/metrics/detect_metrics.py,1a111922f3f0e0999d93124e4cb5c9baa0438672,TODO: parallelize this,https://github.com/Erotemic/netharn/commit/1a111922f3f0e0999d93124e4cb5c9baa0438672,Yes
814,Erotemic/netharn,netharn/data/data_containers.py,b157c56653f4fbaf7a84e851d56cb1a4d5f29c08,"\""\""\"" || Proof-of-concept for porting mmcv DataContainer concept to netharn. Depending || on how well this works these features might be useful as a standalone module or || to contribute to torch proper. ||  || References: ||     https:\/\/github.com\/open-mmlab\/mmcv\/blob\/master\/mmcv\/parallel\/data_container.py ||     https:\/\/github.com\/open-mmlab\/mmcv\/blob\/master\/mmcv\/parallel\/collate.py ||     https:\/\/github.com\/open-mmlab\/mmcv\/blob\/master\/mmcv\/parallel\/scatter_gather.py ||  || FIXME 0 dimension tensors || \""\""\""",https://github.com/Erotemic/netharn/commit/b157c56653f4fbaf7a84e851d56cb1a4d5f29c08,No
815,aldro61/kover,kover/dataset/convert/tsv.py,befa2ebffc86fd19e7f39fde37f38ede912ce93e,TODO: This could be a user specified parameter,https://github.com/aldro61/kover/commit/befa2ebffc86fd19e7f39fde37f38ede912ce93e,Yes
816,aldro61/kover,kover/dataset/convert/tsv.py,6d9473f05560905b8340773914d5bf8e9ff8a4dd,TODO: the block size should be a parameter to control memory usage. The default should be fairly small.,https://github.com/aldro61/kover/commit/6d9473f05560905b8340773914d5bf8e9ff8a4dd,Yes
817,daniel-muthukrishna/astrodash,dash/create_arrays.py,19edb9ec3337f58640da6f478216ed25e427cfd8,TODO: Try parallelising this for speed boost. Or try out SMOTE instead,https://github.com/daniel-muthukrishna/astrodash/commit/19edb9ec3337f58640da6f478216ed25e427cfd8,Yes
818,fpaupier/tensorflow-serving_sidecar,slim/deployment/model_deploy.py,e78e2946749dd804dc3868eee8973161222c3d68,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/fpaupier/tensorflow-serving_sidecar/commit/e78e2946749dd804dc3868eee8973161222c3d68,No
819,Sanqiang/text_simplification,tensor2tensor/data_generators/allen_brain.py,aa326601488d19ed8b6209d53c1179632b99a25e,"\""\""\""Problem definitions for Allen Brain Atlas problems. ||  || Notes: ||  ||   * TODO(cwbeitel): Want to be able to increase up-sampling ratio and\/or ||     in-paint fraction over the course of training. This could be done by ||     defining a range of problems or perhaps more aptly with an hparam ||     that is dialed up depending on training performance. ||  || \""\""\""",https://github.com/Sanqiang/text_simplification/commit/aa326601488d19ed8b6209d53c1179632b99a25e,Yes
820,Sanqiang/text_simplification,tensor2tensor/rl/trainer_model_based.py,aa326601488d19ed8b6209d53c1179632b99a25e,TODO(blazej): this param is unused,https://github.com/Sanqiang/text_simplification/commit/aa326601488d19ed8b6209d53c1179632b99a25e,Yes
821,Sanqiang/text_simplification,trans_data_processor/en/parser/nltk_lite/probability.py,aa326601488d19ed8b6209d53c1179632b99a25e,TODO - add a cut-off parameter; above which the counts are unmodified,https://github.com/Sanqiang/text_simplification/commit/aa326601488d19ed8b6209d53c1179632b99a25e,Yes
822,hwang595/ps_pytorch,pytorch_code/data_parallel_dist/data_parallel_dist.py,dc25acafe0294937addd4d0e988ef23c23556967,TODO: we don't need to replicate params in here. they're always going to,https://github.com/hwang595/ps_pytorch/commit/dc25acafe0294937addd4d0e988ef23c23556967,Yes
823,jason718/game-feature-learning,eval-3rd-party/voc_cls/phikr_caffe/tools/extra/summarize.py,c1844d400fe3d191c3e012f8c6eb83dfc46ff7e9,TODO support rectangular\/ND parameters,https://github.com/jason718/game-feature-learning/commit/c1844d400fe3d191c3e012f8c6eb83dfc46ff7e9,Yes
824,dangitstam/topic-rnn,library/models/topic_rnn.py,db56224ebb5ecd05cebd84d7702261da5a1b44cc,TODO: Add classification layer and freezing parameter.,https://github.com/dangitstam/topic-rnn/commit/db56224ebb5ecd05cebd84d7702261da5a1b44cc,Yes
825,dangitstam/topic-rnn,library/models/topic_rnn.py,7932e236872bb3e38982a3d9ae06d174d5ebd20e,TODO: Add classification layer and freezing parameter.,https://github.com/dangitstam/topic-rnn/commit/7932e236872bb3e38982a3d9ae06d174d5ebd20e,Yes
826,diningphil/CGMM,utils/TrainingUtilities.py,838237057c9cd05abb9e14b4fdb4d25351ea99d9,TODO UPDATE DICT TO SAVE (LIST OF LAYERS and PARAMETERS (IN A DICTIONARY),https://github.com/diningphil/CGMM/commit/838237057c9cd05abb9e14b4fdb4d25351ea99d9,Yes
827,diningphil/CGMM,utils/TrainingUtilities.py,838237057c9cd05abb9e14b4fdb4d25351ea99d9,TODO RESTORE THE ARCHITECTURE (LIST OF LAYERS and PARAMETERS (IN A DICTIONARY),https://github.com/diningphil/CGMM/commit/838237057c9cd05abb9e14b4fdb4d25351ea99d9,Yes
828,rmalav15/Super-SloMo,slomo_video.py,22ca044012102f1c3d9b52094b330b7b702636a1,TODO: Restructure code to separate submodules from slomo,https://github.com/rmalav15/Super-SloMo/commit/22ca044012102f1c3d9b52094b330b7b702636a1,Yes
829,simonfqy/PADME,dcCustom/models/tensor_graph.py,4f3610b7ee05d2bd755fa73ffdd2afdb154f1421,TODO: the following two variables are temporary; for hyperparameter tuning purpose.,https://github.com/simonfqy/PADME/commit/4f3610b7ee05d2bd755fa73ffdd2afdb154f1421,No
830,simonfqy/PADME,dcCustom/models/tensorgraph/tensor_graph.py,2cb677a6ee205673b6e17454b979c3a2cce991a5,TODO: the following two variables are temporary; for hyperparameter tuning purpose.,https://github.com/simonfqy/PADME/commit/2cb677a6ee205673b6e17454b979c3a2cce991a5,No
831,simonfqy/PADME,dcCustom/models/tensorgraph/tensor_graph.py,90a881f4afc0e536b8fc242c9f6646c6cce09b0a,TODO: the following two variables are temporary; for hyperparameter tuning purpose.,https://github.com/simonfqy/PADME/commit/90a881f4afc0e536b8fc242c9f6646c6cce09b0a,No
832,davidhaohanli/Abnormal-Behaviour-Detection,code/core.py,bb693d961e9f2771ed48760cdb1f0204d2dfe28a,TODO PARAM SELECTION,https://github.com/davidhaohanli/Abnormal-Behaviour-Detection/commit/bb693d961e9f2771ed48760cdb1f0204d2dfe28a,No
833,jkkummerfeld/slate,src/config.py,f8abd799e2f03ceaee7d3d93b20dcb3b30ffbcb7,TODO: Have a separate notion of 'single'; 'continuous'; 'discontiuous' (for the item selection); so the user could be linking a pair of discontinuous sets of lines,https://github.com/jkkummerfeld/slate/commit/f8abd799e2f03ceaee7d3d93b20dcb3b30ffbcb7,No
834,VSainteuf/izitorch,trainRack.py,bd2998453ef9eb0f1c359697fc5920c85be1aca6,TODO generalise method and just give a list of metrics as rack parameter,https://github.com/VSainteuf/izitorch/commit/bd2998453ef9eb0f1c359697fc5920c85be1aca6,Yes
835,VSainteuf/izitorch,izitorch/trainRack.py,08578a5799197df51b4178d362e185586d5b3e75,TODO generalise method and just give a list of metrics as rack parameter,https://github.com/VSainteuf/izitorch/commit/08578a5799197df51b4178d362e185586d5b3e75,Yes
836,VSainteuf/izitorch,izitorch/trainRack.py,59a7a1b43fab4833633d447fe6c32555c5f3eb90,TODO generalise method and just give a list of metrics as rack parameter,https://github.com/VSainteuf/izitorch/commit/59a7a1b43fab4833633d447fe6c32555c5f3eb90,Yes
837,VSainteuf/izitorch,izitorch/trainRack.py,93386d64361d0c70a509c9a8a5e10446f6a6df5a,TODO generalise method and just give a list of metrics as rack parameter,https://github.com/VSainteuf/izitorch/commit/93386d64361d0c70a509c9a8a5e10446f6a6df5a,Yes
838,VSainteuf/izitorch,izitorch/trainRack.py,9803a259e88dfd563fd8034ef285cce43ac80f56,TODO Parametrize the evaluation of final peformance,https://github.com/VSainteuf/izitorch/commit/9803a259e88dfd563fd8034ef285cce43ac80f56,Yes
839,amir-jafari/Deep-Learning,Caffe_/1-Simple_Example/PIL/PIL/ImageFile.py,1bc09ffc7eb3e5452577f3f8efe5f3efeae3c136,FIXME: make MAXBLOCK a configuration parameter,https://github.com/amir-jafari/Deep-Learning/commit/1bc09ffc7eb3e5452577f3f8efe5f3efeae3c136,Yes
840,annomator/annomator_1.0,tf_slim_obj_det/deployment/model_deploy.py,042cc36cba515855a4cbc963df4328123d624c9f,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/annomator/annomator_1.0/commit/042cc36cba515855a4cbc963df4328123d624c9f,No
841,Ciaran1981/geospatial-learn,geodata.py,8c0122b135a086ed8e75075ac47f436189639e23,TODO - cythonise loop block processing as separate function then,https://github.com/Ciaran1981/geospatial-learn/commit/8c0122b135a086ed8e75075ac47f436189639e23,No
842,Ciaran1981/geospatial-learn,learning.py,8c0122b135a086ed8e75075ac47f436189639e23,TODO 2- thread or parallelise block\/line processing,https://github.com/Ciaran1981/geospatial-learn/commit/8c0122b135a086ed8e75075ac47f436189639e23,Yes
843,Ciaran1981/geospatial-learn,build/bdist.linux-x86_64/geospatial_learn/geodata.py,7363f0600706186e6e26f593d4c7da385a18de84,TODO - cythonise loop block processing as separate function then,https://github.com/Ciaran1981/geospatial-learn/commit/7363f0600706186e6e26f593d4c7da385a18de84,No
844,Ciaran1981/geospatial-learn,build/bdist.linux-x86_64/geospatial_learn/learning.py,7363f0600706186e6e26f593d4c7da385a18de84,TODO 2- thread or parallelise block\/line processing,https://github.com/Ciaran1981/geospatial-learn/commit/7363f0600706186e6e26f593d4c7da385a18de84,Yes
845,Ciaran1981/geospatial-learn,build/lib/geospatial_learn/geodata.py,3973e2d7092f21a8f6827c279117df638b6a5d9a,TODO - cythonise loop block processing as separate function then,https://github.com/Ciaran1981/geospatial-learn/commit/3973e2d7092f21a8f6827c279117df638b6a5d9a,No
846,Ciaran1981/geospatial-learn,build/lib/geospatial_learn/learning.py,3973e2d7092f21a8f6827c279117df638b6a5d9a,TODO 2- thread or parallelise block\/line processing,https://github.com/Ciaran1981/geospatial-learn/commit/3973e2d7092f21a8f6827c279117df638b6a5d9a,Yes
847,Ciaran1981/geospatial-learn,geospatial_learn/data.py,0a067c58f300ba4eaabea1e60b9c593e9a99cc17,TODO Do we want the metadata in a separate file as well as embedded in the geotiff?,https://github.com/Ciaran1981/geospatial-learn/commit/0a067c58f300ba4eaabea1e60b9c593e9a99cc17,Yes
848,Ciaran1981/geospatial-learn,build/lib/geospatial_learn/data.py,b75f043cca00bb3e17f1f77f27d7ad3af35d1195,TODO Do we want the metadata in a separate file as well as embedded in the geotiff?,https://github.com/Ciaran1981/geospatial-learn/commit/b75f043cca00bb3e17f1f77f27d7ad3af35d1195,Yes
849,bareblackfoot/lddp-tf-faster-rcnn,lib/model/train_val.py,ee512c4533be6d7c26a1f8ebbc1c373114b5bc9f,Needs to restore the other hyperparameters\/states for training; (TODO xinlei) I have,https://github.com/bareblackfoot/lddp-tf-faster-rcnn/commit/ee512c4533be6d7c26a1f8ebbc1c373114b5bc9f,Yes
850,bareblackfoot/lddp-tf-faster-rcnn,lib/model/train_val_vgg16.py,f230e29fe94ccffa151a4a4406005c5be280192e,Needs to restore the other hyperparameters\/states for training; (TODO xinlei) I have,https://github.com/bareblackfoot/lddp-tf-faster-rcnn/commit/f230e29fe94ccffa151a4a4406005c5be280192e,Yes
851,bareblackfoot/lddp-tf-faster-rcnn,lib/model/train_val.py,5ed25ab9bf38c1bc0505dfe83f1a6031a29a5d2a,Needs to restore the other hyper-parameters\/states for training; (TODO xinlei) I have,https://github.com/bareblackfoot/lddp-tf-faster-rcnn/commit/5ed25ab9bf38c1bc0505dfe83f1a6031a29a5d2a,Yes
852,dimimal/semantics_segmentation_of_urban_environments,lib/buildModels.py,f1564a4d137fe5ef49eef41e660179c765d3de46,TODO: Pass CRF Hyperparameters,https://github.com/dimimal/semantics_segmentation_of_urban_environments/commit/f1564a4d137fe5ef49eef41e660179c765d3de46,Yes
853,dtrckd/pymake,pymake/models/hdp/ugs.py,c40b412aac7c40865a09b38a8baf8ab1584e4899,todo: add in the likelihood for hyper-parameter,https://github.com/dtrckd/pymake/commit/c40b412aac7c40865a09b38a8baf8ab1584e4899,Yes
854,dtrckd/pymake,pymake/models/igmm/cgs.py,c40b412aac7c40865a09b38a8baf8ab1584e4899,todo: add in the likelihood for K and hyperparameter,https://github.com/dtrckd/pymake/commit/c40b412aac7c40865a09b38a8baf8ab1584e4899,Yes
855,KMouratidis/EDA_miner,apps/exploration/graphs/graphs2d.py,55ca5bddb5152ba934efac66a989a73b07f99797,TODO: add layout parameters,https://github.com/KMouratidis/EDA_miner/commit/55ca5bddb5152ba934efac66a989a73b07f99797,Yes
856,KMouratidis/EDA_miner,apps/exploration/graphs/kpis.py,55ca5bddb5152ba934efac66a989a73b07f99797,TODO: add layout parameters,https://github.com/KMouratidis/EDA_miner/commit/55ca5bddb5152ba934efac66a989a73b07f99797,Yes
857,mesnico/learning-relationship-aware-visual-features,networkx/networkx/algorithms/bipartite/redundancy.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,TODO This can be trivially parallelized.,https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,Yes
858,mesnico/learning-relationship-aware-visual-features,networkx/networkx/algorithms/isolate.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,TODO This can be parallelized.,https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,Yes
859,mesnico/learning-relationship-aware-visual-features,networkx/networkx/algorithms/tree/mst.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,TODO This can be parallelized; both in the outer loop over,https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,Yes
860,mesnico/learning-relationship-aware-visual-features,networkx/networkx/algorithms/tree/mst.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,TODO This loop can be parallelized; to an extent (the union,https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,Yes
861,mesnico/learning-relationship-aware-visual-features,networkx/networkx/algorithms/vitality.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,TODO This can be trivially parallelized.,https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,Yes
862,mesnico/learning-relationship-aware-visual-features,networkx/networkx/drawing/nx_pydot.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,"FIXME: Document the \""root\"" parameter.",https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,Yes
863,mesnico/learning-relationship-aware-visual-features,networkx/networkx/generators/geometric.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,TODO This can be parallelized.,https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,Yes
864,mpkuse/cartwheel_train,train_netvlad.py,b4ceb6962dfede1926a341156223161c3259f6d3,TODO: codeup a while loop - dry iterations (non training). or may be have a separate,https://github.com/mpkuse/cartwheel_train/commit/b4ceb6962dfede1926a341156223161c3259f6d3,Yes
865,mpkuse/cartwheel_train,4viz.py,a9c51209e3de3e300a9fbdc9571ff308a23d3902,TODO: Add assert here to ensure BAG params are of equal length,https://github.com/mpkuse/cartwheel_train/commit/a9c51209e3de3e300a9fbdc9571ff308a23d3902,Yes
866,hailotech/seg-mentor,fcn_train.py,83b59543441791f0cfc2da5563613a8edbc5a0fd,TODO - parameterize?.... also note 0.1 vs. 0.01 workaround TODO figure out why..),https://github.com/hailotech/seg-mentor/commit/83b59543441791f0cfc2da5563613a8edbc5a0fd,Yes
867,comtravo/ctparse,ctparse/ctparse.py,0b70e9b1bfa3730613304649bdcbe1d8acde0c8a,TODO: the score should be kept separate from the partial parse,https://github.com/comtravo/ctparse/commit/0b70e9b1bfa3730613304649bdcbe1d8acde0c8a,Yes
868,comtravo/ctparse,ctparse/ctparse.py,0b70e9b1bfa3730613304649bdcbe1d8acde0c8a,TODO: We should store scores separately from the production itself,https://github.com/comtravo/ctparse/commit/0b70e9b1bfa3730613304649bdcbe1d8acde0c8a,Yes
869,sbl-sdsc/mmtf-pyspark,mmtfPyspark/interactions/interaction_extractor_df.py,0c1de5b06141caf36dc61e19353bdce5099f2a30,TODO consider adding parameters,https://github.com/sbl-sdsc/mmtf-pyspark/commit/0c1de5b06141caf36dc61e19353bdce5099f2a30,Yes
870,deep-fry/mayo,mayo/estimate.py,b43d8dfc34f33f8e3c64f2512ef4700e52e1e759,TODO parametric gamma element-wise multiply + add overhead,https://github.com/deep-fry/mayo/commit/b43d8dfc34f33f8e3c64f2512ef4700e52e1e759,Yes
871,FreeDiscovery/FreeDiscovery,freediscovery/simhash/base.py,8ac5f23a745fbd015db1a450114d1279218a72b8,TODO parameter 'y' is unused,https://github.com/FreeDiscovery/FreeDiscovery/commit/8ac5f23a745fbd015db1a450114d1279218a72b8,Yes
872,pyro-ppl/funsor,funsor/minipyro.py,92541ea56868c68fcb2210f3e4e674c1f21e3ea3,FIXME This is only correct for reparametrized sites.,https://github.com/pyro-ppl/funsor/commit/92541ea56868c68fcb2210f3e4e674c1f21e3ea3,Yes
873,pyro-ppl/funsor,funsor/minipyro.py,36e57aeaa67c7df9e944485df34b9e285af05378,FIXME This is only correct for reparametrized sites.,https://github.com/pyro-ppl/funsor/commit/36e57aeaa67c7df9e944485df34b9e285af05378,Yes
874,pyro-ppl/funsor,test/test_distribution_generic.py,84286d17a7af1af9d6721d1a5cb92b6c858dcb37,TODO implement RelaxedBernoulli._infer_param_domain for temperature,https://github.com/pyro-ppl/funsor/commit/84286d17a7af1af9d6721d1a5cb92b6c858dcb37,Yes
875,aws-samples/deep-learning-models,models/nlp/electra/run_pretraining.py,70db58fa60894e90f90c9f398c333bfa36da683a,TODO: Make temperature a hyperparameter,https://github.com/aws-samples/deep-learning-models/commit/70db58fa60894e90f90c9f398c333bfa36da683a,Yes
876,estnltk/estnltk,estnltk/wikiXmlParser/internalLink.py,a4defe62574448223cdbde5babc3e396ee51a209,TODO: handle links with separator |,https://github.com/estnltk/estnltk/commit/a4defe62574448223cdbde5babc3e396ee51a209,Yes
877,estnltk/estnltk,estnltk/prettyprinter/prettyprinter.py,976125b470b39f12780b108f50a0b8d0070d409f,TODO: lisada boolean parameeter; millega saab headeri\/footeri lisamist kontrollida,https://github.com/estnltk/estnltk/commit/976125b470b39f12780b108f50a0b8d0070d409f,Yes
878,NLPatVCU/medaCy,medacy/pipeline_components/metamap/metamap.py,73ea73a22deb190456db2e6c13f6dcb37677ba02,TODO implement utilizing code for parallel process mapper from n2c2,https://github.com/NLPatVCU/medaCy/commit/73ea73a22deb190456db2e6c13f6dcb37677ba02,Yes
879,ROCmSoftwarePlatform/Tensile,Scripts/BenchmarkProcess.py,bcf7d884ce3e36128dd12c89ed1ce7de722b2f8e,TODO - print warning when config contains a parameter,https://github.com/ROCmSoftwarePlatform/Tensile/commit/bcf7d884ce3e36128dd12c89ed1ce7de722b2f8e,Yes
880,ROCmSoftwarePlatform/Tensile,Scripts/Structs.py,1bc7ba7e69e6935bdc1deffffd58304bb5432312,TODO limit parameters to those in global; not derrived ones,https://github.com/ROCmSoftwarePlatform/Tensile/commit/1bc7ba7e69e6935bdc1deffffd58304bb5432312,Yes
881,ROCmSoftwarePlatform/Tensile,Scripts/BenchmarkProcess.py,ca30c363cb71fef9ea5371d5e4b6118ab80de4a9,todo having MacroTile as join parameter causes trouble if,https://github.com/ROCmSoftwarePlatform/Tensile/commit/ca30c363cb71fef9ea5371d5e4b6118ab80de4a9,Yes
882,ROCmSoftwarePlatform/Tensile,Scripts/BenchmarkProcess.py,a012352f1b705a5ac65260e2ca66926c3747097b,todo having MacroTile as join parameter causes trouble if,https://github.com/ROCmSoftwarePlatform/Tensile/commit/a012352f1b705a5ac65260e2ca66926c3747097b,Yes
883,ROCmSoftwarePlatform/Tensile,Scripts/Analyze.py,794adadfefa842351143c43c13b2ee6bfb88edac,TODO need to use dilate parameter to make sure we've switched,https://github.com/ROCmSoftwarePlatform/Tensile/commit/794adadfefa842351143c43c13b2ee6bfb88edac,Yes
884,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,74c7db2093e1c05d248c51098896fc511dfc75d0,TODO needs to handle para and perp strides b\/c one address,https://github.com/ROCmSoftwarePlatform/Tensile/commit/74c7db2093e1c05d248c51098896fc511dfc75d0,Yes
885,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,0b3fb6b493f9af6b6fb138c07a491fd431ff15e8,TODO - need to separate prefetches from inside the loop,https://github.com/ROCmSoftwarePlatform/Tensile/commit/0b3fb6b493f9af6b6fb138c07a491fd431ff15e8,Yes
886,ROCmSoftwarePlatform/Tensile,Tensile/SolutionWriter.py,8e230bfad39997daf8dc90a73328800d5b8e6679,TODO - timing with beta kernels is somewhat pessimistic since it has this separate event only on the GSU path.,https://github.com/ROCmSoftwarePlatform/Tensile/commit/8e230bfad39997daf8dc90a73328800d5b8e6679,Yes
887,ROCmSoftwarePlatform/Tensile,Tensile/SolutionWriter.py,f8588847a1a8e6865911a23e41cb56dc09f8c7a4,TODO - timing with beta kernels is somewhat pessimistic since it has this separate event only on the GSU path.,https://github.com/ROCmSoftwarePlatform/Tensile/commit/f8588847a1a8e6865911a23e41cb56dc09f8c7a4,Yes
888,hyperdashio/hyperdash-sdk-py,sdk.py,2e37111d9b4c0bc552c06e9b851bd20f02ffbc82,TODO: We should probably spawn a separate process instead of using a thread,https://github.com/hyperdashio/hyperdash-sdk-py/commit/2e37111d9b4c0bc552c06e9b851bd20f02ffbc82,Yes
889,hyperdashio/hyperdash-sdk-py,hyperdash/experiment.py,7d31c2cc95f40a5e6867a98ce5d78596098eb0bf,TODO: Possibly move this to a separate file so we can still,https://github.com/hyperdashio/hyperdash-sdk-py/commit/7d31c2cc95f40a5e6867a98ce5d78596098eb0bf,Yes
890,blue-oil/blueoil,dlk/python/dlk/core/optimizer.py,985bc1a7a186e4b6fa17777d91a167634ac1f1d3,TODO: pass proper parameters,https://github.com/blue-oil/blueoil/commit/985bc1a7a186e4b6fa17777d91a167634ac1f1d3,Yes
891,IntelAI/models,tests/unit/test_launch_benchmark.py,34f9e35004fa40359e166f23628327108e700c70,TODO: make more efficient! Want to get rid of any example_req_args that exist in request.param[2],https://github.com/IntelAI/models/commit/34f9e35004fa40359e166f23628327108e700c70,Yes
892,IntelAI/models,tests/unit/test_launch_benchmark.py,7c45709c8579de2e2d2653be8606460c68e93964,TODO: make more efficient! Want to get rid of any example_req_args that exist in request.param[2],https://github.com/IntelAI/models/commit/7c45709c8579de2e2d2653be8606460c68e93964,Yes
893,analysiscenter/batchflow,dataset/preprocess.py,17f7532b26ef8c7215c21df5319021520d550bd4,TODO: decorator params: parallelization (e.g. threads; processes; async\/await; greenlets;...),https://github.com/analysiscenter/batchflow/commit/17f7532b26ef8c7215c21df5319021520d550bd4,Yes
894,analysiscenter/batchflow,batchflow/models/torch/visualization.py,365e9b10ef8d85ff383091b1bdb8216a3d452279,TODO : make a separate container for LR,https://github.com/analysiscenter/batchflow/commit/365e9b10ef8d85ff383091b1bdb8216a3d452279,Yes
895,dirty-cat/dirty_cat,dirty_cat/minhash_encoder.py,efa5b704ddde11188d30abc84b9658500e6fea3a,TODO Parallel run here,https://github.com/dirty-cat/dirty_cat/commit/efa5b704ddde11188d30abc84b9658500e6fea3a,Yes
896,CartoDB/cartoframes,cartoframes.py,ad50bfcf78c263a427a21575b3760faca35c0d95,TODO: remove username as a param would be nice.. accessible to write to,https://github.com/CartoDB/cartoframes/commit/ad50bfcf78c263a427a21575b3760faca35c0d95,Yes
897,CartoDB/cartoframes,cartoframes/cartoframes.py,25da5074a0387871f7d40c52be4e898aa9777080,TODO: remove username as a param would be nice.. accessible to write to,https://github.com/CartoDB/cartoframes/commit/25da5074a0387871f7d40c52be4e898aa9777080,Yes
898,skggm/skggm,inverse_covariance/statistical_power.py,54440e59758a5479db133406d4d9a1efcdcf1ce8,TODO: paralellize this,https://github.com/skggm/skggm/commit/54440e59758a5479db133406d4d9a1efcdcf1ce8,Yes
899,skggm/skggm,inverse_covariance/adaptive_graph_lasso.py,a113f904c0b647605b1d1871c924cd35f98e41c9,TODO: add auto_scale param; use False here,https://github.com/skggm/skggm/commit/a113f904c0b647605b1d1871c924cd35f98e41c9,Yes
900,skggm/skggm,inverse_covariance/adaptive_graph_lasso.py,ad70b3f613c73b6817ec744b196637640448e616,TODO: add auto_scale param; use False here,https://github.com/skggm/skggm/commit/ad70b3f613c73b6817ec744b196637640448e616,Yes
901,Epistimio/orion,src/metaopt/algo/space.py,66f5422dd656c04d5da6882678e1f251cba2b019,"\""\""\"" || :mod:`metaopt.algo.space` -- Objects describing a problem's domain || ================================================================== ||  || .. module:: space ||    :platform: Unix ||    :synopsis: Classes for representing the search space of an ||       optimization problem. ||  || There are 3 classes representing possible parameter types. All of them subclass || the base class `Dimension`: ||  ||     * `Real` ||     * `Integer` ||     * `Categorical` ||  || These are instantiated to declare a problem's parameter space. Metaopt registers || them in a ordered dictionary; `Space`; which describes how the parameters should || be in order for `metaopt.algo.base.AbstractAlgorithm` implementations to || communicate with `metaopt.core`. ||  || Parameter values recorded in `metaopt.core.worker.trial.Trial` objects must be || and are in concordance with `metaopt.algo.space` objects. These objects will be || defined by `metaopt.core` using the user script's configuration file. ||  || (TODO) Spaces can be transformed to other spaces; using an appropriate || **compositor** (pattern) subclass of `Dimension`. ||  || Prior distributions; contained in `Dimension` classes; are based on || `scipy.stats.distributions` and should be configured as noted in the || scipy documentation for each specific implentation of a random variable type; || unless noted otherwise! ||  || (TODO) Complete list of distribution names supported and their settings || (TODO) Set up sphinx interlink with scipy to make documentation of priors ||        beautiful! ||  || \""\""\""",https://github.com/Epistimio/orion/commit/66f5422dd656c04d5da6882678e1f251cba2b019,Yes
902,Epistimio/orion,src/orion/core/cli/hunt.py,71d1f9466de2b62092a3b0f52b49d547eac89a91,TODO: simplify when parameter parsing is refactored,https://github.com/Epistimio/orion/commit/71d1f9466de2b62092a3b0f52b49d547eac89a91,Yes
903,ROCmSoftwarePlatform/Tensile,Tensile/SolutionWriter.py,d0dcfa46563a2ade6de59414222052208c249341,TODO - timing with beta kernels is somewhat pessimistic since it has this separate event only on the GSU path.,https://github.com/ROCmSoftwarePlatform/Tensile/commit/d0dcfa46563a2ade6de59414222052208c249341,Yes
904,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,350acef30fb2dcabffdf12d463c0ef1e191b38a2,"and (state[\""LdsBlockSizePerPad%tc\""] % (globalParameters[\""WavefrontWidth\""] * 4) != 0): \/\/ TODO:",https://github.com/ROCmSoftwarePlatform/Tensile/commit/350acef30fb2dcabffdf12d463c0ef1e191b38a2,Yes
905,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,79f2eaec38754e4c0acf3399710f68d2b8586450,"and (state[\""LdsBlockSizePerPad%tc\""] % (globalParameters[\""WavefrontWidth\""] * 4) != 0): \/\/ TODO:",https://github.com/ROCmSoftwarePlatform/Tensile/commit/79f2eaec38754e4c0acf3399710f68d2b8586450,Yes
906,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,98f7030ef914eb00fed788be20386ad7d631c848,"and (state[\""LdsBlockSizePerPad%tc\""] % (globalParameters[\""WavefrontWidth\""] * 4) != 0): \/\/ TODO:",https://github.com/ROCmSoftwarePlatform/Tensile/commit/98f7030ef914eb00fed788be20386ad7d631c848,Yes
907,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,ae969ae0cb63a4b70c9c89beb53b527cb853496a,"and (state[\""LdsBlockSizePerPad%tc\""] % (globalParameters[\""WavefrontWidth\""] * 4) != 0): \/\/ TODO:",https://github.com/ROCmSoftwarePlatform/Tensile/commit/ae969ae0cb63a4b70c9c89beb53b527cb853496a,Yes
908,ROCmSoftwarePlatform/Tensile,Tensile/Common.py,e8fd65172052644345f9a8a2e63c6b56f55c0534,TODO: remove. legacy logic yaml in rocblas contains true and false for this parameter,https://github.com/ROCmSoftwarePlatform/Tensile/commit/e8fd65172052644345f9a8a2e63c6b56f55c0534,Yes
909,msmbuilder/osprey,osprey/strategies.py,9551d8514fb556690c3ca41e4e43407dff97a265,TODO : n_iter; max_iter should be in acquisition params,https://github.com/msmbuilder/osprey/commit/9551d8514fb556690c3ca41e4e43407dff97a265,Yes
910,msmbuilder/osprey,osprey/strategies.py,6e8bb5b43bbd38458a81855a3396c5e9b2ae68a4,TODO make type of model dependent on input param,https://github.com/msmbuilder/osprey/commit/6e8bb5b43bbd38458a81855a3396c5e9b2ae68a4,Yes
911,NVIDIA/Dataset_Utilities,nvdu/viz/nvdu_visualizer.py,d496c98c6fef5bb548660bb0857a14e0a431dc93,TODO: Move this code to a separated function,https://github.com/NVIDIA/Dataset_Utilities/commit/d496c98c6fef5bb548660bb0857a14e0a431dc93,Yes
912,scikit-learn-contrib/stability-selection,stability_selection/stability_selection.py,c045fc8e2ea0293b0aecebb71cb2cd106677cfdd,TODO: Can replace array of alphas with a dict {par_name: par_grid}; so we can use arbitrary parameters,https://github.com/scikit-learn-contrib/stability-selection/commit/c045fc8e2ea0293b0aecebb71cb2cd106677cfdd,Yes
913,Calysto/conx,conx/network.py,9a06508b5b5e0d3cf8a87617d6341bae7947b228,# FIXME: redo checks to separate dataset:,https://github.com/Calysto/conx/commit/9a06508b5b5e0d3cf8a87617d6341bae7947b228,Yes
914,Calysto/conx,conx/network.py,18a8f33b4a05ea233df16315b264f6635a134211,FIXME: change this to plot loss and acc on separate graphs,https://github.com/Calysto/conx/commit/18a8f33b4a05ea233df16315b264f6635a134211,Yes
915,SMTorg/smt,smt/extensions/moe.py,fd62de4bacf57437a96e9d6c87fc9394fcb7ec60,TODO : add parameters for KRG; RMTB; RMTC; KRG ?,https://github.com/SMTorg/smt/commit/fd62de4bacf57437a96e9d6c87fc9394fcb7ec60,Yes
916,SMTorg/smt,smt/surrogate_models/neural_net/demo.py,b4a8e3addfda4664ea1546023686c854e5b2e4bf,TODO: write a function to tune the hyper-parameters above (instead of trial and error),https://github.com/SMTorg/smt/commit/b4a8e3addfda4664ea1546023686c854e5b2e4bf,Yes
917,AcutronicRobotics/ros2learn,experiments/examples/modular_scara_3dof_v3/train_deepq_naf.py,4bee7839a9b8d61e30c07f6ef868316333cba3b9,TODO separate plots for different envs - subplots or completely separate?,https://github.com/AcutronicRobotics/ros2learn/commit/4bee7839a9b8d61e30c07f6ef868316333cba3b9,Yes
918,snuspl/parallax,parallax/parallax/core/python/common/lib.py,8af2efbb22242f811672f9bba7ad240e4a00e675,FIXME support in-graph and between-graph auto parallel,https://github.com/snuspl/parallax/commit/8af2efbb22242f811672f9bba7ad240e4a00e675,Yes
919,ucla-labx/distbelief,train.py,3e93a2b401cf098e33c88a44a4c088b860233340,pull params synchronously for now (TODO: figure out how to express SGD client with async as an actor),https://github.com/ucla-labx/distbelief/commit/3e93a2b401cf098e33c88a44a4c088b860233340,Yes
920,metabrainz/acousticbrainz-server,hl_extractor/job_calc.py,73ef8335fed769486a4030f355cadf2168a543c6,TODO: This has to be a parameter (from profile?),https://github.com/metabrainz/acousticbrainz-server/commit/73ef8335fed769486a4030f355cadf2168a543c6,Yes
921,HewlettPackard/dlcookbook-dlbs,python/dlbs/utils.py,c0b54fb489b018b184dc718e07fc6eac5e1538da,TODO what about parameter type and description?,https://github.com/HewlettPackard/dlcookbook-dlbs/commit/c0b54fb489b018b184dc718e07fc6eac5e1538da,Yes
922,alan-turing-institute/sktime,heavy.py,dc28cc62fc11b04dcc7b4225e27ad814800be55c,TODO: (discuss) I added an extra score param,https://github.com/alan-turing-institute/sktime/commit/dc28cc62fc11b04dcc7b4225e27ad814800be55c,Yes
923,alan-turing-institute/sktime,sktime/highlevel.py,9ac844172c5e02495b98d56dcdbe2b1f6b938c98,TODO replace whole pandas datacontainer with separated metadata container,https://github.com/alan-turing-institute/sktime/commit/9ac844172c5e02495b98d56dcdbe2b1f6b938c98,Yes
924,alan-turing-institute/sktime,sktime/classifiers/proximity_forest/tree.py,bd7ba7ba4e4d11524b34f9d31989a8e7fdfa9049,list of param dicts todo split into two methods,https://github.com/alan-turing-institute/sktime/commit/bd7ba7ba4e4d11524b34f9d31989a8e7fdfa9049,Yes
925,alan-turing-institute/sktime,sktime/highlevel.py,8b4bfcb5fb774d68b158e17424d3865b0e52cb6b,TODO replace whole pandas datacontainer with separated metadata container,https://github.com/alan-turing-institute/sktime/commit/8b4bfcb5fb774d68b158e17424d3865b0e52cb6b,Yes
926,alan-turing-institute/sktime,sktime/highlevel.py,02efab521f3d4721ad6edafd0503fd5bf6f076cc,TODO replace whole pandas datacontainer with separated metadata container,https://github.com/alan-turing-institute/sktime/commit/02efab521f3d4721ad6edafd0503fd5bf6f076cc,Yes
927,alan-turing-institute/sktime,sktime/highlevel.py,058977085dc47027618bb056b9022f329f02622a,TODO replace whole pandas datacontainer with separated metadata container,https://github.com/alan-turing-institute/sktime/commit/058977085dc47027618bb056b9022f329f02622a,Yes
928,alan-turing-institute/sktime,sktime/highlevel.py,d9b9955d6131e94b3fa0eb1250f7adf1fca38756,TODO replace whole pandas datacontainer with separated metadata container,https://github.com/alan-turing-institute/sktime/commit/d9b9955d6131e94b3fa0eb1250f7adf1fca38756,Yes
929,alan-turing-institute/sktime,sktime/forecasting/forecasters.py,3fe27a014d73d6e3ca0a28bc23d1c716f519c6dd,TODO: experimental; major functionality not implemented (input checks; params interface; exogenous variables),https://github.com/alan-turing-institute/sktime/commit/3fe27a014d73d6e3ca0a28bc23d1c716f519c6dd,Yes
930,alan-turing-institute/sktime,sktime/forecasting/forecasters.py,3fe27a014d73d6e3ca0a28bc23d1c716f519c6dd,TODO fix get and set params interface following sklearn double underscore convention,https://github.com/alan-turing-institute/sktime/commit/3fe27a014d73d6e3ca0a28bc23d1c716f519c6dd,Yes
931,alan-turing-institute/sktime,sktime/highlevel.py,3fe27a014d73d6e3ca0a28bc23d1c716f519c6dd,TODO replace whole pandas data container as input argument with separated metadata container,https://github.com/alan-turing-institute/sktime/commit/3fe27a014d73d6e3ca0a28bc23d1c716f519c6dd,Yes
932,alan-turing-institute/sktime,sktime/forecasting/forecasters.py,a71e640e08e02b75996805a9ef7bbd61fd2fc58d,TODO implement set\/get params interface,https://github.com/alan-turing-institute/sktime/commit/a71e640e08e02b75996805a9ef7bbd61fd2fc58d,Yes
933,alan-turing-institute/sktime,sktime/highlevel/tasks.py,7bba5e701ad58bbc3c62d6c703155a8afa2e507b,TODO replace whole pandas data container as input argument with separated metadata container,https://github.com/alan-turing-institute/sktime/commit/7bba5e701ad58bbc3c62d6c703155a8afa2e507b,Yes
934,alan-turing-institute/sktime,sktime/forecasters/compose.py,9658b92814bf6f34beef5f82eefbf218c66bb0a2,TODO: experimental; major functionality not implemented (input checks; params interface; exogenous variables),https://github.com/alan-turing-institute/sktime/commit/9658b92814bf6f34beef5f82eefbf218c66bb0a2,Yes
935,alan-turing-institute/sktime,sktime/forecasters/compose.py,9658b92814bf6f34beef5f82eefbf218c66bb0a2,TODO implement set\/get params interface,https://github.com/alan-turing-institute/sktime/commit/9658b92814bf6f34beef5f82eefbf218c66bb0a2,Yes
936,alan-turing-institute/sktime,sktime/forecasters/compose.py,9658b92814bf6f34beef5f82eefbf218c66bb0a2,TODO fix get and set params interface following sklearn double underscore convention,https://github.com/alan-turing-institute/sktime/commit/9658b92814bf6f34beef5f82eefbf218c66bb0a2,Yes
937,alan-turing-institute/sktime,sktime/classifiers/proximity.py,68a9aac1ab0b7e1db824450aa11914fed7964149,todo get params avoid func pointer - use name,https://github.com/alan-turing-institute/sktime/commit/68a9aac1ab0b7e1db824450aa11914fed7964149,Yes
938,alan-turing-institute/sktime,sktime/classifiers/proximity.py,68a9aac1ab0b7e1db824450aa11914fed7964149,todo set params use func name or func pointer,https://github.com/alan-turing-institute/sktime/commit/68a9aac1ab0b7e1db824450aa11914fed7964149,Yes
939,alan-turing-institute/sktime,sktime/classifiers/proximity.py,80df7efaa34a05db561ce362bd5072b2e45e9fbe,todo get params avoid func pointer - use name,https://github.com/alan-turing-institute/sktime/commit/80df7efaa34a05db561ce362bd5072b2e45e9fbe,Yes
940,alan-turing-institute/sktime,sktime/contrib/distance_based/proximity.py,7f328f3f8e8b11058f0b27875cbf0a7f61742aa0,todo get params avoid func pointer - use name,https://github.com/alan-turing-institute/sktime/commit/7f328f3f8e8b11058f0b27875cbf0a7f61742aa0,Yes
941,alan-turing-institute/sktime,sktime/contrib/distance_based/proximity.py,7f328f3f8e8b11058f0b27875cbf0a7f61742aa0,todo set params use func name or func pointer,https://github.com/alan-turing-institute/sktime/commit/7f328f3f8e8b11058f0b27875cbf0a7f61742aa0,Yes
942,alan-turing-institute/sktime,sktime/classifiers/proximity.py,8bf2f5717fdf58710cf32828060d0721013747c8,todo get params avoid func pointer - use name,https://github.com/alan-turing-institute/sktime/commit/8bf2f5717fdf58710cf32828060d0721013747c8,Yes
943,alan-turing-institute/sktime,sktime/classifiers/proximity.py,8bf2f5717fdf58710cf32828060d0721013747c8,todo set params use func name or func pointer,https://github.com/alan-turing-institute/sktime/commit/8bf2f5717fdf58710cf32828060d0721013747c8,Yes
944,alan-turing-institute/sktime,sktime/contrib/rotation_forest/rotation_forest_reworked.py,ffaee402a91bef4d3d38cd81c2cd2730bc951489,TODO: parallelize,https://github.com/alan-turing-institute/sktime/commit/ffaee402a91bef4d3d38cd81c2cd2730bc951489,Yes
945,alan-turing-institute/sktime,sktime/contrib/rotation_forest/rotation_forest_reworked.py,ffaee402a91bef4d3d38cd81c2cd2730bc951489,TODO parallelize,https://github.com/alan-turing-institute/sktime/commit/ffaee402a91bef4d3d38cd81c2cd2730bc951489,Yes
946,alan-turing-institute/sktime,sktime/contrib/rotation_forest/rotation_forest_reworked.py,f46fd2549713f3eb2847d0453b48f532f4f4d910,TODO: parallelize,https://github.com/alan-turing-institute/sktime/commit/f46fd2549713f3eb2847d0453b48f532f4f4d910,Yes
947,alan-turing-institute/sktime,sktime/contrib/rotation_forest/rotation_forest_reworked.py,0e788486ebb34574d758792778cb04cc9d6dbb9a,TODO: parallelize,https://github.com/alan-turing-institute/sktime/commit/0e788486ebb34574d758792778cb04cc9d6dbb9a,Yes
948,poldracklab/mriqc,mriqc/workflows/functional.py,0ed38c1335992dafb82f88751e9e9dfd228918a6,TODO: handle this parameter,https://github.com/poldracklab/mriqc/commit/0ed38c1335992dafb82f88751e9e9dfd228918a6,Yes
949,KhronosGroup/NNEF-Tools,nnef_tools/io/onnx/onnx_io.py,45b9bf8aa646b75d1fbfda9f29ea6598432f5177,TODO dim_param,https://github.com/KhronosGroup/NNEF-Tools/commit/45b9bf8aa646b75d1fbfda9f29ea6598432f5177,Yes
950,KhronosGroup/NNEF-Tools,nnef_tools/io/input_source.py,5c928df05994ac3894512b5428513709bc72ad6d,TODO separate the creators by type,https://github.com/KhronosGroup/NNEF-Tools/commit/5c928df05994ac3894512b5428513709bc72ad6d,Yes
951,KhronosGroup/NNEF-Tools,nnef_tools/io/onnx/reader.py,4cccc50ab57c7b85034241c8948caaabc1de5f41,TODO dim_param,https://github.com/KhronosGroup/NNEF-Tools/commit/4cccc50ab57c7b85034241c8948caaabc1de5f41,Yes
952,CogComp/cogcomp-nlpy,ccg_nlpy/server/model_wrapper_server_local_pipeline.py,d3c89558bcb8908351e994d4e3bfaafedb95098e,TODO This is a problem with ccg_nlpy text annotation; it does not like newlines (e.g.; marking paragraphs),https://github.com/CogComp/cogcomp-nlpy/commit/d3c89558bcb8908351e994d4e3bfaafedb95098e,Yes
953,CogComp/cogcomp-nlpy,ccg_nlpy/server/model_wrapper_server_remote_pipeline.py,d3c89558bcb8908351e994d4e3bfaafedb95098e,TODO This is a problem with ccg_nlpy text annotation; it does not like newlines (e.g.; marking paragraphs),https://github.com/CogComp/cogcomp-nlpy/commit/d3c89558bcb8908351e994d4e3bfaafedb95098e,Yes
954,CogComp/cogcomp-nlpy,ccg_nlpy/server/multi_model_wrapper_server_local_pipeline.py,d3c89558bcb8908351e994d4e3bfaafedb95098e,TODO This is a problem with ccg_nlpy text annotation; it does not like newlines (e.g.; marking paragraphs),https://github.com/CogComp/cogcomp-nlpy/commit/d3c89558bcb8908351e994d4e3bfaafedb95098e,Yes
955,IntelAI/models,models/image_recognition/tensorflow/squeezenet/fp32/deployment/model_deploy.py,5f0f82d5b1303d17522789a0132c7aa222348420,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/IntelAI/models/commit/5f0f82d5b1303d17522789a0132c7aa222348420,Yes
956,spotty-cloud/spotty,spotty/providers/aws/config/validation.py,49c21eb09f843be96e3dac3306893f042ef3f5df,"TODO: add the \""iops\"" parameter to support the \""io1\"" EBS volume type",https://github.com/spotty-cloud/spotty/commit/49c21eb09f843be96e3dac3306893f042ef3f5df,Yes
957,spotty-cloud/spotty,spotty/config/validation.py,55c464d4e3e394fe9a71663887d3d62254afbb19,TODO: allow to use only certain runtime parameters,https://github.com/spotty-cloud/spotty/commit/55c464d4e3e394fe9a71663887d3d62254afbb19,Yes
958,mondejar/ecg-classification,dnn_mitdb.py,d3d1c177251e749eedb449ac21a2681c67dd6778,TODO modify the optimization parameter it must depends on accuracy taking the average accuracy,https://github.com/mondejar/ecg-classification/commit/d3d1c177251e749eedb449ac21a2681c67dd6778,Yes
959,mondejar/ecg-classification,my_dnn_mitdb.py,8ceb7d449a146e9b66db9e35b6975f612377a74d,TODO modify the optimization parameter it must depends on accuracy taking the average accuracy,https://github.com/mondejar/ecg-classification/commit/8ceb7d449a146e9b66db9e35b6975f612377a74d,Yes
960,mondejar/ecg-classification,python/train_SVM.py,938aff733db5165d8d1f88379b88449cf0537cbe,TODO load best params from cross validation!,https://github.com/mondejar/ecg-classification/commit/938aff733db5165d8d1f88379b88449cf0537cbe,Yes
961,tobegit3hub/ml_implementation,decision_tree/decision_tree_c45.py,51dbcb0ba2248df4a2fbec84faaacaebada7bac3,TODO: don't modify the input parameter,https://github.com/tobegit3hub/ml_implementation/commit/51dbcb0ba2248df4a2fbec84faaacaebada7bac3,Yes
962,tobegit3hub/ml_implementation,decision_tree/decision_tree_id3.py,f66f4581146d341a888eea8c6dca091f225241a3,TODO: don't modify the input parameter,https://github.com/tobegit3hub/ml_implementation/commit/f66f4581146d341a888eea8c6dca091f225241a3,Yes
963,loli/medpy,src/medpy/graphcut/energy_voxel.py,77263060b08102c1ac0e13b07357e9ed6c6ec172,!TODO: Implement a test for the voxel based graph cut. The expected cut is to separate the left 3\/5 of the array,https://github.com/loli/medpy/commit/77263060b08102c1ac0e13b07357e9ed6c6ec172,Yes
964,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/environment.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Allow providing separate train_input; train_target dataframes; or the full df,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
965,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/environment.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Allow providing separate holdout_input; holdout_target dataframes; or the full df,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
966,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/environment.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Allow providing separate test_input; test_target dataframes; or the full df,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
967,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/feature_engineering.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Expand method to include other scaling types by sending string param or callable for apply_scale arg,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
968,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,f20c0f0542911575e6eac34dc1106638c21a6a84,"TODO: Add default acceptable selections\/ranges for algorithms' hyperparameters and add \""default\"" kwargs to \""add\"" methods",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
969,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Check :attr:`module_name`'s library_helper for :attr:`model_initializer` for a default `hyperparameter` list,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
970,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,f20c0f0542911575e6eac34dc1106638c21a6a84,# TODO: Supply callables declaring valid\/invalid relationships between hyperparameters to filter the total choices,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
971,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,a7a9e62101defb39f2d0ea36a8abc9621f53f97f,TODO: :attr:`current_hyperparameters_list` only exists in Informed Protocols,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a7a9e62101defb39f2d0ea36a8abc9621f53f97f,Yes
972,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/recorders.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: Bug when recording `compile_params['optimizer_params']['lr']` when decay\/scheduling used,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
973,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/utils/optimization_utils.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: Bug when matching `guidelines['compile_params']['optimizer_params']` while `optimizer` is a choice,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
974,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/utils/optimization_utils.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: If `optimizer=Categorical(['adam'; 'rmsprop'])`; the dummy `optimizer_params` will be the defaults for 'adam'; ...,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
975,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/utils/optimization_utils.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: because the default `optimizer_params` are different. Need way to all guidelines to be one of a selection of ...,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
976,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/utils/optimization_utils.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: And this problem is currently limited to `optimizer`-`optimizer_params`; so there might not be a general solution,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
977,HunterMcGushion/hyperparameter_hunter,tests/keras_optimization_helper_tests.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: Update below to leverage `_expected_params_0`; plus index numbers,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
978,HunterMcGushion/hyperparameter_hunter,tests/keras_optimization_helper_tests.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: Update below to leverage `_expected_params_1`; plus index numbers,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
979,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/environment.py,f990ea40a8b459d1b96fbe5158b399e48e580c04,TODO: Below should be done here; instead of in `optimization_core.BaseOptimizationProtocol._validate_parameters`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f990ea40a8b459d1b96fbe5158b399e48e580c04,Yes
980,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/result_reader.py,482684fb3d61920146df3c75a053657d77088a1b,TODO: Receive `description` from `get_scored_params` and extract whatever value is required by :attr:`sort`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/482684fb3d61920146df3c75a053657d77088a1b,Yes
981,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/tracers.py,7ff8fcb72a57ac89c199f4c7ce11908b53dbd097,TODO: Generalize below to use `traced_parameter_name` and `translated_names`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/7ff8fcb72a57ac89c199f4c7ce11908b53dbd097,Yes
982,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/tracers.py,7ff8fcb72a57ac89c199f4c7ce11908b53dbd097,TODO: Generalize above to use `traced_parameter_name` and `translated_names`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/7ff8fcb72a57ac89c199f4c7ce11908b53dbd097,Yes
983,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,from hyperparameter_hunter.tracers import TranslateTrace  # TODO: Add when tested with `Mirror`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,Yes
984,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,"@TranslateTrace(\""model\""; (\""model_initializer\""; \""model_init_params\""))  # TODO: Add when tested with `Mirror`",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,Yes
985,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,TODO: ... given; (`model_initializer`; `model_init_params`) should not be; and vice versa,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,Yes
986,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,"TODO: `model_initializer`\/`model_init_params` docstring types += \""default=None\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,Yes
987,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/feature_engineering.py,531e1e2f2f9962527518d812efd345cf6cfd9575,# TODO: Expand method to include other scaling types by sending string param or callable for apply_scale arg,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/531e1e2f2f9962527518d812efd345cf6cfd9575,Yes
988,HunterMcGushion/hyperparameter_hunter,tests/test_experiments.py,46661c62b492abca80c02c1f20213647fbeeba0f,(dummy_folds_0; dummy_cv_params_0; input_data_fixture; target_data_fixture; exp_cv_0);  # TODO: Upgrade to this; since all datasets should yield same indices (except stratified),https://github.com/HunterMcGushion/hyperparameter_hunter/commit/46661c62b492abca80c02c1f20213647fbeeba0f,Yes
989,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/metrics.py,158c94b9f4f2776d9f875e2326a8eb1aa88b564b,"TODO: Above will cause problems if `Environment.metrics_params['oof']` is not \""all\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/158c94b9f4f2776d9f875e2326a8eb1aa88b564b,Yes
990,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/data/data_core.py,aaea5bb6c0452f694e9da3206836d0152b490bb9,"\""\""\""This module defines mechanisms for managing an experiment's various datasets; and each datasets's || inputs; targets; and predictions. ||  || **Important Contents** ||  || In order to maintain the states of different datasets across all divisions of an experiment and || amid transformations that may be applied to the data via || :mod:`~hyperparameter_hunter.feature_engineering`; two main classes are defined herein: ||  || 1. :class:`BaseDataChunk`: ||  ||     * Logical separations between \""columns\"" of data for a given :class:`BaseDataset` ||     * Held and maintained by :class:`BaseDataset` and its descendants ||     * Three primary descendants of :class:`BaseDataChunk`: ||  ||         1. :class:`InputChunk`: Maintains a dataset's input data (and transformations) ||         2. :class:`TargetChunk`: Maintains a dataset's target data (and transformations) ||         3. :class:`PredictionChunk`: Maintains a dataset's predictions (and transformations) ||  ||     * Descendants of :class:`BaseDataChunk` should implement the eight \""on_<division>_<point>\"" ||       callback methods defined by :class:`~hyperparameter_hunter.callbacks.bases.BaseCallback` ||  ||         * Because :class:`BaseDataChunk` subclasses are isolated from the experiment; these methods ||           need not invoke their `super` methods; although they are allowed to if necessary ||  ||     * :class:`NullDataChunk` does nothing but mimic the normal :class:`BaseDataChunk` child structure ||  ||         * Used for :class:`BaseDataset` subclasses lacking a particular data chunk; such as: ||  ||             1) `TestDataset`'s `TargetChunk`; because the targets for a test dataset are unknown; or ||             2) `TrainDataset`'s `PredictionChunk`; because predictions are not made on training data ||  || 2. :class:`BaseDataset`: ||  ||     # TODO: ... ||  || **Dataset Attribute Syntax** ||  || The intricate subclass network bolstering the module's predominant :class:`BaseDataset` subclasses || may be intimidating at first; but don't worry; there's a shortcut. Follow these steps to ensure || proper syntax and a valid result when accessing data from a || :class:`~hyperparameter_hunter.experiments.CVExperiment`: ||  || 1. {`data_train`; `data_oof`; `data_holdout`; `data_test`} - Dataset attribute || 2. {`input`; `target`; `prediction`} - Data chunk || 3. [`T`] - Optional transformation || 4. {`d`; `run`; `fold`; `rep`; `final`} - Division; initial (`d`) or `final` data ||  || By stacking three values (four if following optional step \""3\"") from the above formula; you can || access all of the interesting stuff stored in the datasets from the comfort of your experiment or || :func:`~hyperparameter_hunter.callbacks.bases.lambda_callback`. ||  || Related || ------- || :mod:`hyperparameter_hunter.callbacks.bases` ||     This module defines the core callback method structure mirrored by :class:`BaseDataCore`. ||     Despite the strong logical connection to this module; it is important to remember that the only ||     actual connection between the two modules is in :mod:`hyperparameter_hunter.callbacks.wranglers` || :mod:`hyperparameter_hunter.callbacks.wranglers` ||     # TODO: ... Handlers for the `Dataset`s to invoke callback methods with required parameters ||     This module defines the callback classes that act as handlers for the descendants of ||     :class:`BaseDataset` || :mod:`hyperparameter_hunter.experiments` ||     # TODO: ... || \""\""\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/aaea5bb6c0452f694e9da3206836d0152b490bb9,Yes
991,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/utils/version_utils.py,1df5b957777be40f2fc2d7a370581fd3687d69ea,TODO: Add `parameter: str` kwarg; which; if given; signals that input parameter to the decorated,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/1df5b957777be40f2fc2d7a370581fd3687d69ea,Yes
992,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization/backends/skopt/engine.py,4561dd50c7c92f8a7f664515b48eec97a7e5c037,TODO: Clean up and separate below. Pretty hard to follow the whole thing,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/4561dd50c7c92f8a7f664515b48eec97a7e5c037,Yes
993,HunterMcGushion/hyperparameter_hunter,tests/test_optimization/test_backends/test_skopt/test_engine.py,4561dd50c7c92f8a7f664515b48eec97a7e5c037,TODO: Refactor - Split into separate error test,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/4561dd50c7c92f8a7f664515b48eec97a7e5c037,Yes
994,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/environment.py,df50741a02aa5f68ad59e82e5bbba6ac1bc3ba12,TODO: Make `cv_params` optional. If not given; use default `cv_type` parameters. Test,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/df50741a02aa5f68ad59e82e5bbba6ac1bc3ba12,Yes
995,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/environment.py,f5bfc7d3e87731ac95d12f3828c233e1dd3744e4,TODO: Clean up this logic duplicated by `update_custom_environment_params`' setting of,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f5bfc7d3e87731ac95d12f3828c233e1dd3744e4,Yes
996,vi3k6i5/GuidedLDA,guidedlda/guidedlda.py,cf0b68234ce9333740bd967e8e10135f309dcfd9,TODO: this loop is parallelizable,https://github.com/vi3k6i5/GuidedLDA/commit/cf0b68234ce9333740bd967e8e10135f309dcfd9,Yes
997,rafellerc/Pytorch-SiamFC,tracking/tracking_graph.py,34b7652607804ea2854a6beb147769c7cec562fd,TODO give the user the choice of the number of scales (use parameters),https://github.com/rafellerc/Pytorch-SiamFC/commit/34b7652607804ea2854a6beb147769c7cec562fd,Yes
998,edublancas/sklearn-evaluation,doc/sphinxext/ipython_sphinxext/ipython_directive.py,74238f95ebe8fbaa62becadef967c6a0ee7989e7,"\""\""\"" || Sphinx directive to support embedded IPython code. ||  || This directive allows pasting of entire interactive IPython sessions; prompts || and all; and their code will actually get re-executed at doc build time; with || all prompts renumbered sequentially. It also allows you to input code as a pure || python input by giving the argument python to the directive. The output looks || like an interactive ipython section. ||  || To enable this directive; simply list it in your Sphinx ``conf.py`` file || (making sure the directory where you placed it is visible to sphinx; as is || needed for all Sphinx directives). For example; to enable syntax highlighting || and the IPython directive:: ||  ||     extensions = ['IPython.sphinxext.ipython_console_highlighting'; ||                   'IPython.sphinxext.ipython_directive'] ||  || The IPython directive outputs code-blocks with the language 'ipython'. So || if you do not have the syntax highlighting extension enabled as well; then || all rendered code-blocks will be uncolored. By default this directive assumes || that your prompts are unchanged IPython ones; but this can be customized. || The configurable options that can be placed in conf.py are: ||  || ipython_savefig_dir: ||     The directory in which to save the figures. This is relative to the ||     Sphinx source directory. The default is `html_static_path`. || ipython_rgxin: ||     The compiled regular expression to denote the start of IPython input ||     lines. The default is re.compile('In \\[(\\d+)\\]:\\s?(.*)\\s*'). You ||     shouldn't need to change this. || ipython_rgxout: ||     The compiled regular expression to denote the start of IPython output ||     lines. The default is re.compile('Out\\[(\\d+)\\]:\\s?(.*)\\s*'). You ||     shouldn't need to change this. || ipython_promptin: ||     The string to represent the IPython input prompt in the generated ReST. ||     The default is 'In [%d]:'. This expects that the line numbers are used ||     in the prompt. || ipython_promptout: ||     The string to represent the IPython prompt in the generated ReST. The ||     default is 'Out [%d]:'. This expects that the line numbers are used ||     in the prompt. || ipython_mplbackend: ||     The string which specifies if the embedded Sphinx shell should import ||     Matplotlib and set the backend. The value specifies a backend that is ||     passed to `matplotlib.use()` before any lines in `ipython_execlines` are ||     executed. If not specified in conf.py; then the default value of 'agg' is ||     used. To use the IPython directive without matplotlib as a dependency; set ||     the value to `None`. It may end up that matplotlib is still imported ||     if the user specifies so in `ipython_execlines` or makes use of the ||     @savefig pseudo decorator. || ipython_execlines: ||     A list of strings to be exec'd in the embedded Sphinx shell. Typical ||     usage is to make certain packages always available. Set this to an empty ||     list if you wish to have no imports always available. If specified in ||     conf.py as `None`; then it has the effect of making no imports available. ||     If omitted from conf.py altogether; then the default value of ||     ['import numpy as np'; 'import matplotlib.pyplot as plt'] is used. || ipython_holdcount ||     When the @suppress pseudo-decorator is used; the execution count can be ||     incremented or not. The default behavior is to hold the execution count; ||     corresponding to a value of `True`. Set this to `False` to increment ||     the execution count after each suppressed command. ||  || As an example; to use the IPython directive when `matplotlib` is not available; || one sets the backend to `None`:: ||  ||     ipython_mplbackend = None ||  || An example usage of the directive is: ||  || .. code-block:: rst ||  ||     .. ipython:: ||  ||         In [1]: x = 1 ||  ||         In [2]: y = x**2 ||  ||         In [3]: print(y) ||  || See http:\/\/matplotlib.org\/sampledoc\/ipython_directive.html for additional || documentation. ||  || Pseudo-Decorators || ================= ||  || Note: Only one decorator is supported per input. If more than one decorator || is specified; then only the last one is used. ||  || In addition to the Pseudo-Decorators\/options described at the above link; || several enhancements have been made. The directive will emit a message to the || console at build-time if code-execution resulted in an exception or warning. || You can suppress these on a per-block basis by specifying the :okexcept: || or :okwarning: options: ||  || .. code-block:: rst ||  ||     .. ipython:: ||         :okexcept: ||         :okwarning: ||  ||         In [1]: 1\/0 ||         In [2]: # raise warning. ||  || ToDo || ---- ||  || - Turn the ad-hoc test() function into a real test suite. || - Break up ipython-specific functionality from matplotlib stuff into better ||   separated code. ||  || Authors || ------- ||  || - John D Hunter: orignal author. || - Fernando Perez: refactoring; documentation; cleanups; port to 0.11. || - V\u00E1clav\u0160milauer <eudoxos-AT-arcig.cz>: Prompt generalizations. || - Skipper Seabold; refactoring; cleanups; pure python addition || \""\""\""",https://github.com/edublancas/sklearn-evaluation/commit/74238f95ebe8fbaa62becadef967c6a0ee7989e7,Yes
999,Bartzi/see,evaluation/evaluator.py,91657b60027df6a076022a39e63d1dcc788fff99,TODO Parallelize,https://github.com/Bartzi/see/commit/91657b60027df6a076022a39e63d1dcc788fff99,Yes
1000,Bartzi/see,metrics/ctc_metrics.py,91657b60027df6a076022a39e63d1dcc788fff99,TODO Parallelize,https://github.com/Bartzi/see/commit/91657b60027df6a076022a39e63d1dcc788fff99,Yes
1001,Bartzi/see,metrics/loss_metrics.py,91657b60027df6a076022a39e63d1dcc788fff99,TODO Parallelize,https://github.com/Bartzi/see/commit/91657b60027df6a076022a39e63d1dcc788fff99,Yes
1002,Bartzi/see,metrics/svhn_ctc_metrics.py,91657b60027df6a076022a39e63d1dcc788fff99,TODO Parallelize,https://github.com/Bartzi/see/commit/91657b60027df6a076022a39e63d1dcc788fff99,Yes
1003,Bartzi/see,metrics/textrec_metrics.py,91657b60027df6a076022a39e63d1dcc788fff99,TODO Parallelize,https://github.com/Bartzi/see/commit/91657b60027df6a076022a39e63d1dcc788fff99,Yes
1004,ShuangLI59/person_search,caffe-fast-rcnn/tools/extra/summarize.py,52350f294541ceee9cb5b3c04ab728a7babf0bed,TODO support rectangular\/ND parameters,https://github.com/ShuangLI59/person_search/commit/52350f294541ceee9cb5b3c04ab728a7babf0bed,Yes
1005,xvjiarui/GCNet,mmdet/models/losses/ghm_loss.py,32df98e970c8848d3d9fd72492aba8bc030377d8,TODO: support reduction parameter,https://github.com/xvjiarui/GCNet/commit/32df98e970c8848d3d9fd72492aba8bc030377d8,Yes
1006,astorfi/3D-convolutional-speaker-recognition,1-development/deployment/model_deploy.py,af8b21a2ac3a04cbe60a749846a63ce1584112b8,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/astorfi/3D-convolutional-speaker-recognition/commit/af8b21a2ac3a04cbe60a749846a63ce1584112b8,Yes
1007,astorfi/3D-convolutional-speaker-recognition,2-enrollment/deployment/model_deploy.py,af8b21a2ac3a04cbe60a749846a63ce1584112b8,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/astorfi/3D-convolutional-speaker-recognition/commit/af8b21a2ac3a04cbe60a749846a63ce1584112b8,Yes
1008,astorfi/3D-convolutional-speaker-recognition,3-evaluation/deployment/model_deploy.py,af8b21a2ac3a04cbe60a749846a63ce1584112b8,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/astorfi/3D-convolutional-speaker-recognition/commit/af8b21a2ac3a04cbe60a749846a63ce1584112b8,Yes
1009,deepmedic/deepmedic,deepmedic/frontEnd/configParsing/testSessionParams.py,a84f61744c77e8310dddab69f473211ffd0ced5f,TODO: Merge with same in trainSessionParams,https://github.com/deepmedic/deepmedic/commit/a84f61744c77e8310dddab69f473211ffd0ced5f,Yes
1010,deepmedic/deepmedic,deepmedic/frontEnd/configParsing/trainSessionParams.py,a84f61744c77e8310dddab69f473211ffd0ced5f,TODO: Merge with same in testSessionParams,https://github.com/deepmedic/deepmedic/commit/a84f61744c77e8310dddab69f473211ffd0ced5f,Yes
1011,deepmedic/deepmedic,deepmedic/frontEnd/configParsing/modelParams.py,a60fb9a6b76e7f0a462fc0924024c707bae9dba5,The below rec_field is ONLY for checking correctness of the passed parameters. TODO: Remove,https://github.com/deepmedic/deepmedic/commit/a60fb9a6b76e7f0a462fc0924024c707bae9dba5,Yes
1012,DeepLabCut/DeepLabCut,deeplabcut/pose_estimation_tensorflow/predict_videos.py,e5c3384897b39e3e33ca440ffef9a7830bb02a70,TODO: check if metadata are identical (same parameters!),https://github.com/DeepLabCut/DeepLabCut/commit/e5c3384897b39e3e33ca440ffef9a7830bb02a70,Yes
1013,DeepLabCut/DeepLabCut,deeplabcut/pose_estimation_tensorflow/predict_videos.py,e5c3384897b39e3e33ca440ffef9a7830bb02a70,TODO: get cropping parameters and utilize!,https://github.com/DeepLabCut/DeepLabCut/commit/e5c3384897b39e3e33ca440ffef9a7830bb02a70,Yes
1014,DeepLabCut/DeepLabCut,deeplabcut/refine_training_dataset/stitch.py,dd8682c48eff68808b4618ff1ca027d380651e2b,TODO Split into separate tracklets if inds are not continuous,https://github.com/DeepLabCut/DeepLabCut/commit/dd8682c48eff68808b4618ff1ca027d380651e2b,Yes
1015,DeepLabCut/DeepLabCut,deeplabcut/pose_estimation_tensorflow/predict_videos.py,9c3d9ba05502b5f553bc8abc44d08e09e1851e68,TODO: get cropping parameters and utilize!,https://github.com/DeepLabCut/DeepLabCut/commit/9c3d9ba05502b5f553bc8abc44d08e09e1851e68,Yes
1016,r9y9/deepvoice3_pytorch,hparams.py,3c3099ff02ba13937fd3e8b1949c3d546c856556,TODO: add more configurable hparams,https://github.com/r9y9/deepvoice3_pytorch/commit/3c3099ff02ba13937fd3e8b1949c3d546c856556,Yes
1017,shenweichen/DeepCTR,tf_model/deepfm.py,15096ea01f1f1e584b574a6dd36058745c595cb6,TODO:  self.init_std\/ math.sqrt(float(dim))  #self.params['feature_dim'],https://github.com/shenweichen/DeepCTR/commit/15096ea01f1f1e584b574a6dd36058745c595cb6,Yes
1018,amdegroot/ssd.pytorch,train.py,c9c5b8da3654bc049b3b2007ea9b2eee93b0eaa1,TODO change this to original params,https://github.com/amdegroot/ssd.pytorch/commit/c9c5b8da3654bc049b3b2007ea9b2eee93b0eaa1,Yes
1019,amdegroot/ssd.pytorch,train.py,6c47f628426b0330f37edb4b1da08f81b87aec50,TODO change this to original params,https://github.com/amdegroot/ssd.pytorch/commit/6c47f628426b0330f37edb4b1da08f81b87aec50,Yes
1020,AKSHAYUBHAT/DeepVideoAnalytics,repos/object_detection/meta_architectures/ssd_meta_arch.py,19b2103de8a502a6569c6a1dce0d75c5465d66dd,TODO: revisit whether to always use batch size as the number of parallel,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/19b2103de8a502a6569c6a1dce0d75c5465d66dd,Yes
1021,AKSHAYUBHAT/DeepVideoAnalytics,repos/slim/deployment/model_deploy.py,19b2103de8a502a6569c6a1dce0d75c5465d66dd,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/19b2103de8a502a6569c6a1dce0d75c5465d66dd,Yes
1022,bbfamily/abu,abupy/MLBu/ABuMLGrid.py,815d6c3a2584b4e4e0878d3fdb2776976840cb4c,FIXME \u8FD9\u91CC\u5047\u5B9A\u4E86\u6240\u6709param_range\u7684\u5143\u7D20\u7C7B\u578B\u90FD\u662F\u6570\u503C\u7C7B\u578B\uFF0C\u9700\u8981\u5224\u5B9A\uFF0C\u5E76\u4E14\u6839\u636E\u60C5\u51B5\u662F\u5426\u9700\u8981\u6392\u5E8F,https://github.com/bbfamily/abu/commit/815d6c3a2584b4e4e0878d3fdb2776976840cb4c,Yes
1023,lanpa/tensorboardX,tensorboardX/summary.py,dce4fb1181d86b8d667d914defa59fe0886fb02b,TODO: expose other parameters in the future.,https://github.com/lanpa/tensorboardX/commit/dce4fb1181d86b8d667d914defa59fe0886fb02b,Yes
1024,lanpa/tensorboardX,tensorboardX/pytorch_graph.py,23de6f0cd811b9a4e9b3f7db3723f609dea13ced,TODO: When it comes to shared parameter; will it still work?,https://github.com/lanpa/tensorboardX/commit/23de6f0cd811b9a4e9b3f7db3723f609dea13ced,Yes
1025,sloria/TextBlob,nltk-3.0a0/nltk/classify/svm.py,d539a164ed1cac8239b632a778946248a00c0c42,TODO: implement passing of SVMlight parameters from train() to learn(),https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
1026,aleju/imgaug,imgaug/augmenters.py,3c882db1f425462e40300cbbf7c28bb6e13129a3,"\""\""\""Creates an augmenter that pronounces all edges in the image. ||  || Parameters || ---------- || alpha : int; float; tuple of two ints\/floats or StochasticParameter ||     Visibility of the edge map. At 0; only the original image is visible; ||     at 1.0 only the edge map is visible. || name : TODO || deterministic : TODO || random_state : TODO ||  || Example: ||     aug = EdgeDetect(alpha=(0.0; 1.0)) ||     image_aug = aug.augment_image(image) || \""\""\""",https://github.com/aleju/imgaug/commit/3c882db1f425462e40300cbbf7c28bb6e13129a3,Yes
1027,aleju/imgaug,imgaug/augmenters.py,3c882db1f425462e40300cbbf7c28bb6e13129a3,"\""\""\""Creates an augmenter that pronounces edges that have certain directions. ||  || Parameters || ---------- || alpha : int; float; tuple of two ints\/floats or StochasticParameter ||     Visibility of the edge map. At 0; only the original image is visible; ||     at 1.0 only the edge map is visible. || direction : int; float; tuple of two ints\/floats or StochasticParameter; optional ||     Angle of edges to pronounce; where 0 represents 0 degrees and 1.0 ||     represents 360 degrees (both clockwise). ||     Default value is (0.0; 1.0); i.e. pick a random angle per image. || name : TODO || deterministic : TODO || random_state : TODO ||  || Example: ||     aug = DirectedEdgeDetect(alpha=(0.0; 1.0); direction=(0.0; 1.0)) ||     image_aug = aug.augment_image(image) || \""\""\""",https://github.com/aleju/imgaug/commit/3c882db1f425462e40300cbbf7c28bb6e13129a3,Yes
1028,aleju/imgaug,imgaug/augmenters.py,64a64d92131191c603597643bacccbd4497326cb,"\""\""\""Creates an augmenter that pronounces all edges in the image. ||  || Parameters || ---------- || alpha : int; float; tuple of two ints\/floats or StochasticParameter ||     Visibility of the edge map. At 0; only the original image is visible; ||     at 1.0 only the edge map is visible. || name : TODO || deterministic : TODO || random_state : TODO ||  || Example: ||     aug = EdgeDetect(alpha=(0.0; 1.0)) ||     image_aug = aug.augment_image(image) || \""\""\""",https://github.com/aleju/imgaug/commit/64a64d92131191c603597643bacccbd4497326cb,Yes
1029,aleju/imgaug,imgaug/augmenters.py,64a64d92131191c603597643bacccbd4497326cb,"\""\""\""Creates an augmenter that pronounces edges that have certain directions. ||  || Parameters || ---------- || alpha : int; float; tuple of two ints\/floats or StochasticParameter ||     Visibility of the edge map. At 0; only the original image is visible; ||     at 1.0 only the edge map is visible. || direction : int; float; tuple of two ints\/floats or StochasticParameter; optional ||     Angle of edges to pronounce; where 0 represents 0 degrees and 1.0 ||     represents 360 degrees (both clockwise). ||     Default value is (0.0; 1.0); i.e. pick a random angle per image. || name : TODO || deterministic : TODO || random_state : TODO ||  || Example: ||     aug = DirectedEdgeDetect(alpha=(0.0; 1.0); direction=(0.0; 1.0)) ||     image_aug = aug.augment_image(image) || \""\""\""",https://github.com/aleju/imgaug/commit/64a64d92131191c603597643bacccbd4497326cb,Yes
1030,aleju/imgaug,imgaug/augmenters.py,b64cb5a84e6d061b696eca7c54efee57a53e510f,TODO add parameter for handling multiple images ((a) next to each other,https://github.com/aleju/imgaug/commit/b64cb5a84e6d061b696eca7c54efee57a53e510f,Yes
1031,aleju/imgaug,imgaug/augmenters.py,b5c81b2bea8704a00416a0b09a7deb387ed15d01,"TODO \""images\"" parameter deviates from augment_images (3d array is here",https://github.com/aleju/imgaug/commit/b5c81b2bea8704a00416a0b09a7deb387ed15d01,Yes
1032,aleju/imgaug,imgaug/augmenters.py,2898606d13c2197e713d94a81f0ac263a57c3c08,TODO removed deterministic and random_state here as parameters; because this,https://github.com/aleju/imgaug/commit/2898606d13c2197e713d94a81f0ac263a57c3c08,Yes
1033,aleju/imgaug,imgaug/augmenters/color.py,ec46c7e629d9fc233f89f12cc51df00409697bd3,TODO removed deterministic and random_state here as parameters; because this,https://github.com/aleju/imgaug/commit/ec46c7e629d9fc233f89f12cc51df00409697bd3,Yes
1034,aleju/imgaug,imgaug/augmenters/meta.py,ec46c7e629d9fc233f89f12cc51df00409697bd3,TODO add parameter for handling multiple images ((a) next to each other,https://github.com/aleju/imgaug/commit/ec46c7e629d9fc233f89f12cc51df00409697bd3,Yes
1035,aleju/imgaug,imgaug/augmenters/meta.py,ec46c7e629d9fc233f89f12cc51df00409697bd3,"TODO \""images\"" parameter deviates from augment_images (3d array is here",https://github.com/aleju/imgaug/commit/ec46c7e629d9fc233f89f12cc51df00409697bd3,Yes
1036,aleju/imgaug,imgaug/parameters.py,fc9122db09b392671d1937b0cf53ce52b11d51dd,TODO this might fail if the same parameter is added,https://github.com/aleju/imgaug/commit/fc9122db09b392671d1937b0cf53ce52b11d51dd,Yes
1037,aleju/imgaug,imgaug/parameters.py,fc9122db09b392671d1937b0cf53ce52b11d51dd,TODO this will fail if a parameter cant handle size=(N;),https://github.com/aleju/imgaug/commit/fc9122db09b392671d1937b0cf53ce52b11d51dd,Yes
1038,aleju/imgaug,imgaug/augmenters/meta.py,a3e7bec31a1252db3e51657a9ca54effd78fffc7,TODO change this to a stochastic parameter,https://github.com/aleju/imgaug/commit/a3e7bec31a1252db3e51657a9ca54effd78fffc7,Yes
1039,aleju/imgaug,imgaug/imgaug.py,00e8ae4aee6a9da5520798646762e2a24b02188e,TODO separate this into draw_face_on_image() and draw_border_on_image(),https://github.com/aleju/imgaug/commit/00e8ae4aee6a9da5520798646762e2a24b02188e,Yes
1040,aleju/imgaug,imgaug/augmenters/geometric.py,b9ddfd71c2c6d777f51f7e8c2364fc6831d719e5,TODO numpy.rot90() also has an axes parameter. Does it make sense to add that here?,https://github.com/aleju/imgaug/commit/b9ddfd71c2c6d777f51f7e8c2364fc6831d719e5,Yes
1041,aleju/imgaug,imgaug/augmenters/contrast.py,daa3d33b3e6f4c3ea5e25455119d8c4001edc797,TODO add parameter `tile_grid_size_percent`,https://github.com/aleju/imgaug/commit/daa3d33b3e6f4c3ea5e25455119d8c4001edc797,Yes
1042,aleju/imgaug,imgaug/augmenters/contrast.py,c54231a5b036c2f2444f95f33b7097607a97669e,TODO add parameter `tile_grid_size_percent`,https://github.com/aleju/imgaug/commit/c54231a5b036c2f2444f95f33b7097607a97669e,Yes
1043,aleju/imgaug,imgaug/imgaug.py,675b9e4fa381bdca4f74da903d1982e6e60a55e8,TODO convert this to x\/y params?,https://github.com/aleju/imgaug/commit/675b9e4fa381bdca4f74da903d1982e6e60a55e8,Yes
1044,aleju/imgaug,imgaug/augmenters/arithmetic.py,ec140bae4b88e88fbfc2001ec99e9f258da04184,TODO add separate per_channels for mask and replacement,https://github.com/aleju/imgaug/commit/ec140bae4b88e88fbfc2001ec99e9f258da04184,Yes
1045,aleju/imgaug,imgaug/augmentables/bbs.py,82febec2db268d6b237bcf84e517c3f0234a2cc4,TODO convert this to x\/y params?,https://github.com/aleju/imgaug/commit/82febec2db268d6b237bcf84e517c3f0234a2cc4,Yes
1046,aleju/imgaug,imgaug/augmentables/polys.py,3a7cb068e0b17d20747e0d84b1dedd535898255d,TODO separate this into draw_face_on_image() and draw_border_on_image(),https://github.com/aleju/imgaug/commit/3a7cb068e0b17d20747e0d84b1dedd535898255d,Yes
1047,aleju/imgaug,imgaug/augmentables/lines.py,630ba422a0d70fe0392f28f462fdac3f45199439,TODO convert this to x\/y params?,https://github.com/aleju/imgaug/commit/630ba422a0d70fe0392f28f462fdac3f45199439,Yes
1048,aleju/imgaug,imgaug/augmentables/polys.py,dd722158770cb60f820886bcd99dc9edca70a0bb,TODO separate this into draw_face_on_image() and draw_border_on_image(),https://github.com/aleju/imgaug/commit/dd722158770cb60f820886bcd99dc9edca70a0bb,Yes
1049,aleju/imgaug,imgaug/augmenters/pool.py,b9959f2f43a68ffc67057ae24331396592377d8b,TODO rename kernel size parameters in all augmenters to kernel_size,https://github.com/aleju/imgaug/commit/b9959f2f43a68ffc67057ae24331396592377d8b,Yes
1050,aleju/imgaug,imgaug/augmenters/segmentation.py,a90d8881a45ebeef631d8cbcdb62c97c8d3fcc5b,TODO add compactness parameter,https://github.com/aleju/imgaug/commit/a90d8881a45ebeef631d8cbcdb62c97c8d3fcc5b,Yes
1051,aleju/imgaug,imgaug/parameters.py,01f0f03f17c29775498ab9f8e6d283145982f2f6,TODO docstring for parameters is outdated,https://github.com/aleju/imgaug/commit/01f0f03f17c29775498ab9f8e6d283145982f2f6,Yes
1052,aleju/imgaug,test/augmenters/test_size.py,9db57673552fb8d989cb81a2a3472fed8df5b2bf,TODO add test for shorter side being tuple; list; stochastic parameter,https://github.com/aleju/imgaug/commit/9db57673552fb8d989cb81a2a3472fed8df5b2bf,Yes
1053,aleju/imgaug,test/augmenters/test_size.py,9db57673552fb8d989cb81a2a3472fed8df5b2bf,TODO add test for longer side being tuple; list; stochastic parameter,https://github.com/aleju/imgaug/commit/9db57673552fb8d989cb81a2a3472fed8df5b2bf,Yes
1054,aleju/imgaug,imgaug/augmenters/arithmetic.py,c0dfdcc02a847258b404cd95b9b88682c42b695d,TODO maybe introduce to stochastic parameters some way to,https://github.com/aleju/imgaug/commit/c0dfdcc02a847258b404cd95b9b88682c42b695d,Yes
1055,aleju/imgaug,imgaug/augmenters/geometric.py,97210bb14facb542d03bf3520d0e56702cf696d2,TODO use iap.handle_categorical_string_param() here,https://github.com/aleju/imgaug/commit/97210bb14facb542d03bf3520d0e56702cf696d2,Yes
1056,aleju/imgaug,imgaug/augmenters/geometric.py,a6f0cda8c7a4b526e158d51f8e8661b3e7e4401c,TODO enable support for stochastic parameters in,https://github.com/aleju/imgaug/commit/a6f0cda8c7a4b526e158d51f8e8661b3e7e4401c,Yes
1057,aleju/imgaug,imgaug/parameters.py,c0216586743cf3f95b5148f972d94bd3955ddbe4,TODO replace two-value parameters used in tests with this,https://github.com/aleju/imgaug/commit/c0216586743cf3f95b5148f972d94bd3955ddbe4,Yes
1058,aleju/imgaug,imgaug/augmenters/geometric.py,7848fdc1bb95b5d9f51b8481d983d1a7ae3256e9,TODO enable support for stochastic parameters in,https://github.com/aleju/imgaug/commit/7848fdc1bb95b5d9f51b8481d983d1a7ae3256e9,Yes
1059,aleju/imgaug,imgaug/augmentables/bbs.py,ed210e388b8bc715648f480f58db34979db2cea9,TODO convert this to x\/y params?,https://github.com/aleju/imgaug/commit/ed210e388b8bc715648f480f58db34979db2cea9,Yes
1060,aleju/imgaug,imgaug/augmentables/bbs.py,1340df38991c7c5b28a57ae2ec4c0fd1a72b86df,TODO convert this to x\/y params?,https://github.com/aleju/imgaug/commit/1340df38991c7c5b28a57ae2ec4c0fd1a72b86df,Yes
1061,aleju/imgaug,imgaug/augmentables/lines.py,565d6e017f76a38dd823454a63428c17c54f9aef,TODO convert this to x\/y params?,https://github.com/aleju/imgaug/commit/565d6e017f76a38dd823454a63428c17c54f9aef,Yes
1062,deepfakes/faceswap,scripts/convert.py,810bd0bce7e63c3bbcb4e3ba3cd3804287f55795,TODO: This switch between 64 and 128 is a hack for now. We should have a separate cli option for size,https://github.com/deepfakes/faceswap/commit/810bd0bce7e63c3bbcb4e3ba3cd3804287f55795,Yes
1063,deepfakes/faceswap,scripts/convert.py,ee6bc402249b00fc443680dfa5b202492ba6ec91,TODO: This switch between 64 and 128 is a hack for now. We should have a separate cli option for size,https://github.com/deepfakes/faceswap/commit/ee6bc402249b00fc443680dfa5b202492ba6ec91,Yes
1064,mesnico/RelationNetworks-CLEVR,train.py,430d484cd23ecd5c24b45bc638ff5268628a2408,TODO: there may be problems caused by pytorch issue #3805 if using DataParallel,https://github.com/mesnico/RelationNetworks-CLEVR/commit/430d484cd23ecd5c24b45bc638ff5268628a2408,Yes
1065,mesnico/RelationNetworks-CLEVR,train.py,308688e544ed08afce7adbee7df3f1f4f97225bb,TODO: there may be problems caused by pytorch issue #3805 if using DataParallel,https://github.com/mesnico/RelationNetworks-CLEVR/commit/308688e544ed08afce7adbee7df3f1f4f97225bb,Yes
1066,jasonwu0731/trade-dst,models/TRADE.py,bba6c529ec5166fe65aee77fedede4de13c88da4,TODO: Parallel this part to make it train faster,https://github.com/jasonwu0731/trade-dst/commit/bba6c529ec5166fe65aee77fedede4de13c88da4,Yes
1067,nikonikolov/rltf,plot/plotter.py,841f22a8235758fc2834ae3e19f638d327cda50e,TODO: Add functionality for plotting on separate figures,https://github.com/nikonikolov/rltf/commit/841f22a8235758fc2834ae3e19f638d327cda50e,Yes
1068,nikonikolov/rltf,plot/plotter.py,a51bd393889fce84f98e7d3c190abd71a3c85a6e,TODO: Add functionality for plotting on separate figures,https://github.com/nikonikolov/rltf/commit/a51bd393889fce84f98e7d3c190abd71a3c85a6e,Yes
1069,lopuhin/transformer-lm,lm_web_ui/main.py,2b0e0579f09f5d253b0de9a64d3e48886d40f8a2,TODO paragraphs,https://github.com/lopuhin/transformer-lm/commit/2b0e0579f09f5d253b0de9a64d3e48886d40f8a2,Yes
1070,lopuhin/transformer-lm,lm_web_ui/main.py,4013a93d31e67b19180bdee3c65c1ff92f60a435,TODO paragraphs,https://github.com/lopuhin/transformer-lm/commit/4013a93d31e67b19180bdee3c65c1ff92f60a435,Yes
1071,Shun14/TextBoxes_plusplus_Tensorflow,deployment/model_deploy.py,dc5d7b7c5bc9df657bf66122c54685af158fc3a6,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||     g = tf.Graph() ||  ||     # Set up DeploymentConfig ||     config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||     # Create the global step on the device storing the variables. ||     with tf.device(config.variables_device()): ||         global_step = slim.create_global_step() ||  ||     # Define the inputs ||     with tf.device(config.inputs_device()): ||         images; labels = LoadData(...) ||         inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||     # Define the optimizer. ||     with tf.device(config.optimizer_device()): ||         optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||     # Define the model including the loss. ||     def model_fn(inputs_queue): ||         images; labels = inputs_queue.dequeue() ||         predictions = CreateNetwork(images) ||         slim.losses.log_loss(predictions; labels) ||  ||     model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                    optimizer=optimizer) ||  ||     # Run training. ||     slim.learning.train(model_dp.train_op; my_log_dir; ||                                             summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||     * outputs: The return values of the calls to `model_fn()`. ||     * scope: The scope used to create the clone. ||     * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||     * train_op: An operation that run the optimizer training op and include ||         all the update ops created by `model_fn`. Present only if an optimizer ||         was specified. ||     * summary_op: An operation that run the summaries created by `model_fn` ||         and process_gradients. ||     * total_loss: A `Tensor` that contains the sum of all losses created by ||         `model_fn` plus the regularization losses. ||     * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||     * num_clones: Number of model clones to deploy in each replica. ||     * clone_on_cpu: True if clones should be placed on CPU. ||     * replica_id: Integer.  Index of the replica for which the model is ||             deployed.  Usually 0 for the chief replica. ||     * num_replicas: Number of replicas to use. ||     * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||     * worker_job_name: A name for the worker job. ||     * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||     - describe side effect to the graph. ||     - what happens to summaries and update_ops. ||     - which graph collections are altered. ||     - write a tutorial on how to use this. ||     - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/Shun14/TextBoxes_plusplus_Tensorflow/commit/dc5d7b7c5bc9df657bf66122c54685af158fc3a6,Yes
1072,wuhao5688/RNN-TrajModel,trajmodel.py,93533d4a764089c54ad3d5e83809c42e90f6beb3,"fwd_params = [v for v in tf.all_variables() if v.name.startswith(self.model_scope + \""\/\"" + var_scope)]  # TODO",https://github.com/wuhao5688/RNN-TrajModel/commit/93533d4a764089c54ad3d5e83809c42e90f6beb3,Yes
1073,wuhao5688/RNN-TrajModel,trajmodel.py,93533d4a764089c54ad3d5e83809c42e90f6beb3,"params = [v for v in tf.all_variables() if v.name.startswith(self.model_scope + \""\/\"" + var_scope)]  # TODO",https://github.com/wuhao5688/RNN-TrajModel/commit/93533d4a764089c54ad3d5e83809c42e90f6beb3,Yes
1074,mdangschat/ctc-asr,python/model.py,88240146f8df5317ff356cd2b9d00a61c595df00,TODO: If conv layers don't change in parameters from layer to layer; wrap them in a helper.,https://github.com/mdangschat/ctc-asr/commit/88240146f8df5317ff356cd2b9d00a61c595df00,Yes
1075,mdangschat/ctc-asr,python/train.py,2d656b64e946252aaf3b56f7821563c4791fbe00,TODO move to params.py ?,https://github.com/mdangschat/ctc-asr/commit/2d656b64e946252aaf3b56f7821563c4791fbe00,Yes
1076,gabrielspmoreira/chameleon_recsys,nar_module/nar/benchmarks/gru4rec/gru4rec2.py,c2c3a7f7c3421d2fbb3a9d1987cf068ba8e8c10a,TODO: Parametrize Last Clicks buffer size,https://github.com/gabrielspmoreira/chameleon_recsys/commit/c2c3a7f7c3421d2fbb3a9d1987cf068ba8e8c10a,Yes
1077,wenwei202/iss-rnns,attention/main.py,732ffb02b01a01e1030ee8fe70cbb4ffb0006302,TODO: set other parameters; e.g.,https://github.com/wenwei202/iss-rnns/commit/732ffb02b01a01e1030ee8fe70cbb4ffb0006302,Yes
1078,wenwei202/iss-rnns,attention/main.py,732ffb02b01a01e1030ee8fe70cbb4ffb0006302,TODO : Add any other parameter that induces a lot of computations,https://github.com/wenwei202/iss-rnns/commit/732ffb02b01a01e1030ee8fe70cbb4ffb0006302,Yes
1079,wenwei202/iss-rnns,attention/model.py,732ffb02b01a01e1030ee8fe70cbb4ffb0006302,TODO : put more parameters,https://github.com/wenwei202/iss-rnns/commit/732ffb02b01a01e1030ee8fe70cbb4ffb0006302,Yes
1080,wenwei202/iss-rnns,model/model.py,bd840ca83e0e0ae7c28c84a3eb1ea3e66008d4cb,TODO : put more parameters,https://github.com/wenwei202/iss-rnns/commit/bd840ca83e0e0ae7c28c84a3eb1ea3e66008d4cb,Yes
1081,wenwei202/iss-rnns,cnn/main.py,89739b12e5e7ba27f54575fbc76c8982d4a1a60b,TODO: set other parameters; e.g.,https://github.com/wenwei202/iss-rnns/commit/89739b12e5e7ba27f54575fbc76c8982d4a1a60b,Yes
1082,wenwei202/iss-rnns,cnn/main.py,89739b12e5e7ba27f54575fbc76c8982d4a1a60b,TODO : Add any other parameter that induces a lot of computations,https://github.com/wenwei202/iss-rnns/commit/89739b12e5e7ba27f54575fbc76c8982d4a1a60b,Yes
1083,wenwei202/iss-rnns,cnn/model.py,89739b12e5e7ba27f54575fbc76c8982d4a1a60b,TODO : put more parameters,https://github.com/wenwei202/iss-rnns/commit/89739b12e5e7ba27f54575fbc76c8982d4a1a60b,Yes
1084,dsindex/etagger,inference/export.py,7e9bf4104ba5e4a31f922cb0f401d755bb951108,FIXME can't be referred by 'trans_params:0'. why?,https://github.com/dsindex/etagger/commit/7e9bf4104ba5e4a31f922cb0f401d755bb951108,Yes
1085,ComputationalCryoEM/ASPIRE-Python,src/aspire/source/__init__.py,6bffc4e2e1bd68eb4fc0db27c0f50c6318f5c148,TODO: Support optional start\/num parameters,https://github.com/ComputationalCryoEM/ASPIRE-Python/commit/6bffc4e2e1bd68eb4fc0db27c0f50c6318f5c148,Yes
1086,skyportal/skyportal,skyportal/utils/offset_util.py,be798b6c1794b4896acf21b8e8aa8424e6a91bc2,TODO: put this in geocentric coords to account for parallax,https://github.com/skyportal/skyportal/commit/be798b6c1794b4896acf21b8e8aa8424e6a91bc2,Yes
1087,skyportal/skyportal,skyportal/utils/offset.py,97d983a4f3deb9c672e99d978b54f41bb202d48a,TODO: put this in geocentric coords to account for parallax,https://github.com/skyportal/skyportal/commit/97d983a4f3deb9c672e99d978b54f41bb202d48a,Yes
1088,tasoc/starclass,starclass/classifiers/selfsom.py,01b42007be3cb844bbc04123d73dbf47cf37a538,TODO expose distance function as parameter,https://github.com/tasoc/starclass/commit/01b42007be3cb844bbc04123d73dbf47cf37a538,Yes
1089,tasoc/starclass,run_training.py,fd0d9ba0035e75aa1e483a7018794b8fbb20662d,TODO: Run in paralllel,https://github.com/tasoc/starclass/commit/fd0d9ba0035e75aa1e483a7018794b8fbb20662d,Yes
1090,tasoc/starclass,run_training.py,695cb85e29b5aa21b1045071bd42407eebf90ae7,TODO: Run in paralllel?,https://github.com/tasoc/starclass/commit/695cb85e29b5aa21b1045071bd42407eebf90ae7,Yes
1091,Acellera/moleculekit,moleculekit/molecule.py,41c1ef369d95d2a9574937af97bb5e35cd00ebc3,TODO: Should pass all parameters here!!!,https://github.com/Acellera/moleculekit/commit/41c1ef369d95d2a9574937af97bb5e35cd00ebc3,Yes
1092,Acellera/moleculekit,moleculekit/molecule.py,41c1ef369d95d2a9574937af97bb5e35cd00ebc3,TODO: Make a PR where fileloc becomes (2; nframes) numpy array so we don't handle it separately,https://github.com/Acellera/moleculekit/commit/41c1ef369d95d2a9574937af97bb5e35cd00ebc3,Yes
1093,Acellera/moleculekit,moleculekit/molecule.py,41c1ef369d95d2a9574937af97bb5e35cd00ebc3,TODO do not use parameterize data,https://github.com/Acellera/moleculekit/commit/41c1ef369d95d2a9574937af97bb5e35cd00ebc3,Yes
1094,astrazeneca-cgr-publications/mantis-ml-release,mantis_ml/bin/__main__.py,35cea49df32b536b266e668182a7ac49dc64ad41,TODO: remove boruta from all -- allow it as a separate step,https://github.com/astrazeneca-cgr-publications/mantis-ml-release/commit/35cea49df32b536b266e668182a7ac49dc64ad41,Yes
1095,BorgwardtLab/P-WL,baseline.py,244db90c190f358ab52d5401b9d6ba57d948d1b4,TODO: shamelessly copied from `main.py`; should become a separate,https://github.com/BorgwardtLab/P-WL/commit/244db90c190f358ab52d5401b9d6ba57d948d1b4,Yes
1096,Roboy/tss18-robotsinmusicalimprovisation,VAE/VAEsemane.py,b3b4dcd00cd5369c39a700e9d7dfdedd19cf699f,TODO reparameterize to get new sequences here with GUI??,https://github.com/Roboy/tss18-robotsinmusicalimprovisation/commit/b3b4dcd00cd5369c39a700e9d7dfdedd19cf699f,Yes
1097,Roboy/tss18-robotsinmusicalimprovisation,gui/rimi_gui.py,701dcdfbab07eff356989e84337f93bcf00b414a,TODO reparameterize to get new sequences here with GUI??,https://github.com/Roboy/tss18-robotsinmusicalimprovisation/commit/701dcdfbab07eff356989e84337f93bcf00b414a,Yes
1098,AllenInstitute/mouse_connectivity_models,voxel_model/regressors/nonparametric/nadaraya_watson.py,7586a2446b39d17be3999f2f15357590a5473133,TODO: check _check_param_grid in proper spot,https://github.com/AllenInstitute/mouse_connectivity_models/commit/7586a2446b39d17be3999f2f15357590a5473133,Yes
1099,AllenInstitute/mouse_connectivity_models,mcmodels/core/base.py,c2b92ac74b5d4fe27779d3356f6ed9fa035c1a48,TODO: update to show parameters,https://github.com/AllenInstitute/mouse_connectivity_models/commit/c2b92ac74b5d4fe27779d3356f6ed9fa035c1a48,Yes
1100,spacetelescope/jwql,jwql/instrument_monitors/common_monitors/readnoise_monitor.py,63d793447bb46eb62784d9bf0b844211e1560afb,readnoise_dictionary = crds_tools.get_reffiles(parameters; ['readnoise']; download=True)  # TODO do i need to download this?,https://github.com/spacetelescope/jwql/commit/63d793447bb46eb62784d9bf0b844211e1560afb,Yes
1101,ml-feeds/arxiv-ml-reviews,arxivmlrev/searcher.py,26d9e023dab31ed3f0c15144d8e7e802fbdfa6e4,TODO: Try adding terms: Past; Present and Future; Comparative Study; A Primer on,https://github.com/ml-feeds/arxiv-ml-reviews/commit/26d9e023dab31ed3f0c15144d8e7e802fbdfa6e4,Yes
1102,pandoraboxchain/pyrrha-pynode,pynode/core/broker.py,3749d859963afc674c1a67c752e729deae4bbfe7,todo job address is necessary ADD it to method call parameters,https://github.com/pandoraboxchain/pyrrha-pynode/commit/3749d859963afc674c1a67c752e729deae4bbfe7,Yes
1103,pandoraboxchain/pyrrha-pynode,pynode/core/broker.py,14de531d5524b35f3dfdb9aab5a91743bbe56d11,TODO add parameter for translate restart count,https://github.com/pandoraboxchain/pyrrha-pynode/commit/14de531d5524b35f3dfdb9aab5a91743bbe56d11,Yes
1104,trungnt13/odin-ai,odin/bay/distributions/zero_inflated.py,b5c6abaaf6c69b6ef67d1318f972339a7cd17716,TODO: NotImplementedError: <class 'odin.bay.distributions.zero_inflated.ZeroInflated'> does not support batch slicing; must implement _params_event_ndims.,https://github.com/trungnt13/odin-ai/commit/b5c6abaaf6c69b6ef67d1318f972339a7cd17716,Yes
1105,Neuraxio/Neuraxle,neuraxle/union.py,88cf1d133d11ed8fec0acbe0202d3fefa5ed8003,TODO: parallel.,https://github.com/Neuraxio/Neuraxle/commit/88cf1d133d11ed8fec0acbe0202d3fefa5ed8003,Yes
1106,Neuraxio/Neuraxle,testing/test_pipeline.py,65680a90ee9b8f0cb64b83cfcd16ab3ac198772a,"assert the_step.get_hyperparams()[\""learning_rate\""] == 12  # TODO: debug why wouldn't this work",https://github.com/Neuraxio/Neuraxle/commit/65680a90ee9b8f0cb64b83cfcd16ab3ac198772a,Yes
1107,Neuraxio/Neuraxle,testing/test_pipeline.py,a25d14d03508cb7973ed77cbd144e11b8ff4af8b,"assert the_step.get_hyperparams()[\""learning_rate\""] == 12  # TODO: debug why wouldn't this work",https://github.com/Neuraxio/Neuraxle/commit/a25d14d03508cb7973ed77cbd144e11b8ff4af8b,Yes
1108,Neuraxio/Neuraxle,neuraxle/base.py,0c7f962d7f34c3759778a192ffda10d6f7e09dbd,TODO: how to set_params on contained step?,https://github.com/Neuraxio/Neuraxle/commit/0c7f962d7f34c3759778a192ffda10d6f7e09dbd,Yes
1109,Neuraxio/Neuraxle,neuraxle/steps/util.py,0c7f962d7f34c3759778a192ffda10d6f7e09dbd,TODO: set params on wrapped.,https://github.com/Neuraxio/Neuraxle/commit/0c7f962d7f34c3759778a192ffda10d6f7e09dbd,Yes
1110,Neuraxio/Neuraxle,neuraxle/metaopt/auto_ml.py,88a2d8ec393032b1ed9a5548a546522de17f5099,TODO: make hyperparams space serializable,https://github.com/Neuraxio/Neuraxle/commit/88a2d8ec393032b1ed9a5548a546522de17f5099,Yes
1111,Neuraxio/Neuraxle,neuraxle/metaopt/tpe.py,ddf7dd7511d981e8b87442f6fd0b97cd0ff03317,TODO: Maybe they use the likelyhood to sum over all possible parameters to find the max so it become a join distribution of all hyperparameters; would make sense.,https://github.com/Neuraxio/Neuraxle/commit/ddf7dd7511d981e8b87442f6fd0b97cd0ff03317,Yes
1112,SimGus/Chatette,src/units/words.py,3c86bf1653808ad8df68e74e7eafe8b1693ebece,FIXME: wrong parameters for overriding,https://github.com/SimGus/Chatette/commit/3c86bf1653808ad8df68e74e7eafe8b1693ebece,Yes
1113,snakeztc/NeuralDialog-LaRL,latent_dialog/enc2dec/base_modules.py,7840453b5153c5e1c1289ac6daa7ca01cd36b080,TODO Trick for initializing LSTM gate parameters,https://github.com/snakeztc/NeuralDialog-LaRL/commit/7840453b5153c5e1c1289ac6daa7ca01cd36b080,Yes
1114,araffin/srl-zoo,evaluation/Utils.py,701b04a5a70821b044e0e078b44c4300316ad513,TODO add param for relative path vs just folder names,https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,Yes
1115,araffin/srl-zoo,evaluation/generateNNImages.py,701b04a5a70821b044e0e078b44c4300316ad513,TODO process separate test set,https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,Yes
1116,araffin/srl-zoo,evaluation/utils.py,9523719a3757bd73f99de9385affc5388ec288ab,TODO add param for relative path vs just folder names,https://github.com/araffin/srl-zoo/commit/9523719a3757bd73f99de9385affc5388ec288ab,Yes
1117,araffin/srl-zoo,const.py,1b48736e4499aa6960c5faed727effdfd6d7e18e,TODO add param for relative path vs just folder names,https://github.com/araffin/srl-zoo/commit/1b48736e4499aa6960c5faed727effdfd6d7e18e,Yes
1118,araffin/srl-zoo,utils.py,e03f1711d8b00bbb778cb37149d938dbe5adb92a,TODO add param for relative path vs just folder names,https://github.com/araffin/srl-zoo/commit/e03f1711d8b00bbb778cb37149d938dbe5adb92a,Yes
1119,araffin/srl-zoo,main.py,c42ccf51ac4e4b4d10f1248580c82924a0492178,A future enhancement TODO for running on multiple GPU: CUDA_VISIBLE_DEVICES=2;3 python main.py   and then also model = torch.nn.DataParallel(model; device_ids=[0;1]).cuda(),https://github.com/araffin/srl-zoo/commit/c42ccf51ac4e4b4d10f1248580c82924a0492178,Yes
1120,araffin/srl-zoo,main.py,b9801ed7b2541c0b86b1543ffffc7a12b20da858,A future enhancement TODO for running on multiple GPU: CUDA_VISIBLE_DEVICES=2;3 python main.py   and then also model = torch.nn.DataParallel(model; device_ids=[0;1]).cuda(),https://github.com/araffin/srl-zoo/commit/b9801ed7b2541c0b86b1543ffffc7a12b20da858,Yes
1121,araffin/srl-zoo,train.py,413b4121a079a9b103bc064cd0082cb67fe3929b,A future enhancement TODO for running on multiple GPU: CUDA_VISIBLE_DEVICES=2;3 python main.py   and then also model = torch.nn.DataParallel(model; device_ids=[0;1]).cuda(),https://github.com/araffin/srl-zoo/commit/413b4121a079a9b103bc064cd0082cb67fe3929b,Yes
1122,davidsbatista/BREDS,Snowball/Sentence.py,61d2b6e3d8853a341c89fd28a1f75e546d9a0a69,TODO: read the context values from parameters.cfg,https://github.com/davidsbatista/BREDS/commit/61d2b6e3d8853a341c89fd28a1f75e546d9a0a69,Yes
1123,davidsbatista/BREDS,Snowball/Sentence.py,61d2b6e3d8853a341c89fd28a1f75e546d9a0a69,TODO: read the context window size from parameters.cfg,https://github.com/davidsbatista/BREDS/commit/61d2b6e3d8853a341c89fd28a1f75e546d9a0a69,Yes
1124,davidsbatista/BREDS,build-ground-truth/Sentence.py,1c9a94999da7e6f12430749b940f804c6eff131a,TODO: read the context values from parameters.cfg,https://github.com/davidsbatista/BREDS/commit/1c9a94999da7e6f12430749b940f804c6eff131a,Yes
1125,davidsbatista/BREDS,build-ground-truth/Sentence.py,1c9a94999da7e6f12430749b940f804c6eff131a,TODO: read the context window size from parameters.cfg,https://github.com/davidsbatista/BREDS/commit/1c9a94999da7e6f12430749b940f804c6eff131a,Yes
1126,davidsbatista/BREDS,build-ground-truth/evaluate.py,0ccf6e0f6437ee6502037eafb3604120a6c8bd96,TODO: paralalizar,https://github.com/davidsbatista/BREDS/commit/0ccf6e0f6437ee6502037eafb3604120a6c8bd96,Yes
1127,davidsbatista/BREDS,build-ground-truth/evaluate.py,0ccf6e0f6437ee6502037eafb3604120a6c8bd96,TODO: paralelizar,https://github.com/davidsbatista/BREDS/commit/0ccf6e0f6437ee6502037eafb3604120a6c8bd96,Yes
1128,davidsbatista/BREDS,build-ground-truth/evaluate.py,b0b1110f1bdc80fadbbb85c383e8a2a87596b76a,TODO: est\u00E1 hard-coded para rela\u00E7\u00E3o: founder; para outros casos; aplicar uma medida de similaridade,https://github.com/davidsbatista/BREDS/commit/b0b1110f1bdc80fadbbb85c383e8a2a87596b76a,Yes
1129,davidsbatista/BREDS,build-ground-truth/evaluate.py,b0b1110f1bdc80fadbbb85c383e8a2a87596b76a,TODO: paralelizar,https://github.com/davidsbatista/BREDS/commit/b0b1110f1bdc80fadbbb85c383e8a2a87596b76a,Yes
1130,davidsbatista/BREDS,build-ground-truth/evaluate.py,338854f46839d75a8b8a7510ae6af97842e63ca7,TODO: s\u00F3 para uma rela\u00E7\u00E3o pode-se fazer um dump e evitar andar sempre a calcular,https://github.com/davidsbatista/BREDS/commit/338854f46839d75a8b8a7510ae6af97842e63ca7,Yes
1131,davidsbatista/BREDS,automatic-evaluation/evaluate-tiago.py,d228126a2642ac91fbd663b4e73d3b3724a2db33,TODO: est\u00E1 hard-coded para rela\u00E7\u00E3o: founder; para caso geral; aplicar uma medida de similaridade,https://github.com/davidsbatista/BREDS/commit/d228126a2642ac91fbd663b4e73d3b3724a2db33,Yes
1132,davidsbatista/BREDS,automatic-evaluation/evaluate-tiago.py,d228126a2642ac91fbd663b4e73d3b3724a2db33,TODO: s\u00F3 para uma rela\u00E7\u00E3o pode-se fazer um dump e evitar andar sempre a calcular,https://github.com/davidsbatista/BREDS/commit/d228126a2642ac91fbd663b4e73d3b3724a2db33,Yes
1133,davidsbatista/BREDS,automatic-evaluation/evaluate-tiago2.py,d228126a2642ac91fbd663b4e73d3b3724a2db33,TODO: est\u00E1 hard-coded para rela\u00E7\u00E3o: founder; para caso geral; aplicar uma medida de similaridade,https://github.com/davidsbatista/BREDS/commit/d228126a2642ac91fbd663b4e73d3b3724a2db33,Yes
1134,davidsbatista/BREDS,automatic-evaluation/evaluate-tiago2.py,d228126a2642ac91fbd663b4e73d3b3724a2db33,TODO: s\u00F3 para uma rela\u00E7\u00E3o pode-se fazer um dump e evitar andar sempre a calcular,https://github.com/davidsbatista/BREDS/commit/d228126a2642ac91fbd663b4e73d3b3724a2db33,Yes
1135,davidsbatista/BREDS,automatic-evaluation/evaluate-tiago3.py,d228126a2642ac91fbd663b4e73d3b3724a2db33,TODO: est\u00E1 hard-coded para rela\u00E7\u00E3o: founder; para caso geral; aplicar uma medida de similaridade,https://github.com/davidsbatista/BREDS/commit/d228126a2642ac91fbd663b4e73d3b3724a2db33,Yes
1136,davidsbatista/BREDS,automatic-evaluation/evaluate-tiago3.py,d228126a2642ac91fbd663b4e73d3b3724a2db33,TODO: s\u00F3 para uma rela\u00E7\u00E3o pode-se fazer um dump e evitar andar sempre a calcular,https://github.com/davidsbatista/BREDS/commit/d228126a2642ac91fbd663b4e73d3b3724a2db33,Yes
1137,davidsbatista/BREDS,automatic-evaluation/evaluate.py,9a6146c55036dc1cb9e5611d2541034a084231c4,TODO: fazer o dump para cada tipo de rela\u00E7\u00E3o diferente,https://github.com/davidsbatista/BREDS/commit/9a6146c55036dc1cb9e5611d2541034a084231c4,Yes
1138,davidsbatista/BREDS,automatic-evaluation/evaluate.py,df80d4569a3f8d0cbabad0e1cfc59df732338f0a,TODO: generalizar isto para todos os tipos de rela\u00E7\u00F5es,https://github.com/davidsbatista/BREDS/commit/df80d4569a3f8d0cbabad0e1cfc59df732338f0a,Yes
1139,davidsbatista/BREDS,BREDS/BREDS.py,cdc4ef4227f3573ec60a87a0d8f99e2f2132463a,TODO: usar o self.config para ler os params eps=0.1; min_samples=2,https://github.com/davidsbatista/BREDS/commit/cdc4ef4227f3573ec60a87a0d8f99e2f2132463a,Yes
1140,davidsbatista/BREDS,BREDS/BREDS.py,cd4e0f3d675ef72318564f329ec9b0e530b8ce21,TODO: isto pode ser paralelizado,https://github.com/davidsbatista/BREDS/commit/cd4e0f3d675ef72318564f329ec9b0e530b8ce21,Yes
1141,davidsbatista/BREDS,BREDS.py,d86685661bffd59a01ef128df06aba902ef12f5c,TODO: isto pode ser paralelizado,https://github.com/davidsbatista/BREDS/commit/d86685661bffd59a01ef128df06aba902ef12f5c,Yes
1142,davidsbatista/BREDS,large-scale-evaluation.py,f52c0309b63018121eaf799267e52f1789b3b433,TODO: escrever para um ficheiro com base no ficheiro de entrada,https://github.com/davidsbatista/BREDS/commit/f52c0309b63018121eaf799267e52f1789b3b433,Yes
1143,davidsbatista/BREDS,BREDS.py,fe8095b9d5bbc780b9995010d3077f8f24088812,TODO: normalizar valores das matrizes para [0;1],https://github.com/davidsbatista/BREDS/commit/fe8095b9d5bbc780b9995010d3077f8f24088812,Yes
1144,davidsbatista/BREDS,BREDS.py,86396b22c2d701bd801c4ef75d767a6cfb0f5720,TODO: se h\u00E1 novos tuplos v\u00E1lidos extraidos ou os patterns a alterarem a confianca; entao continua; senao para,https://github.com/davidsbatista/BREDS/commit/86396b22c2d701bd801c4ef75d767a6cfb0f5720,Yes
1145,davidsbatista/BREDS,BREDS-FCM.py,0f5ac8793e3b301887718fb974d107286d186af2,TODO: se h\u00E1 novos tuplos v\u00E1lidos extraidos ou os patterns a alterarem a confianca; entao continua; senao para,https://github.com/davidsbatista/BREDS/commit/0f5ac8793e3b301887718fb974d107286d186af2,Yes
1146,davidsbatista/BREDS,BREDS-new-sentence.py,715c1d38103e20fa04459bf9b566fe4b371558f3,TODO: se h\u00E1 novos tuplos v\u00E1lidos extraidos ou os patterns a alterarem a confianca; entao continua; senao para,https://github.com/davidsbatista/BREDS/commit/715c1d38103e20fa04459bf9b566fe4b371558f3,Yes
1147,davidsbatista/BREDS,BREDS-parallel.py,37eff7fdeed0d8665091161d35e09555479d5a2f,TODO: para paralelizar o finding instances,https://github.com/davidsbatista/BREDS/commit/37eff7fdeed0d8665091161d35e09555479d5a2f,Yes
1148,davidsbatista/BREDS,BREDS-parallel.py,37eff7fdeed0d8665091161d35e09555479d5a2f,TODO: se h\u00E1 novos tuplos v\u00E1lidos extraidos ou os patterns a alterarem a confianca; entao continua; senao para,https://github.com/davidsbatista/BREDS/commit/37eff7fdeed0d8665091161d35e09555479d5a2f,Yes
1149,davidsbatista/BREDS,BREDS-parallel-fe-clustering.py,4ccccfa6fed87c372089cfce8223b9cbbd9f6b5e,TODO: Patterns novos t\u00EAm que ser comparados pairwise e merged cosoante as similaridades,https://github.com/davidsbatista/BREDS/commit/4ccccfa6fed87c372089cfce8223b9cbbd9f6b5e,Yes
1150,davidsbatista/BREDS,BREDS-parallel.py,f426023bd06ed1e8700c3b6fff422caed5b9f652,TODO: Patterns novos t\u00EAm que ser comparados pairwise e merged cosoante as similaridades,https://github.com/davidsbatista/BREDS/commit/f426023bd06ed1e8700c3b6fff422caed5b9f652,Yes
1151,davidsbatista/BREDS,BREDS-parallel.py,de48a7e043f19626f7c1925811bc49108b6e997e,TODO: apenas comparar tuplos diferentes,https://github.com/davidsbatista/BREDS/commit/de48a7e043f19626f7c1925811bc49108b6e997e,Yes
1152,josedolz/HyperDenseNet,src/HyperDenseNet/HyperDenseNet.py,18c35dd81927707d8e0b17b357b38540a266b33e,TODO: Get this directly as len(_self.networkLayers[l_i].params),https://github.com/josedolz/HyperDenseNet/commit/18c35dd81927707d8e0b17b357b38540a266b33e,Yes
1153,YannDubs/disentangling-vae,viz/latent_traversals.py,f69b9d7f921b080499476d0057770ac3bff8a52f,TODO: Treating this separately for now with the idea that an exception will be raised later,https://github.com/YannDubs/disentangling-vae/commit/f69b9d7f921b080499476d0057770ac3bff8a52f,Yes
1154,proycon/clam,jobservice/parameters.py,b7b156a3e1e48075a2c3dd79e94391aecbb9a860,TODO: implement reading parameters from XML; returns a class derived from AbstractParameter; xmlnode is lxml Element or string,https://github.com/proycon/clam/commit/b7b156a3e1e48075a2c3dd79e94391aecbb9a860,Yes
1155,proycon/clam,common/parameters.py,8645dd05ced070fbf8e4f75f8f6a76c4fa01a1d9,extra parsing for choice parameter (TODO: put in a better spot),https://github.com/proycon/clam/commit/8645dd05ced070fbf8e4f75f8f6a76c4fa01a1d9,Yes
1156,proycon/clam,common/data.py,098d75ff0ece9eadc3c5f172fc59c7dc50d42a73,extra parsing for choice parameter (TODO: put in a better spot),https://github.com/proycon/clam/commit/098d75ff0ece9eadc3c5f172fc59c7dc50d42a73,Yes
1157,proycon/clam,common/parameters.py,893952a164febbfc20c53e42e79ebde68ae83df1,extra parsing for choice parameter (TODO: put in a better spot),https://github.com/proycon/clam/commit/893952a164febbfc20c53e42e79ebde68ae83df1,Yes
1158,proycon/clam,common/parameters.py,38d54e5606cfce79dc912b153496163485945646,extra parsing for choice parameter (TODO: put in a better spot),https://github.com/proycon/clam/commit/38d54e5606cfce79dc912b153496163485945646,Yes
1159,proycon/clam,common/parameters.py,5994e609b8c8c84251ff8952661e66ca11c59f37,extra parsing for choice parameter (TODO: put in a better spot),https://github.com/proycon/clam/commit/5994e609b8c8c84251ff8952661e66ca11c59f37,Yes
1160,proycon/clam,common/parameters.py,bc16a29c14f0959bc782b8269c321e78f7fe0e36,extra parsing for choice parameter (TODO: put in a better spot),https://github.com/proycon/clam/commit/bc16a29c14f0959bc782b8269c321e78f7fe0e36,Yes
1161,regel/loudml,loudml/prometheus.py,e89cd86a525f2a49f068d417832312634a5fb725,TODO: need to handle bottomk(X) param,https://github.com/regel/loudml/commit/e89cd86a525f2a49f068d417832312634a5fb725,Yes
1162,regel/loudml,loudml/prometheus.py,e89cd86a525f2a49f068d417832312634a5fb725,TODO: need to handle topk(X) param,https://github.com/regel/loudml/commit/e89cd86a525f2a49f068d417832312634a5fb725,Yes
1163,maciejczyzewski/neural-chessboard,train.py,eb915e9ed37a252eb87b482e4c2c8a92776d5591,"FIXME: dataset preparation from \""d\"" script (labels)",https://github.com/maciejczyzewski/neural-chessboard/commit/eb915e9ed37a252eb87b482e4c2c8a92776d5591,Yes
1164,drckf/paysage,paysage/paysage/models.py,b09b6c78761245e3662f3d3dcef5b8d1361d0aaf,"\""\""\""   || #TODO: || class HopfieldModel(LatentModel): ||      ||     def __init__(self; nvis; nhid): ||         self.layers = {} ||         self.layers['visible'] = layers.IsingLayer(nvis) ||         self.layers['hidden'] = layers.GaussianLayer(nhid) ||          ||         self.params = {} ||         self.params['weights'] = numpy.random.normal(loc=0.0; scale=1.0; size=(self.layers['visible'].len; self.layers['hidden'].len)).astype(dtype=numpy.float32) ||         self.params['bias'] = numpy.ones_like(self.layers['visible'].loc)   ||  ||  || class HookeMachine(LatentModel): ||      ||     def __init__(self; nvis; nhid; vis_type='gauss'; hid_type='expo'):    ||         assert vis_type.lower() in ['gauss'; 'ising'] ||         assert hid_type.lower() in ['expo'; 'bern'] ||          ||         self.layers = {} ||         self.layers['visible'] = layers.get(vis_type)(nvis) ||         self.layers['hidden'] = layers.get(hid_type)(nhid) ||          ||         self.params = {} ||         self.params['weights'] = numpy.random.normal(loc=0.0; scale=1.0; size=(self.layers['visible'].len; self.layers['hidden'].len)).astype(dtype=numpy.float32) ||         self.params['bias'] = numpy.ones_like(self.layers['hidden'].loc)   ||         self.params['T'] = numpy.ones(1; dtype=numpy.float32) ||                  ||         self.deriv = {} ||         self.deriv['weights'] = numpy.zeros_like(self.params['weights']) ||         self.deriv['bias'] = numpy.zeros_like(self.params['bias']) ||         self.params['T'] = numpy.zeros_like(self.params['T']) ||          ||         self.set_vis(numpy.zeros_like(self.layers['visible'].loc)) ||          ||     def set_vis(self; vis): ||         self.vis = vis ||         self.diff = (self.params['weights'].T - vis).T ||         self.squared_dist = numpy.sum(self.diff ** 2; axis=0) ||         self.layers['hidden'].update_params(self.params['bias'] + self.squared_dist \/ (2 * self.params['T'])) ||         self.energy = -numpy.sum(numpy.log(self.layers['hidden'].partition_function()))         ||          ||     def visible_conditional_params(self; hid): ||         total = numpy.sum(hid) ||         loc = numpy.dot(self.params['weights']; hid) \/ total ||         scale = self.params['T'] \/ total * numpy.ones_like(self.layers['visible'].loc) ||         return (loc; scale) ||          ||     def update_visible_params(self; hid): ||         self.layers['visible'].update_params(*self.visible_conditional_params(hid)) ||          ||     def derivatives(self; vis; key): ||         self.update_hidden_params(vis) ||         hidden_mean = self.layers['hidden'].mean() ||         if key == 'bias': ||             # del H(v; k) \/ del b ||             return hidden_mean ||         elif key == 'weights': ||             # del H(v; k) \/ del W ||             return (self.difference(vis) * hidden_mean.T) \/ self.params['T'] ||         elif key == 'T': ||             # del H(v;k) \/ del T ||             return numpy.dot(hidden_mean.T; self.squared_distance(vis)) ||         else: ||             raise ValueError('unknown key: {}'.format(key)) ||     \""\""\""",https://github.com/drckf/paysage/commit/b09b6c78761245e3662f3d3dcef5b8d1361d0aaf,Yes
1165,drckf/paysage,paysage/models/hidden.py,a6316e764c26589184c846a9c96c18fce8fb935a,TODO: implement parameter constraints,https://github.com/drckf/paysage/commit/a6316e764c26589184c846a9c96c18fce8fb935a,Yes
1166,drckf/paysage,paysage/layers.py,2b610c4c66426f7502dd9a92f1f47fa5fc0ea7b0,TODO : params,https://github.com/drckf/paysage/commit/2b610c4c66426f7502dd9a92f1f47fa5fc0ea7b0,Yes
1167,drckf/paysage,paysage/layers.py,a9dcc1431c4f40013dff34beebb8c935322119dd,TODO : params,https://github.com/drckf/paysage/commit/a9dcc1431c4f40013dff34beebb8c935322119dd,Yes
1168,drckf/paysage,paysage/layers.py,3f2c69b49f6285e30a13c9dae31d27dae81d91dd,TODO : params,https://github.com/drckf/paysage/commit/3f2c69b49f6285e30a13c9dae31d27dae81d91dd,Yes
1169,drckf/paysage,paysage/layers.py,dacda95bdb01af8ff5badbcd671920c19d187cbf,TODO : params,https://github.com/drckf/paysage/commit/dacda95bdb01af8ff5badbcd671920c19d187cbf,Yes
1170,drckf/paysage,paysage/layers.py,9962067e1b9886c546abc0af8d4d00fc31cab1ef,TODO : params,https://github.com/drckf/paysage/commit/9962067e1b9886c546abc0af8d4d00fc31cab1ef,Yes
1171,drckf/paysage,paysage/layers.py,1179c89fb1cd563db6c7d32d0d54cc2039d5128c,TODO : params,https://github.com/drckf/paysage/commit/1179c89fb1cd563db6c7d32d0d54cc2039d5128c,Yes
1172,alvations/sacremoses,sacremoses/cli.py,e535a7e495228904722f467f6e6837b22e0e0fcc,FIXME: parallelize job don't work properly for MosesTruecaser.truecase,https://github.com/alvations/sacremoses/commit/e535a7e495228904722f467f6e6837b22e0e0fcc,Yes
1173,maximecb/gym-miniworld,gym_miniworld/miniworld.py,e7d1f8b760d84e36bb9c78f9ea0e81ca4edfc467,TODO: make this a param to gen_tex_coords?,https://github.com/maximecb/gym-miniworld/commit/e7d1f8b760d84e36bb9c78f9ea0e81ca4edfc467,Yes
1174,maximecb/gym-miniworld,gym_miniworld/envs/simtoreal1.py,ab110db8c3d81d0e74b4777d106ccc89f7e52d03,TODO: modify lighting parameters,https://github.com/maximecb/gym-miniworld/commit/ab110db8c3d81d0e74b4777d106ccc89f7e52d03,Yes
1175,maximecb/gym-miniworld,gym_miniworld/envs/simtoreal2.py,ceb973d07ee3187769e57cfedf669cad641377c9,TODO: modify lighting parameters,https://github.com/maximecb/gym-miniworld/commit/ceb973d07ee3187769e57cfedf669cad641377c9,Yes
1176,quadrismegistus/prosodic,lib/Parse.py,b6886467753a49b9cbb2edf6667b847aaecc3600,# @TODO: parameterize this: break ties by favoring the more binary parse,https://github.com/quadrismegistus/prosodic/commit/b6886467753a49b9cbb2edf6667b847aaecc3600,Yes
1177,quadrismegistus/prosodic,lib/lexconvert.py,7a6f7c226deb397b9d81b40cfa517573f2c96ee6,TODO: manual does not say what the maximum length is; longest parameter in examples is 80 bytes; should we use inline_format to make each WORD a separate command?,https://github.com/quadrismegistus/prosodic/commit/7a6f7c226deb397b9d81b40cfa517573f2c96ee6,Yes
1178,quadrismegistus/prosodic,lib/lexconvert.py,7a6f7c226deb397b9d81b40cfa517573f2c96ee6,oops; lost synchronisation the other way (TODO: show this per-paragraph? but don't call eSpeak too many times if processing many short paragraphs),https://github.com/quadrismegistus/prosodic/commit/7a6f7c226deb397b9d81b40cfa517573f2c96ee6,Yes
1179,quadrismegistus/prosodic,lib/lexconvert.py,7a6f7c226deb397b9d81b40cfa517573f2c96ee6,(TODO: this assumes stress marks are at end of syllable rather than immediately after vowel; correct for Festival; check others; probably a harmless assumption though; mac-uk is better with syllable separators although espeak basically ignores them),https://github.com/quadrismegistus/prosodic/commit/7a6f7c226deb397b9d81b40cfa517573f2c96ee6,Yes
1180,quadrismegistus/prosodic,lib/lexconvert.py,7a6f7c226deb397b9d81b40cfa517573f2c96ee6,TODO: won't work for formats that don't have a phoneme separator (doesn't really matter for eSpeak though),https://github.com/quadrismegistus/prosodic/commit/7a6f7c226deb397b9d81b40cfa517573f2c96ee6,Yes
1181,clusterking/clusterking,cluster.py,95e6b8ccb1aa73f98c480c2b8880c2824dedc0e1,todo: probably we don't need that separately actually,https://github.com/clusterking/clusterking/commit/95e6b8ccb1aa73f98c480c2b8880c2824dedc0e1,Yes
1182,ECRL/ECNet,ecnet_server.py,0c671f050733424962575768192be345b8bcac8c,## NOT FUNCTIONAL - TODO: Add function for limiting parameters to data utils file\t\t,https://github.com/ECRL/ECNet/commit/0c671f050733424962575768192be345b8bcac8c,Yes
1183,ELS-RD/anonymisation,ner/train.py,6d4da29b5fed35325440baefb743f616b02f189a,TODO look for header info inside paragraphs,https://github.com/ELS-RD/anonymisation/commit/6d4da29b5fed35325440baefb743f616b02f189a,Yes
1184,ELS-RD/anonymisation,ner/train.py,6d4da29b5fed35325440baefb743f616b02f189a,TODO remove any paragraph without any annotation?,https://github.com/ELS-RD/anonymisation/commit/6d4da29b5fed35325440baefb743f616b02f189a,Yes
1185,ELS-RD/anonymisation,entities_viewer_spacy.py,8605e578571c95f4b159f5efd445568400764082,TODO port number should be a parameter,https://github.com/ELS-RD/anonymisation/commit/8605e578571c95f4b159f5efd445568400764082,Yes
1186,ELS-RD/anonymisation,fine_tune_pre_trained_model.py,535af3abd59b9ee4dca977d3be908a3c4154eb0e,results = recompose_paragraphs(results)  # TODO why this step is necessary?,https://github.com/ELS-RD/anonymisation/commit/535af3abd59b9ee4dca977d3be908a3c4154eb0e,Yes
1187,intel/dffml,model/scikit/dffml_model_scikit/scikit_models.py,7afd85a85ee48f1ba45631b2e193d51c91eaab61,TODO if param.default is an array then Args needs to get a,https://github.com/intel/dffml/commit/7afd85a85ee48f1ba45631b2e193d51c91eaab61,Yes
1188,intel/dffml,dffml/df/types.py,95229ff8049b9b34b5cd319a919d88b33e12ea0a,TODO Add optional parameter Input.target which specifies the operation,https://github.com/intel/dffml/commit/95229ff8049b9b34b5cd319a919d88b33e12ea0a,Yes
1189,drivendataorg/zamba,zamba/models/cnnensemble_model.py,b14f5836ce11b50ebf6623e1eb527e81c094e8e9,TODO: slow part; would benefit from parallel execution at least per fold,https://github.com/drivendataorg/zamba/commit/b14f5836ce11b50ebf6623e1eb527e81c094e8e9,Yes
1190,sonidosmutantes/apicultor,MockRedPanalAPI_service.py,ecde3284639a0d2f22c6c4f8f5c872662b87a550,"TODO: refactorizar para pasarle a la funci\u00F3n un \""comparator\"" en un objeto (para no duplicar c\u00F3digo)",https://github.com/sonidosmutantes/apicultor/commit/ecde3284639a0d2f22c6c4f8f5c872662b87a550,Yes
1191,sonidosmutantes/apicultor,RandomSegmentation.py,6927fa71843994757e3e20c793d2ed42c7736402,TODO: take files dir as parameter,https://github.com/sonidosmutantes/apicultor/commit/6927fa71843994757e3e20c793d2ed42c7736402,Yes
1192,sonidosmutantes/apicultor,apicultor/RandomSegmentation.py,2722a75c6507ce376b1f5983271866cabdb2a924,TODO: take files dir as parameter,https://github.com/sonidosmutantes/apicultor/commit/2722a75c6507ce376b1f5983271866cabdb2a924,Yes
1193,sonidosmutantes/apicultor,JsonMirFilesData.py,eae3ef43d1b1aa7881d1527cedc4c6ae42b6e3dc,"TODO: refactorizar para pasarle a la funci\u00F3n un \""comparator\"" en un objeto (para no duplicar c\u00F3digo)",https://github.com/sonidosmutantes/apicultor/commit/eae3ef43d1b1aa7881d1527cedc4c6ae42b6e3dc,Yes
1194,sonidosmutantes/apicultor,MirDbApi/IMirDbApi.py,21d3a68a389501a92846800754ce365d6de9e79d,TODO: add as a parameter and callback function processing in other methods,https://github.com/sonidosmutantes/apicultor/commit/21d3a68a389501a92846800754ce365d6de9e79d,Yes
1195,sonidosmutantes/apicultor,mirdbapi/FreesoundDB.py,21d3a68a389501a92846800754ce365d6de9e79d,TODO: add as a parameter and callback function processing in other methods,https://github.com/sonidosmutantes/apicultor/commit/21d3a68a389501a92846800754ce365d6de9e79d,Yes
1196,sonidosmutantes/apicultor,apicultor/segmentation/RandomSegmentation.py,752b4bd37627edc215f26c6f1a7b79f70c315b7d,TODO: take files dir as parameter,https://github.com/sonidosmutantes/apicultor/commit/752b4bd37627edc215f26c6f1a7b79f70c315b7d,Yes
1197,sonidosmutantes/apicultor,segmentation/FixedSegmentation.py,8c21c08cbf86627fd43108b8be38305114b8bbd0,TODO: take files dir as parameter,https://github.com/sonidosmutantes/apicultor/commit/8c21c08cbf86627fd43108b8be38305114b8bbd0,Yes
1198,WMD-group/SMACT,smact/builder.py,0ceab844146cd962813ea7fe5c4280e846c23bc1,TODO Use actual lattice parameter,https://github.com/WMD-group/SMACT/commit/0ceab844146cd962813ea7fe5c4280e846c23bc1,Yes
1199,WMD-group/SMACT,smact/structure_prediction/structure.py,26e30058a89d855b4121a87202eaf19656d2a47d,TODO Use actual lattice parameter,https://github.com/WMD-group/SMACT/commit/26e30058a89d855b4121a87202eaf19656d2a47d,Yes
1200,WMD-group/SMACT,smact/builder.py,59324c885ee4e0f9ff493df79ecf1703a17c1e15,TODO Use actual lattice parameter,https://github.com/WMD-group/SMACT/commit/59324c885ee4e0f9ff493df79ecf1703a17c1e15,Yes
1201,WMD-group/SMACT,smact/structure_prediction/probability_models.py,f18278088a532c4edede4d6e9ccaa0a60bb7b712,"\""\""\""Probability models for species substitution. ||  || Implements base class :class:`SubstitutionModel`; || which can be extended to allow for development of new || lambda tables. An example of such an extension; || :class:`RadiusModel`; is also implemented. ||  || Todo: ||     * Allow for parallelism in lambda table calculations ||       by implementing a `sub_probs` abstractmethod ||       that :meth:`SubstitutionModel.gen_lambda` uses; ||       if available. ||  || \""\""\""",https://github.com/WMD-group/SMACT/commit/f18278088a532c4edede4d6e9ccaa0a60bb7b712,Yes
1202,hypnosapos/cartpole-rl-remote,cartpole/runner_remote.py,382f4591a79cb9d52de6c2d6268ff75c033fe0a7,TODO Parameter,https://github.com/hypnosapos/cartpole-rl-remote/commit/382f4591a79cb9d52de6c2d6268ff75c033fe0a7,Yes
1203,yoshida-lab/XenonPy,xenonpy/descriptor/base.py,95654980bf7d1d0cf04dba7ebf000ae92ce955fd,todo: Dose fit_transform need to pass paras to transform?,https://github.com/yoshida-lab/XenonPy/commit/95654980bf7d1d0cf04dba7ebf000ae92ce955fd,Yes
1204,betterlife/betterlifepsi,psi/app/views/shipping.py,84e9e5c04c45ec088693c96d0ef70c8698d14e55,TODO.xqliu Move the judgetment stragegy to a separate method to avoid repeat myself!!!,https://github.com/betterlife/betterlifepsi/commit/84e9e5c04c45ec088693c96d0ef70c8698d14e55,Yes
1205,hackingmaterials/automatminer,matbench/data/tests/test_data.py,85297fe4e122d903190fdef74127e132e1210259,todo: needs separate tests for each load function...,https://github.com/hackingmaterials/automatminer/commit/85297fe4e122d903190fdef74127e132e1210259,Yes
1206,hackingmaterials/automatminer,matbench/automl/adaptors.py,6bb13f6c4bcf27cd9acf2a997110911d1d52faa2,todo: combinations of model params).,https://github.com/hackingmaterials/automatminer/commit/6bb13f6c4bcf27cd9acf2a997110911d1d52faa2,Yes
1207,hackingmaterials/automatminer,automatminer/automl/adaptors.py,fefd6ba4eea465bf8a5bdab66a485599d5187324,todo: combinations of model params).,https://github.com/hackingmaterials/automatminer/commit/fefd6ba4eea465bf8a5bdab66a485599d5187324,Yes
1208,mackelab/delfi,delfi/generator/BaseGenerator.py,7b4ab5ed3db6e772da1449bfe746324aedbf6bff,TODO: for n_reps > 1 duplicate params; reshape stats array,https://github.com/mackelab/delfi/commit/7b4ab5ed3db6e772da1449bfe746324aedbf6bff,Yes
1209,mackelab/delfi,delfi/generator/BaseGenerator.py,7b4ab5ed3db6e772da1449bfe746324aedbf6bff,TODO: check if parameter is inside of support of prior when,https://github.com/mackelab/delfi/commit/7b4ab5ed3db6e772da1449bfe746324aedbf6bff,Yes
1210,mackelab/delfi,delfi/generator/MPGenerator.py,1696070fd94df04a0c0cb7c4c5f765a302e1f8f9,TODO: for n_reps > 1 duplicate params; reshape stats array,https://github.com/mackelab/delfi/commit/1696070fd94df04a0c0cb7c4c5f765a302e1f8f9,Yes
1211,ucloud/uai-sdk,uaitrain/arch_conf/base_conf.py,841997ee514a2e846192b2eefcb5cb70e23b2dfc,todo: add cmd line parameters outside baseconfig; according to different aiframe,https://github.com/ucloud/uai-sdk/commit/841997ee514a2e846192b2eefcb5cb70e23b2dfc,Yes
1212,ucloud/uai-sdk,examples/tensorflow/train/slim/code/deployment/model_deploy.py,33018523cc2b4efe7b9872aebb7d2211a9d68482,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/ucloud/uai-sdk/commit/33018523cc2b4efe7b9872aebb7d2211a9d68482,Yes
1213,nengo/nengo-dl,nengo_lasagne/nengo_lasagne/builder.py,75c58f100d2b5b7bcadc1b6661d53f860bf1c9cd,TODO: throw warnings here about the parameters being ignored,https://github.com/nengo/nengo-dl/commit/75c58f100d2b5b7bcadc1b6661d53f860bf1c9cd,Yes
1214,nengo/nengo-dl,nengo_lasagne/nengo_lasagne/layers.py,b30b68df70708ec6a81a3d5c6f5d963f6a29b082,TODO: explore optimization parameters (e.g. rollout),https://github.com/nengo/nengo-dl/commit/b30b68df70708ec6a81a3d5c6f5d963f6a29b082,Yes
1215,nengo/nengo-dl,nengo_deeplearning/simulator.py,5927076ae38eca69d46c82c66a2efaebf2d047a4,TODO: more parallel iterations,https://github.com/nengo/nengo-dl/commit/5927076ae38eca69d46c82c66a2efaebf2d047a4,Yes
1216,nengo/nengo-dl,nengo_deeplearning/tensor_graph.py,71be78879a42b35bf6b289549211f2c34730a4b8,TODO: more parallel iterations,https://github.com/nengo/nengo-dl/commit/71be78879a42b35bf6b289549211f2c34730a4b8,Yes
1217,nengo/nengo-dl,nengo_dl/tests/test_simulator.py,f56616396da6224b63c6df5474714e938a9561a0,TODO: separate this into two different inputs to test that,https://github.com/nengo/nengo-dl/commit/f56616396da6224b63c6df5474714e938a9561a0,Yes
1218,nengo/nengo-dl,nengo_dl/tensor_graph.py,f1d8b838a6b8ee7b43e2642ffd0fde8a7fecc8de,TODO: parallel iterations work in eager mode,https://github.com/nengo/nengo-dl/commit/f1d8b838a6b8ee7b43e2642ffd0fde8a7fecc8de,Yes
1219,nengo/nengo-dl,nengo_dl/tensor_graph.py,1cdb1e558cd02b18b839b7f5644e37b43967ba6f,TODO: it would be nicer if buildconfig was static (i.e. find a separate,https://github.com/nengo/nengo-dl/commit/1cdb1e558cd02b18b839b7f5644e37b43967ba6f,Yes
1220,scvae/scvae,scvae/scvae.py,13dbb8c6adf8f1e1ea6abb99d82e161e91cda395,TODO Split into separate functions,https://github.com/scvae/scvae/commit/13dbb8c6adf8f1e1ea6abb99d82e161e91cda395,Yes
1221,scvae/scvae,scvae/cross_analysis.py,daa18b69417d0dbfc295a6dd39d1fb1f9e324d43,TODO Split into separate functions,https://github.com/scvae/scvae/commit/daa18b69417d0dbfc295a6dd39d1fb1f9e324d43,Yes
1222,vecto-ai/vecto,vecto/corpus/corpus.py,e0d1334ef6d31ba65c3e5c19f511b612e9a2989d,todo lower should be parameter,https://github.com/vecto-ai/vecto/commit/e0d1334ef6d31ba65c3e5c19f511b612e9a2989d,Yes
1223,vecto-ai/vecto,vecto/benchmarks/text_classification/text_datasets.py,6e17a6e4f455c9032c3ead2dc6c610d04f0c15ee,TODO: deal with shrink parameter,https://github.com/vecto-ai/vecto/commit/6e17a6e4f455c9032c3ead2dc6c610d04f0c15ee,Yes
1224,vecto-ai/vecto,vecto/benchmarks/text_classification/text_datasets.py,446309996bee3da9b0d9c40f0b80ad9bad22f5f0,TODO: deal with shrink parameter,https://github.com/vecto-ai/vecto/commit/446309996bee3da9b0d9c40f0b80ad9bad22f5f0,Yes
1225,hachmannlab/chemml,chemml/models/keras/graphconvlayers.py,0a1a45f6322422737a325c32ed1caa8458890cbe,TODO: Add GraphWiseDropout layer; that creates masks for each degree separately.,https://github.com/hachmannlab/chemml/commit/0a1a45f6322422737a325c32ed1caa8458890cbe,Yes
1226,IBM/mi-prometheus,problems/image_text_to_class/clevr.py,b06f7620f2977700ef699b9a774376cd4008f489,TODO: We will have to separate this file into 1 file per sample to support multiprocessing!,https://github.com/IBM/mi-prometheus/commit/b06f7620f2977700ef699b9a774376cd4008f489,Yes
1227,IBM/mi-prometheus,problems/image_text_to_class/clevr.py,9af1ec6de010e66e2b9d3832f2683852de63adc4,TODO: We will have to separate this file into 1 file per sample to support multiprocessing!,https://github.com/IBM/mi-prometheus/commit/9af1ec6de010e66e2b9d3832f2683852de63adc4,Yes
1228,IBM/mi-prometheus,problems/image_text_to_class/clevr.py,80593f8153c1e020fbd1b378bbf8ae83528bf680,TODO: We will have to separate this file into 1 file per sample to support multiprocessing!,https://github.com/IBM/mi-prometheus/commit/80593f8153c1e020fbd1b378bbf8ae83528bf680,Yes
1229,IBM/mi-prometheus,trainer.py,1cdfe57ea74f885f241ef8298d744d5a146ce9d2,"\""\""\"" || trainer.py: Contains the code implementation of the main worker of mi-prometheus. || This worker in particular is called the `episodic trainer` and will take care of training || a specified model on a specified problem for a given number of episodes (among other adjustable || parameters). ||  || #TODO: Enhance this description and documentation. ||  || \""\""\""",https://github.com/IBM/mi-prometheus/commit/1cdfe57ea74f885f241ef8298d744d5a146ce9d2,Yes
1230,IBM/mi-prometheus,miprometheus/workers/tester.py,6f2c4b99e812a3802e1e861659a2df66b8a24f7d,TODO: Fix bug where we have to delete the key from the config AND default params,https://github.com/IBM/mi-prometheus/commit/6f2c4b99e812a3802e1e861659a2df66b8a24f7d,Yes
1231,NRCan/geo-deep-learning,images_to_echantillons.py,07f82cad109e3d36f5fd292b795c13865d7449f6,TODO lire les parametres dans un fichier txt.,https://github.com/NRCan/geo-deep-learning/commit/07f82cad109e3d36f5fd292b795c13865d7449f6,Yes
1232,NRCan/geo-deep-learning,inference.py,d728da5afb04d213a3da93e334ca9093d54bd5a1,TODO: remove working_folder parameter in all templates,https://github.com/NRCan/geo-deep-learning/commit/d728da5afb04d213a3da93e334ca9093d54bd5a1,Yes
1233,NRCan/geo-deep-learning,utils/visualization.py,d728da5afb04d213a3da93e334ca9093d54bd5a1,FIXME: function parameters should not come in as different types if inference or not.,https://github.com/NRCan/geo-deep-learning/commit/d728da5afb04d213a3da93e334ca9093d54bd5a1,Yes
1234,NRCan/geo-deep-learning,inference.py,5865c25bd83b5f14ec843b43e6f878d4b4517c1e,TODO: remove working_folder parameter in all templates,https://github.com/NRCan/geo-deep-learning/commit/5865c25bd83b5f14ec843b43e6f878d4b4517c1e,Yes
1235,cesium-ml/cesium_web,cesium_app/flask_server.py,4053b1c36868b71bca3cf443c23c0226b61c3f6a,TODO split out constant params \/ params to optimize,https://github.com/cesium-ml/cesium_web/commit/4053b1c36868b71bca3cf443c23c0226b61c3f6a,Yes
1236,cesium-ml/cesium_web,cesium_app/flask_server.py,22caf42a02cfa02acade02a2c1f7fb8a0b2bb87e,TODO where do task server params live?,https://github.com/cesium-ml/cesium_web/commit/22caf42a02cfa02acade02a2c1f7fb8a0b2bb87e,Yes
1237,cesium-ml/cesium_web,cesium_app/handlers/model.py,bfa604cf80c88d7fe9b3ced2d839061d352b3930,TODO split out constant params \/ params to optimize,https://github.com/cesium-ml/cesium_web/commit/bfa604cf80c88d7fe9b3ced2d839061d352b3930,Yes
1238,eellak/gsoc2018-3gm,src/parser.py,7f06392f0f4886a7a01da05bd5fdf5f889b4f44b,TODO split to paragraphs,https://github.com/eellak/gsoc2018-3gm/commit/7f06392f0f4886a7a01da05bd5fdf5f889b4f44b,Yes
1239,eellak/gsoc2018-3gm,src/syntax.py,67026d5021f20c5e227b670a5f461bb37e0bc531,TODO separate phrases from paragraphs and articles so they always exist in extracts,https://github.com/eellak/gsoc2018-3gm/commit/67026d5021f20c5e227b670a5f461bb37e0bc531,Yes
1240,eellak/gsoc2018-3gm,src/syntax.py,007c218c4a32f7f272ab999b68933f83feceaf24,TODO separate phrases from paragraphs and articles so they always exist in extracts,https://github.com/eellak/gsoc2018-3gm/commit/007c218c4a32f7f272ab999b68933f83feceaf24,Yes
1241,NatLibFi/Annif,annif/cli.py,0fba48fb8da4c7f287bde4bf0f797ebf9d475a68,TODO: Allow overriding a parameter of a specific backend of ensemble,https://github.com/NatLibFi/Annif/commit/0fba48fb8da4c7f287bde4bf0f797ebf9d475a68,Yes
1242,NatLibFi/Annif,annif/cli.py,ece825fdbbe436156b64c820f75cfd9838e0f10b,TODO: Allow overriding a parameter of a specific backend of ensemble,https://github.com/NatLibFi/Annif/commit/ece825fdbbe436156b64c820f75cfd9838e0f10b,Yes
1243,IBM/MAX-Object-Detector,training/training_code/object_detection/slim/deployment/model_deploy.py,e7aef82865713ee7bd109156a2eb2fadd57f2387,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/IBM/MAX-Object-Detector/commit/e7aef82865713ee7bd109156a2eb2fadd57f2387,Yes
1244,ucam-smt/sgnmt,cam/sgnmt/decoding/combination.py,82d4d8867de80d54c3d837a1b3b4d74eeaf9a2fb,"\""\""\""This module contains strategies to convert a score breakdown to || the total score. This is commonly specified via the || --combination_scheme parameter. ||  || TODO: The breakdown2score interface is not very elegant; and has some ||       overlap with the interpolation_strategy implementations. || \""\""\""",https://github.com/ucam-smt/sgnmt/commit/82d4d8867de80d54c3d837a1b3b4d74eeaf9a2fb,Yes
1245,delph-in/pydelphin,delphin/itsdb.py,ec768b557e6c4d7be4982e5a536742fa06f2cb2b,todo: use gzip parameter,https://github.com/delph-in/pydelphin/commit/ec768b557e6c4d7be4982e5a536742fa06f2cb2b,Yes
1246,delph-in/pydelphin,delphin/itsdb.py,3eca205eea2dd1f2121b156c400da28b3c1e7908,todo: use gzip parameter,https://github.com/delph-in/pydelphin/commit/3eca205eea2dd1f2121b156c400da28b3c1e7908,Yes
1247,BaderLab/saber,kari/sequence_processing_model.py,e9638310312cb0b3a327b5318ee51ead06cad5a9,TODO (johngiorgi): consider smarter default values for paramas,https://github.com/BaderLab/saber/commit/e9638310312cb0b3a327b5318ee51ead06cad5a9,Yes
1248,BaderLab/saber,kari/sequence_processing_model.py,ce3ede7fe4844cfc2d29365796e23f0adea2d97a,TODO (johngiorgi): make model checkpointing a config param,https://github.com/BaderLab/saber/commit/ce3ede7fe4844cfc2d29365796e23f0adea2d97a,Yes
1249,BaderLab/saber,kari/models/simple_lstm_crf_ner.py,03a14f5eeef0e6ff2f148ba9ac6649ae4aac9922,TODO (johngiorgi) set up parameter for embedding output dimension,https://github.com/BaderLab/saber/commit/03a14f5eeef0e6ff2f148ba9ac6649ae4aac9922,Yes
1250,BaderLab/saber,kari/sequence_processor.py,c9811274642b4da4438c9d059ad2eeb19fd920f1,TODO (johngiorgi): make model checkpointing a config param,https://github.com/BaderLab/saber/commit/c9811274642b4da4438c9d059ad2eeb19fd920f1,Yes
1251,BaderLab/saber,saber/utils_models.py,025f6d72a8f9aabd4e065430e81a6fc19c74643a,TODO (johngiorgi) add verbosity parameter for printing model summary,https://github.com/BaderLab/saber/commit/025f6d72a8f9aabd4e065430e81a6fc19c74643a,Yes
1252,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_4.py,ae7b50a959c1432ce14e2a984d0cf7e3dddbc786,TODO: once SKLL hyperparameters can be passed; replace this code,https://github.com/EducationalTestingService/rsmtool/commit/ae7b50a959c1432ce14e2a984d0cf7e3dddbc786,Yes
1253,jhu-lcsr/costar_plan,costar_google_brainrobotdata/grasp_dataset.py,6fb19d1020ebc6fa8725a5d1e885dc3a3430c7fc,TODO(ahundt) MOVE THE CODE ABOVE INTO _get_simple_parallel_dataset_ops,https://github.com/jhu-lcsr/costar_plan/commit/6fb19d1020ebc6fa8725a5d1e885dc3a3430c7fc,Yes
1254,jhu-lcsr/costar_plan,costar_google_brainrobotdata/grasp_dataset.py,b81c8575a9e01e64f890fee3f30c3cef99dd72ac,TODO(ahundt) MOVE THE CODE ABOVE INTO _get_simple_parallel_dataset_ops(),https://github.com/jhu-lcsr/costar_plan/commit/b81c8575a9e01e64f890fee3f30c3cef99dd72ac,Yes
1255,jhu-lcsr/costar_plan,costar_google_brainrobotdata/grasp_train.py,5f2a4295f027596de6b7b0303f1883d5635f20af,TODO(ahundt) consider making image_model_weights shared vs separate configurable,https://github.com/jhu-lcsr/costar_plan/commit/5f2a4295f027596de6b7b0303f1883d5635f20af,Yes
1256,jhu-lcsr/costar_plan,costar_google_brainrobotdata/cornell_grasp_train_classification.py,255eab1cd9236c5f0ba357cee5361897e0c7e7e5,Quite good kfold; best hyperparams from 2018-04 2000 model hyperopt run TODO(ahundt) add details from kfold run,https://github.com/jhu-lcsr/costar_plan/commit/255eab1cd9236c5f0ba357cee5361897e0c7e7e5,Yes
1257,google-research/google-research,tcc/evaluation/few_shot_classification.py,d4cc5636f1150e304ff4c478e0017f7175f93f29,TODO(debidatta): Move these hard coded params to config after expts.,https://github.com/google-research/google-research/commit/d4cc5636f1150e304ff4c478e0017f7175f93f29,Yes
1258,google-research/google-research,strategic_exploration/hrl/graph_update.py,cfca7c2cc9af9bf531a770830212264e916ccc83,TODO: In parallel mode; remove this.,https://github.com/google-research/google-research/commit/cfca7c2cc9af9bf531a770830212264e916ccc83,Yes
1259,google-research/google-research,ravens/ravens/gripper.py,6c81474714c1842c19c4971cdede294e17827250,Parallel-Jaw Two-Finger Gripper (TODO: fix),https://github.com/google-research/google-research/commit/6c81474714c1842c19c4971cdede294e17827250,Yes
1260,pandas-profiling/pandas-profiling,src/pandas_profiling/report/presentation/flavours/widget/variable.py,123e443a484e6ea27adbdf55e1d357984039254e,TODO: use `ignore` param,https://github.com/pandas-profiling/pandas-profiling/commit/123e443a484e6ea27adbdf55e1d357984039254e,Yes
1261,OpenNMT/OpenNMT-py,word_language_model/main.py,6fde116a1bb2563cf51ed607fb32d6fb90bf0c75,FIXME: we need better reset_parameters methods in stdlib,https://github.com/OpenNMT/OpenNMT-py/commit/6fde116a1bb2563cf51ed607fb32d6fb90bf0c75,Yes
1262,OpenNMT/OpenNMT-py,word_language_model/model.py,d15ec955693c8daa6c975ea3b4897d7cfe63dfdb,FIXME: add stdv named argument to reset_parameters,https://github.com/OpenNMT/OpenNMT-py/commit/d15ec955693c8daa6c975ea3b4897d7cfe63dfdb,Yes
1263,OpenNMT/OpenNMT-py,OpenNMT/onmt/utils/Parallel.py,3bd3fb3abc023cef914a0121fdc01dafb9818f05,TODO - this is memory costly since we need to clone full parameters from one GPU to another,https://github.com/OpenNMT/OpenNMT-py/commit/3bd3fb3abc023cef914a0121fdc01dafb9818f05,Yes
1264,OpenNMT/OpenNMT-py,OpenNMT/onmt/utils/Parallel.py,45bd1a929be8222c01fc7f07dc544f0160f60e78,# TODO - this is memory costly since we need to clone full parameters from one GPU to another,https://github.com/OpenNMT/OpenNMT-py/commit/45bd1a929be8222c01fc7f07dc544f0160f60e78,Yes
1265,mlflow/mlflow,mlflow/pyfunc/__init__.py,7a69ba64ef269b2a5534e16fe888050a83224ef3,"\""\""\""Export \/ Import of generic python models. ||  || This module defines generic filesystem format for python models and provides utilities || for saving and loading to and from this format. The format is self contained in a sense || that it includes all necessary information for anyone to load it and use it. Dependencies || are either stored directly with the model or referenced via a conda environment. ||  || The convention for pyfunc models is to have a predict method or function with the following || signature ||  || predict(data: pandas.DataFrame) -> pandas.DataFrame ||  || This convention is relied upon by other mlflow components. ||  || Pyfunc model format is defined as a directory structure containing all required data; code and || configuration: ||  || .\/dst-path\/ ||     .\/MLmodel - config ||     <code> - any code packaged with the model (specified in the conf file; see below) ||     <data> - any data packaged with the model (specified in the conf file; see below) ||     <env>  - conda environment definition (specified in the conf file; see below) ||  || It must contain MLmodel file in its root with \""python_function\"" format with the following || parameters: ||  ||    - loader_module [required]: ||          Python module that can load the model. Expected as module identifier ||           e.g. ``mlflow.sklearn``; it will be imported via importlib.import_module. ||          The imported module must contain function with the following signature: ||  ||               load_pyfunc(path: string) -> <pyfunc model> ||  ||          The path argument is specified by the data parameter and may refer to a file or directory. ||  ||    - code [optional]: ||         relative path to a directory containing the code packaged with this model. ||         All files and directories inside this directory are added to the python path ||         prior to importing the model loader. ||  ||    - data [optional]: ||          relative path to a file or directory containing model data. ||          the path is passed to the model loader. ||  ||    - env [optional]: ||          relative path to an exported conda environment. If present this environment ||          should be activated prior to running the model. ||  || Example: ||  || ``` || >tree example\/sklearn_iris\/mlruns\/run1\/outputs\/linear-lr || \u251C\u2500\u2500 MLmodel || \u251C\u2500\u2500 code || \u2502\u00A0\u00A0 \u251C\u2500\u2500 sklearn_iris.py || \u2502\u00A0\u00A0 || \u251C\u2500\u2500 data || \u2502\u00A0\u00A0 \u2514\u2500\u2500 model.pkl || \u2514\u2500\u2500 mlflow_env.yml ||  || >cat example\/sklearn_iris\/mlruns\/run1\/outputs\/linear-lr\/MLmodel || python_function: ||   code: code ||   data: data\/model.pkl ||   env: mlflow_env.yml ||   main: sklearn_iris ||  || ``` || Todo: || * Get default conda_env of the project. || \""\""\""",https://github.com/mlflow/mlflow/commit/7a69ba64ef269b2a5534e16fe888050a83224ef3,Yes
1266,mlflow/mlflow,mlflow/store/sqlalchemy_store.py,bf4ccba3f001c0e1187a15d0f02ca251769c73c8,ToDo: Consider prior checks for null; type; param name validations; ... etc.,https://github.com/mlflow/mlflow/commit/bf4ccba3f001c0e1187a15d0f02ca251769c73c8,Yes
1267,mlflow/mlflow,mlflow/pytorch/pickle_module.py,2a3764010a3ac7cda53f6df8290983c75633920b,"\""\""\"" || This module imports contents from CloudPickle in a way that is compatible with the || ``pickle_module`` parameter of PyTorch's model persistence function: ``torch.save`` || (see https:\/\/github.com\/pytorch\/pytorch\/blob\/692898fe379c9092f5e380797c32305145cd06e1\/torch\/ || serialization.py#L192). It is included as a distinct module from :mod:`mlflow.pytorch` to avoid || polluting the namespace with wildcard imports. ||  || Calling ``torch.save(...; pickle_module=mlflow.pytorch.pickle_module)`` will persist PyTorch model || definitions using CloudPickle; leveraging improved pickling functionality such as the ability || to capture class definitions in the \""__main__\"" scope. ||  || TODO: Remove this module or make it an alias of CloudPickle when CloudPickle and PyTorch have || compatible pickling APIs. || \""\""\""",https://github.com/mlflow/mlflow/commit/2a3764010a3ac7cda53f6df8290983c75633920b,Yes
1268,mlflow/mlflow,mlflow/sklearn/__init__.py,77ebf43f8ff523ceb5b511a6e182c103be2d989f,TODO: We should not log nested estimator parameters for,https://github.com/mlflow/mlflow/commit/77ebf43f8ff523ceb5b511a6e182c103be2d989f,Yes
1269,mlflow/mlflow,tests/keras/test_keras_model_export.py,322b766ee19f5ae7aa018ce25b3596474dc139f9,TODO: Unskip these parameters once https:\/\/github.com\/h5py\/h5py\/issues\/1732 is fixed,https://github.com/mlflow/mlflow/commit/322b766ee19f5ae7aa018ce25b3596474dc139f9,Yes
1270,ludwig-ai/ludwig,tests/ludwig/models/modules/test_encoder.py,c01767ddffaba3dbedb6ac52c883f9383a7f7efb,TODO Figure out the output size for parallel 1d conv,https://github.com/ludwig-ai/ludwig/commit/c01767ddffaba3dbedb6ac52c883f9383a7f7efb,Yes
1271,ludwig-ai/ludwig,ludwig/models/model.py,b1268d78355c994594a3629539ef9de276909117,todo tf2: reintroduce optimizer parameters,https://github.com/ludwig-ai/ludwig/commit/b1268d78355c994594a3629539ef9de276909117,Yes
1272,ludwig-ai/ludwig,ludwig/models/model.py,ef667de0580f28c3c55352523e5fb0f4659d61c2,todo tf2: reintroduce optimizer parameters,https://github.com/ludwig-ai/ludwig/commit/ef667de0580f28c3c55352523e5fb0f4659d61c2,Yes
1273,ludwig-ai/ludwig,ludwig/models/modules/loss_modules.py,d3f677b7357e4e08614f91981e4f8f1ed40fc291,todo tf2 need to determine how to handle this; possible in separate function,https://github.com/ludwig-ai/ludwig/commit/d3f677b7357e4e08614f91981e4f8f1ed40fc291,Yes
1274,ludwig-ai/ludwig,ludwig/hyperopt.py,0d2fc91ac51ff4d7726b9eade5d27b53c4693c1f,TODO: Yields a set of parameters names and their values.,https://github.com/ludwig-ai/ludwig/commit/0d2fc91ac51ff4d7726b9eade5d27b53c4693c1f,Yes
1275,ludwig-ai/ludwig,ludwig/models/ecd.py,a0d860d4ec3854c37c352613bf93913b6302327f,TODO add parameter for how much regularization,https://github.com/ludwig-ai/ludwig/commit/a0d860d4ec3854c37c352613bf93913b6302327f,Yes
1276,ludwig-ai/ludwig,ludwig/models/modules/sequence_decoders.py,2950d48e53aa9a3e026894f39be8f50fe47de01d,todo tf2 add other required parameters for Dense layer,https://github.com/ludwig-ai/ludwig/commit/2950d48e53aa9a3e026894f39be8f50fe47de01d,Yes
1277,ludwig-ai/ludwig,ludwig/models/model.py,d35f8f67473a98df298ac96a17b6d8ccf170c136,todo tf2: reintroduce optimizer parameters,https://github.com/ludwig-ai/ludwig/commit/d35f8f67473a98df298ac96a17b6d8ccf170c136,Yes
1278,ludwig-ai/ludwig,ludwig/models/model.py,91bc289ceee24916533e81e56b5497413b4a95fe,todo tf2: reintroduce optimizer parameters,https://github.com/ludwig-ai/ludwig/commit/91bc289ceee24916533e81e56b5497413b4a95fe,Yes
1279,ludwig-ai/ludwig,ludwig/models/model.py,2ad48fd896a8c46ccfb91c3ffd448f0c442985ee,todo tf2: reintroduce optimizer parameters,https://github.com/ludwig-ai/ludwig/commit/2ad48fd896a8c46ccfb91c3ffd448f0c442985ee,Yes
1280,ludwig-ai/ludwig,ludwig/models/model.py,acb8bed03b3603a65a157b17315629e4b0a932eb,todo tf2: reintroduce optimizer parameters,https://github.com/ludwig-ai/ludwig/commit/acb8bed03b3603a65a157b17315629e4b0a932eb,Yes
1281,ludwig-ai/ludwig,ludwig/models/modules/optimization_modules.py,3ad7af3ea2ed7f79f24c8f09dbfcf08a08b339e5,todo tf2: improve this class with better names and parameters,https://github.com/ludwig-ai/ludwig/commit/3ad7af3ea2ed7f79f24c8f09dbfcf08a08b339e5,Yes
1282,ludwig-ai/ludwig,ludwig/models/model.py,cd2cbe84f7578f843795be0968c65be5c1181935,todo tf2: reintroduce optimizer parameters,https://github.com/ludwig-ai/ludwig/commit/cd2cbe84f7578f843795be0968c65be5c1181935,Yes
1283,ludwig-ai/ludwig,ludwig/api.py,b7a1ade7b7ac4c835f31877cd532d3fe0deb1bbc,todo refactoring: maybe replace the self.model_definition paramter,https://github.com/ludwig-ai/ludwig/commit/b7a1ade7b7ac4c835f31877cd532d3fe0deb1bbc,Yes
1284,ludwig-ai/ludwig,ludwig/data/preprocessing.py,496009ec369f8d78311afde2b62123596122f076,TODO dask: find a way to better parallelize this operation,https://github.com/ludwig-ai/ludwig/commit/496009ec369f8d78311afde2b62123596122f076,Yes
1285,ludwig-ai/ludwig,ludwig/data/preprocessing.py,bb7bd0505d9b2dd5ad428b1929ed77c24ccf4917,todo figure out if global_preprocessing_parameters is needed,https://github.com/ludwig-ai/ludwig/commit/bb7bd0505d9b2dd5ad428b1929ed77c24ccf4917,Yes
1286,ludwig-ai/ludwig,ludwig/data/preprocessing.py,bb7bd0505d9b2dd5ad428b1929ed77c24ccf4917,todo figure out if additional parameters are needed,https://github.com/ludwig-ai/ludwig/commit/bb7bd0505d9b2dd5ad428b1929ed77c24ccf4917,Yes
1287,ludwig-ai/ludwig,ludwig/data/preprocessing.py,79e6559fd7607bab4b18aba132dffeecf036060c,todo figure out if global_preprocessing_parameters is needed,https://github.com/ludwig-ai/ludwig/commit/79e6559fd7607bab4b18aba132dffeecf036060c,Yes
1288,ludwig-ai/ludwig,ludwig/data/preprocessing.py,79e6559fd7607bab4b18aba132dffeecf036060c,todo figure out if additional parameters are needed,https://github.com/ludwig-ai/ludwig/commit/79e6559fd7607bab4b18aba132dffeecf036060c,Yes
1289,pyro-ppl/pyro,pyro/infer/iwelbo.py,6ea088a1b39787e5d3ab87381300f85b993e891d,TODO: Make this more efficient with respect to params,https://github.com/pyro-ppl/pyro/commit/6ea088a1b39787e5d3ab87381300f85b993e891d,Yes
1290,pyro-ppl/pyro,pyro/infer/kl_qp.py,6ea088a1b39787e5d3ab87381300f85b993e891d,TODO: Make this more efficient with respect to params,https://github.com/pyro-ppl/pyro/commit/6ea088a1b39787e5d3ab87381300f85b993e891d,Yes
1291,pyro-ppl/pyro,pyro/infer/trace_klqp.py,91ed170d419456483c7c972b379c2b1ca7b5f165,TODO: Make this more efficient with respect to params,https://github.com/pyro-ppl/pyro/commit/91ed170d419456483c7c972b379c2b1ca7b5f165,Yes
1292,pyro-ppl/pyro,pyro/infer/trace_klqp.py,6d49fd973974e89328d3db482faee7eee0f71587,TODO get reparam right,https://github.com/pyro-ppl/pyro/commit/6d49fd973974e89328d3db482faee7eee0f71587,Yes
1293,pyro-ppl/pyro,pyro/infer/search.py,9756b2052a7b53bbaa35435b1357e0eef1a698e7,TODO parallelize,https://github.com/pyro-ppl/pyro/commit/9756b2052a7b53bbaa35435b1357e0eef1a698e7,Yes
1294,pyro-ppl/pyro,pyro/infer/search.py,96559fb9a00247d7e112904822ba55d1b331751a,TODO parallelize,https://github.com/pyro-ppl/pyro/commit/96559fb9a00247d7e112904822ba55d1b331751a,Yes
1295,pyro-ppl/pyro,pyro/infer/importance.py,9c6e69fb6093a03804d0d0b462ec36c620b3c48b,TODO parallelize,https://github.com/pyro-ppl/pyro/commit/9c6e69fb6093a03804d0d0b462ec36c620b3c48b,Yes
1296,pyro-ppl/pyro,pyro/infer/search.py,9c6e69fb6093a03804d0d0b462ec36c620b3c48b,TODO parallelize,https://github.com/pyro-ppl/pyro/commit/9c6e69fb6093a03804d0d0b462ec36c620b3c48b,Yes
1297,pyro-ppl/pyro,pyro/distributions/torch_wrapper.py,952e138954cb72676e363e37935617b9b344f75f,TODO define .reparameterized; .enumerable properties once RandomPrimitive is gone,https://github.com/pyro-ppl/pyro/commit/952e138954cb72676e363e37935617b9b344f75f,Yes
1298,pyro-ppl/pyro,pyro/contrib/gp/__init__.py,168517d164aff0909f3798a8eff43bd2f60ca7e6,TODO: use `constraint_to` inside `pyro.param(...)` when available,https://github.com/pyro-ppl/pyro/commit/168517d164aff0909f3798a8eff43bd2f60ca7e6,Yes
1299,pyro-ppl/pyro,pyro/distributions/torch/__init__.py,6a55af873d7c44f98502e13d773cd92cbd6c5bd8,TODO rename parameters so these can be imported automatically.,https://github.com/pyro-ppl/pyro/commit/6a55af873d7c44f98502e13d773cd92cbd6c5bd8,Yes
1300,pyro-ppl/pyro,tests/test_examples.py,1bbfb48aa280dab36ea8b1ff17826665e1fd9eb6,TODO fix examples to work with --enum-discrete=parallel,https://github.com/pyro-ppl/pyro/commit/1bbfb48aa280dab36ea8b1ff17826665e1fd9eb6,Yes
1301,pyro-ppl/pyro,pyro/util.py,fe32be3b80a9224ba623d1752078642886d65b21,TODO Check parallel dimensions on the left of max_iarange_nesting.,https://github.com/pyro-ppl/pyro/commit/fe32be3b80a9224ba623d1752078642886d65b21,Yes
1302,pyro-ppl/pyro,tests/infer/test_conjugate_gradients.py,748e3446727295857a3603c25c7fb66816b6902a,TODO increase precision and number of particles once latter is parallelized properly,https://github.com/pyro-ppl/pyro/commit/748e3446727295857a3603c25c7fb66816b6902a,Yes
1303,pyro-ppl/pyro,pyro/contrib/gp/util.py,5e42018b4694388b9f245ca82eb6ed4c65797882,TODO: use `constraint_to` inside `pyro.param(...)` when available,https://github.com/pyro-ppl/pyro/commit/5e42018b4694388b9f245ca82eb6ed4c65797882,Yes
1304,pyro-ppl/pyro,tests/integration_tests/test_advi.py,2bdeb5c1808bd04fbc82cd0f25b621aa16aded79,TODO speed up with parallel num_particles > 1,https://github.com/pyro-ppl/pyro/commit/2bdeb5c1808bd04fbc82cd0f25b621aa16aded79,Yes
1305,pyro-ppl/pyro,pyro/infer/mcmc/util.py,0113e0b349f12ddd29d4163802eeee01bdd77b42,TODO: expose init_strategy using separate functions.,https://github.com/pyro-ppl/pyro/commit/0113e0b349f12ddd29d4163802eeee01bdd77b42,Yes
1306,ad12/DOSMA,file_constants.py,89335c55d1ea2da659931399c4cd805420647b66,TODO (erubin3): Change the parameter files needed for registration,https://github.com/ad12/DOSMA/commit/89335c55d1ea2da659931399c4cd805420647b66,Yes
1307,astier/model-free-episodic-control,main.py,02ca7d6c330a8f8a3a3fb6ca369b7697c5e612f5,TODO parameters as json-config,https://github.com/astier/model-free-episodic-control/commit/02ca7d6c330a8f8a3a3fb6ca369b7697c5e612f5,Yes
1308,brendanhasz/probflow,src/probflow/core.py,e80c95f1c6dca0447d57d87b54c37f124aee0263,TODO: recurse down the model; setting param._session = sess for each parameter,https://github.com/brendanhasz/probflow/commit/e80c95f1c6dca0447d57d87b54c37f124aee0263,Yes
1309,brendanhasz/probflow,src/probflow/core.py,952505c94c7c4422a8762f0e6f2b4cb41889eef6,TODO: recurse down the model; setting param._session = sess for each parameter,https://github.com/brendanhasz/probflow/commit/952505c94c7c4422a8762f0e6f2b4cb41889eef6,Yes
1310,brendanhasz/probflow,src/probflow/layers.py,5450bb84847a860f518dafc76bb735d298d29603,TODO: this won't work; Parameter.build also needs the batch_shape!,https://github.com/brendanhasz/probflow/commit/5450bb84847a860f518dafc76bb735d298d29603,Yes
1311,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,a2188004018a6c6e14784932675bfd521c8081e8,TODO: test 2D X and params,https://github.com/brendanhasz/probflow/commit/a2188004018a6c6e14784932675bfd521c8081e8,Yes
1312,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,1c0df3d4f9131d1193faee4321bf3ace08039b2b,TODO: only record one param,https://github.com/brendanhasz/probflow/commit/1c0df3d4f9131d1193faee4321bf3ace08039b2b,Yes
1313,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,6651bc37e129d195fb99f028162b00a8072fff9b,TODO: test 2D X and params,https://github.com/brendanhasz/probflow/commit/6651bc37e129d195fb99f028162b00a8072fff9b,Yes
1314,brendanhasz/probflow,src/probflow/parameters.py,a2db7740ccffe67f66c82fc83c05b31c9a78a74c,TODO: DeterministicParameter,https://github.com/brendanhasz/probflow/commit/a2db7740ccffe67f66c82fc83c05b31c9a78a74c,Yes
1315,brendanhasz/probflow,src/probflow/modules.py,733fe5906b0189b5457b71ea55b4283b71683322,TODO: look for variables NOT in parameters too,https://github.com/brendanhasz/probflow/commit/733fe5906b0189b5457b71ea55b4283b71683322,Yes
1316,brendanhasz/probflow,src/probflow/modules/module.py,6b961353b37189581ef76086b6e103633e947b84,TODO: look for variables NOT in parameters too,https://github.com/brendanhasz/probflow/commit/6b961353b37189581ef76086b6e103633e947b84,Yes
1317,cyschneck/Hydra,raw_text_processing.py,7fb6dff72bf33bb692e445eb2552f32cb2e73e93,TODO: set up with average paragraph length as size_sentences,https://github.com/cyschneck/Hydra/commit/7fb6dff72bf33bb692e445eb2552f32cb2e73e93,Yes
1318,hoya012/shake-shake-tensorflow,models/nn.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME if you change 'first_channel' parameter,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,Yes
1319,hoya012/shake-shake-tensorflow,test.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Test hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,Yes
1320,hoya012/shake-shake-tensorflow,train.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Training hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,Yes
1321,hoya012/shake-shake-tensorflow,train.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Regularization hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,Yes
1322,hoya012/shake-shake-tensorflow,train.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Evaluation hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,Yes
1323,M4gicT0/hybrid-dataset-factory,render.py,1351ef919b2fd39eb7e57a4dc78511a9ce49e80d,''' ||     ----- TODO ----- ||  || - Random positioning of the gate || - Boundaries definition for the gate || - Camera calibration (use the correct parameters) || - Project on transparent background || - Overlay with background image || - Histogram equalization of both images (hue; saturation; luminence ?...) || - Motion blur (shader ?) || - Anti alisasing (shader ?) || - Ship it! ||  || ''',https://github.com/M4gicT0/hybrid-dataset-factory/commit/1351ef919b2fd39eb7e57a4dc78511a9ce49e80d,Yes
1324,M4gicT0/hybrid-dataset-factory,dataset_factory.py,34679e0b552e7e146c4277f139f4aa7d7b6829b0,''' ||     ----- TODO ----- ||  || [ ] Match the perspective via camera height estimation (with camera || calibration) || [ ] WHY IS IT SO UGLY???! || [ ] Thread it! || [x] Random positioning of the gate || [x] Boundaries definition for the gate (relative to the mesh's size) || [x] Compute the center of the gate || [ ] Compute the presence of the gate in the image frame || [?] Compute the distance to the gate || [ ] Camera calibration (use the correct parameters) || [x] Project on transparent background || [x] Overlay with background image || [ ] Model the camera distortion || [ ] Apply the distortion to the OpenGL projection || [ ] Histogram equalization of both images (hue; saturation; luminence ?...) || [ ] Motion blur (shader ?) || [ ] Anti alisasing (shader ?) || [ ] Ship it! ||  || ''',https://github.com/M4gicT0/hybrid-dataset-factory/commit/34679e0b552e7e146c4277f139f4aa7d7b6829b0,Yes
1325,Shenggan/DeepCell-Keras,deepcell/tifffile/tifffile.py,e43b302cf0952a147a44dfd5af9cbbc053c94071,TODO: handle photometric == 'separated' (CMYK),https://github.com/Shenggan/DeepCell-Keras/commit/e43b302cf0952a147a44dfd5af9cbbc053c94071,Yes
1326,thiagopbueno/tf-plan,tfplan/planners/environment.py,784b842d6593be8a5c158400c49378b00f0ea0ab,TODO: use dtype parameter,https://github.com/thiagopbueno/tf-plan/commit/784b842d6593be8a5c158400c49378b00f0ea0ab,Yes
1327,woongbinchoi/English-Premier-League-Prediction,predict.py,73bedb4efce299342e56f055e6ee15ae9355c435,TODO: Create the parameters list you wish to tune,https://github.com/woongbinchoi/English-Premier-League-Prediction/commit/73bedb4efce299342e56f055e6ee15ae9355c435,Yes
1328,zappybiby/EuroTruckAutopilot,image_process.py,39fdde432e2bdf391a3cab46c97ff01d9deb9352,TODO: Canny paramaters are the worst,https://github.com/zappybiby/EuroTruckAutopilot/commit/39fdde432e2bdf391a3cab46c97ff01d9deb9352,Yes
1329,apacha/Mensural-Detector,tensorflow_object_detection/meta_architectures/ssd_meta_arch.py,22404eb2931793c36fd2ecf361b81166eed6726b,TODO: revisit whether to always use batch size as the number of parallel,https://github.com/apacha/Mensural-Detector/commit/22404eb2931793c36fd2ecf361b81166eed6726b,Yes
1330,apacha/Mensural-Detector,slim/deployment/model_deploy.py,f54532c1ddedb69c7531d13ad793c251a87957d3,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/apacha/Mensural-Detector/commit/f54532c1ddedb69c7531d13ad793c251a87957d3,Yes
1331,flairNLP/flair,flair/embeddings.py,c9a75e887c1517cbea603a527406ff9dd05ca97c,token_ids = [torch.LongTensor(self.spm.SampleEncodeAsIds(sentence.to_original_text(); self.length; self.alpha[self.training])) for sentence in sentences]  # TODO: expose these params; we want this to be random in training; but fixed in inference,https://github.com/flairNLP/flair/commit/c9a75e887c1517cbea603a527406ff9dd05ca97c,Yes
1332,HazyResearch/fonduer,async_annotations.py,4416afd52a8fe4366dc05514ba9fdec35c66f4c2,TODO: move this for-loop computation to database for automatic parallelization;,https://github.com/HazyResearch/fonduer/commit/4416afd52a8fe4366dc05514ba9fdec35c66f4c2,Yes
1333,HazyResearch/fonduer,fonduer/supervision/async_annotations.py,eba02e1e4415e08c412e2174de2ac16d2a146c02,TODO: move this for-loop computation to database for automatic parallelization;,https://github.com/HazyResearch/fonduer/commit/eba02e1e4415e08c412e2174de2ac16d2a146c02,Yes
1334,HazyResearch/fonduer,fonduer/parser/spacy_parser.py,33e55ce2ea328489062384ca09697d8dd2a2265d,TODO: We could do this in parallel. Test speedup in the future,https://github.com/HazyResearch/fonduer/commit/33e55ce2ea328489062384ca09697d8dd2a2265d,Yes
1335,HealthCatalyst/healthcareai-py,hcpytools/deploy_supervised_model.py,39002d735f4e8d32729496eefff353014e5cc68c,TODO: add try catch to improve message about what parameters to change in case of error,https://github.com/HealthCatalyst/healthcareai-py/commit/39002d735f4e8d32729496eefff353014e5cc68c,Yes
1336,HealthCatalyst/healthcareai-py,Example4_advanced.py,234f35ccfd182aaa2b37dfa0de6574cdfb91d6c3,TODO these are bogus hyperparams for random forest,https://github.com/HealthCatalyst/healthcareai-py/commit/234f35ccfd182aaa2b37dfa0de6574cdfb91d6c3,Yes
1337,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,c462b034b0765b50bd7c0b2d83256c05f4bd9f8a,TODO sensible default hyperparameter grid,https://github.com/HealthCatalyst/healthcareai-py/commit/c462b034b0765b50bd7c0b2d83256c05f4bd9f8a,Yes
1338,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,58175722ed069a677c04034d139a216bc66641ff,TODO add sensible SGD classifier hyperparameter grid,https://github.com/HealthCatalyst/healthcareai-py/commit/58175722ed069a677c04034d139a216bc66641ff,Yes
1339,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,58175722ed069a677c04034d139a216bc66641ff,TODO add sensible hyperparameter grid,https://github.com/HealthCatalyst/healthcareai-py/commit/58175722ed069a677c04034d139a216bc66641ff,Yes
1340,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,0d0ec283df9b7f610a054fa8a286e80919eff3a9,TODO this may need to ferret out each classification score separately,https://github.com/HealthCatalyst/healthcareai-py/commit/0d0ec283df9b7f610a054fa8a286e80919eff3a9,Yes
1341,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,a36fc5f5aa194f0775860b87d2e06cfce8534026,TODO Could these be trained separately then after the best is found; train the factor model and add to TSM?,https://github.com/HealthCatalyst/healthcareai-py/commit/a36fc5f5aa194f0775860b87d2e06cfce8534026,Yes
1342,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,a36fc5f5aa194f0775860b87d2e06cfce8534026,TODO should the factor model be either 1) optional or 2) separate?,https://github.com/HealthCatalyst/healthcareai-py/commit/a36fc5f5aa194f0775860b87d2e06cfce8534026,Yes
1343,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,7a22e6e88fd7599f4ed528610053d2b42d2a5cf6,TODO should the factor model be either 1) optional or 2) separate?,https://github.com/HealthCatalyst/healthcareai-py/commit/7a22e6e88fd7599f4ed528610053d2b42d2a5cf6,Yes
1344,HealthCatalyst/healthcareai-py,healthcareai/advanced_trainer.py,21c2ac2bd22098b16e2e0fab91e0be6ce4c9bae9,TODO add sensible KNN hyperparameter grid,https://github.com/HealthCatalyst/healthcareai-py/commit/21c2ac2bd22098b16e2e0fab91e0be6ce4c9bae9,Yes
1345,HealthCatalyst/healthcareai-py,healthcareai/trainer.py,1b1f6e035cee6325152c2cae579ab951aca45413,TODO consider making a scoring parameter (which will necessitate some more logic,https://github.com/HealthCatalyst/healthcareai-py/commit/1b1f6e035cee6325152c2cae579ab951aca45413,Yes
1346,hellohaptik/chatbot_ner,ner_v1/detectors/textual/name/name_detection.py,430ae55b3f9887439150f70c871fdb6a5b073365,TODO: Tokenization issue where trailing '.'s are considered a separate token,https://github.com/hellohaptik/chatbot_ner/commit/430ae55b3f9887439150f70c871fdb6a5b073365,Yes
1347,hellohaptik/chatbot_ner,datastore/datastore.py,1ccbae3f9e59b47d763f14f7aaa13a881b907574,FIXME: Inconsistent data format with other APIs or confusing parameter names!,https://github.com/hellohaptik/chatbot_ner/commit/1ccbae3f9e59b47d763f14f7aaa13a881b907574,Yes
1348,hellohaptik/chatbot_ner,datastore/datastore.py,79bf165ea8acaccf5834d28b0199fe264e78f560,FIXME: Inconsistent data format with other APIs or confusing parameter names!,https://github.com/hellohaptik/chatbot_ner/commit/79bf165ea8acaccf5834d28b0199fe264e78f560,Yes
1349,hellohaptik/chatbot_ner,es_datastore/tests/test_elastic_search.py,49584a11b398a7cf26957f927e4166b3827816fc,:TODO: configure parameters here,https://github.com/hellohaptik/chatbot_ner/commit/49584a11b398a7cf26957f927e4166b3827816fc,Yes
1350,X-DataInitiative/tick,tick/optim/prox/__init__.py,86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,TODO: add ProxSeparable,https://github.com/X-DataInitiative/tick/commit/86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,Yes
1351,X-DataInitiative/tick,tick/survival/convolutional_sccs.py,44925fde378750b918604bb92a599cc9cf1db317,TODO later: exploit new options of SVRG (parallel fit; variance_reduction...),https://github.com/X-DataInitiative/tick/commit/44925fde378750b918604bb92a599cc9cf1db317,Yes
1352,X-DataInitiative/tick,tick/survival/convolutional_sccs.py,44925fde378750b918604bb92a599cc9cf1db317,TODO later: parallelize CV,https://github.com/X-DataInitiative/tick/commit/44925fde378750b918604bb92a599cc9cf1db317,Yes
1353,X-DataInitiative/tick,tick/survival/convolutional_sccs.py,44925fde378750b918604bb92a599cc9cf1db317,TODO later: parallelize bootstrap (everything should be pickable...),https://github.com/X-DataInitiative/tick/commit/44925fde378750b918604bb92a599cc9cf1db317,Yes
1354,X-DataInitiative/tick,tick/survival/convolutional_sccs.py,44925fde378750b918604bb92a599cc9cf1db317,TODO later: fix parallel preprocessing,https://github.com/X-DataInitiative/tick/commit/44925fde378750b918604bb92a599cc9cf1db317,Yes
1355,X-DataInitiative/tick,tick/survival/simu_sccs.py,44925fde378750b918604bb92a599cc9cf1db317,TODO later: add properties for these parameters,https://github.com/X-DataInitiative/tick/commit/44925fde378750b918604bb92a599cc9cf1db317,Yes
1356,X-DataInitiative/tick,tick/survival/simu_sccs.py,44925fde378750b918604bb92a599cc9cf1db317,TODO later: allow to change this parameter,https://github.com/X-DataInitiative/tick/commit/44925fde378750b918604bb92a599cc9cf1db317,Yes
1357,anki/vector-python-sdk,anki_vector/lights.py,66aa0966ea388fe1739c468856ec5d13a4775827,TODO Needs docs; param types; sample code,https://github.com/anki/vector-python-sdk/commit/66aa0966ea388fe1739c468856ec5d13a4775827,Yes
1358,anki/vector-python-sdk,anki_vector/lights.py,66aa0966ea388fe1739c468856ec5d13a4775827,TODO needs docs; param types. Should this be private? Maybe a more descriptive name?,https://github.com/anki/vector-python-sdk/commit/66aa0966ea388fe1739c468856ec5d13a4775827,Yes
1359,anki/vector-python-sdk,anki_vector/robot.py,66aa0966ea388fe1739c468856ec5d13a4775827,TODO For both Robot and AsyncRobot; consider adding equivalent of use_3d_viewer param so OpenGLViewer starts automatically.,https://github.com/anki/vector-python-sdk/commit/66aa0966ea388fe1739c468856ec5d13a4775827,Yes
1360,bigmlcom/python,bigml/predicate.py,994243aa2e7cc0ef092126628e1c68576d20eb5a,TODO: items have different attributes and separator,https://github.com/bigmlcom/python/commit/994243aa2e7cc0ef092126628e1c68576d20eb5a,Yes
1361,dipy/dipy,dipy/align/metrics.py,fbe206e8459e6be7196ff4012cbd6660ebd7a104,Fixme: this parameter must be given as input,https://github.com/dipy/dipy/commit/fbe206e8459e6be7196ff4012cbd6660ebd7a104,Yes
1362,msracver/Deep-Exemplar-based-Colorization,similarity_subnet/tools/extra/summarize.py,3c7bc401373ece785bb4b773356335bd8656f71f,TODO support rectangular\/ND parameters,https://github.com/msracver/Deep-Exemplar-based-Colorization/commit/3c7bc401373ece785bb4b773356335bd8656f71f,Yes
1363,datmo/datmo,datmo/controller/environment/driver/dockerenv.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: separate methods for instantiation into init function below,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,Yes
1364,datmo/datmo,datmo/controller/task.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,"TODO: Fix DAL to keep objects in sync and remove \""environment_file_collection_id\"" as passed param",https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,Yes
1365,datmo/datmo,datmo/controller/task.py,a5bebabfc9cea45640626c3973e636b6f3ac5027,"TODO: Fix DAL to keep objects in sync and remove \""environment_file_collection_id\"" as passed param",https://github.com/datmo/datmo/commit/a5bebabfc9cea45640626c3973e636b6f3ac5027,Yes
1366,datmo/datmo,datmo/monitoring.py,cc7c84114561406516b3b6b40057534164017fc6,TODO: separate deployment into another file,https://github.com/datmo/datmo/commit/cc7c84114561406516b3b6b40057534164017fc6,Yes
1367,datmo/datmo,datmo/tests/test_monitoring.py,cc7c84114561406516b3b6b40057534164017fc6,TODO: separate deployment into another file,https://github.com/datmo/datmo/commit/cc7c84114561406516b3b6b40057534164017fc6,Yes
1368,h2oai/h2o4gpu,src/interface_py/h2o4gpu/solvers/xgboost.py,7ad340085f67262fc158cb663eaecef43cd6d01e,TODO: num_parallel_tree = 100; subsample = 0.5; colsample_bytree =0.5; nrounds = 1;,https://github.com/h2oai/h2o4gpu/commit/7ad340085f67262fc158cb663eaecef43cd6d01e,Yes
1369,h2oai/h2o4gpu,src/interface_py/h2o4gpu/solvers/linear_regression.py,c924ae97313324eedaa3230f55e0cbf40b07f6ba,TODO score for DAAL? input parameters are not clear,https://github.com/h2oai/h2o4gpu/commit/c924ae97313324eedaa3230f55e0cbf40b07f6ba,Yes
1370,h2oai/h2o4gpu,src/interface_py/h2o4gpu/solvers/linear_regression.py,3220358eb98946aa01da977d0d41c39ade31783f,TODO score for DAAL? input parameters are not clear,https://github.com/h2oai/h2o4gpu/commit/3220358eb98946aa01da977d0d41c39ade31783f,Yes
1371,RobotLocomotion/pytorch-dense-correspondence,dataset/labelfusion.py,a4690a73c829be630f8c3a2cf2d10d1cc689b28f,Pete Todo: separate train\/val?,https://github.com/RobotLocomotion/pytorch-dense-correspondence/commit/a4690a73c829be630f8c3a2cf2d10d1cc689b28f,Yes
1372,mme/vergeml,vergeml/env.py,b5cffa8a8e532d64bc38e58baa51ec38bf30285e,TODO reserved: hyperparameters and results,https://github.com/mme/vergeml/commit/b5cffa8a8e532d64bc38e58baa51ec38bf30285e,Yes
1373,mme/vergeml,vergeml/operation.py,b5cffa8a8e532d64bc38e58baa51ec38bf30285e,TODO should all operations take the apply parameter?,https://github.com/mme/vergeml/commit/b5cffa8a8e532d64bc38e58baa51ec38bf30285e,Yes
1374,mila-iqia/babyai,scripts/train_intelligent_expert.py,94c5ff8c61413d202a87033afe4c6a6a2f1e903d,TODO: auto-adjust this parameter in function of success rate,https://github.com/mila-iqia/babyai/commit/94c5ff8c61413d202a87033afe4c6a6a2f1e903d,Yes
1375,scikit-learn-contrib/lightning,lightning/kmp.py,3ac2560eb1b22f0e5096c427c787f2a284c74cff,FIXME: this can be parallelized by using stateful iterators,https://github.com/scikit-learn-contrib/lightning/commit/3ac2560eb1b22f0e5096c427c787f2a284c74cff,Yes
1376,jupyter/nbdime,nbdime/diffing/deep.py,ce953884ae1799445dd8648d8029be44f9faaca5,TODO: Providing separate comparison predicate for,https://github.com/jupyter/nbdime/commit/ce953884ae1799445dd8648d8029be44f9faaca5,Yes
1377,jupyter/nbdime,nbdime/diffing/generic.py,880eb41b4df6fc9c5332f84585adbd323f8d9ae0,TODO: Providing separate comparison predicate for,https://github.com/jupyter/nbdime/commit/880eb41b4df6fc9c5332f84585adbd323f8d9ae0,Yes
1378,jupyter/nbdime,nbdime/merging/notebooks.py,025a4ff28e30a879226f08cc6303624b9d50347c,Pass on diff to external difftool (TODO: store in global metadata at end instead of in separate diff dicts?),https://github.com/jupyter/nbdime/commit/025a4ff28e30a879226f08cc6303624b9d50347c,Yes
1379,jupyter/nbdime,nbdime/merging/generic.py,cb97d3ba10f91388ed27410a946447f3961a6d5d,Workaround for string merge (FIXME: Make a separate function for string merge),https://github.com/jupyter/nbdime/commit/cb97d3ba10f91388ed27410a946447f3961a6d5d,Yes
1380,jupyter/nbdime,nbdime/merging/generic.py,4ed3bca332a12a506723100ebac396b075c4c405,FIXME: Add removals to conflicts in subdecisions instead of treating them as separate diffs here?,https://github.com/jupyter/nbdime/commit/4ed3bca332a12a506723100ebac396b075c4c405,Yes
1381,deepchem/deepchem,deep_chem/utils/dataset.py,403bcfd1c12e996d93e95502842e73570a60d8b8,TODO(rbharath\/enf): These need to be better integrated with new OO paradigm.,https://github.com/deepchem/deepchem/commit/403bcfd1c12e996d93e95502842e73570a60d8b8,Yes
1382,deepchem/deepchem,deepchem/models/tensorflow_models/__init__.py,21831e01cda03c1b32db3c171903eecc505e5db7,TODO(rbharath): config and model_params overlap significantly. Maybe just,https://github.com/deepchem/deepchem/commit/21831e01cda03c1b32db3c171903eecc505e5db7,Yes
1383,deepchem/deepchem,deepchem/molnet/load_function/pdbbind_datasets.py,215e2f118ca8394ad3ce68a7bcd032f8844cffaf,TODO: This is not the correct setting. Set hyperparameters correctly,https://github.com/deepchem/deepchem/commit/215e2f118ca8394ad3ce68a7bcd032f8844cffaf,Yes
1384,deepchem/deepchem,deepchem/hyper/gaussian_process.py,628d726e260b65e78fe371cd0d1f1b8640d937e6,"FIXME: Signature of \""hyperparam_search\"" incompatible with supertype \""HyperparamOpt\""",https://github.com/deepchem/deepchem/commit/628d726e260b65e78fe371cd0d1f1b8640d937e6,Yes
1385,deepchem/deepchem,deepchem/hyper/grid_search.py,628d726e260b65e78fe371cd0d1f1b8640d937e6,"FIXME: Signature of \""hyperparam_search\"" incompatible with supertype \""HyperparamOpt\""",https://github.com/deepchem/deepchem/commit/628d726e260b65e78fe371cd0d1f1b8640d937e6,Yes
1386,deepchem/deepchem,deepchem/hyper/grid_search.py,81309ea8b83c3b59a795982c865f3a6ff334c29f,"FIXME: Signature of \""hyperparam_search\"" incompatible with supertype \""HyperparamOpt\""",https://github.com/deepchem/deepchem/commit/81309ea8b83c3b59a795982c865f3a6ff334c29f,Yes
1387,dmlc/gluon-nlp,tests/test_layers.py,70a188776f7470c838dd22b1636462b75573a734,TODO This test even passes without sharing the parameters. It needs to be improved.,https://github.com/dmlc/gluon-nlp/commit/70a188776f7470c838dd22b1636462b75573a734,Yes
1388,IDSIA/sacred,sacred/utils.py,047eb9dd373ca0d59251a1b6cdb80f8817786eb8,TODO: Remove this and put into separate package (maybe a plugin?),https://github.com/IDSIA/sacred/commit/047eb9dd373ca0d59251a1b6cdb80f8817786eb8,Yes
1389,IDSIA/sacred,sacred/observers/sql.py,c73e5934da368992edd30827810f1eebecd9c13f,TODO: move to separate table?,https://github.com/IDSIA/sacred/commit/c73e5934da368992edd30827810f1eebecd9c13f,Yes
1390,IDSIA/sacred,sacred/observers/sql_bases.py,7fc2cf64377e3f0f1abf4840cb3b363e410b4c9a,TODO: move to separate table?,https://github.com/IDSIA/sacred/commit/7fc2cf64377e3f0f1abf4840cb3b363e410b4c9a,Yes
1391,DistrictDataLabs/yellowbrick,yellowbrick/regressor.py,742f93774c7ffa12b73dadb4389ba5edeb0c0e86,TODO: make test size a parameter and do better data storage on viz.,https://github.com/DistrictDataLabs/yellowbrick/commit/742f93774c7ffa12b73dadb4389ba5edeb0c0e86,Yes
1392,DistrictDataLabs/yellowbrick,yellowbrick/regressor.py,742f93774c7ffa12b73dadb4389ba5edeb0c0e86,TODO: better parameters based on the plot or; normalize; then push -1 to 1,https://github.com/DistrictDataLabs/yellowbrick/commit/742f93774c7ffa12b73dadb4389ba5edeb0c0e86,Yes
1393,DistrictDataLabs/yellowbrick,yellowbrick/bestfit.py,56236f3d3c7cafcad716a14f36d98bd66eeedd94,TODO: determin if xlim or X.min(); X.max() are better params,https://github.com/DistrictDataLabs/yellowbrick/commit/56236f3d3c7cafcad716a14f36d98bd66eeedd94,Yes
1394,DistrictDataLabs/yellowbrick,yellowbrick/features/radviz.py,01d599606a916492a1a43325a13dd30959a0c452,TODO: This class is identical to the Parallel Coordinates version;,https://github.com/DistrictDataLabs/yellowbrick/commit/01d599606a916492a1a43325a13dd30959a0c452,Yes
1395,DistrictDataLabs/yellowbrick,yellowbrick/features/radviz.py,01d599606a916492a1a43325a13dd30959a0c452,TODO: make this a separate function,https://github.com/DistrictDataLabs/yellowbrick/commit/01d599606a916492a1a43325a13dd30959a0c452,Yes
1396,DistrictDataLabs/yellowbrick,yellowbrick/features/rankd.py,ee754dc49da225164a2de4920916b366a8125624,TODO: This class is identical to the Parallel Coordinates version;,https://github.com/DistrictDataLabs/yellowbrick/commit/ee754dc49da225164a2de4920916b366a8125624,Yes
1397,DistrictDataLabs/yellowbrick,yellowbrick/features/scatter.py,fc94ec4cd6e0dc6204cda30b07a02a875f512a3a,TODO: make this a separate function,https://github.com/DistrictDataLabs/yellowbrick/commit/fc94ec4cd6e0dc6204cda30b07a02a875f512a3a,Yes
1398,DistrictDataLabs/yellowbrick,yellowbrick/classifier/boundaries.py,ee91da75a55036c540e64c321c8875e08c2f0262,TODO: make this a separate function,https://github.com/DistrictDataLabs/yellowbrick/commit/ee91da75a55036c540e64c321c8875e08c2f0262,Yes
1399,DistrictDataLabs/yellowbrick,tests/test_regressor/test_alphas.py,6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,TODO: parametrize with models when unittest dependency removed,https://github.com/DistrictDataLabs/yellowbrick/commit/6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,Yes
1400,DistrictDataLabs/yellowbrick,tests/test_regressor/test_alphas.py,6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,TODO: parametrize with models when unittest dependency removed (new test case),https://github.com/DistrictDataLabs/yellowbrick/commit/6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,Yes
1401,DistrictDataLabs/yellowbrick,tests/test_regressor/test_alphas.py,6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,TODO: parameterize with classifier; clusterer; decomposition,https://github.com/DistrictDataLabs/yellowbrick/commit/6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,Yes
1402,DistrictDataLabs/yellowbrick,yellowbrick/classifier/threshold.py,1e04216661ac623686b0ccde55df448d70d8de12,TODO: parallelize trials with joblib (using sklearn utility),https://github.com/DistrictDataLabs/yellowbrick/commit/1e04216661ac623686b0ccde55df448d70d8de12,Yes
1403,DistrictDataLabs/yellowbrick,yellowbrick/contrib/classifier/boundaries.py,a60bc41d74bd9f43f1baa5c2a1ca686397764126,TODO: make this a separate function,https://github.com/DistrictDataLabs/yellowbrick/commit/a60bc41d74bd9f43f1baa5c2a1ca686397764126,Yes
1404,DistrictDataLabs/yellowbrick,tests/test_features/test_rfecv.py,6e1099daee47e664f4b128c0f25af0177d6fca77,TODO: parametrize when unittest is removed,https://github.com/DistrictDataLabs/yellowbrick/commit/6e1099daee47e664f4b128c0f25af0177d6fca77,Yes
1405,pyannote/pyannote-audio,pyannote/audio/embedding/callbacks.py,617c79d64ddf7bb3475347bb102dcc7c1d2e2ec1,TODO \/ pass layer_index as parameter,https://github.com/pyannote/pyannote-audio/commit/617c79d64ddf7bb3475347bb102dcc7c1d2e2ec1,Yes
1406,pyannote/pyannote-audio,pyannote/audio/embedding/callbacks.py,892c19db20a2829b1ae935cc1a3abfd38384f010,TODO pass layer_index as parameter,https://github.com/pyannote/pyannote-audio/commit/892c19db20a2829b1ae935cc1a3abfd38384f010,Yes
1407,pyannote/pyannote-audio,pyannote/audio/features/utils.py,9e9c83cb71c42b23f79e625cc5b96ab241f70704,FIXME switch to Py3.5 and use glob 'recursive' parameter,https://github.com/pyannote/pyannote-audio/commit/9e9c83cb71c42b23f79e625cc5b96ab241f70704,Yes
1408,pyannote/pyannote-audio,scripts/speaker_embedding.py,abbfdc5b04ec1824e42cebb472626d234ff7bd36,TODO parallelize using multiprocessing,https://github.com/pyannote/pyannote-audio/commit/abbfdc5b04ec1824e42cebb472626d234ff7bd36,Yes
1409,pyannote/pyannote-audio,pyannote/audio/applications/bic_clustering.py,16fb800c2d3f6d7198ad237adcab8f60828d0982,do not parallelize clustering as it seems (TODO: check) to be,https://github.com/pyannote/pyannote-audio/commit/16fb800c2d3f6d7198ad237adcab8f60828d0982,Yes
1410,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/triplet_loss.py,4cf3746c401f53e222acdd42fd737e31157de8cb,"TODO. tune this \""10\"" hyperparameter",https://github.com/pyannote/pyannote-audio/commit/4cf3746c401f53e222acdd42fd737e31157de8cb,Yes
1411,pyannote/pyannote-audio,pyannote/audio/pipeline/speaker_diarization.py,b33c518531391823e32f2f53b62d8db3263f71b5,FIXME: be smarter about this parameter,https://github.com/pyannote/pyannote-audio/commit/b33c518531391823e32f2f53b62d8db3263f71b5,Yes
1412,pyannote/pyannote-audio,pyannote/audio/applications/change_detection.py,72afdcf46210570909bad4e3e7dfd4d81a5b4ba0,TODO -- parallelize this,https://github.com/pyannote/pyannote-audio/commit/72afdcf46210570909bad4e3e7dfd4d81a5b4ba0,Yes
1413,pyannote/pyannote-audio,pyannote/audio/applications/speech_detection.py,72afdcf46210570909bad4e3e7dfd4d81a5b4ba0,TODO -- parallelize this,https://github.com/pyannote/pyannote-audio/commit/72afdcf46210570909bad4e3e7dfd4d81a5b4ba0,Yes
1414,pyannote/pyannote-audio,pyannote/audio/pipeline/speaker_diarization.py,f8a5b5a43184a5f6939ea8d89c17d64cef49a122,FIXME: be smarter about this parameter,https://github.com/pyannote/pyannote-audio/commit/f8a5b5a43184a5f6939ea8d89c17d64cef49a122,Yes
1415,pyannote/pyannote-audio,pyannote/audio/pipeline/base.py,49db9f2ded56bf3c4dd0f67b558731a626c7be09,TODO -- parallelize this,https://github.com/pyannote/pyannote-audio/commit/49db9f2ded56bf3c4dd0f67b558731a626c7be09,Yes
1416,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/centroid_loss.py,36985df5599ff54855549689f6a6bf79f2b7969f,"TODO. tune this \""10\"" hyperparameter",https://github.com/pyannote/pyannote-audio/commit/36985df5599ff54855549689f6a6bf79f2b7969f,Yes
1417,pyannote/pyannote-audio,pyannote/audio/features/precomputed.py,63d81fee7aaaf856531f42bc514d0d90dfb03eeb,FIXME switch to Py3.5 and use glob 'recursive' parameter,https://github.com/pyannote/pyannote-audio/commit/63d81fee7aaaf856531f42bc514d0d90dfb03eeb,Yes
1418,pyannote/pyannote-audio,pyannote/audio/signal.py,c45b9b317e7d4967d371b7de55514c52c264caef,FIXME: embarrasingly parallel,https://github.com/pyannote/pyannote-audio/commit/c45b9b317e7d4967d371b7de55514c52c264caef,Yes
1419,pyannote/pyannote-audio,pyannote/audio/applications/pyannote_audio.py,608aae0a1c0a8eca6e202a67f447330bd866e7a7,FIXME: parallel is broken in pyannote.metrics,https://github.com/pyannote/pyannote-audio/commit/608aae0a1c0a8eca6e202a67f447330bd866e7a7,Yes
1420,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/contrastive_loss.py,55bcf77105abbaa9f775cc7f6e03b993e60d1c09,FIXME homogeneize the meaning of margin parameter,https://github.com/pyannote/pyannote-audio/commit/55bcf77105abbaa9f775cc7f6e03b993e60d1c09,Yes
1421,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/triplet_loss.py,55bcf77105abbaa9f775cc7f6e03b993e60d1c09,FIXME homogeneize the meaning of margin parameter,https://github.com/pyannote/pyannote-audio/commit/55bcf77105abbaa9f775cc7f6e03b993e60d1c09,Yes
1422,HDI-Project/ATM,atm/metrics.py,2af4e398324c6454ce8b73eb93a9638c77cc1730,TODO: make the rank parameter configurable,https://github.com/HDI-Project/ATM/commit/2af4e398324c6454ce8b73eb93a9638c77cc1730,Yes
1423,theislab/scanpy,scanpy/__init__.py,c22e48abe45a6ccca5918bbf689637caa4b31250,TODO: solve this in a nicer way; also get an ordered dict for params,https://github.com/theislab/scanpy/commit/c22e48abe45a6ccca5918bbf689637caa4b31250,Yes
1424,theislab/scanpy,scanpy/plotting/anndata.py,dfd48901c5b08228c41dc03614cf916f8acf6fb6,TODO: I don't know why but we have to update the plotting parameters here,https://github.com/theislab/scanpy/commit/dfd48901c5b08228c41dc03614cf916f8acf6fb6,Yes
1425,theislab/scanpy,scanpy/plotting/tools/__init__.py,4aaf3fe50d48e2aaf7a3398c80df2610aa997a75,FIXME computed_distribution: missing default parameter?,https://github.com/theislab/scanpy/commit/4aaf3fe50d48e2aaf7a3398c80df2610aa997a75,Yes
1426,theislab/scanpy,scanpy/metrics/_gearys_c.py,b3a2a6d45bff507625ae3f42d89ddd13389f6bf0,TODO: These are faster if we can compile them in parallel mode. However;,https://github.com/theislab/scanpy/commit/b3a2a6d45bff507625ae3f42d89ddd13389f6bf0,Yes
1427,summanlp/textrank,textrank/textrank_word.py,154d7a7f63025c573fef9cd875e05b7f260f8233,TODO: cambiar la sliding window por una cola; asi cuando la ventana se mueve uno a la derecha; no compara,https://github.com/summanlp/textrank/commit/154d7a7f63025c573fef9cd875e05b7f260f8233,Yes
1428,summanlp/textrank,textrank/textrank_word.py,154d7a7f63025c573fef9cd875e05b7f260f8233,"TODO: Aca se puede cambiar; para que la ventana se \""calcule\"" respecto de las palabras ya lemmatizadas",https://github.com/summanlp/textrank/commit/154d7a7f63025c573fef9cd875e05b7f260f8233,Yes
1429,summanlp/textrank,textrank/textrank_word.py,726cbcdede6f115dec4c6be205d9ac3dc8605ca0,TODO uso text.split para que quede el texto original; con las comas; asi no une conceptos separados,https://github.com/summanlp/textrank/commit/726cbcdede6f115dec4c6be205d9ac3dc8605ca0,Yes
1430,catalyst-team/catalyst,catalyst/dl/experiment/gan.py,67c6a26a9aa81d29a0fa7f174eb5b1e96c6b1618,TODO: Check for phase in state_params?,https://github.com/catalyst-team/catalyst/commit/67c6a26a9aa81d29a0fa7f174eb5b1e96c6b1618,Yes
1431,catalyst-team/catalyst,catalyst/contrib/dl/runner/wandb.py,30bfb7e8c5dce5144ee8570b87db962d32cf030e,@TODO: add params for artefacts logging,https://github.com/catalyst-team/catalyst/commit/30bfb7e8c5dce5144ee8570b87db962d32cf030e,Yes
1432,catalyst-team/catalyst,catalyst/utils/metrics/accuracy.py,a5f8fe9852efb9e0daaaac6b345a9c785daf9a56,TODO: move to separate function,https://github.com/catalyst-team/catalyst/commit/a5f8fe9852efb9e0daaaac6b345a9c785daf9a56,Yes
1433,catalyst-team/catalyst,catalyst/callbacks/scheduler.py,9c467e3eb582a24a35a80f04a48b190a88fc5034,todo: consider saving lr and momentum for all param groups ?,https://github.com/catalyst-team/catalyst/commit/9c467e3eb582a24a35a80f04a48b190a88fc5034,Yes
1434,probcomp/bayeslite,src/core.py,c207b2f6c966d43a21ad35b84835bb6bf51e3cae,TODO: A lot of code repated here. Separate.,https://github.com/probcomp/bayeslite/commit/c207b2f6c966d43a21ad35b84835bb6bf51e3cae,Yes
1435,probcomp/bayeslite,src/core.py,026fc78db82dcbfe76c7e29f930e8a944f12f161,TODO: A lot of code repated here. Separate.,https://github.com/probcomp/bayeslite/commit/026fc78db82dcbfe76c7e29f930e8a944f12f161,Yes
1436,autonomio/talos,talos/commands/deploy.py,b7646a7a66a1bffdec72179d0c9b59d795f90e05,TODO: needs to also deploy the hyperparameter configuration,https://github.com/autonomio/talos/commit/b7646a7a66a1bffdec72179d0c9b59d795f90e05,Yes
1437,DragonComputer/Dragonfire,dragonfire/conversational/__init__.py,1b000c167a7d08dafa0673308ded2f494c5a12fa,TODO: Also update learning parameters eventually,https://github.com/DragonComputer/Dragonfire/commit/1b000c167a7d08dafa0673308ded2f494c5a12fa,Yes
1438,DragonComputer/Dragonfire,dragonfire/conversational/trainer.py,1b000c167a7d08dafa0673308ded2f494c5a12fa,TODO: Save params\/results ? or already inside training args ?,https://github.com/DragonComputer/Dragonfire/commit/1b000c167a7d08dafa0673308ded2f494c5a12fa,Yes
1439,mne-tools/mne-python,mne/beamformer/_tf_lcmv.py,03ec52c2006089466244a346b1a5aa4f4da7a1f4,TODO: Which of these parameters should really be exposed? All?,https://github.com/mne-tools/mne-python/commit/03ec52c2006089466244a346b1a5aa4f4da7a1f4,Yes
1440,mne-tools/mne-python,mne/beamformer/_lcmv.py,eb4d5344df6a4b76e5b651efce2232828d1d2ace,TODO: Which of these parameters should really be exposed? All?,https://github.com/mne-tools/mne-python/commit/eb4d5344df6a4b76e5b651efce2232828d1d2ace,Yes
1441,mne-tools/mne-python,mne/beamformer/_lcmv.py,dcf84c291a2c5ee795a28268dc6df31c6836d52e,TODO: Make sure all parameters are used!,https://github.com/mne-tools/mne-python/commit/dcf84c291a2c5ee795a28268dc6df31c6836d52e,Yes
1442,mne-tools/mne-python,mne/beamformer/_lcmv.py,e55e30ca245bcf4c7d85f1f2d57333a2e5e3400f,TODO: Make actual use of the picks parameter!,https://github.com/mne-tools/mne-python/commit/e55e30ca245bcf4c7d85f1f2d57333a2e5e3400f,Yes
1443,mne-tools/mne-python,mne/preprocessing/maxwell.py,a5d0c22a100637d2913ba3e289c7bb1c6dca7e17,TODO: Figure out all parameters required,https://github.com/mne-tools/mne-python/commit/a5d0c22a100637d2913ba3e289c7bb1c6dca7e17,Yes
1444,mne-tools/mne-python,mne/preprocessing/maxwell.py,a5d0c22a100637d2913ba3e289c7bb1c6dca7e17,TODO: Error checks on input parameters,https://github.com/mne-tools/mne-python/commit/a5d0c22a100637d2913ba3e289c7bb1c6dca7e17,Yes
1445,mne-tools/mne-python,mne/preprocessing/maxwell.py,dbd61f421717a3b1fbc68b4d77d795ee47b27599,TODO: Add error checks on input parameters,https://github.com/mne-tools/mne-python/commit/dbd61f421717a3b1fbc68b4d77d795ee47b27599,Yes
1446,mne-tools/mne-python,mne/preprocessing/maxwell.py,56200e8ca6a8cef886c2438b768b016659aad551,TODO: Add error checks on input parameters,https://github.com/mne-tools/mne-python/commit/56200e8ca6a8cef886c2438b768b016659aad551,Yes
1447,mne-tools/mne-python,mne/simulation/epochs.py,eae960399ceac19e76489215164b8ae0ace73f8d,TODO: check if other default params should be included,https://github.com/mne-tools/mne-python/commit/eae960399ceac19e76489215164b8ae0ace73f8d,Yes
1448,mne-tools/mne-python,mne/simulation/raw.py,887071492b56c33efb9c6be5f9c292907e658b67,TODO: Should conditionals depend on fwds or param bool flags?,https://github.com/mne-tools/mne-python/commit/887071492b56c33efb9c6be5f9c292907e658b67,Yes
1449,mne-tools/mne-python,mne/simulation/raw.py,51b01843b582c14347122334ae930da7229a0b5f,TODO: Should conditionals depend on fwds or param bool flags?,https://github.com/mne-tools/mne-python/commit/51b01843b582c14347122334ae930da7229a0b5f,Yes
1450,mne-tools/mne-python,mne/simulation/raw.py,378ae50b483bcbe68240609d7ffe9b332234e3de,TODO: Should conditionals depend on fwds or param bool flags?,https://github.com/mne-tools/mne-python/commit/378ae50b483bcbe68240609d7ffe9b332234e3de,Yes
1451,mne-tools/mne-python,mne/decoding/time_gen.py,8bf84fbc2280f5d495d2528e6ec251229101e117,FIXME Note that this means that TimeDecoding.predict isn't parallel,https://github.com/mne-tools/mne-python/commit/8bf84fbc2280f5d495d2528e6ec251229101e117,Yes
1452,mne-tools/mne-python,mne/decoding/ems.py,e17e31fc42874461648c40a8b5d11e3aced74ab7,FIXME this parallization should be removed.,https://github.com/mne-tools/mne-python/commit/e17e31fc42874461648c40a8b5d11e3aced74ab7,Yes
1453,mne-tools/mne-python,mne/decoding/search_light.py,c2a521a26b1bf875ba3d9bf941578be803a1c78c,FIXME: The parallization is a bit high; it might be memory consuming.,https://github.com/mne-tools/mne-python/commit/c2a521a26b1bf875ba3d9bf941578be803a1c78c,Yes
1454,mne-tools/mne-python,tutorials/intro/plot_introduction.py,2682e32be43af610bdb67f5eabfbfcb1472a3e7f,.. TODO edit prev. paragraph when projectors tutorial is added: ...those are,https://github.com/mne-tools/mne-python/commit/2682e32be43af610bdb67f5eabfbfcb1472a3e7f,Yes
1455,mne-tools/mne-python,tutorials/raw/plot_annotating_raw.py,532f6589e35c4519dc16bcf8768ed758e6b3fe45,.. TODO when raw tutorial updated; add this to end of paragraph:,https://github.com/mne-tools/mne-python/commit/532f6589e35c4519dc16bcf8768ed758e6b3fe45,Yes
1456,mne-tools/mne-python,tutorials/raw/plot_annotating_raw.py,532f6589e35c4519dc16bcf8768ed758e6b3fe45,".. TODO: change \""projection\"" in prev. para to this when tut is ready:",https://github.com/mne-tools/mne-python/commit/532f6589e35c4519dc16bcf8768ed758e6b3fe45,Yes
1457,mne-tools/mne-python,tutorials/misc/plot_sensor_locations.py,3a89b550c95ea7d55031d68503e16b6db296809c,.. TODO sample data doesn't have separate .hsp or .elp files; so can't demo,https://github.com/mne-tools/mne-python/commit/3a89b550c95ea7d55031d68503e16b6db296809c,Yes
1458,mne-tools/mne-python,mne/viz/_brain/surface.py,ad305e0d51287e66b89e66cfdb49ba7620b5fad8,TODO: delete self.grey_curv after cortex parameter,https://github.com/mne-tools/mne-python/commit/ad305e0d51287e66b89e66cfdb49ba7620b5fad8,Yes
1459,mozilla/TTS,tests/test_layers.py,11e789532905beb702b492432dc72b136a37587e,FIXME: several missing required parameters for Decoder ctor,https://github.com/mozilla/TTS/commit/11e789532905beb702b492432dc72b136a37587e,Yes
1460,mozilla/TTS,tests/test_tacotron2_model.py,11e789532905beb702b492432dc72b136a37587e,FIXME: missing num_speakers parameter to Tacotron2 ctor,https://github.com/mozilla/TTS/commit/11e789532905beb702b492432dc72b136a37587e,Yes
1461,mozilla/TTS,tests/test_tacotron_model.py,11e789532905beb702b492432dc72b136a37587e,FIXME: missing num_speakers parameter to Tacotron ctor,https://github.com/mozilla/TTS/commit/11e789532905beb702b492432dc72b136a37587e,Yes
1462,mozilla/TTS,tests/test_layers.py,9a61dfa155086c78d8b5ffe3c6cd22a6fd457d23,FIXME: several missing required parameters for Decoder ctor,https://github.com/mozilla/TTS/commit/9a61dfa155086c78d8b5ffe3c6cd22a6fd457d23,Yes
1463,mozilla/TTS,tests/test_tacotron_model.py,057a50a889d5166270afb195e906c5262b09e04a,FIXME: missing num_speakers parameter to Tacotron ctor,https://github.com/mozilla/TTS/commit/057a50a889d5166270afb195e906c5262b09e04a,Yes
1464,gunthercox/ChatterBot,chatterbot/adapters/input/input_format_adapter.py,26d585fe050778106e99d0af391252d9ca00e088,TODO: Should this just be a parameter for the Terminal adapter?,https://github.com/gunthercox/ChatterBot/commit/26d585fe050778106e99d0af391252d9ca00e088,Yes
1465,gunthercox/ChatterBot,chatterbot/adapters/output/output_format_adapter.py,26d585fe050778106e99d0af391252d9ca00e088,TODO: Should this just be a parameter for the Terminal adapter?,https://github.com/gunthercox/ChatterBot/commit/26d585fe050778106e99d0af391252d9ca00e088,Yes
1466,microsoft/graspologic,graphstats/embed/ase.py,deda18c6c2708ca900bbf2d72394d06c580ce7da,TODO other parameters here?,https://github.com/microsoft/graspologic/commit/deda18c6c2708ca900bbf2d72394d06c580ce7da,Yes
1467,microsoft/graspologic,graspy/embed/ase.py,f553bd38de79ebec7a9c2fc60d5b2d45d4762409,TODO other parameters here?,https://github.com/microsoft/graspologic/commit/f553bd38de79ebec7a9c2fc60d5b2d45d4762409,Yes
1468,openml/openml-python,tests/test_flows/test_flow.py,71ec3fc53277fd117f714baf251914f084b9ff7f,TODO: Test if parameters are set correctly!,https://github.com/openml/openml-python/commit/71ec3fc53277fd117f714baf251914f084b9ff7f,Yes
1469,openml/openml-python,tests/test_flows/test_flow.py,f9bf4f2a123e37f74fb2d0dbd084eed369e3c29b,TODO: Test if parameters are set correctly!,https://github.com/openml/openml-python/commit/f9bf4f2a123e37f74fb2d0dbd084eed369e3c29b,Yes
1470,openml/openml-python,openml/runs/functions.py,b54427299e4fd4121875deea02d2dc230df8556d,TODO: if someone needs it; he can use the parameter,https://github.com/openml/openml-python/commit/b54427299e4fd4121875deea02d2dc230df8556d,Yes
1471,openml/openml-python,tests/test_runs/test_run_functions.py,cd7d74bd15d642bbd1ee6a0e0dedac49c24e5cf7,TODO add test about initializing a model from a run given a parameter distribution - also,https://github.com/openml/openml-python/commit/cd7d74bd15d642bbd1ee6a0e0dedac49c24e5cf7,Yes
1472,scikit-multiflow/scikit-multiflow,skmultiflow/classification/trees/arf_hoeffding_tree.py,988129bfba353dfc802641df2976716ca1ccd7a7,TODO Add HT parameters to ARF Hoeffding Tree constructor signature,https://github.com/scikit-multiflow/scikit-multiflow/commit/988129bfba353dfc802641df2976716ca1ccd7a7,Yes
1473,scikit-multiflow/scikit-multiflow,skmultiflow/classification/trees/arf_hoeffding_tree.py,988129bfba353dfc802641df2976716ca1ccd7a7,TODO Pass all HT parameters once they are available at the ARFHT class level,https://github.com/scikit-multiflow/scikit-multiflow/commit/988129bfba353dfc802641df2976716ca1ccd7a7,Yes
1474,scikit-multiflow/scikit-multiflow,skmultiflow/classification/meta/adaptive_random_forests.py,899feb8f1035dc6f4dfa2b52cf478d4db9b3856a,TODO Pass all HT parameters once they are available at the ARFHT class level,https://github.com/scikit-multiflow/scikit-multiflow/commit/899feb8f1035dc6f4dfa2b52cf478d4db9b3856a,Yes
1475,scikit-multiflow/scikit-multiflow,src/skmultiflow/meta/adaptive_random_forest_regressor.py,56798201b77d1546bec413b57e3561c5f13f5122,TODO: check necessity of this parameter,https://github.com/scikit-multiflow/scikit-multiflow/commit/56798201b77d1546bec413b57e3561c5f13f5122,Yes
1476,scikit-multiflow/scikit-multiflow,src/skmultiflow/meta/adaptive_random_forest_regressor.py,4f1afcc6a96382fcf1ae922a32cf7000acebef2e,TODO: check necessity of this parameter,https://github.com/scikit-multiflow/scikit-multiflow/commit/4f1afcc6a96382fcf1ae922a32cf7000acebef2e,Yes
1477,onnx/onnx-caffe2,onnx_caffe2/backend.py,ba3351fb6f195e5c4ef4d9055fe1530defd028ab,TODO implement support for return_params in gru_cell.GRU.,https://github.com/onnx/onnx-caffe2/commit/ba3351fb6f195e5c4ef4d9055fe1530defd028ab,Yes
1478,microsoft/NimbusML,src/python/nimbusml/pipeline.py,793739b8ab6b002bfccdbda2bedf6f915ce6125b,todo: ideally all the nodes have the same name for params,https://github.com/microsoft/NimbusML/commit/793739b8ab6b002bfccdbda2bedf6f915ce6125b,Yes
1479,microsoft/NimbusML,src/python/nimbusml/base_predictor.py,d08b702ade3e4d8bf487fe583f9632c40a7a774b,todo: ideally all the nodes have the same name for params,https://github.com/microsoft/NimbusML/commit/d08b702ade3e4d8bf487fe583f9632c40a7a774b,Yes
1480,tensorflow/addons,tensorflow_addons/image/translate_ops_test.py,061b888280ca0e3efa5fc84c146d448354851fb7,TODO: Parameterize on dtypes,https://github.com/tensorflow/addons/commit/061b888280ca0e3efa5fc84c146d448354851fb7,Yes
1481,menpo/menpo,pybug/align/base.py,469d1b36b559d5526025956efe77f1f5e0bfe36b,TODO this should be separated out into visualize.,https://github.com/menpo/menpo/commit/469d1b36b559d5526025956efe77f1f5e0bfe36b,Yes
1482,menpo/menpo,menpo/fitmultilevel/aam/base.py,c151cb3fe9187ff852769ff7d4df1c7774fa05fa,TODO: Add residual as parameter; when residuals are properly defined,https://github.com/menpo/menpo/commit/c151cb3fe9187ff852769ff7d4df1c7774fa05fa,Yes
1483,menpo/menpo,menpo/fitmultilevel/clm/base.py,40865be71a6c9602f222f1f26ca3cc62aec9d480,TODO: Add residual as parameter; when residuals are properly defined,https://github.com/menpo/menpo/commit/40865be71a6c9602f222f1f26ca3cc62aec9d480,Yes
1484,menpo/menpo,menpo/fitmultilevel/aam/fitter.py,c77e99a3b7319e1fdb7232e4b0bcea0b9539505d,TODO: Add residual as parameter; when residuals are properly defined,https://github.com/menpo/menpo/commit/c77e99a3b7319e1fdb7232e4b0bcea0b9539505d,Yes
1485,menpo/menpo,menpo/fitmultilevel/clm/fitter.py,496ce4665da6a7863af4a986efd2f51e6a38cf12,TODO: Add residual as parameter; when residuals are properly defined,https://github.com/menpo/menpo/commit/496ce4665da6a7863af4a986efd2f51e6a38cf12,Yes
1486,menpo/menpo,menpo/fitmultilevel/aam/base.py,b5916a9166db3da1a81d3aad0b8f76c40ab5d362,TODO: Add residual as parameter; when residuals are properly defined,https://github.com/menpo/menpo/commit/b5916a9166db3da1a81d3aad0b8f76c40ab5d362,Yes
1487,menpo/menpo,menpo/fitmultilevel/atm/fitter.py,0678bbfa769f9b517cb600de93bc70ab8ae7c0fa,TODO: Add residual as parameter; when residuals are properly defined,https://github.com/menpo/menpo/commit/0678bbfa769f9b517cb600de93bc70ab8ae7c0fa,Yes
1488,feedly/transfer-nlp,transfer_nlp/runners/runnersABC.py,5770f658a72c31c646854a10d69fed99c81e3c31,Register useful parameters and objects useful for model instantiation #TODO: do proper testing on this part,https://github.com/feedly/transfer-nlp/commit/5770f658a72c31c646854a10d69fed99c81e3c31,Yes
1489,feedly/transfer-nlp,transfer_nlp/predictors/predictor.py,8c483e1a28f37259b68a2f1a946ffaeb2e4fe0f3,Register useful parameters and objects useful for model instantiation #TODO: do proper testing on this part,https://github.com/feedly/transfer-nlp/commit/8c483e1a28f37259b68a2f1a946ffaeb2e4fe0f3,Yes
1490,chainer/chainer-chemistry,examples/qm9/train_qm9.py,c426e08351a5bf2e8989aaf8d79b76fceb177e05,TODO: Review default parameter,https://github.com/chainer/chainer-chemistry/commit/c426e08351a5bf2e8989aaf8d79b76fceb177e05,Yes
1491,chainer/chainer-chemistry,examples/qm9/train_qm9.py,1ed6735f1468f2a148b87a0949e7d6a1935fc598,TODO: Review default parameter,https://github.com/chainer/chainer-chemistry/commit/1ed6735f1468f2a148b87a0949e7d6a1935fc598,Yes
1492,online-ml/river,skmultiflow/classification/trees/arf_hoeffding_tree.py,988129bfba353dfc802641df2976716ca1ccd7a7,TODO Add HT parameters to ARF Hoeffding Tree constructor signature,https://github.com/online-ml/river/commit/988129bfba353dfc802641df2976716ca1ccd7a7,Yes
1493,online-ml/river,skmultiflow/classification/trees/arf_hoeffding_tree.py,988129bfba353dfc802641df2976716ca1ccd7a7,TODO Pass all HT parameters once they are available at the ARFHT class level,https://github.com/online-ml/river/commit/988129bfba353dfc802641df2976716ca1ccd7a7,Yes
1494,online-ml/river,skmultiflow/classification/meta/adaptive_random_forests.py,899feb8f1035dc6f4dfa2b52cf478d4db9b3856a,TODO Pass all HT parameters once they are available at the ARFHT class level,https://github.com/online-ml/river/commit/899feb8f1035dc6f4dfa2b52cf478d4db9b3856a,Yes
1495,online-ml/river,src/skmultiflow/meta/adaptive_random_forest_regressor.py,56798201b77d1546bec413b57e3561c5f13f5122,TODO: check necessity of this parameter,https://github.com/online-ml/river/commit/56798201b77d1546bec413b57e3561c5f13f5122,Yes
1496,online-ml/river,src/skmultiflow/meta/adaptive_random_forest_regressor.py,4f1afcc6a96382fcf1ae922a32cf7000acebef2e,TODO: check necessity of this parameter,https://github.com/online-ml/river/commit/4f1afcc6a96382fcf1ae922a32cf7000acebef2e,Yes
1497,online-ml/river,docs/scripts/index_api.py,f8f92de0b408e21f2395ef3885268f4ef0915bc7,TODO: remove >>> and ... in code blocks; put output in a separate fenced block,https://github.com/online-ml/river/commit/f8f92de0b408e21f2395ef3885268f4ef0915bc7,Yes
1498,CPJKU/madmom,cp/features/beats.py,763a0a784d916e863de806ce1481d77fe82a856b,TODO: make this processing parallel or numpyfy if possible,https://github.com/CPJKU/madmom/commit/763a0a784d916e863de806ce1481d77fe82a856b,Yes
1499,CPJKU/madmom,cp/features/beats.py,c900f7d07032ad002439e420b45213513e6dc872,TODO: make this processing parallel or numpyfy if possible,https://github.com/CPJKU/madmom/commit/c900f7d07032ad002439e420b45213513e6dc872,Yes
1500,CPJKU/madmom,madmom/utils/helpers.py,1fe757a266ddb8c2eee8f34ecda6927bd47486df,TODO: remove duplicate code with files(); add pattern parameter to files(),https://github.com/CPJKU/madmom/commit/1fe757a266ddb8c2eee8f34ecda6927bd47486df,Yes
1501,CPJKU/madmom,madmom/features/tempo.py,14462f03e9588bac30770d9f14d000a6ac7a9abc,TODO: make this processing parallel or numpyfy if possible,https://github.com/CPJKU/madmom/commit/14462f03e9588bac30770d9f14d000a6ac7a9abc,Yes
1502,CPJKU/madmom,madmom/features/onsets.py,20831660617b5953a2b7cdb88612c425adac37fa,TODO: the signal processing parameters should be included in and,https://github.com/CPJKU/madmom/commit/20831660617b5953a2b7cdb88612c425adac37fa,Yes
1503,CPJKU/madmom,madmom/features/notes.py,60ed6dc3183a584806be86dea282453366a72ac0,TODO: the signal processing parameters should be included in and,https://github.com/CPJKU/madmom/commit/60ed6dc3183a584806be86dea282453366a72ac0,Yes
1504,CPJKU/madmom,madmom/features/__init__.py,65971295ced9dcf4d4ec230bf7ba20ab8be830e5,TODO: the signal processing parameters should be included in and,https://github.com/CPJKU/madmom/commit/65971295ced9dcf4d4ec230bf7ba20ab8be830e5,Yes
1505,CPJKU/madmom,madmom/evaluation/__init__.py,41d61a1523572f8f390c3c24e30f8a5490de440b,TODO: add a generic totable() function which accepts columns separator;,https://github.com/CPJKU/madmom/commit/41d61a1523572f8f390c3c24e30f8a5490de440b,Yes
1506,CPJKU/madmom,madmom/features/beats.py,d4e69605b83c1f7153b6aa77b515e9ef2a62450d,TODO: make this parametric,https://github.com/CPJKU/madmom/commit/d4e69605b83c1f7153b6aa77b515e9ef2a62450d,Yes
1507,CPJKU/madmom,tests/test_bin.py,9be5595820f0bf72fec9fafe40a824f608b04ad0,TODO: parametrize tests; don't know how to do with nose; should be simple,https://github.com/CPJKU/madmom/commit/9be5595820f0bf72fec9fafe40a824f608b04ad0,Yes
1508,CPJKU/madmom,madmom/features/downbeats.py,cf513993cf1338edc515d9a2c9bc734cab72655f,TODO: make this parametric,https://github.com/CPJKU/madmom/commit/cf513993cf1338edc515d9a2c9bc734cab72655f,Yes
1509,CPJKU/madmom,madmom/features/downbeats.py,59af3b5a1a2463379f90da271461d29856551035,TODO: make this parametric,https://github.com/CPJKU/madmom/commit/59af3b5a1a2463379f90da271461d29856551035,Yes
1510,nyu-mll/jiant,src/trainer.py,e1bf3cd5ea60e131ad9401fd30e8de843ef8aadb,TODO: Make this an explicit parameter rather than hard-coding.,https://github.com/nyu-mll/jiant/commit/e1bf3cd5ea60e131ad9401fd30e8de843ef8aadb,Yes
1511,nyu-mll/jiant,src/trainer.py,2ecd33d725ab6dd76b078a075c3dd578a8171922,TODO: Make this an explicit parameter rather than hard-coding.,https://github.com/nyu-mll/jiant/commit/2ecd33d725ab6dd76b078a075c3dd578a8171922,Yes
1512,nyu-mll/jiant,src/trainer.py,dc7b91fe477b1b09d49403f92fb647ead6af0690,TODO: Make this an explicit parameter rather than hard-coding.,https://github.com/nyu-mll/jiant/commit/dc7b91fe477b1b09d49403f92fb647ead6af0690,Yes
1513,nyu-mll/jiant,src/trainer.py,5ce94a38a3078bc009080c23b05fd49df61e9efa,TODO: Make this an explicit parameter rather than hard-coding.,https://github.com/nyu-mll/jiant/commit/5ce94a38a3078bc009080c23b05fd49df61e9efa,Yes
1514,nyu-mll/jiant,src/trainer.py,1901e07981f9c172927244b074c3e04c87ef2dd2,TODO: Make this an explicit parameter rather than hard-coding.,https://github.com/nyu-mll/jiant/commit/1901e07981f9c172927244b074c3e04c87ef2dd2,Yes
1515,nyu-mll/jiant,src/trainer.py,b51a3727e7c3ce8f346daca376b3fc1ae018d9ac,TODO: Make this an explicit parameter rather than hard-coding.,https://github.com/nyu-mll/jiant/commit/b51a3727e7c3ce8f346daca376b3fc1ae018d9ac,Yes
1516,nyu-mll/jiant,src/trainer.py,36bf874fcd055d6e12be1c2cb4a054a1e0a26f3c,TODO: Make this an explicit parameter rather than hard-coding.,https://github.com/nyu-mll/jiant/commit/36bf874fcd055d6e12be1c2cb4a054a1e0a26f3c,Yes
1517,nyu-mll/jiant,jiant/proj/simple/preprocessing.py,e339ccd7ae38f47002309b93216edbbda84a86d3,TODO: Expose parameters  (Issue #49),https://github.com/nyu-mll/jiant/commit/e339ccd7ae38f47002309b93216edbbda84a86d3,Yes
1518,nyu-mll/jiant,jiant/shared/initialization.py,0dfa7c526e8237ed1711c99d38c525a055b4332e,TODO break local_rank == -1 and no_cuda into separate cases to make the logic easier to read.,https://github.com/nyu-mll/jiant/commit/0dfa7c526e8237ed1711c99d38c525a055b4332e,Yes
1519,scikit-learn-contrib/metric-learn,metric_learn/constraints.py,3c57c64e139ed25454901df2c9611cb14fb40cdd,@TODO: remove seed from params and use numpy RandomState,https://github.com/scikit-learn-contrib/metric-learn/commit/3c57c64e139ed25454901df2c9611cb14fb40cdd,Yes
1520,scikit-learn-contrib/metric-learn,metric_learn/rca.py,3c57c64e139ed25454901df2c9611cb14fb40cdd,@TODO: remove seed from param. See @TODO in constraints\/chunks,https://github.com/scikit-learn-contrib/metric-learn/commit/3c57c64e139ed25454901df2c9611cb14fb40cdd,Yes
1521,oracle/Skater,skater/core/visualizer/relevance_visualizer.py,ce125e717e492e1b5a2722a4a1176d9ac331e0c2,TODO extend support for Word Cloud params,https://github.com/oracle/Skater/commit/ce125e717e492e1b5a2722a4a1176d9ac331e0c2,Yes
1522,Accenture/AmpliGraph,ampligraph/latent_features/loss_functions.py,3de385c15a8f21e816f3ef1958e1607882fed6eb,TODO (consider add regularization term - now handled in params constructor),https://github.com/Accenture/AmpliGraph/commit/3de385c15a8f21e816f3ef1958e1607882fed6eb,Yes
1523,Accenture/AmpliGraph,ampligraph/latent_features/loss_functions.py,354d1f743d76b05ae7be83574930dcf2c5479481,TODO: Remove need for eta in parameters (its tied to loss_registry),https://github.com/Accenture/AmpliGraph/commit/354d1f743d76b05ae7be83574930dcf2c5479481,Yes
1524,awslabs/sockeye,sockeye/train.py,85f21bdb1361d8e3b19a96b141842e6a0cf05b07,TODO: The loading for continuation of the scheduler is done separately from the other parts,https://github.com/awslabs/sockeye/commit/85f21bdb1361d8e3b19a96b141842e6a0cf05b07,Yes
1525,awslabs/sockeye,sockeye/layers.py,bb588ecbe874ae29ede33af2709e251910778bb3,TODO: remove with next major version update to use mx.gluon.nn.LayerNorm (which uses different parameter naming).,https://github.com/awslabs/sockeye/commit/bb588ecbe874ae29ede33af2709e251910778bb3,Yes
1526,awslabs/sockeye,sockeye/layers.py,f5e9ec7b130505e29093e52456d77e5765f8cb70,TODO: remove with next major version update to use mx.gluon.nn.LayerNorm (which uses different parameter naming).,https://github.com/awslabs/sockeye/commit/f5e9ec7b130505e29093e52456d77e5765f8cb70,Yes
1527,awslabs/sockeye,sockeye/model.py,e4553d392a8b67c88bf9628384ae956916b06ea2,TODO: check for missing parameters somehow (we allowed scaling to be missing),https://github.com/awslabs/sockeye/commit/e4553d392a8b67c88bf9628384ae956916b06ea2,Yes
1528,awslabs/sockeye,sockeye/constants.py,bf89a7eeabd433b603a1d50895ff18269c9eac04,TODO: make this configurable in the model; separately per target factor.,https://github.com/awslabs/sockeye/commit/bf89a7eeabd433b603a1d50895ff18269c9eac04,Yes
1529,optuna/optuna,pfnopt/pruners.py,dfff9a867c90943ef83bd02d04f6b8299bb6e96b,TODO: parameterize,https://github.com/optuna/optuna/commit/dfff9a867c90943ef83bd02d04f6b8299bb6e96b,Yes
1530,explosion/thinc,thinc/shims/mxnet.py,31e027da8140d8db1c3355a1fbbadb2fcc91de81,TODO: state_dict equiv in mxnet? collect_params().copy() maybe?,https://github.com/explosion/thinc/commit/31e027da8140d8db1c3355a1fbbadb2fcc91de81,Yes
1531,microsoft/dowhy,dowhy/causal_estimators/linear_regression_estimator.py,0a50b1033ad4ac7cac4d056aa1519959fcaa014e,TODO make treatment_value and control value also as local parameters,https://github.com/microsoft/dowhy/commit/0a50b1033ad4ac7cac4d056aa1519959fcaa014e,Yes
1532,microsoft/dowhy,dowhy/causal_estimators/regression_estimator.py,113031a7d84af7bc28dfc4152ea78f489558f9a0,TODO make treatment_value and control value also as local parameters,https://github.com/microsoft/dowhy/commit/113031a7d84af7bc28dfc4152ea78f489558f9a0,Yes
1533,microsoft/dowhy,dowhy/causal_model.py,20c20a2d8184a6fa3d561646a4947df56c91434e,TODO: This add_params needs to move to the estimator class,https://github.com/microsoft/dowhy/commit/20c20a2d8184a6fa3d561646a4947df56c91434e,Yes
1534,chartbeat-labs/textacy,textacy/fileio/utils.py,3c86a21f479e93746e72aafbfb8da022967ce183,TODO: remove these params in; say; v0.4,https://github.com/chartbeat-labs/textacy/commit/3c86a21f479e93746e72aafbfb8da022967ce183,Yes
1535,nilearn/nilearn,nisl/io/nifti_region.py,0909a1101d564e780ff489326924f1055a68b4a1,TODO: add a parameter to choose computation method. Either use,https://github.com/nilearn/nilearn/commit/0909a1101d564e780ff489326924f1055a68b4a1,Yes
1536,nilearn/nilearn,nisl/io/nifti_region.py,f80ad2e8dafb628d9fcd0ac4883959cac379a4a4,FIXME: useless copy if input parameter niimg is a string.,https://github.com/nilearn/nilearn/commit/f80ad2e8dafb628d9fcd0ac4883959cac379a4a4,Yes
1537,nilearn/nilearn,nisl/resampling.py,ebf6faeaa4230a4ece9970bb7ece6e4372eb1de5,"FIXME: \""import copy\"" overwrites input parameter \""copy\""!",https://github.com/nilearn/nilearn/commit/ebf6faeaa4230a4ece9970bb7ece6e4372eb1de5,Yes
1538,nilearn/nilearn,nisl/io/nifti_region.py,ef950b8ed6aaeac52feca9bd077f887a16dcf852,FIXME: useless copy if input parameter niimg is a string.,https://github.com/nilearn/nilearn/commit/ef950b8ed6aaeac52feca9bd077f887a16dcf852,Yes
1539,nilearn/nilearn,nistats/second_level_model.py,644619db2bd4922a9e2c2710fbb5c344c01ed93a,TODO: manage changing mask parameters,https://github.com/nilearn/nilearn/commit/644619db2bd4922a9e2c2710fbb5c344c01ed93a,Yes
1540,cltk/cltk,cltk/tests/test_arabic_utils.py,101faecbdb6ec22ad627f602e249f5f42d161d5f,test separate function  TODO: testme,https://github.com/cltk/cltk/commit/101faecbdb6ec22ad627f602e249f5f42d161d5f,Yes
1541,cltk/cltk,scripts/download_misc_dependencies.py,2258bb6ac217df21b64dc5fd511bc8d53b9d8285,TODO: add command line params for what langs (all or just one); useful for build server,https://github.com/cltk/cltk/commit/2258bb6ac217df21b64dc5fd511bc8d53b9d8285,Yes
1542,cltk/cltk,src/cltkv1/tokenizers/latin/params.py,ce6ba260f38c522d1642e2be98d56e38a73f1933,"\""\""\"" Params: Latin ||  || TODO: Some of these are only used for training. PRAENOMINA for training punkt tokenizer (als ABBREVIATIONS; CALENDAR; MISC) || TODO: The enclitic exceptions (que_exceptions and below) can all be deleted ||  || \""\""\""",https://github.com/cltk/cltk/commit/ce6ba260f38c522d1642e2be98d56e38a73f1933,Yes
1543,IntelPython/sdc,hpat/__init__.py,9ad382429c64f8d888acfe0e30f79f402ff4a1bd,FIXME: support parallel setitem,https://github.com/IntelPython/sdc/commit/9ad382429c64f8d888acfe0e30f79f402ff4a1bd,Yes
1544,IntelPython/sdc,hpat/distributed_analysis.py,65fd2b8bed1eb0ca2e2f9186377d75009ac66666,no parallel to parallel array set (TODO),https://github.com/IntelPython/sdc/commit/65fd2b8bed1eb0ca2e2f9186377d75009ac66666,Yes
1545,IntelPython/sdc,hpat/hiframes_aggregate.py,977c516a1de2e221165e671925a74efa2e247bf6,key is returned in parallel local agg phase (TODO: avoid if key is output already),https://github.com/IntelPython/sdc/commit/977c516a1de2e221165e671925a74efa2e247bf6,Yes
1546,IntelPython/sdc,hpat/tests/test_hiframes.py,12dc0226a5a0f3300f5d1bf2896ba306e664b05b,TODO: better parallel sort test,https://github.com/IntelPython/sdc/commit/12dc0226a5a0f3300f5d1bf2896ba306e664b05b,Yes
1547,IntelPython/sdc,hpat/hiframes_aggregate.py,15a7b29b1a138396e165dab419341548bf43b519,# key is returned in parallel local agg phase (TODO: avoid if key is output already),https://github.com/IntelPython/sdc/commit/15a7b29b1a138396e165dab419341548bf43b519,Yes
1548,IntelPython/sdc,hpat/pd_series_ext.py,a62dcacc582943649d237ae26a26d33ab85af4d3,TODO: create a separate DatetimeIndex type from Series,https://github.com/IntelPython/sdc/commit/a62dcacc582943649d237ae26a26d33ab85af4d3,Yes
1549,IntelPython/sdc,hpat/hiframes_typed.py,14cd92d9456a633b2878564a91c5beb5ef0fd450,TODO: remove after support arr.shape in parallel,https://github.com/IntelPython/sdc/commit/14cd92d9456a633b2878564a91c5beb5ef0fd450,Yes
1550,IntelPython/sdc,hpat/hiframes_sort.py,65990538a93e58d4a380dbaf983bab2b0ff94531,arrays in the parallel case; TODO: fix),https://github.com/IntelPython/sdc/commit/65990538a93e58d4a380dbaf983bab2b0ff94531,Yes
1551,IntelPython/sdc,hpat/hiframes_join.py,9a8e4ac2ff8bf1f88f81f3c9aa08a070c1174ffb,TODO: support separate keys?,https://github.com/IntelPython/sdc/commit/9a8e4ac2ff8bf1f88f81f3c9aa08a070c1174ffb,Yes
1552,IntelPython/sdc,hpat/hiframes_typed.py,f83578b0bfedb8c0e6608096bfabbec2bc88b21e,TODO: support default whitespace separator,https://github.com/IntelPython/sdc/commit/f83578b0bfedb8c0e6608096bfabbec2bc88b21e,Yes
1553,IntelPython/sdc,hpat/hiframes/hiframes_typed.py,635888b78950e4dbb282125c1b3310b7d74998bc,FIXME e.g. test_series_nlargest_parallel1 np.int32(),https://github.com/IntelPython/sdc/commit/635888b78950e4dbb282125c1b3310b7d74998bc,Yes
1554,IntelPython/sdc,hpat/hiframes/pd_series_ext.py,a46be3c524041e5ff75a6ba5cb863c54f9061b3d,TODO: separate array type for Categorical data,https://github.com/IntelPython/sdc/commit/a46be3c524041e5ff75a6ba5cb863c54f9061b3d,Yes
1555,IntelPython/sdc,hpat/decorators.py,167ea12c1f6eb795d21ff60555aa2b1fe56c3676,FIXME: support parallel setitem,https://github.com/IntelPython/sdc/commit/167ea12c1f6eb795d21ff60555aa2b1fe56c3676,Yes
1556,IntelPython/sdc,hpat/hiframes/api.py,27987829653df54650c25b3e51d127167970985d,TODO: use separate index type instead of just storing array,https://github.com/IntelPython/sdc/commit/27987829653df54650c25b3e51d127167970985d,Yes
1557,IntelPython/sdc,hpat/hiframes/pd_dataframe_ext.py,057d5376c94afa9b75c0511bcf9dd3bf8a96ca98,TODO: use separate index type instead of just storing array,https://github.com/IntelPython/sdc/commit/057d5376c94afa9b75c0511bcf9dd3bf8a96ca98,Yes
1558,IntelPython/sdc,hpat/distributed.py,1c34f97215dec05cb9e21ce33948ef403199d99e,TODO: test parallel,https://github.com/IntelPython/sdc/commit/1c34f97215dec05cb9e21ce33948ef403199d99e,Yes
1559,IntelPython/sdc,hpat/config.py,62314491b148c51e7c27e13aded283a0622c47f4,TODO: make sure h5py\/hdf5 supports parallel,https://github.com/IntelPython/sdc/commit/62314491b148c51e7c27e13aded283a0622c47f4,Yes
1560,IntelPython/sdc,hpat/hiframes/pd_series_ext.py,1c3b32dfda4c8568a9720a74cc32684302899f2b,TODO: call it from numba.targets.arrayobj; need separate function in numba,https://github.com/IntelPython/sdc/commit/1c3b32dfda4c8568a9720a74cc32684302899f2b,Yes
1561,IntelPython/sdc,hpat/datatypes/hpat_pandas_series_functions.py,63a4215f5cb07db175b1e000ae45c1ebdca8531f,TODO Needs to implement parameters value check,https://github.com/IntelPython/sdc/commit/63a4215f5cb07db175b1e000ae45c1ebdca8531f,Yes
1562,IntelPython/sdc,hpat/datatypes/hpat_pandas_series_functions.py,d71e2407175995d2e795046c4ba0c3474050fd76,TODO Needs to implement parameters value check,https://github.com/IntelPython/sdc/commit/d71e2407175995d2e795046c4ba0c3474050fd76,Yes
1563,IntelPython/sdc,sdc/hiframes/pd_series_type.py,b57c9adebb90fe95458d2c2238928d2e38c3ee9c,TODO: call it from numba.targets.arrayobj; need separate function in numba,https://github.com/IntelPython/sdc/commit/b57c9adebb90fe95458d2c2238928d2e38c3ee9c,Yes
1564,IntelPython/sdc,sdc/rewrites/read_csv_consts.py,845799488da7c6959ae0d119c9452f4db1d56db5,TODO: 1. save instructions of build_map; build_list for read_csv params,https://github.com/IntelPython/sdc/commit/845799488da7c6959ae0d119c9452f4db1d56db5,Yes
1565,IntelPython/sdc,sdc/datatypes/categorical/pdimpl.py,e0619659131a8647ad8e5738a776ce77d1ce5c9a,TODO: support other parameters (only values now),https://github.com/IntelPython/sdc/commit/e0619659131a8647ad8e5738a776ce77d1ce5c9a,Yes
1566,fastnlp/fastNLP,fastNLP/core/utils.py,6309eafd25084c4c1f33113a05b9c03d2eaaf0b1,TODO \u8FD9\u4E2A\u51FD\u6570\u5B58\u5728\u4E00\u5B9A\u7684\u98CE\u9669\uFF0C\u56E0\u4E3A\u540C\u4E00\u4E2A\u6A21\u578B\u53EF\u80FD\u5B58\u5728\u67D0\u4E9Bparameter\u4E0D\u5728\u663E\u5361\u4E2D\uFF0C\u6BD4\u5982BertEmbedding,https://github.com/fastnlp/fastNLP/commit/6309eafd25084c4c1f33113a05b9c03d2eaaf0b1,Yes
1567,kornia/kornia,torchgeometry/feature/harris.py,84cc1287fcd9df2a437a2d25a61f171097047a76,TODO: add as signature parameter,https://github.com/kornia/kornia/commit/84cc1287fcd9df2a437a2d25a61f171097047a76,Yes
1568,kornia/kornia,torchgeometry/feature/harris.py,84cc1287fcd9df2a437a2d25a61f171097047a76,TODO: add as signature parameter ?,https://github.com/kornia/kornia/commit/84cc1287fcd9df2a437a2d25a61f171097047a76,Yes
1569,kornia/kornia,kornia/augmentation/functional.py,7976f384889e79b8f5aef3bc9a3c495961e60db6,TODO: this params should be at some point; learnable tensors,https://github.com/kornia/kornia/commit/7976f384889e79b8f5aef3bc9a3c495961e60db6,Yes
1570,kornia/kornia,test/augmentation/test_augmentation.py,7976f384889e79b8f5aef3bc9a3c495961e60db6,TODO: Gradcheck for param random gen failed. Suspect get_motion_kernel2d issue.,https://github.com/kornia/kornia/commit/7976f384889e79b8f5aef3bc9a3c495961e60db6,Yes
1571,kornia/kornia,test/augmentation/test_motionblur.py,766bd71d6cca7313988b02784be6d56834e8c744,TODO: Gradcheck for param random gen failed. Suspect get_motion_kernel2d issue.,https://github.com/kornia/kornia/commit/766bd71d6cca7313988b02784be6d56834e8c744,Yes
1572,kornia/kornia,kornia/augmentation/container/video.py,5ea5760e41a3faa385027f9229db49dfcd62481e,TODO: revise colorjitter order param in the future to align the standard.,https://github.com/kornia/kornia/commit/5ea5760e41a3faa385027f9229db49dfcd62481e,Yes
1573,bsc-wdc/dislib,dislib/data/array.py,41fe9592be0b2ef3175ae652e457a672917cc36b,TODO try avoid this with 2 params like: top_left_shape &,https://github.com/bsc-wdc/dislib/commit/41fe9592be0b2ef3175ae652e457a672917cc36b,Yes
1574,bsc-wdc/dislib,dislib/data/array.py,00dc8d5bde0d139ad4d947b2ef53530ee0b1176e,TODO: parse\/interpret the rows\/cols parameters;,https://github.com/bsc-wdc/dislib/commit/00dc8d5bde0d139ad4d947b2ef53530ee0b1176e,Yes
1575,ecohealthalliance/EpiTator,epitator/count_annotator_alt.py,28a481eee95f145287c01db676ad84397b03812c,TODO: This needs to not do this if there is non-quantity text separating these tokens.,https://github.com/ecohealthalliance/EpiTator/commit/28a481eee95f145287c01db676ad84397b03812c,Yes
1576,openopt/copt,copt/stochastic.py,fdc3936416402e556690a52a63a7ee3ffc5f0cbf,TODO: could be parallelized,https://github.com/openopt/copt/commit/fdc3936416402e556690a52a63a7ee3ffc5f0cbf,Yes
1577,openopt/copt,copt/stochastic.py,90f15f4390c4ae4c4f1ec00c670e64dfbe31c6d5,TODO: could be parallelized,https://github.com/openopt/copt/commit/90f15f4390c4ae4c4f1ec00c670e64dfbe31c6d5,Yes
1578,openopt/copt,copt/stochastic.py,46ccf408c5abc796182736412abff89232e3659e,TODO: could be parallelized,https://github.com/openopt/copt/commit/46ccf408c5abc796182736412abff89232e3659e,Yes
1579,DIVA-DIA/DeepDIVA,template/CIFAR_CNN_classifier.py,7875d579a8aeffd60b5ab70ded47cc9d74353145,TODO make way that the model and the criterion are also passed as parameter with introspection thingy as the optimizer,https://github.com/DIVA-DIA/DeepDIVA/commit/7875d579a8aeffd60b5ab70ded47cc9d74353145,Yes
1580,DIVA-DIA/DeepDIVA,template/CIFAR_CNN_classifier.py,fb5793a66a4a05b8408328f705e3ab58d8d2584e,TODO load a ds passed from parameter NICELY,https://github.com/DIVA-DIA/DeepDIVA/commit/fb5793a66a4a05b8408328f705e3ab58d8d2584e,Yes
1581,DIVA-DIA/DeepDIVA,init/initializer.py,459512f99163e38d4a136a7cab0f9d3194f30e50,TODO two parameters should be A() and B() where A is used to init everything and B only the last layer.,https://github.com/DIVA-DIA/DeepDIVA/commit/459512f99163e38d4a136a7cab0f9d3194f30e50,Yes
1582,DIVA-DIA/DeepDIVA,init/initializer.py,d9499fb498bb06a8cb10ffa38e299854c439f3db,TODO two parameters should be A() and B() where A is used to init everything and B only the last layer.,https://github.com/DIVA-DIA/DeepDIVA/commit/d9499fb498bb06a8cb10ffa38e299854c439f3db,Yes
1583,DIVA-DIA/DeepDIVA,init/initializer.py,7a35957d4ea0e94a30a79f4ee5e23389cd498cdf,TODO two parameters should be A() and B() where A is used to init everything and B only the last layer.,https://github.com/DIVA-DIA/DeepDIVA/commit/7a35957d4ea0e94a30a79f4ee5e23389cd498cdf,Yes
1584,DIVA-DIA/DeepDIVA,template/CIFAR_CNN_classifier.py,6cb1f7802245c51b0807f8278a64fbbbdd5990b9,TODO load a ds passed from parameter NICELY,https://github.com/DIVA-DIA/DeepDIVA/commit/6cb1f7802245c51b0807f8278a64fbbbdd5990b9,Yes
1585,DIVA-DIA/DeepDIVA,template/standard.py,81906fc71337ac21282ed2a5afa1147264c767a4,TODO load a ds passed from parameter NICELY,https://github.com/DIVA-DIA/DeepDIVA/commit/81906fc71337ac21282ed2a5afa1147264c767a4,Yes
1586,DIVA-DIA/DeepDIVA,template/standard.py,81906fc71337ac21282ed2a5afa1147264c767a4,TODO make way that the model and the criterion are also passed as parameter with introspection thingy as the optimizer,https://github.com/DIVA-DIA/DeepDIVA/commit/81906fc71337ac21282ed2a5afa1147264c767a4,Yes
1587,DIVA-DIA/DeepDIVA,template/standard.py,f92916ae2eeac5abe9ee29f807bcd2c5107626ca,TODO make way that the model and the criterion are also passed as parameter with introspection thingy as the optimizer,https://github.com/DIVA-DIA/DeepDIVA/commit/f92916ae2eeac5abe9ee29f807bcd2c5107626ca,Yes
1588,DIVA-DIA/DeepDIVA,template/runner/triplet/evaluate.py,d7f29fcc9f5f2417aab2c1d6e166b5561ff69d64,TODO: Make it parameterized to use top_n or FPR,https://github.com/DIVA-DIA/DeepDIVA/commit/d7f29fcc9f5f2417aab2c1d6e166b5561ff69d64,Yes
1589,DIVA-DIA/DeepDIVA,template/runner/convolutional_auto_encoder/setup.py,1d60331a22936564d0c4b7bba0c6f7a685d441b9,TODO: parameterize this out,https://github.com/DIVA-DIA/DeepDIVA/commit/1d60331a22936564d0c4b7bba0c6f7a685d441b9,Yes
1590,DIVA-DIA/DeepDIVA,template/setup.py,d9c48c532ad91cd2b27e6a584aa7d2f5f0bd0385,TODO: Remove or make param: map_location,https://github.com/DIVA-DIA/DeepDIVA/commit/d9c48c532ad91cd2b27e6a584aa7d2f5f0bd0385,Yes
1591,DIVA-DIA/DeepDIVA,template/setup.py,1f9efcacc11c53e2e0acf477d51b51d02873c179,TODO: Remove or make param: map_location,https://github.com/DIVA-DIA/DeepDIVA/commit/1f9efcacc11c53e2e0acf477d51b51d02873c179,Yes
1592,DIVA-DIA/DeepDIVA,template/setup.py,2b9dd6b0116a088a2a4fd3c9a5bd4c38509a3faa,TODO: Remove or make param: map_location,https://github.com/DIVA-DIA/DeepDIVA/commit/2b9dd6b0116a088a2a4fd3c9a5bd4c38509a3faa,Yes
1593,mercury-ml-team/mercury-ml,tests/test_common/test_data_set.py,4978a277bd99dea8d2a2e83c9e5936ca51d058e6,data_wrapper_params =  {k:{} for k in list(self.__dict__.keys())} #TODO: does this work?,https://github.com/mercury-ml-team/mercury-ml/commit/4978a277bd99dea8d2a2e83c9e5936ca51d058e6,Yes
1594,sdv-dev/Copulas,copulas/copulas.py,a0f7700fa16bd17eba3f018cb5d528545966b160,FIXME imports are missing and self.param doesn't exist,https://github.com/sdv-dev/Copulas/commit/a0f7700fa16bd17eba3f018cb5d528545966b160,Yes
1595,comic/grand-challenge.org,django/comicsite/models.py,dc82b6c7ff9e81b1999ae5b6d990baa67b93bf27,TODO: Sjoerd - Is it correct to define the params below as class params; or should these be in an init method?,https://github.com/comic/grand-challenge.org/commit/dc82b6c7ff9e81b1999ae5b6d990baa67b93bf27,Yes
1596,comic/grand-challenge.org,app/grandchallenge/algorithms/models.py,ec79efdcc2ad4202e3a9e56be7e5d46c2e32129a,TODO: Split out the ipynb description as a separate object,https://github.com/comic/grand-challenge.org/commit/ec79efdcc2ad4202e3a9e56be7e5d46c2e32129a,Yes
1597,openAGI/tefla,tefla/core/decoder.py,d1584118e319a0fb5b0b150fe5d63e6c788168ba,TODO: Make this a parameter: We may or may not want this.,https://github.com/openAGI/tefla/commit/d1584118e319a0fb5b0b150fe5d63e6c788168ba,Yes
1598,larq/larq,larq/layers.py,c36fc76a4dbccc6663db01041dc3ffe0a4f426e7,TODO: find a good way remove duplication between QuantizerBase; QuantizerDepthwiseBase and QuantizerSeparableBase,https://github.com/larq/larq/commit/c36fc76a4dbccc6663db01041dc3ffe0a4f426e7,Yes
1599,larq/larq,larq/layers_base.py,81d9b37cdfabdb43a7c516da4102d8de011aaaac,TODO: find a good way remove duplication between QuantizerBase; QuantizerDepthwiseBase and QuantizerSeparableBase,https://github.com/larq/larq/commit/81d9b37cdfabdb43a7c516da4102d8de011aaaac,Yes
1600,openml/automlbenchmark,runbenchmark.py,0795e1bf838177f1cd9e950bb5fdf2d96f996630,todo: allow a custom automlbenchmark_config.json in user directory: maybe this would allow removal of parameters like region; indir; outdir,https://github.com/openml/automlbenchmark/commit/0795e1bf838177f1cd9e950bb5fdf2d96f996630,Yes
1601,openml/automlbenchmark,automl/aws.py,2da86b32b16e40af86b2c422b0be375d181a1973,todo: parallelization improvement -> in many situations; creating a job for each fold may end up being much slower,https://github.com/openml/automlbenchmark/commit/2da86b32b16e40af86b2c422b0be375d181a1973,Yes
1602,openml/automlbenchmark,automl/results.py,ccc5d22b82b079806341c3fc7e8c3d0e10c2f416,params=str(framework_def.params); # TODO: enable this?,https://github.com/openml/automlbenchmark/commit/ccc5d22b82b079806341c3fc7e8c3d0e10c2f416,Yes
1603,SPFlow/SPFlow,src/spn/algorithms/Inference.py,17850af896582ee071cccda2c4362d06fca17aac,TODO: parallelize here,https://github.com/SPFlow/SPFlow/commit/17850af896582ee071cccda2c4362d06fca17aac,Yes
1604,SPFlow/SPFlow,src/spn/algorithms/Inference.py,758c72bd7c70e35f2f0bc106d7e9cab224a90e49,TODO: parallelize here,https://github.com/SPFlow/SPFlow/commit/758c72bd7c70e35f2f0bc106d7e9cab224a90e49,Yes
1605,SPFlow/SPFlow,src/spn/algorithms/Posteriors.py,758c72bd7c70e35f2f0bc106d7e9cab224a90e49,TODO: this is the same as in update the posterior parameters,https://github.com/SPFlow/SPFlow/commit/758c72bd7c70e35f2f0bc106d7e9cab224a90e49,Yes
1606,SPFlow/SPFlow,src/spn/algorithms/EM.py,394e46450265829907cdc6322600ad1f37828bac,TODO: do in parallel,https://github.com/SPFlow/SPFlow/commit/394e46450265829907cdc6322600ad1f37828bac,Yes
1607,Rostlab/nalaf,source/test.py,8b1ee30eadb0e9a8e447227f9020337eb1a67623,TODO pre-processing: strip new lines; just paragraphs,https://github.com/Rostlab/nalaf/commit/8b1ee30eadb0e9a8e447227f9020337eb1a67623,Yes
1608,Rostlab/nalaf,source/test.py,b28ad3ead46066154d92d6b968ff4de03a228c54,TODO import through parameters,https://github.com/Rostlab/nalaf/commit/b28ad3ead46066154d92d6b968ff4de03a228c54,Yes
1609,Rostlab/nalaf,source/test.py,72e3be9518b0f01432f6d2895238ec1a7d23ba69,TODO opt. parameter for whole documents,https://github.com/Rostlab/nalaf/commit/72e3be9518b0f01432f6d2895238ec1a7d23ba69,Yes
1610,Rostlab/nalaf,source/test.py,72e3be9518b0f01432f6d2895238ec1a7d23ba69,TODO find_all since there is more than just one paragraph,https://github.com/Rostlab/nalaf/commit/72e3be9518b0f01432f6d2895238ec1a7d23ba69,Yes
1611,Rostlab/nalaf,nala/test.py,15fceb0c50dc2d395caddb0528326710b7669584,TODO import through parameters,https://github.com/Rostlab/nalaf/commit/15fceb0c50dc2d395caddb0528326710b7669584,Yes
1612,Rostlab/nalaf,nala/structures/data.py,3025428549bf8b9394fb02e759a68e533f591ad6,TODO (4) nl mentions total vs min lettre parameter @graph @parameters,https://github.com/Rostlab/nalaf/commit/3025428549bf8b9394fb02e759a68e533f591ad6,Yes
1613,Rostlab/nalaf,nala/structures/data.py,3025428549bf8b9394fb02e759a68e533f591ad6,TODO (5) parametrizable min length (12..36) @parameters,https://github.com/Rostlab/nalaf/commit/3025428549bf8b9394fb02e759a68e533f591ad6,Yes
1614,Rostlab/nalaf,nala/utils/writers.py,74402de9d462f8bf452ee55e2e33b56fe3687bf3,TODO subplots for params,https://github.com/Rostlab/nalaf/commit/74402de9d462f8bf452ee55e2e33b56fe3687bf3,Yes
1615,Rostlab/nalaf,nala/utils/writers.py,74402de9d462f8bf452ee55e2e33b56fe3687bf3,TODO plt.axhan or sth like that for highlighting area of inclusive method with param,https://github.com/Rostlab/nalaf/commit/74402de9d462f8bf452ee55e2e33b56fe3687bf3,Yes
1616,Rostlab/nalaf,nala/utils/writers.py,1d4c586c776a1624cb1bfd3de843971f3457b989,TODO subplots for params,https://github.com/Rostlab/nalaf/commit/1d4c586c776a1624cb1bfd3de843971f3457b989,Yes
1617,Rostlab/nalaf,tests/test_features.py,5fde23fe506f88f8b1a5513dc60bd6167eea5325,TODO implement separate test functions for each feature that is already implemented in test_generate,https://github.com/Rostlab/nalaf/commit/5fde23fe506f88f8b1a5513dc60bd6167eea5325,Yes
1618,Rostlab/nalaf,tests/test_features.py,3a38b34cb332007524ebf47fbcdcd8fc65973e43,TODO implement separate test functions for each feature that is already implemented in test_generate,https://github.com/Rostlab/nalaf/commit/3a38b34cb332007524ebf47fbcdcd8fc65973e43,Yes
1619,Rostlab/nalaf,nala/utils/writers.py,890766743fe0a0b29cac409a877cf19249a330a4,TODO make interesting bars as param not hard coded,https://github.com/Rostlab/nalaf/commit/890766743fe0a0b29cac409a877cf19249a330a4,Yes
1620,Rostlab/nalaf,demo.py,a210b2e272269683924acacb27d5fb1034150c5f,TODO add param,https://github.com/Rostlab/nalaf/commit/a210b2e272269683924acacb27d5fb1034150c5f,Yes
1621,Rostlab/nalaf,nala/learning/evaluators.py,5b6580deca1bef80dc9107be0eaad18f836e5e60,TODO Clean up this; separate function,https://github.com/Rostlab/nalaf/commit/5b6580deca1bef80dc9107be0eaad18f836e5e60,Yes
1622,Rostlab/nalaf,nala/utils/pattern_eval.py,0876a2c30e19db560f475ac5aae3e07f1dbd86e4,todo param file to save to,https://github.com/Rostlab/nalaf/commit/0876a2c30e19db560f475ac5aae3e07f1dbd86e4,Yes
1623,Rostlab/nalaf,nala/bootstrapping/document_filters.py,fa6bf6c361267f89317590181797a4f1859543bc,todo write to file param + saving to manually annotate and find tp + fp for performance eval on each pattern,https://github.com/Rostlab/nalaf/commit/fa6bf6c361267f89317590181797a4f1859543bc,Yes
1624,Rostlab/nalaf,nala/structures/data.py,6331342413d88ab8146e71ec9b82c72059e7824e,TODO make parameterisable to just check for pure nl mentions,https://github.com/Rostlab/nalaf/commit/6331342413d88ab8146e71ec9b82c72059e7824e,Yes
1625,Rostlab/nalaf,nala/bootstrapping/document_filters.py,ebc888015369745abb197d3738c59a1e8d47df4d,todo write to file param + saving to manually annotate and find tp + fp for performance eval on each pattern,https://github.com/Rostlab/nalaf/commit/ebc888015369745abb197d3738c59a1e8d47df4d,Yes
1626,graknlabs/kglib,kglib/kgcn/learn/learn_IT.py,88690ec7bb66691e4d0b1e8b7c97e7e3f0277472,TODO Remove 'input' and 'solution' fields; only needed for plotting which should be separated,https://github.com/graknlabs/kglib/commit/88690ec7bb66691e4d0b1e8b7c97e7e3f0277472,Yes
1627,graknlabs/kglib,kglib/utils/grakn/object/thing.py,88690ec7bb66691e4d0b1e8b7c97e7e3f0277472,TODO Make attribute a separate class,https://github.com/graknlabs/kglib/commit/88690ec7bb66691e4d0b1e8b7c97e7e3f0277472,Yes
1628,graknlabs/kglib,kglib/kgcn/learn/learn_IT.py,8f015cdf63a86262e05bc242efbe432c71396eec,TODO Remove 'input' and 'solution' fields; only needed for plotting which should be separated,https://github.com/graknlabs/kglib/commit/8f015cdf63a86262e05bc242efbe432c71396eec,Yes
1629,neptune-ai/neptune-client,neptune_client_prototype/__init__.py,629b08421f3a430f4a6c9d8bc5513404ca6e064f,TODO confirming: parameter type stays fixed once the structure is created?,https://github.com/neptune-ai/neptune-client/commit/629b08421f3a430f4a6c9d8bc5513404ca6e064f,Yes
1630,IGITUGraz/L2L,ltl/optimizers/optimizer.py,b7640142cda11372881958b521f3d287a450f5bd,TODO: Set eval_pop to the values of parameters you want to evaluate in the next cycle,https://github.com/IGITUGraz/L2L/commit/b7640142cda11372881958b521f3d287a450f5bd,Yes
1631,IGITUGraz/L2L,bin/ltl-template.py,43c176fae4287db40ab4c1a5f91ad7860b91b115,TODO: Change the names; ids and parameters passed in,https://github.com/IGITUGraz/L2L/commit/43c176fae4287db40ab4c1a5f91ad7860b91b115,Yes
1632,Aifred-Health/Vulcan,vulcanai/dataloaders/base_dataloader.py,636fca330010e1437a702a115ab28d287a1586bc,TODO: this will be a lot of params.,https://github.com/Aifred-Health/Vulcan/commit/636fca330010e1437a702a115ab28d287a1586bc,Yes
1633,Aifred-Health/Vulcan,vulcanai/engines/base_engine.py,636fca330010e1437a702a115ab28d287a1586bc,TODO: this will be a lot of params.,https://github.com/Aifred-Health/Vulcan/commit/636fca330010e1437a702a115ab28d287a1586bc,Yes
1634,Aifred-Health/Vulcan,vulcanai/models/base_model.py,636fca330010e1437a702a115ab28d287a1586bc,TODO: this will be a lot of params.,https://github.com/Aifred-Health/Vulcan/commit/636fca330010e1437a702a115ab28d287a1586bc,Yes
1635,Aifred-Health/Vulcan,vulcanai2/models/base_model.py,e281d04e77519923e38af2b17df487aace66b449,TODO: this will be a lot of params.,https://github.com/Aifred-Health/Vulcan/commit/e281d04e77519923e38af2b17df487aace66b449,Yes
1636,Aifred-Health/Vulcan,vulcanai2/models/AbstractNetwork.py,56bfa5aebecedaaed14d4d858b4adfdfafbed214,TODO: how to deal with passing parameters especially if custom; given the note here: https:\/\/pytorch.org\/docs\/stable\/optim.html,https://github.com/Aifred-Health/Vulcan/commit/56bfa5aebecedaaed14d4d858b4adfdfafbed214,Yes
1637,Aifred-Health/Vulcan,vulcanai2/models/BaseNetwork.py,ce84010a82a5d65a373dacac6b85c7084ebcfa38,TODO: include plot as parameter,https://github.com/Aifred-Health/Vulcan/commit/ce84010a82a5d65a373dacac6b85c7084ebcfa38,Yes
1638,Aifred-Health/Vulcan,vulcanai2/models/BaseNetwork.py,6344b9cc271163e168674f76585b5288682a09f1,TODO: include plot as parameter,https://github.com/Aifred-Health/Vulcan/commit/6344b9cc271163e168674f76585b5288682a09f1,Yes
1639,Aifred-Health/Vulcan,vulcanai2/datasets/tabulardataset.py,f5dbd783b9da29e4309b564971860848d6b861d0,TODO: edit this method that creates a split given different filepaths or objects so that the params match,https://github.com/Aifred-Health/Vulcan/commit/f5dbd783b9da29e4309b564971860848d6b861d0,Yes
1640,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,a76f355efbbeca06b556ffb08d7ebcd91979eeb2,TODO: Priya Please define parameters.,https://github.com/Aifred-Health/Vulcan/commit/a76f355efbbeca06b556ffb08d7ebcd91979eeb2,Yes
1641,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,a76f355efbbeca06b556ffb08d7ebcd91979eeb2,TODO: this param doesn't actually exist,https://github.com/Aifred-Health/Vulcan/commit/a76f355efbbeca06b556ffb08d7ebcd91979eeb2,Yes
1642,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,a76f355efbbeca06b556ffb08d7ebcd91979eeb2,TODO: Priya: why do we need activation and pred_activation as parameters here?,https://github.com/Aifred-Health/Vulcan/commit/a76f355efbbeca06b556ffb08d7ebcd91979eeb2,Yes
1643,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,21acea06419eed157ee71f7bb7db75e0447d846c,TODO: this is kinda dumb also you're not passing params... they shouldn't have given you a dataloader,https://github.com/Aifred-Health/Vulcan/commit/21acea06419eed157ee71f7bb7db75e0447d846c,Yes
1644,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,10880bcd1ff5b1d68104db3f070c634b5699d689,TODO: this is kinda dumb also you're not passing params... they shouldn't have given you a dataloader,https://github.com/Aifred-Health/Vulcan/commit/10880bcd1ff5b1d68104db3f070c634b5699d689,Yes
1645,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,73f1b9de7f8fdae8fddd5bcd1cd3472a9cd9ee52,TODO: deal with repeated default parameters,https://github.com/Aifred-Health/Vulcan/commit/73f1b9de7f8fdae8fddd5bcd1cd3472a9cd9ee52,Yes
1646,Aifred-Health/Vulcan,vulcanai2/models/metrics.py,73f1b9de7f8fdae8fddd5bcd1cd3472a9cd9ee52,TODO: improve the copying of parameters,https://github.com/Aifred-Health/Vulcan/commit/73f1b9de7f8fdae8fddd5bcd1cd3472a9cd9ee52,Yes
1647,Aifred-Health/Vulcan,vulcanai2/models/metrics.py,73f1b9de7f8fdae8fddd5bcd1cd3472a9cd9ee52,TODO: properly pass params,https://github.com/Aifred-Health/Vulcan/commit/73f1b9de7f8fdae8fddd5bcd1cd3472a9cd9ee52,Yes
1648,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,ac75d6823e58bc870944f0c8a4fda68749108d99,TODO: deal with repeated default parameters,https://github.com/Aifred-Health/Vulcan/commit/ac75d6823e58bc870944f0c8a4fda68749108d99,Yes
1649,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,e3b9ac7129c1c406eb171bb112c6351cfe6fc201,TODO: deal with repeated default parameters,https://github.com/Aifred-Health/Vulcan/commit/e3b9ac7129c1c406eb171bb112c6351cfe6fc201,Yes
1650,Aifred-Health/Vulcan,vulcanai2/models/metrics.py,e3b9ac7129c1c406eb171bb112c6351cfe6fc201,#TODO: improve the copying of parameters,https://github.com/Aifred-Health/Vulcan/commit/e3b9ac7129c1c406eb171bb112c6351cfe6fc201,Yes
1651,Aifred-Health/Vulcan,vulcanai2/models/metrics.py,e3b9ac7129c1c406eb171bb112c6351cfe6fc201,TODO: properly pass params,https://github.com/Aifred-Health/Vulcan/commit/e3b9ac7129c1c406eb171bb112c6351cfe6fc201,Yes
1652,Aifred-Health/Vulcan,vulcanai2/datasets/tabulardataset.py,253ee8f86d9449d322f61ddeea19baea789a0047,TODO: edit this method that creates a split given different filepaths or objects so that the params match,https://github.com/Aifred-Health/Vulcan/commit/253ee8f86d9449d322f61ddeea19baea789a0047,Yes
1653,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,df4232124e55a35c0f9197decdd65cb29331d810,TODO: deal with repeated default parameters,https://github.com/Aifred-Health/Vulcan/commit/df4232124e55a35c0f9197decdd65cb29331d810,Yes
1654,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,18706df2da167c0fdc9ee15e930292be74ea2356,TODO: deal with repeated default parameters,https://github.com/Aifred-Health/Vulcan/commit/18706df2da167c0fdc9ee15e930292be74ea2356,Yes
1655,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,481ffd6dddd71cb5cdfc59c0d6823074be6342fc,TODO: deal with repeated default parameters,https://github.com/Aifred-Health/Vulcan/commit/481ffd6dddd71cb5cdfc59c0d6823074be6342fc,Yes
1656,Aifred-Health/Vulcan,vulcanai2/datasets/tabulardataset.py,6bfa36b133a7acb270c6425498fd721c7823de0f,TODO: edit this method that creates a split given different filepaths or objects so that the params match,https://github.com/Aifred-Health/Vulcan/commit/6bfa36b133a7acb270c6425498fd721c7823de0f,Yes
1657,Aifred-Health/Vulcan,vulcanai/models/basenetwork.py,39b5ad778d0f672b2c763b878b0ed159e8d69479,TODO: deal with repeated default parameters,https://github.com/Aifred-Health/Vulcan/commit/39b5ad778d0f672b2c763b878b0ed159e8d69479,Yes
1658,Aifred-Health/Vulcan,vulcanai/models/metrics.py,39b5ad778d0f672b2c763b878b0ed159e8d69479,#TODO: improve the copying of parameters,https://github.com/Aifred-Health/Vulcan/commit/39b5ad778d0f672b2c763b878b0ed159e8d69479,Yes
1659,Aifred-Health/Vulcan,vulcanai/models/metrics.py,39b5ad778d0f672b2c763b878b0ed159e8d69479,TODO: properly pass params,https://github.com/Aifred-Health/Vulcan/commit/39b5ad778d0f672b2c763b878b0ed159e8d69479,Yes
1660,Aifred-Health/Vulcan,vulcanai/models/basenetwork.py,3808f589c2773c493de0df6bb05b87c9594ce32d,TODO: need to update transform callable params to match that of,https://github.com/Aifred-Health/Vulcan/commit/3808f589c2773c493de0df6bb05b87c9594ce32d,Yes
1661,mideind/GreynirCorrect,src/reynir_correct/errtokenizer.py,00f132c653c219aebd98185a78be105a34284c73,TODO could add normalize_ellipsis as a parameter here,https://github.com/mideind/GreynirCorrect/commit/00f132c653c219aebd98185a78be105a34284c73,Yes
1662,nltk/nltk,src/nltk/probability.py,60d523138223c7ac0a978c62a1889e685d73606a,TODO - add a cut-off parameter; above which the counts are unmodified,https://github.com/nltk/nltk/commit/60d523138223c7ac0a978c62a1889e685d73606a,Yes
1663,nltk/nltk,lite/nltk_lite/probability.py,f9fc8bcdafd590084a7cf6e77c57981a224d07ea,"\""\""\"" || Classes for representing and processing probabilistic information. ||  || The L{FreqDist} class is used to encode X{frequency distributions}; || which count the number of times that each outcome of an experiment || occurs. ||  || The L{ProbDistI} class defines a standard interface for X{probability || distributions}; which encode the probability of each outcome for an || experiment.  There are two types of probability distribution: ||  ||   - X{derived probability distributions} are created from frequency ||     distributions.  They attempt to model the probability distribution ||     that generated the frequency distribution. ||   - X{analytic probability distributions} are created directly from ||     parameters (such as variance). ||  || The L{ConditionalFreqDist} class and L{ConditionalProbDistI} interface || are used to encode conditional distributions.  Conditional probability || distributions can be derived or analytic; but currently the only || implementation of the C{ConditionalProbDistI} interface is || L{ConditionalProbDist}; a derived distribution. ||  || The L{ProbabilisticMixIn} class is a mix-in class that can be used to || associate probabilities with data classes (such as C{Token} or || C{Tree}). ||  || @group Frequency Distributions: FreqDist || @group Derived Probability Distributions: ProbDistI; MLEProbDist; ||     LidstoneProbDist; LaplaceProbDist; ELEProbDist; HeldoutProbDist; ||     CrossValidationProbDist || @group Analyitic Probability Distributions: UniformProbDist || @group Conditional Distributions: ConditionalFreqDist; ||     ConditionalProbDistI; ConditionalProbDist || @group Probabilistic Mix-In: ProbabilisticMixIn || @sort: FreqDist; ProbDistI; MLEProbDist; LidstoneProbDist; LaplaceProbDist;  ||     ELEProbDist; HeldoutProbDist; CrossValidationProbDist; UniformProbDist; ||     ConditionalFreqDist; ConditionalProbDistI; ConditionalProbDist ||  || @todo: Better handling of log probabilities. || \""\""\""",https://github.com/nltk/nltk/commit/f9fc8bcdafd590084a7cf6e77c57981a224d07ea,Yes
1664,nltk/nltk,lite/nltk_lite/probability.py,f9fc8bcdafd590084a7cf6e77c57981a224d07ea,TODO - add a cut-off parameter; above which the counts are unmodified,https://github.com/nltk/nltk/commit/f9fc8bcdafd590084a7cf6e77c57981a224d07ea,Yes
1665,nltk/nltk,nltk_lite/probability.py,603384c5576ae07862766cd332c9ab5059a029ae,TODO - add a cut-off parameter; above which the counts are unmodified,https://github.com/nltk/nltk/commit/603384c5576ae07862766cd332c9ab5059a029ae,Yes
1666,nltk/nltk,nltk/featstruct.py,1c976cdd388634b4c3cfb24110083d35a21837ff,"\""\""\"" || Basic data classes for representing feature structures.  A X{feature || structure} is a mapping from feature names to feature values; where: ||  ||   - Each X{feature name} is a case sensitive string. ||   - Each X{feature value} can be a base value (such as a string); a ||     variable; or a nested feature structure. ||  || Feature structures are typically used to represent partial information || about objects.  A feature name that is not mapped to a value stands || for a feature whose value is unknown (I{not} a feature without a || value).  Two feature structures that represent (potentially || overlapping) information about the same object can be combined by || X{unification}.  When two inconsistent feature structures are unified; || the unification fails and returns C{None}. ||  || Features can be specified using X{feature paths}; or tuples of feature || names that specify path through the nested feature structures to a || value.  Feature structures may contain reentrant feature values.  A || X{reentrant feature value} is a single feature value that can be || accessed via multiple feature paths.  Unification preserves the || reentrance relations imposed by both of the unified feature || structures.  In the feature structure resulting from unification; any || modifications to a reentrant feature value will be visible using any || of its feature paths.  Feature structures may also contain X{cyclic || feature values}; i.e.; values that recursively contain themself. ||  || Feature structure variables are encoded using the L{nltk.sem.Variable} || class.  The variables' values are tracked using a X{bindings} || dictionary; which maps variables to their values.  When two feature || structures are unified; a fresh bindings dictionary is created to || track their values; and before unification completes; all bound || variables are replaced by their values.  Thus; the bindings || dictionaries are usually strictly internal to the unification process. || However; it is possible to track the bindings of variables if you || choose to; by supplying your own initial bindings dictionary to the || L{unify() <FeatStruct.unify>} method. ||  || When unbound variables are unified with one another; they become || X{aliased}.  This is encoded by binding one variable to the other. ||  || @todo: add a fail parameter to unify?  This would be a function that ||    would be called if unificaiton fails; it could either raise a ||    UnificationFailure error; or return a value.  How would this be ||    useful?  Well; one example is that it could be used to find a ||    \""diff\"" between two feature structures -- i.e.; a list of all ||    feature paths with different values.  Anyway; the old version had ||    it.  Ask steven why it was introduced? ||  || @todo: Figure out yaml support.  Do we need any? ||  || @todo: support for mutable feature structures? ||  || @todo: define __div__ for feature structures? ||  || relative to category; we don't define... ||   - .symbol (we're not a Nonterminal) ||   - .head() ||   - .feature_names(); .has_features() -- eh ||   - .to_yaml() and .from_yaml() ||   - parsing of cfgs.. ||  || \""\""\""",https://github.com/nltk/nltk/commit/1c976cdd388634b4c3cfb24110083d35a21837ff,Yes
1667,nltk/nltk,nltk/classify/svm.py,8793ed7e0cdccf4c319a7270bb05f04e8e6dbd8f,TODO: implement passing of SVMlight parameters from train() to learn(),https://github.com/nltk/nltk/commit/8793ed7e0cdccf4c319a7270bb05f04e8e6dbd8f,Yes
1668,tensorflow/tensor2tensor,tensor2tensor/data_generators/librispeech.py,5365113cc17db280974f7c80e8c6847aec235fe8,TODO: clean up hparams,https://github.com/tensorflow/tensor2tensor/commit/5365113cc17db280974f7c80e8c6847aec235fe8,Yes
1669,tensorflow/tensor2tensor,tensor2tensor/data_generators/librispeech.py,6d9b5e1cc01518c033569090faf6fbe519517971,TODO: clean up hparams,https://github.com/tensorflow/tensor2tensor/commit/6d9b5e1cc01518c033569090faf6fbe519517971,Yes
1670,tensorflow/tensor2tensor,tensor2tensor/data_generators/gym.py,3cf6515f55749369e5665de2454b71fbc2dfefc5,Todo: think how to pass parameters,https://github.com/tensorflow/tensor2tensor/commit/3cf6515f55749369e5665de2454b71fbc2dfefc5,Yes
1671,tensorflow/tensor2tensor,tensor2tensor/rl/envs/utils.py,b1f5614cf066afc294a6bbdf54cb421a2d3716cc,TODO: pm->B\u0142a\u017Cej. Should the paramters be infered.,https://github.com/tensorflow/tensor2tensor/commit/b1f5614cf066afc294a6bbdf54cb421a2d3716cc,Yes
1672,tensorflow/tensor2tensor,tensor2tensor/rl/envs/utils.py,f7c28e8089ff66f8915493f22bc81c71b3cb6acb,TODO: pm->B\u0142a\u017Cej. Should the paramters be infered.,https://github.com/tensorflow/tensor2tensor/commit/f7c28e8089ff66f8915493f22bc81c71b3cb6acb,Yes
1673,tensorflow/tensor2tensor,tensor2tensor/rl/envs/utils.py,32396a9ce6699bb5222f17ee4bd6e5bcd729942f,TODO: pm->B\u0142a\u017Cej. Should the paramters be infered.,https://github.com/tensorflow/tensor2tensor/commit/32396a9ce6699bb5222f17ee4bd6e5bcd729942f,Yes
1674,tensorflow/tensor2tensor,tensor2tensor/data_generators/allen_brain.py,7967b446c5a0f7e7ea2e6b39150abf850abec463,"\""\""\""Problem definitions for Allen Brain Atlas problems. ||  || Notes: ||  ||   * TODO(cwbeitel): Want to be able to increase up-sampling ratio and\/or ||     in-paint fraction over the course of training. This could be done by ||     defining a range of problems or perhaps more aptly with an hparam ||     that is dialed up depending on training performance. ||  || \""\""\""",https://github.com/tensorflow/tensor2tensor/commit/7967b446c5a0f7e7ea2e6b39150abf850abec463,Yes
1675,tensorflow/tensor2tensor,tensor2tensor/rl/trainer_model_based_params.py,1b16365fef49747163939c86686043eb195c061f,TODO(blazej): this param is unused,https://github.com/tensorflow/tensor2tensor/commit/1b16365fef49747163939c86686043eb195c061f,Yes
1676,tensorflow/tensor2tensor,tensor2tensor/rl/trainer_model_based.py,7c14ecd1210c0a56fcde63c227b392688f821883,TODO(blazej): this param is unused,https://github.com/tensorflow/tensor2tensor/commit/7c14ecd1210c0a56fcde63c227b392688f821883,Yes
1677,tensorflow/tensor2tensor,tensor2tensor/layers/bayes.py,c864603bad91503bd198a7a410603bf2d85d9d6a,TODO(trandustin): Hack to store parameters so KL reg. can operate on them.,https://github.com/tensorflow/tensor2tensor/commit/c864603bad91503bd198a7a410603bf2d85d9d6a,Yes
1678,tensorflow/tensor2tensor,tensor2tensor/data_generators/gym_env.py,f14cf8d3d571b9b811f29b3c9bf5d72b87619880,TODO(afrozm): Why is this separated out from _preprocess_observations?,https://github.com/tensorflow/tensor2tensor/commit/f14cf8d3d571b9b811f29b3c9bf5d72b87619880,Yes
1679,tensorflow/tensor2tensor,tensor2tensor/trax/models/research/transformer_revnet.py,8c23cbb2f3634d7ba2d9ade1c88b935e07197218,TODO(kitaev): Figure out why parameter sharing doesn't work (if this,https://github.com/tensorflow/tensor2tensor/commit/8c23cbb2f3634d7ba2d9ade1c88b935e07197218,Yes
1680,tensorflow/tensor2tensor,tensor2tensor/trax/optimizers/base.py,210282baf89580a081ef6952264fb1ea50d22300,TODO(levskaya): refactor to use newer RL friendly parameter passing.,https://github.com/tensorflow/tensor2tensor/commit/210282baf89580a081ef6952264fb1ea50d22300,Yes
1681,RaRe-Technologies/gensim,src/gensim/models/lsimodel.py,867acdb02d141aa7cf6cf29b8aa42ce0b90e0c4c,FIXME: keep the tUp rotations separate ala eq (11)? this would mean discarding every P as soon as we hit the target rank; is that ok?,https://github.com/RaRe-Technologies/gensim/commit/867acdb02d141aa7cf6cf29b8aa42ce0b90e0c4c,Yes
1682,RaRe-Technologies/gensim,src/gensim/models/lsi_dispatcher.py,a83ca9100d0ce5d25972a893bf8cec624888667f,TODO: merge in parallel; so that we're done in `log_2(workers)` merges;,https://github.com/RaRe-Technologies/gensim/commit/a83ca9100d0ce5d25972a893bf8cec624888667f,Yes
1683,RaRe-Technologies/gensim,src/gensim/corpora/wikiExternParsingCorpus.py,d77450c7df7e5877947727d0c8264bf7928d5e65,todo: make filtering optional with a parameter,https://github.com/RaRe-Technologies/gensim/commit/d77450c7df7e5877947727d0c8264bf7928d5e65,Yes
1684,RaRe-Technologies/gensim,src/gensim/corpora/wikiExternalParsingCorpus.py,f767972fe4f68698dfafd6e0f55e3df9797e5260,TODO: make filtering optional with a parameter,https://github.com/RaRe-Technologies/gensim/commit/f767972fe4f68698dfafd6e0f55e3df9797e5260,Yes
1685,RaRe-Technologies/gensim,gensim/similarities/simserver.py,6d8151cff261cbe5a367d956f8aee64835afa1b4,TODO: use subclassing\/injection for different methods; instead of param?,https://github.com/RaRe-Technologies/gensim/commit/6d8151cff261cbe5a367d956f8aee64835afa1b4,Yes
1686,RaRe-Technologies/gensim,gensim/utils.py,aff353fdbff2297442a0083f8f98f1b137a1be14,TODO convert to csr\/csc => stores .indices .data .indptr arrays separately,https://github.com/RaRe-Technologies/gensim/commit/aff353fdbff2297442a0083f8f98f1b137a1be14,Yes
1687,RaRe-Technologies/gensim,gensim/models/ldamodelmulticore.py,a41f896f0d9cffc624c2876de729b2f6af3f739d,TODO: full explicit init params; incl. defaults,https://github.com/RaRe-Technologies/gensim/commit/a41f896f0d9cffc624c2876de729b2f6af3f739d,Yes
1688,RaRe-Technologies/gensim,gensim/models/ldamodelmulticore.py,fc0d6811a61c34b852a46b68523df7aa81d84081,"\""\""\"" || Latent Dirichlet Allocation (LDA) in Python; using all cores to parallelize and || speed up model training. ||  || The parallelization uses multiprocessing; in case this doesn't work for you for || some reason; try `LdaModel` which is an equivalent; but more straightforward and || single-core implementation. ||  || FIXME wiki timings ||  || This module allows both LDA model estimation from a training corpus and inference of topic || distribution on new; unseen documents. The model can also be updated with new documents || for online training. ||  || The core estimation code is based on the `onlineldavb.py` script by M. Hoffman [1]_; see || **Hoffman; Blei; Bach: Online Learning for Latent Dirichlet Allocation; NIPS 2010.** ||  || The algorithm: ||  || * is **streamed**: training documents may come in sequentially; no random access required; || * runs in **constant memory** w.r.t. the number of documents: size of the ||   training corpus does not affect memory footprint; can process corpora larger than RAM; and || * is **distributed**: makes use of a cluster of machines; if available; to ||   speed up model estimation. ||  || .. [1] http:\/\/www.cs.princeton.edu\/~mdhoffma || \""\""\""",https://github.com/RaRe-Technologies/gensim/commit/fc0d6811a61c34b852a46b68523df7aa81d84081,Yes
1689,RaRe-Technologies/gensim,gensim/models/doc2vec.py,09a30b3c7f0e5c702affb0faf5e2f5d28d6bbe44,## TODO: save docvecs in same separate-numpy-file style,https://github.com/RaRe-Technologies/gensim/commit/09a30b3c7f0e5c702affb0faf5e2f5d28d6bbe44,Yes
1690,PetrochukM/PyTorch-NLP,lib/checkpoint.py,0cdbcfbe4a5508e902d60152001b29fb0f331fe4,TODO: This should be recusive looking for any module with flatten_parameters,https://github.com/PetrochukM/PyTorch-NLP/commit/0cdbcfbe4a5508e902d60152001b29fb0f331fe4,Yes
1691,PetrochukM/PyTorch-NLP,lib/configurable.py,0cdbcfbe4a5508e902d60152001b29fb0f331fe4,TODO: Does not print all parameters; FIX,https://github.com/PetrochukM/PyTorch-NLP/commit/0cdbcfbe4a5508e902d60152001b29fb0f331fe4,Yes
1692,PetrochukM/PyTorch-NLP,lib/nn/seq_decoder.py,0cdbcfbe4a5508e902d60152001b29fb0f331fe4,TODO: Fix by having a is_cuda parameter,https://github.com/PetrochukM/PyTorch-NLP/commit/0cdbcfbe4a5508e902d60152001b29fb0f331fe4,Yes
1693,PetrochukM/PyTorch-NLP,examples/end_to_end/main.py,c6c351965bf22965765f837c4b83bfdb02cb4743,TODO: Return the best loss if hyperparameter tunning.,https://github.com/PetrochukM/PyTorch-NLP/commit/c6c351965bf22965765f837c4b83bfdb02cb4743,Yes
1694,yzhao062/pyod,pyod/models/feature_bagging.py,230d9a40712456048b5c8f685dd0a38f330d438f,TODO: add a check for estimator_param,https://github.com/yzhao062/pyod/commit/230d9a40712456048b5c8f685dd0a38f330d438f,Yes
1695,yzhao062/pyod,pyod/models/feature_bagging.py,851da67b32b52b4c7ebb8d60e906ad8466600ab2,TODO: should support parallelization at the model level,https://github.com/yzhao062/pyod/commit/851da67b32b52b4c7ebb8d60e906ad8466600ab2,Yes
1696,yzhao062/pyod,pyod/models/knn.py,8a655def2f6a57e864bd1500647338db4c710e87,TODO: algorithm parameter is deprecated and will be removed in 0.7.6.,https://github.com/yzhao062/pyod/commit/8a655def2f6a57e864bd1500647338db4c710e87,Yes
1697,yzhao062/pyod,pyod/models/knn.py,8a655def2f6a57e864bd1500647338db4c710e87,TODO: since Ball_tree is used by default; may introduce its parameters.,https://github.com/yzhao062/pyod/commit/8a655def2f6a57e864bd1500647338db4c710e87,Yes
1698,yzhao062/pyod,pyod/models/knn.py,497e788909428842867da3a55f45cad297240a62,TODO: algorithm parameter is deprecated and will be removed in 0.7.6.,https://github.com/yzhao062/pyod/commit/497e788909428842867da3a55f45cad297240a62,Yes
1699,yzhao062/pyod,pyod/models/knn.py,497e788909428842867da3a55f45cad297240a62,TODO: since Ball_tree is used by default; may introduce its parameters.,https://github.com/yzhao062/pyod/commit/497e788909428842867da3a55f45cad297240a62,Yes
1700,ddbourgin/numpy-ml,neural_nets/models/vae.py,b6fdb165ffb3b447c5d53b879ce15c1720ddf87b,TODO: parallelize inner loop,https://github.com/ddbourgin/numpy-ml/commit/b6fdb165ffb3b447c5d53b879ce15c1720ddf87b,Yes
1701,prihoda/golem,golem/core/parsing/golem_extractor.py,fd1d47c4ac5dd7888655102104aec121d72a6e7b,TODO use a separate thread (pool) to remove TF memory overhead,https://github.com/prihoda/golem/commit/fd1d47c4ac5dd7888655102104aec121d72a6e7b,Yes
1702,prihoda/golem,golem/core/dialog_manager.py,15e67b0a5b01369a0b7dc506cf2203ef56e76c39,TODO we can either have ':init' or a bool parameter,https://github.com/prihoda/golem/commit/15e67b0a5b01369a0b7dc506cf2203ef56e76c39,Yes
1703,prihoda/golem,golem/core/dialog_manager.py,5986a4f8134c67e22c1e34412f3d570a25455af3,TODO we can either have ':init' or a bool parameter,https://github.com/prihoda/golem/commit/5986a4f8134c67e22c1e34412f3d570a25455af3,Yes
1704,prihoda/golem,golem/core/dialog_manager.py,c0873577bc7742eb141a238c43cc11f696e9f2d2,TODO we can either have ':init' or a bool parameter,https://github.com/prihoda/golem/commit/c0873577bc7742eb141a238c43cc11f696e9f2d2,Yes
1705,rsokl/MyGrad,mygrad/nnet/layers/batchnorm.py,8703db0b886f2c5e0f9874ea64b532f90c276baf,TODO: Remove affine parameters from Operation,https://github.com/rsokl/MyGrad/commit/8703db0b886f2c5e0f9874ea64b532f90c276baf,Yes
1706,raamana/neuropredict,rhst.py,cbb7ecb70c7cb5ad2abfa3eb8130ad021734fa8c,TODO generate visualizations for each feature set as well as a comparative summary!,https://github.com/raamana/neuropredict/commit/cbb7ecb70c7cb5ad2abfa3eb8130ad021734fa8c,Yes
1707,raamana/neuropredict,rhst.py,c5a153e9325a513d855483c27889404a60a54a2b,TODO implement a multi-process version as differnt rep's are embarrasingly parallel,https://github.com/raamana/neuropredict/commit/c5a153e9325a513d855483c27889404a60a54a2b,Yes
1708,raamana/neuropredict,neuropredict/rhst.py,ea8604b6967062a2d60bb8aa2f3d633bb0265890,TODO implement a multi-process version as differnt rep's are embarrasingly parallel,https://github.com/raamana/neuropredict/commit/ea8604b6967062a2d60bb8aa2f3d633bb0265890,Yes
1709,raamana/neuropredict,neuropredict/rhst.py,ea8604b6967062a2d60bb8aa2f3d633bb0265890,TODO generate visualizations for each feature set as well as a comparative summary!,https://github.com/raamana/neuropredict/commit/ea8604b6967062a2d60bb8aa2f3d633bb0265890,Yes
1710,raamana/neuropredict,neuropredict/visualize.py,0137716674260ec622b909dfa76d03e34b897b6d,TODO separate this calculation for use in exporting of results,https://github.com/raamana/neuropredict/commit/0137716674260ec622b909dfa76d03e34b897b6d,Yes
1711,raamana/neuropredict,neuropredict/rhst.py,3dfab81c1e8f2bc07fcd58cac91fb0a578ee58ab,TODO try parallelizing this. Think about how it interacts with parallel processing at CV rep level,https://github.com/raamana/neuropredict/commit/3dfab81c1e8f2bc07fcd58cac91fb0a578ee58ab,Yes
1712,raamana/neuropredict,neuropredict/neuropredict.py,faa4f6868f3e19acc82c1d44eb7e07e1c385f4a5,TODO need to be able to parallelize at subgroup- and method-level,https://github.com/raamana/neuropredict/commit/faa4f6868f3e19acc82c1d44eb7e07e1c385f4a5,Yes
1713,raamana/neuropredict,neuropredict/rhst.py,faa4f6868f3e19acc82c1d44eb7e07e1c385f4a5,TODO to achieve feature- or method-level parallization;,https://github.com/raamana/neuropredict/commit/faa4f6868f3e19acc82c1d44eb7e07e1c385f4a5,Yes
1714,raamana/neuropredict,neuropredict/run_workflow.py,7c6ec11298238e4a152d6b14ca0ede304d697a50,TODO need to be able to parallelize at subgroup- and method-level,https://github.com/raamana/neuropredict/commit/7c6ec11298238e4a152d6b14ca0ede304d697a50,Yes
1715,raamana/neuropredict,neuropredict/run_workflow.py,93e317be2135ad6882c7a6b4b1ccc221569e5266,TODO need to be able to parallelize at subgroup- and method-level,https://github.com/raamana/neuropredict/commit/93e317be2135ad6882c7a6b4b1ccc221569e5266,Yes
1716,raamana/neuropredict,neuropredict/rhst.py,34154175007ac9740d0d366ff4c767796e3d2f67,TODO returning preprocessor blindly without any parameters,https://github.com/raamana/neuropredict/commit/34154175007ac9740d0d366ff4c767796e3d2f67,Yes
1717,raamana/neuropredict,neuropredict/algorithms.py,d2a9d7cd2c7f0bc4789e3923ebf7a296060688fd,TODO returning preprocessor blindly without any parameters,https://github.com/raamana/neuropredict/commit/d2a9d7cd2c7f0bc4789e3923ebf7a296060688fd,Yes
1718,raamana/neuropredict,neuropredict/algorithms.py,266cf74e75e84ed3a786a76a3e15cb6ca7bad148,TODO eigen_solver; path_method could be hyper params for Isomap,https://github.com/raamana/neuropredict/commit/266cf74e75e84ed3a786a76a3e15cb6ca7bad148,Yes
1719,raamana/neuropredict,neuropredict/base.py,5ade4e71d2093b858e08aa2c9efc2e67b089b684,TODO find ways to parallelize - there is a separate branch for this,https://github.com/raamana/neuropredict/commit/5ade4e71d2093b858e08aa2c9efc2e67b089b684,Yes
1720,omimo/PyMO,pymo/rotation_tools.py,e1a5b1e261525394fe073af6462023aba5e0fac9,TODO: Check exp map params,https://github.com/omimo/PyMO/commit/e1a5b1e261525394fe073af6462023aba5e0fac9,Yes
1721,bhargavvader/pycobra,pycobra/cobra.py,2822360217b1bb78cc76ef7265f914c36f8b902a,"\""\""\"" || TODO: ||  || 1) Vectorise\/cythonise the predict method ||     - could it also be parallelised? || 2) Scikit-Learn conventions ||     - fit - does this kind of thing work here? || 3) Wrappers for other popular ML\/stat libraries ||     - mlpy ||     - all NN libraries ||     - ensembles; etc ||     - your own array as well || 4) Support Set algo: use indice_info from visualisations || \""\""\""",https://github.com/bhargavvader/pycobra/commit/2822360217b1bb78cc76ef7265f914c36f8b902a,No
1722,silvandeleemput/memcnn,memcnn/experiment/tests/test_manager.py,55fa6124c1b935927569735d8247df1f5ddb6ce4,TODO fix state dict optimizer parameters asserts,https://github.com/silvandeleemput/memcnn/commit/55fa6124c1b935927569735d8247df1f5ddb6ce4,Yes
1723,adalca/neurite,src/pytools/callbacks.py,edc4a31383288e8722778e1a88c3d5c136055116,TODO: Separate the part that outputs slices.,https://github.com/adalca/neurite/commit/edc4a31383288e8722778e1a88c3d5c136055116,Yes
1724,ysig/GraKeL,grakel/graph_kernels.py,b4913894b3ad9c51eabe4d59fa659cc61cc7cd6c,TODO: maybe apply parallelization?,https://github.com/ysig/GraKeL/commit/b4913894b3ad9c51eabe4d59fa659cc61cc7cd6c,Yes
1725,nschaetti/EchoTorch,echotorch/nn/conceptors/IncSPESN.py,d99a6affee2a69b9d27d0a9245e919b81bed0141,TODO: Add a parameter for averaged output,https://github.com/nschaetti/EchoTorch/commit/d99a6affee2a69b9d27d0a9245e919b81bed0141,Yes
1726,Mariewelt/OpenChem,openchem/models/MolecularRNN.py,a6e855145cc27c3705c70dc5a9c15f4dac855e82,TODO: implement required params,https://github.com/Mariewelt/OpenChem/commit/a6e855145cc27c3705c70dc5a9c15f4dac855e82,Yes
1727,keiffster/program-y,src/programy/extensions/weather/weather.py,dd21e108b9862f7667f7310338852c1ad9612d42,"TODO Use 'when\"" paramter to extract datapoints",https://github.com/keiffster/program-y/commit/dd21e108b9862f7667f7310338852c1ad9612d42,Yes
1728,PyTorchLightning/pytorch-lightning,examples/new_project_templates/trainer_gpu_cluster_template.py,bba51dde8cd742aa243c33e33ac99a63ffbf5b34,TODO: make 1 param,https://github.com/PyTorchLightning/pytorch-lightning/commit/bba51dde8cd742aa243c33e33ac99a63ffbf5b34,Yes
1729,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/trainer.py,805ff37e8c7709e2b355c1d182f35de8c327c01c,TODO: temporary; need to decide if tune or separate object,https://github.com/PyTorchLightning/pytorch-lightning/commit/805ff37e8c7709e2b355c1d182f35de8c327c01c,Yes
1730,PyTorchLightning/pytorch-lightning,pytorch_lightning/accelerators/base_backend.py,38b96776380977163a6df664c2226eee8c349b93,TODO: separate TPU case from here,https://github.com/PyTorchLightning/pytorch-lightning/commit/38b96776380977163a6df664c2226eee8c349b93,Yes
1731,PyTorchLightning/pytorch-lightning,pytorch_lightning/accelerators/tpu_backend.py,38b96776380977163a6df664c2226eee8c349b93,TODO: separate TPU case from here,https://github.com/PyTorchLightning/pytorch-lightning/commit/38b96776380977163a6df664c2226eee8c349b93,Yes
1732,PyTorchLightning/pytorch-lightning,tests/test_deprecated.py,c50c225f05f9a27687d5d5244c8d46a43900c635,TODO: remove bool from Trainer.profiler param in v1.3.0; update profiler_connector.py,https://github.com/PyTorchLightning/pytorch-lightning/commit/c50c225f05f9a27687d5d5244c8d46a43900c635,Yes
1733,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/ddp_sequential_plugin.py,ef8ef12fd0b1fa9318aa7a9930389bab8c8ef5d5,TODO currently no support for vertical model parallel,https://github.com/PyTorchLightning/pytorch-lightning/commit/ef8ef12fd0b1fa9318aa7a9930389bab8c8ef5d5,Yes
1734,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/training_type/rpc_sequential.py,9064b83caf817c624ad5241080e5a68d678a5ea6,TODO currently no support for vertical model parallel,https://github.com/PyTorchLightning/pytorch-lightning/commit/9064b83caf817c624ad5241080e5a68d678a5ea6,Yes
1735,J535D165/recordlinkage,sampledata/generate.py,d666c8446d9ff77ec401f9bf3237523d326bc86e,"\""\""\""Module generate.py - Auxiliary program to create records using various ||                         frequency tables and introduce duplicates with errors. ||  ||    USAGE: ||      python generate.py [output_file] [num_originals] [num_duplicates] ||                         [max_duplicate_per_record] [distribution]      ||  ||    ARGUMENTS: ||      output_file               Name of the output file (currently this is a ||                                CSV file). ||      num_originals             Number of original records to be created. ||      num_duplicates            Number of duplicate records to be created. ||      max_duplicate_per_record  The maximal number of duplicates that can be ||                                created for one original record. ||      distribution              The probability distribution used to create ||                                the duplicates (i.e the number of duplicates for ||                                one original). ||                                Possible are: - uniform ||                                              - poisson ||                                              - zipf ||  ||    DESCRIPTION: ||      This program can be used to create a data set with records that contain ||      randomly created names and addresses (using frequency files); dates and ||      identifier numbers. Duplicate records will then be created following a ||      given probability distribution; with various errors introduced. ||  ||      Various parameters on how theses duplicates are created can be given ||      within the program; see below. ||  ||  ||    TODO: ||      - Fix ZIPF distribution ||  ||      - Allow various probability distributions for fields oftype 'date' and ||        'iden' (using a new keyword in field dictionaries). ||  ||      - Try to find real world error distributions for typographical errors and ||        integrate them into the random error creation ||  ||      - Add random word spilling between fields (similar to field swapping) ||      - Improve performance (loading and creating frequency tables) || \""\""\""",https://github.com/J535D165/recordlinkage/commit/d666c8446d9ff77ec401f9bf3237523d326bc86e,No
1736,J535D165/recordlinkage,recordlinkage/algorithms/nb_sklearn.py,ead3d959376b993b8bffd2be798ff00a8cf6a1c7,TODO: set the init parameters by the user,https://github.com/J535D165/recordlinkage/commit/ead3d959376b993b8bffd2be798ff00a8cf6a1c7,Yes
1737,shawnwun/RNNLG,nn/basic.py,49e8e66eadaabbaecd75b5436cb7957512631b5a,TODO: load numpy parameters,https://github.com/shawnwun/RNNLG/commit/49e8e66eadaabbaecd75b5436cb7957512631b5a,Yes
1738,kengz/SLM-Lab,slm_lab/experiment/control.py,8f5f62f14e777cf2f88fe71360581be95d643163,TODO spec resolver for params per trial,https://github.com/kengz/SLM-Lab/commit/8f5f62f14e777cf2f88fe71360581be95d643163,Yes
1739,kengz/SLM-Lab,slm_lab/agent/net/feedforward.py,4df11055e61fa6c9fede6b2114c8ce05de9a035e,TODO parametrize output layer activation too,https://github.com/kengz/SLM-Lab/commit/4df11055e61fa6c9fede6b2114c8ce05de9a035e,Yes
1740,kengz/SLM-Lab,slm_lab/agent/net/mlp_policy.py,99f54b4f5d398727527b83c720a42156c61d592c,TODO restore param gaussian_fixed_var=True,https://github.com/kengz/SLM-Lab/commit/99f54b4f5d398727527b83c720a42156c61d592c,Yes
1741,kengz/SLM-Lab,slm_lab/agent/algorithm/ppo.py,a6bad8048ab0300d05b6dca26212698d5eb26dc5,TODO check all param consistency,https://github.com/kengz/SLM-Lab/commit/a6bad8048ab0300d05b6dca26212698d5eb26dc5,Yes
1742,kengz/SLM-Lab,slm_lab/agent/algorithm/policy_util.py,e7584bc18e845e296e315bb1c29c13b6d70ee419,TODO do as multitail list pdparams in the future to control activation,https://github.com/kengz/SLM-Lab/commit/e7584bc18e845e296e315bb1c29c13b6d70ee419,Yes
1743,tslearn-team/tslearn,tslearn/svm.py,746f0147006e2c3f4a1004cc0586a8e85338ce03,# TODO: change or mutate the parameter kernel from gak to,https://github.com/tslearn-team/tslearn/commit/746f0147006e2c3f4a1004cc0586a8e85338ce03,Yes
1744,jmwoloso/pychattr,pychattr/models/channel_attribution/heuristic.py,b4f5060bb31fd9f089e396c0434df6a9f735527b,TODO: param\/input validation,https://github.com/jmwoloso/pychattr/commit/b4f5060bb31fd9f089e396c0434df6a9f735527b,Yes
1745,jmwoloso/pychattr,v0.1.0/channel_attribution/pychattr.py,9f3bd4ddb261f53093c016feef8af5978f514800,"TODO: parameterize \""channel_name\""?",https://github.com/jmwoloso/pychattr/commit/9f3bd4ddb261f53093c016feef8af5978f514800,Yes
1746,JohnVinyard/zounds,zounds/analyze/feature/template.py,99ddbee138b293afb76f087aea0ba11b845e13ea,TODO: Make this an __init__ parameter,https://github.com/JohnVinyard/zounds/commit/99ddbee138b293afb76f087aea0ba11b845e13ea,Yes
1747,JohnVinyard/zounds,zounds/analyze2/feature/template.py,b66c7c8205fb4669f2f5fab75d91e212e1e60c46,TODO: Make this an __init__ parameter,https://github.com/JohnVinyard/zounds/commit/b66c7c8205fb4669f2f5fab75d91e212e1e60c46,Yes
1748,JohnVinyard/zounds,zounds/model/framesearch.py,ea2b2653b48af11ce05a4e26cc98a16a9f5ab6de,TODO: Parallelize the search,https://github.com/JohnVinyard/zounds/commit/ea2b2653b48af11ce05a4e26cc98a16a9f5ab6de,Yes
1749,ryfeus/gcf-packs,pandas_numpy/sources/numpy/lib/_datasource.py,255a05a5980efb8b096c283d79872d0695886161,TODO: Add a ``subdir`` parameter for specifying the subdirectory,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
1750,ryfeus/gcf-packs,pandas_numpy/sources/pandas/core/generic.py,255a05a5980efb8b096c283d79872d0695886161,TODO: define separate funcs for DataFrame; Series and Panel so you can,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
1751,ryfeus/gcf-packs,pandas_numpy/sources/pandas/core/internals.py,255a05a5980efb8b096c283d79872d0695886161,FIXME: refactor; clearly separate broadcasting & zip-like assignment,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
1752,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/test_base.py,255a05a5980efb8b096c283d79872d0695886161,TODO: make this a dedicated test with parametrized methods,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
1753,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/test_base.py,255a05a5980efb8b096c283d79872d0695886161,TODO: Parametrize numeric and str tests after self.strIndex fixture,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
1754,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/series/test_arithmetic.py,255a05a5980efb8b096c283d79872d0695886161,TODO: parametrize; better name,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
1755,ryfeus/gcf-packs,tensorflow2.0/source/numpy/lib/_datasource.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,TODO: Add a ``subdir`` parameter for specifying the subdirectory,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
1756,ryfeus/gcf-packs,tensorflow2.0/source/tensorflow/python/ops/partitioned_variables.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"\""\""\""Helper functions for creating partitioned variables. ||  || This is a convenient abstraction to partition a large variable across || multiple smaller variables that can be assigned to different devices. ||  || The full variable can be reconstructed by concatenating the smaller variables. || Using partitioned variables instead of a single variable is mostly a || performance choice.  It however also has an impact on: ||  || 1. Random initialization; as the random number generator is called once per ||    slice || 2. Updates; as they happen in parallel across slices ||  || A key design goal is to allow a different graph to repartition a variable || with the same name but different slicings; including possibly no partitions. ||  || TODO(touts): If an initializer provides a seed; the seed must be changed || deterministically for each slice; maybe by adding one to it; otherwise each || slice will use the same values.  Maybe this can be done by passing the || slice offsets to the initializer functions. ||  || Typical usage: ||  || ```python || # Create a list of partitioned variables with: || vs = create_partitioned_variables( ||     <shape>; <slicing>; <initializer>; name=<optional-name>) ||  || # Pass the list as inputs to embedding_lookup for sharded; parallel lookup: || y = embedding_lookup(vs; ids; partition_strategy=\""div\"") ||  || # Or fetch the variables in parallel to speed up large matmuls: || z = matmul(x; concat(slice_dim; vs)) || ``` || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
1757,ryfeus/gcf-packs,tensorflow2.0/source/tensorflow/python/tpu/tpu_embedding.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,TODO(shizhiw): move `optimization_parameters` into `_optimizer_handler`,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
1758,cschoeller/pycandle,training/callbacks.py,28c1f772edfe442fdd4854303d33b81218421788,TODO print per param group?,https://github.com/cschoeller/pycandle/commit/28c1f772edfe442fdd4854303d33b81218421788,Yes
1759,serengil/chefboost,chefboost/training/Training.py,0bda5524f4ab0a1ef28a65103e1930d945f03db9,TODO: elif checks might be above than if statements in parallel,https://github.com/serengil/chefboost/commit/0bda5524f4ab0a1ef28a65103e1930d945f03db9,Yes
1760,serengil/chefboost,chefboost/training/Training.py,81d4ee5098f9106707140cce63c3875a3a19b66c,TODO: just apply parallelism for the 1st level?,https://github.com/serengil/chefboost/commit/81d4ee5098f9106707140cce63c3875a3a19b66c,Yes
1761,serengil/chefboost,chefboost/tuning/randomforest.py,374a6d4dd045e5722b4058133472f9201df52cf0,TODO: reconstruct for parallel run is problematic. you should reconstruct based on tree id.,https://github.com/serengil/chefboost/commit/374a6d4dd045e5722b4058133472f9201df52cf0,Yes
1762,serengil/chefboost,chefboost/training/Training-Draft.py,f42220860f040cbafd2d2648bc4a250ef03f6639,TODO: elif checks might be above than if statements in parallel,https://github.com/serengil/chefboost/commit/f42220860f040cbafd2d2648bc4a250ef03f6639,Yes
1763,explosion/spaCy,spacy/cli/train.py,240e0a62cae2994bb55d4a1e57256330d23377c0,TODO: separate checks from loading,https://github.com/explosion/spaCy/commit/240e0a62cae2994bb55d4a1e57256330d23377c0,Yes
1764,explosion/spaCy,spacy/cli/train.py,defe1e7213ea3a974fd9131dd89f99739cb99d89,TODO: separate checks from loading,https://github.com/explosion/spaCy/commit/defe1e7213ea3a974fd9131dd89f99739cb99d89,Yes
1765,explosion/spaCy,spacy/cli/train.py,872938ec761ecc44694e1a31d498a864b2f53995,TODO: separate checks from loading,https://github.com/explosion/spaCy/commit/872938ec761ecc44694e1a31d498a864b2f53995,Yes
1766,explosion/spaCy,spacy/cli/train.py,9ee1c54f40e901533ef16cd148556cbf83cca6a7,TODO: separate checks from loading,https://github.com/explosion/spaCy/commit/9ee1c54f40e901533ef16cd148556cbf83cca6a7,Yes
1767,explosion/spaCy,spacy/cli/train.py,43b960c01b0c64e56859ad5eb304a5422af46516,TODO: separate checks from loading,https://github.com/explosion/spaCy/commit/43b960c01b0c64e56859ad5eb304a5422af46516,Yes
1768,explosion/spaCy,spacy/cli/train.py,b795f02fbdd095d7c326dab0a823c94815a17484,TODO: refactor this so we don't have to run it separately in here,https://github.com/explosion/spaCy/commit/b795f02fbdd095d7c326dab0a823c94815a17484,Yes
1769,explosion/spaCy,spacy/cli/train.py,122cb020010a3f4e34e696726206eb92ea9974e8,TODO: refactor this so we don't have to run it separately in here,https://github.com/explosion/spaCy/commit/122cb020010a3f4e34e696726206eb92ea9974e8,Yes
1770,explosion/spaCy,spacy/cli/train.py,122cb020010a3f4e34e696726206eb92ea9974e8,TODO: separate checks from loading,https://github.com/explosion/spaCy/commit/122cb020010a3f4e34e696726206eb92ea9974e8,Yes
1771,explosion/spaCy,spacy/cli/_util.py,553bfea6418e76c28b8786de35df7a3df0e0b56a,TODO: separate checks from loading,https://github.com/explosion/spaCy/commit/553bfea6418e76c28b8786de35df7a3df0e0b56a,Yes
1772,explosion/spaCy,spacy/cli/train.py,e44a7519cdac903a64b0dec5e98b8b828952d4b9,TODO: separate checks from loading,https://github.com/explosion/spaCy/commit/e44a7519cdac903a64b0dec5e98b8b828952d4b9,Yes
1773,explosion/spaCy,spacy/training/loop.py,822ea4ef619072a94ce565bf78add9f9ea9d2866,TODO: refactor this so we don't have to run it separately in here,https://github.com/explosion/spaCy/commit/822ea4ef619072a94ce565bf78add9f9ea9d2866,Yes
1774,explosion/spaCy,spacy/cli/train.py,c1c841940cd600046e4e58e12b7ed78f761ae232,TODO: refactor this so we don't have to run it separately in here,https://github.com/explosion/spaCy/commit/c1c841940cd600046e4e58e12b7ed78f761ae232,Yes
1775,explosion/spaCy,spacy/cli/train.py,c1c841940cd600046e4e58e12b7ed78f761ae232,TODO: separate checks from loading,https://github.com/explosion/spaCy/commit/c1c841940cd600046e4e58e12b7ed78f761ae232,Yes
1776,scikit-learn/scikit-learn,scikits/learn/pyem/gmm_em.py,9fe7ff776888b2adac5775878a7bb10e30ef81ff,TODO: separate regularizer from EM class ?,https://github.com/scikit-learn/scikit-learn/commit/9fe7ff776888b2adac5775878a7bb10e30ef81ff,Yes
1777,scikit-learn/scikit-learn,scikits/learn/datasets/mlcomp.py,434eed65b4b2eb5f0f8814467576cd808742ccfd,TODO: use joblib.Parallel or multiprocessing to parallelize the following,https://github.com/scikit-learn/scikit-learn/commit/434eed65b4b2eb5f0f8814467576cd808742ccfd,Yes
1778,scikit-learn/scikit-learn,scikits/learn/datasets/mlcomp.py,26bdc720b71c530560bad0b0826130bede6c7d40,TODO: use joblib.Parallel or multiprocessing to parallelize the following,https://github.com/scikit-learn/scikit-learn/commit/26bdc720b71c530560bad0b0826130bede6c7d40,Yes
1779,scikit-learn/scikit-learn,scikits/learn/datasets/mlcomp.py,b7a1afb3933e09eb4fe2985a4ce184d3b3c8ed7b,TODO: use joblib.Parallel or multiprocessing to parallelize the following,https://github.com/scikit-learn/scikit-learn/commit/b7a1afb3933e09eb4fe2985a4ce184d3b3c8ed7b,Yes
1780,scikit-learn/scikit-learn,scikits/learn/datasets/mlcomp.py,314c6870f10f46434ab3cde4776ca95b2a6787c8,TODO: use joblib.Parallel or multiprocessing to parallelize the following,https://github.com/scikit-learn/scikit-learn/commit/314c6870f10f46434ab3cde4776ca95b2a6787c8,Yes
1781,scikit-learn/scikit-learn,scikits/learn/datasets/mlcomp.py,d0828deba072cc9d343f24c91bdd23c75d27ceda,TODO: use joblib.Parallel or multiprocessing to parallelize the following,https://github.com/scikit-learn/scikit-learn/commit/d0828deba072cc9d343f24c91bdd23c75d27ceda,Yes
1782,scikit-learn/scikit-learn,scikits/learn/sgd/sparse/sgd.py,dad8deb08e17aa7577792d0e8781ac030d620c86,TODO: parallel training using joblib.,https://github.com/scikit-learn/scikit-learn/commit/dad8deb08e17aa7577792d0e8781ac030d620c86,Yes
1783,scikit-learn/scikit-learn,scikits/learn/feature_extraction/text.py,5bdc3ab5e67b51769a0edb2dfc22ce47454cd699,TODO: parallelize the following loop with joblib?,https://github.com/scikit-learn/scikit-learn/commit/5bdc3ab5e67b51769a0edb2dfc22ce47454cd699,Yes
1784,scikit-learn/scikit-learn,sklearn/neighbors/base.py,bc02942bc322b04221679fa9ddb0dd64f3004a80,FIXME: include float parameter p for using different distance metrics.,https://github.com/scikit-learn/scikit-learn/commit/bc02942bc322b04221679fa9ddb0dd64f3004a80,Yes
1785,scikit-learn/scikit-learn,sklearn/neighbors/base.py,dacfd8bd5d943cb899ed8cd423aaf11b4f27c186,FIXME: include float parameter p for using different distance metrics.,https://github.com/scikit-learn/scikit-learn/commit/dacfd8bd5d943cb899ed8cd423aaf11b4f27c186,Yes
1786,scikit-learn/scikit-learn,sklearn/neighbors/base.py,7f9b7834686c7498c6977c7363b87c9cc4ea64a1,FIXME: include float parameter p for using different distance metrics.,https://github.com/scikit-learn/scikit-learn/commit/7f9b7834686c7498c6977c7363b87c9cc4ea64a1,Yes
1787,scikit-learn/scikit-learn,sklearn/neighbors/base.py,8861833ec9ee67656670af66605991c44207ff81,FIXME: include float parameter p for using different distance metrics.,https://github.com/scikit-learn/scikit-learn/commit/8861833ec9ee67656670af66605991c44207ff81,Yes
1788,scikit-learn/scikit-learn,sklearn/neighbors/base.py,eaac7aa19f8e3188b8eccc0be593ac2ae1c87a9f,FIXME: include float parameter p for using different distance metrics.,https://github.com/scikit-learn/scikit-learn/commit/eaac7aa19f8e3188b8eccc0be593ac2ae1c87a9f,Yes
1789,scikit-learn/scikit-learn,sklearn/cross_validation.py,34c85ed995907f018279decb36be41fe2d29fba6,todo shuffle parameter of StratifiedKFold not exposed,https://github.com/scikit-learn/scikit-learn/commit/34c85ed995907f018279decb36be41fe2d29fba6,Yes
1790,scikit-learn/scikit-learn,sklearn/decomposition/online_lda.py,62ad838be046b9624068ba0882f100e536fcd303,TODO: make Parallel._effective_n_jobs public instead?,https://github.com/scikit-learn/scikit-learn/commit/62ad838be046b9624068ba0882f100e536fcd303,Yes
1791,scikit-learn/scikit-learn,sklearn/model_selection/tests/test_validation.py,8e2c2aa35d234ba5d14f6c2492e2e5ca57de8af6,FIXME issue in error_score parameter,https://github.com/scikit-learn/scikit-learn/commit/8e2c2aa35d234ba5d14f6c2492e2e5ca57de8af6,Yes
1792,scikit-learn/scikit-learn,sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,9f68c992aede232d539339ef10fa1aadda532d2a,TODO: This could be done in parallel,https://github.com/scikit-learn/scikit-learn/commit/9f68c992aede232d539339ef10fa1aadda532d2a,Yes
1793,scikit-learn/scikit-learn,sklearn/ensemble/_hist_gradient_boosting/loss.py,9f68c992aede232d539339ef10fa1aadda532d2a,TODO: This could be done in parallel,https://github.com/scikit-learn/scikit-learn/commit/9f68c992aede232d539339ef10fa1aadda532d2a,Yes
1794,scikit-learn/scikit-learn,sklearn/feature_extraction/tests/test_text.py,deec2baf00a0b34db48d08bdecb3f211b0872ab7,FIXME Remove copy parameter support in 0.24,https://github.com/scikit-learn/scikit-learn/commit/deec2baf00a0b34db48d08bdecb3f211b0872ab7,Yes
1795,scikit-learn/scikit-learn,sklearn/feature_extraction/text.py,deec2baf00a0b34db48d08bdecb3f211b0872ab7,FIXME Remove copy parameter support in 0.24,https://github.com/scikit-learn/scikit-learn/commit/deec2baf00a0b34db48d08bdecb3f211b0872ab7,Yes
1796,scikit-learn/scikit-learn,sklearn/tests/test_common.py,d9a12aa0a98140490676dac27094f9f6569b8c1a,TODO: meta-estimators like GridSearchCV has required parameters,https://github.com/scikit-learn/scikit-learn/commit/d9a12aa0a98140490676dac27094f9f6569b8c1a,Yes
1797,scikit-learn/scikit-learn,sklearn/ensemble/_hist_gradient_boosting/loss.py,5cf88db24491112d2b8672f75df22f65a140d167,TODO: ideally this should be computed in parallel over the leaves,https://github.com/scikit-learn/scikit-learn/commit/5cf88db24491112d2b8672f75df22f65a140d167,Yes
1798,scikit-learn/scikit-learn,sklearn/tree/tree.py,186629b60aa208d125a21132b58bd8d6b7565264,TODO: the tree shouldn't need this param,https://github.com/scikit-learn/scikit-learn/commit/186629b60aa208d125a21132b58bd8d6b7565264,Yes
1799,scikit-learn/scikit-learn,sklearn/tests/test_pipeline.py,16f4ac90f0732988e3b7efe0c937eaff70e99692,TODO: Remove parametrization in 0.24 when None is removed for FeatureUnion,https://github.com/scikit-learn/scikit-learn/commit/16f4ac90f0732988e3b7efe0c937eaff70e99692,Yes
1800,scikit-learn/scikit-learn,sklearn/ensemble/tests/test_voting.py,939fa3cccefe708db7a81c5248db32a1d600bf8d,TODO: Remove parametrization in 0.24 when None is removed in Voting*,https://github.com/scikit-learn/scikit-learn/commit/939fa3cccefe708db7a81c5248db32a1d600bf8d,Yes
1801,scikit-learn/scikit-learn,sklearn/ensemble/tests/test_voting.py,7c47337f7b15a5368c922ed1781a267bf66c7367,TODO: Remove drop parametrize in 0.24 when None is removed in Voting*,https://github.com/scikit-learn/scikit-learn/commit/7c47337f7b15a5368c922ed1781a267bf66c7367,Yes
1802,scikit-learn/scikit-learn,sklearn/linear_model/_base.py,306826f7b6bf5fd61af74062c0ba8f0f21aa3dae,FIXME in 1.2: parameter 'normalize' should be removed from linear models,https://github.com/scikit-learn/scikit-learn/commit/306826f7b6bf5fd61af74062c0ba8f0f21aa3dae,Yes
1803,lai-bluejay/diego,diego/preprocessor/auto_binning.py,33ea32953c6219d750942a0ebae7a81ec7247b62,TODO add hyperparameter to gbdt binning,https://github.com/lai-bluejay/diego/commit/33ea32953c6219d750942a0ebae7a81ec7247b62,Yes
1804,lai-bluejay/diego,diego/preprocessor/local_uncertainty_sampling.py,33ea32953c6219d750942a0ebae7a81ec7247b62,TODO add hyperparameter to gbdt binning,https://github.com/lai-bluejay/diego/commit/33ea32953c6219d750942a0ebae7a81ec7247b62,Yes
1805,lai-bluejay/diego,diego/classifier.py,12e33a920ac8d0cb28d2717fe03d26d42dcf74b8,TODO add hyperparameter to gbdt binning,https://github.com/lai-bluejay/diego/commit/12e33a920ac8d0cb28d2717fe03d26d42dcf74b8,Yes
1806,lai-bluejay/diego,diego/classifier/logistic_regression.py,3cf932b8ce784661425d5b08cbd338f8e0b9da16,TODO remove svrg; performance no good with same parameter space with sgd,https://github.com/lai-bluejay/diego/commit/3cf932b8ce784661425d5b08cbd338f8e0b9da16,Yes
1807,lzfelix/minuet,minuet/minuet.py,98b90c10f7b2b5d2ccbd6bce3fd2eaaec2213ff1,TODO: fix hyperparameters reloading,https://github.com/lzfelix/minuet/commit/98b90c10f7b2b5d2ccbd6bce3fd2eaaec2213ff1,Yes
1808,plstcharles/thelper,src/thelper/train.py,aac5d8b089b18e5ae70d53795017364934a3dd79,todo: run eval in parallel (i.e. at the same time as training?),https://github.com/plstcharles/thelper/commit/aac5d8b089b18e5ae70d53795017364934a3dd79,Yes
1809,plstcharles/thelper,thelper/nn/resnet.py,2c9a8deb64e34458d6e8d3e73d8d6b4e5c770822,TODO: add pretrained param to toggle loading weights from imagenet before applying task?,https://github.com/plstcharles/thelper/commit/2c9a8deb64e34458d6e8d3e73d8d6b4e5c770822,Yes
1810,plstcharles/thelper,thelper/nn/resnet.py,808e40487254553e3d0c5176faaf458e6c1d91ed,TODO: add pretrained param to toggle loading weights from imagenet before applying task?,https://github.com/plstcharles/thelper/commit/808e40487254553e3d0c5176faaf458e6c1d91ed,Yes
1811,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/ANN_simulation.py,e04a40e5698fba0ad62e5c3b77b3dac81264f5f2,TODO: currently they are not run in parallel; fix this later,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/e04a40e5698fba0ad62e5c3b77b3dac81264f5f2,Yes
1812,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/ANN_simulation_trp_cage.py,0f7dc1afffb031619f0c6f3ed6355a5648e48f8f,TODO: currently they are not run in parallel; fix this later,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/0f7dc1afffb031619f0c6f3ed6355a5648e48f8f,Yes
1813,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/ANN_simulation.py,be935c802b1e9090b6bb5e555d2c485d1b24a382,TODO: move the last two parameters into somewhere else (not hard-coded),https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/be935c802b1e9090b6bb5e555d2c485d1b24a382,Yes
1814,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/autoencoders.py,77d63fbc95f2db520e1aecec56faa57b873e96a9,TODO: 1. use better training parameters. 2. use consistant activation functions; 3. consider how to do this for hierarchical case,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/77d63fbc95f2db520e1aecec56faa57b873e96a9,Yes
1815,manuwhs/Trapyng,Examples/1. TimeData Examples/1.1. main_Basic.py,df8c673a0b2963d29f4402c83912b5c8320475db,TODO: Allow to specify extra info to the subplots like to erate the separation between graphs so that they share shit.,https://github.com/manuwhs/Trapyng/commit/df8c673a0b2963d29f4402c83912b5c8320475db,Yes
1816,manuwhs/Trapyng,Examples/5. ML Systems Examples/1. main_features_experiments.py,df8c673a0b2963d29f4402c83912b5c8320475db,# TODO: idea; a high variance; indicates consinuity in the parameters,https://github.com/manuwhs/Trapyng/commit/df8c673a0b2963d29f4402c83912b5c8320475db,Yes
1817,manuwhs/Trapyng,Other/libs (copy)/graph/graph_3D.py,df8c673a0b2963d29f4402c83912b5c8320475db,TODO: make this a parameters,https://github.com/manuwhs/Trapyng/commit/df8c673a0b2963d29f4402c83912b5c8320475db,Yes
1818,manuwhs/Trapyng,libs/graph/graph_3D.py,df8c673a0b2963d29f4402c83912b5c8320475db,TODO: make this a parameters,https://github.com/manuwhs/Trapyng/commit/df8c673a0b2963d29f4402c83912b5c8320475db,Yes
1819,manuwhs/Trapyng,Examples/3.3 EURUSD_example/partidos_politicos.py,6b25ff82f383b3dac092795657538789ae7160c8,"\""\""\"" || CONCLUSION: DADO UN THRESHOLD; EL NUMERO DE VOTOS DEL ULTIMO ESCANHO DADO; LA MEJOR CONFIGURACION POSIBLE ES TENERLO TODO REPARTIDO. ||  || OTRA INFORMACION: EL THRESHOLD FINAL CAMBIA EN FUNCION DE LA COMPETENCIA.  ||  || PREGUNTA: ES POSIBLE QUE SEA MEJOR TENER UNA DISTRIBUCION MAS REPARTIDA DEL VOTO PARA QUE EL THRESHOLD SE SITUE UN POCO MAYOR  ||  ||  || \""\""\""",https://github.com/manuwhs/Trapyng/commit/6b25ff82f383b3dac092795657538789ae7160c8,Yes
1820,jmwoloso/pychattr,pychattr/channel_attribution.py,1892bf85ca963496bcf129917bf8c82fb920751f,"TODO: parameterize \""channel_name\""",https://github.com/jmwoloso/pychattr/commit/1892bf85ca963496bcf129917bf8c82fb920751f,Yes
1821,jmwoloso/pychattr,pychattr/channel_attribution.py,c5febc03006534df6597a264537920f6be133be2,"TODO: parameterize \""channel_name\""?",https://github.com/jmwoloso/pychattr/commit/c5febc03006534df6597a264537920f6be133be2,Yes
1822,jmwoloso/pychattr,dev/channel_attribution.py,bb7f15686f2d50cd3a770d63887f82ae04acb322,"TODO: parameterize \""channel_name\""?",https://github.com/jmwoloso/pychattr/commit/bb7f15686f2d50cd3a770d63887f82ae04acb322,Yes
1823,thu-ml/zhusuan,zhusuan/sgmcmc.py,3fb4e7502eee3c0bcefdd067422903c5863c6726,TODO: 1. enable matrix parameter 2. enable mass setting\uFFFF,https://github.com/thu-ml/zhusuan/commit/3fb4e7502eee3c0bcefdd067422903c5863c6726,Yes
1824,biolab/orange3,Orange/data/variable.py,1c65f8552ef4a590cffa50cc43e5777708cc2509,"\""\""\"" || ======================== || Variables (``variable``) || ======================== ||  || Data instances in Orange can contain several types of variables: || :ref:`discrete <discrete>`; :ref:`continuous <continuous>`; || :ref:`strings <string>`; and :ref:`Python <Python>` and types derived from it. || The latter represent arbitrary Python objects. || The names; types; values (where applicable); functions for computing the || variable value from values of other variables; and other properties of the || variables are stored in descriptor classes defined in this module. ||  || Variable descriptors || -------------------- ||  || Variable descriptors can be constructed either directly; using  || constructors and passing attributes as parameters; or by a  || factory function :func:`Orange.data.variable.make`; which either  || retrieves an existing descriptor or constructs a new one. ||  || .. class:: Variable ||  ||     An abstract base class for variable descriptors. ||  ||     .. attribute:: name ||  ||         The name of the variable. Variable names do not need to be unique since two ||         variables are considered the same only if they have the same descriptor ||         (e.g. even multiple variables in the same table can have the same name). ||         This should; however; be avoided since it may result in unpredictable ||         behavior. ||      ||     .. attribute:: var_type ||         ||         Variable type; it can be Orange.data.Type.Discrete; ||         Orange.data.Type.Continuous; Orange.data.Type.String or ||         Orange.data.Type.Other.   ||  ||     .. attribute:: get_value_from ||  ||         A function (an instance of :obj:`Orange.classification.Classifier`) which computes ||         a value of the variable from values of one or more other variables. This ||         is used; for instance; in discretization where the variables describing ||         the discretized variable are computed from the original variable.  ||  ||     .. attribute:: ordered ||      ||         A flag telling whether the values of a discrete variable are ordered. At ||         the moment; no built-in method treats ordinal variables differently than ||         nominal ones. ||      ||     .. attribute:: distributed ||      ||         A flag telling whether the values of the variables are distributions. ||         As for the flag ordered; no methods treat such variables in any special ||         manner. ||      ||     .. attribute:: random_generator ||      ||         A local random number generator used by method ||         :obj:`Variable.random_value`. ||      ||     .. attribute:: default_meta_id ||      ||         A proposed (but not guaranteed) meta id to be used for that variable. ||         This is used; for instance; by the data loader for tab-delimited file ||         format instead of assigning an arbitrary new value; or by ||         :obj:`Orange.data.new_meta_id` if the variable is passed as an argument.  ||          ||     .. attribute:: attributes ||          ||         A dictionary which allows the user to store additional information ||         about the variable. All values should be strings. See the section  ||         about :ref:`storing additional information <attributes>`. ||  ||     .. method:: __call__(obj) ||      ||            Convert a string; number; or other suitable object into a variable ||            value. ||             ||            :param obj: An object to be converted into a variable value ||            :type o: any suitable ||            :rtype: :class:`Orange.data.Value` ||         ||     .. method:: randomvalue() ||  ||            Return a random value for the variable. ||         ||            :rtype: :class:`Orange.data.Value` ||         ||     .. method:: compute_value(inst) ||  ||            Compute the value of the variable given the instance by calling ||            obj:`~Variable.get_value_from` through a mechanism that prevents deadlocks by ||            circular calls. ||  ||            :rtype: :class:`Orange.data.Value` ||  || .. _discrete: || .. class:: Discrete ||  ||     Bases: :class:`Variable` ||     ||     Descriptor for discrete variables. ||      ||     .. attribute:: values ||      ||         A list with symbolic names for variables' values. Values are stored as ||         indices referring to this list. Therefore; modifying this list  ||         instantly changes the (symbolic) names of values as they are printed out or ||         referred to by user. ||      ||         .. note:: ||          ||             The size of the list is also used to indicate the number of ||             possible values for this variable. Changing the size - especially ||             shrinking the list - can have disastrous effects and is therefore not ||             really recommended. Also; do not add values to the list by ||             calling its append or extend method: call the :obj:`add_value` ||             method instead. ||  ||             It is also assumed that this attribute is always defined (but can ||             be empty); so never set it to None. ||      ||     .. attribute:: base_value ||  ||             Stores the base value for the variable as an index in `values`. ||             This can be; for instance; a \""normal\"" value; such as \""no ||             complications\"" as opposed to abnormal \""low blood pressure\"". The ||             base value is used by certain statistics; continuization etc. ||             potentially; learning algorithms. The default is -1 which means that ||             there is no base value. ||      ||     .. method:: add_value ||      ||             Add a value to values. Always call this function instead of ||             appending to values. ||  || .. _continuous: || .. class:: Continuous ||  ||     Bases: :class:`Variable` ||  ||     Descriptor for continuous variables. ||      ||     .. attribute:: number_of_decimals ||      ||         The number of decimals used when the value is printed out; converted to ||         a string or saved to a file. ||      ||     .. attribute:: scientific_format ||      ||         If ``True``; the value is printed in scientific format whenever it ||         would have more than 5 digits. In this case; :obj:`number_of_decimals` is ||         ignored. ||  ||     .. attribute:: adjust_decimals ||      ||         Tells Orange to monitor the number of decimals when the value is ||         converted from a string (when the values are read from a file or ||         converted by; e.g. ``inst[0]=\""3.14\""``):  ||         0: the number of decimals is not adjusted automatically; ||         1: the number of decimals is (and has already) been adjusted; ||         2: automatic adjustment is enabled; but no values have been converted yet. ||  ||         By default; adjustment of the number of decimals goes as follows: ||      ||         If the variable was constructed when data was read from a file; it will  ||         be printed with the same number of decimals as the largest number of  ||         decimals encountered in the file. If scientific notation occurs in the  ||         file; :obj:`scientific_format` will be set to ``True`` and scientific format  ||         will be used for values too large or too small.  ||      ||         If the variable is created in a script; it will have; by default; three ||         decimal places. This can be changed either by setting the value ||         from a string (e.g. ``inst[0]=\""3.14\""``; but not ``inst[0]=3.14``) or by ||         manually setting the :obj:`number_of_decimals`. ||  ||     .. attribute:: start_value; end_value; step_value ||      ||         The range used for :obj:`randomvalue`. ||  || .. _String: || .. class:: String ||  ||     Bases: :class:`Variable` ||  ||     Descriptor for variables that contain strings. No method can use them for  ||     learning; some will complain and others will silently ignore them when they  ||     encounter them. They can be; however; useful for meta-attributes; if  ||     instances in a dataset have unique IDs; the most efficient way to store them  ||     is to read them as meta-attributes. In general; never use discrete  ||     attributes with many (say; more than 50) values. Such attributes are  ||     probably not of any use for learning and should be stored as string ||     attributes. ||  ||     When converting strings into values and back; empty strings are treated  ||     differently than usual. For other types; an empty string can be used to ||     denote undefined values; while :obj:`String` will take empty strings ||     as empty strings -- except when loading or saving into file. ||     Empty strings in files are interpreted as undefined; to specify an empty ||     string; enclose the string in double quotes; these are removed when the ||     string is loaded. ||  || .. _Python: || .. class:: Python ||  ||     Bases: :class:`Variable` ||  ||     Base class for descriptors defined in Python. It is fully functional ||     and can be used as a descriptor for attributes that contain arbitrary Python ||     values. Since this is an advanced topic; PythonVariables are described on a  ||     separate page. !!TODO!! ||      ||      || Variables computed from other variables || --------------------------------------- ||  || Values of variables are often computed from other variables; such as in || discretization. The mechanism described below usually functions behind the scenes; || so understanding it is required only for implementing specific transformations. ||  || Monk 1 is a well-known dataset with target concept ``y := a==b or e==1``. || It can help the learning algorithm if the four-valued attribute ``e`` is || replaced with a binary attribute having values `\""1\""` and `\""not 1\""`. The || new variable will be computed from the old one on the fly.  ||  || .. literalinclude:: code\/variable-get_value_from.py ||     :lines: 7-17 ||      || The new variable is named ``e2``; we define it with a descriptor of type  || :obj:`Discrete`; with appropriate name and values ``\""not 1\""`` and ``1`` (we  || chose this order so that the ``not 1``'s index is ``0``; which can be; if  || needed; interpreted as ``False``). Finally; we tell e2 to use  || ``checkE`` to compute its value when needed; by assigning ``checkE`` to  || ``e2.get_value_from``.  ||  || ``checkE`` is a function that is passed an instance and another argument we  || do not care about here. If the instance's ``e`` equals ``1``; the function  || returns value ``1``; otherwise it returns ``not 1``. Both are returned as  || values; not plain strings. ||  || In most circumstances the value of ``e2`` can be computed on the fly - we can  || pretend that the variable exists in the data; although it does not (but  || can be computed from it). For instance; we can compute the information gain of || variable ``e2`` or its distribution without actually constructing data containing || the new variable. ||  || .. literalinclude:: code\/variable-get_value_from.py ||     :lines: 19-22 ||  || There are methods which cannot compute values on the fly because it would be || too complex or time consuming. In such cases; the data need to be converted || to a new :obj:`Orange.data.Table`:: ||  ||     new_domain = Orange.data.Domain([data.domain[\""a\""]; data.domain[\""b\""]; e2; data.domain.class_var]) ||     new_data = Orange.data.Table(new_domain; data)  ||  || Automatic computation is useful when the data is split into training and  || testing examples. Training instances can be modified by adding; removing  || and transforming variables (in a typical setup; continuous variables  || are discretized prior to learning; therefore the original variables are  || replaced by new ones). Test instances; on the other hand; are left as they  || are. When they are classified; the classifier automatically converts the  || testing instances into the new domain; which includes recomputation of  || transformed variables.  ||  || .. literalinclude:: code\/variable-get_value_from.py ||     :lines: 24- ||  || .. _attributes: ||  || Storing additional variables || ----------------------------- ||  || All variables have a field :obj:`~Variable.attributes`; a dictionary || which can contain strings. Although the current implementation allows all || types of value we strongly advise to use only strings. An example: ||  || .. literalinclude:: code\/attributes.py ||  || These attributes can only be saved to a .tab file. They are listed in the || third line in <name>=<value> format; after other attribute specifications || (such as \""meta\"" or \""class\""); and are separated by spaces.  ||  || .. _variable_descriptor_reuse: ||  || Reuse of descriptors || -------------------- ||  || There are situations when variable descriptors need to be reused. Typically; the  || user loads some training examples; trains a classifier; and then loads a separate || test set. For the classifier to recognize the variables in the second data set; || the descriptors; not just the names; need to be the same.  ||  || When constructing new descriptors for data read from a file or during unpickling; || Orange checks whether an appropriate descriptor (with the same name and; in case || of discrete variables; also values) already exists and reuses it. When new || descriptors are constructed by explicitly calling the above constructors; this || always creates new descriptors and thus new variables; although a variable with || the same name may already exist. ||  || The search for an existing variable is based on four attributes: the variable's name; || type; ordered values; and unordered values. As for the latter two; the values can  || be explicitly ordered by the user; e.g. in the second line of the tab-delimited  || file. For instance; sizes can be ordered as small; medium; or big. ||  || The search for existing variables can end with one of the following statuses. ||  || .. data:: Orange.data.variable.MakeStatus.NotFound (4) ||  ||     The variable with that name and type does not exist.  ||  || .. data:: Orange.data.variable.MakeStatus.Incompatible (3) ||  ||     There are variables with matching name and type; but their ||     values are incompatible with the prescribed ordered values. For example; ||     if the existing variable already has values [\""a\""; \""b\""] and the new one ||     wants [\""b\""; \""a\""]; the old variable cannot be reused. The existing list can; ||     however be appended with the new values; so searching for [\""a\""; \""b\""; \""c\""] would ||     succeed. Likewise a search for [\""a\""] would be successful; since the extra existing value ||     does not matter. The formal rule is thus that the values are compatible iff ``existing_values[:len(ordered_values)] == ordered_values[:len(existing_values)]``. ||  || .. data:: Orange.data.variable.MakeStatus.NoRecognizedValues (2) ||  ||     There is a matching variable; yet it has none of the values that the new ||     variable will have (this is obviously possible only if the new variable has ||     no prescribed ordered values). For instance; we search for a variable ||     \""sex\"" with values \""male\"" and \""female\""; while there is a variable of the same  ||     name with values \""M\"" and \""F\"" (or; well; \""no\"" and \""yes\"" :). Reuse of this  ||     variable is possible; though this should probably be a new variable since it  ||     obviously comes from a different data set. If we do decide to reuse the variable; the  ||     old variable will get some unneeded new values and the new one will inherit  ||     some from the old. ||  || .. data:: Orange.data.variable.MakeStatus.MissingValues (1) ||  ||     There is a matching variable with some of the values that the new one  ||     requires; but some values are missing. This situation is neither uncommon  ||     nor suspicious: in case of separate training and testing data sets there may ||     be values which occur in one set but not in the other. ||  || .. data:: Orange.data.variable.MakeStatus.OK (0) ||  ||     There is a perfect match which contains all the prescribed values in the ||     correct order. The existing variable may have some extra values; though. ||  || Continuous variables can obviously have only two statuses;  || :obj:`~Orange.data.variable.MakeStatus.NotFound` or :obj:`~Orange.data.variable.MakeStatus.OK`. ||  || When loading the data using :obj:`Orange.data.Table`; Orange takes the safest  || approach and; by default; reuses everything that is compatible up to  || and including :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`. Unintended reuse would be obvious from the || variable having too many values; which the user can notice and fix. More on that  || in the page on `loading data`. !!TODO!! ||  || There are two functions for reusing the variables instead of creating new ones. ||  || .. function:: Orange.data.variable.make(name; type; ordered_values; unordered_values[; create_new_on]) ||  ||     Find and return an existing variable or create a new one if none of the existing ||     variables matches the given name; type and values. ||      ||     The optional `create_new_on` specifies the status at which a new variable is ||     created. The status must be at most :obj:`~Orange.data.variable.MakeStatus.Incompatible` since incompatible (or ||     non-existing) variables cannot be reused. If it is set lower; for instance  ||     to :obj:`~Orange.data.variable.MakeStatus.MissingValues`; a new variable is created even if there exists ||     a variable which is only missing the same values. If set to :obj:`~Orange.data.variable.MakeStatus.OK`; the function ||     always creates a new variable. ||      ||     The function returns a tuple containing a variable descriptor and the ||     status of the best matching variable. So; if ``create_new_on`` is set to ||     :obj:`~Orange.data.variable.MakeStatus.MissingValues`; and there exists a variable whose status is; say; ||     :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`; a variable would be created; while the second  ||     element of the tuple would contain :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`. If; on the other ||     hand; there exists a variable which is perfectly OK; its descriptor is  ||     returned and the returned status is :obj:`~Orange.data.variable.MakeStatus.OK`. The function returns no  ||     indicator whether the returned variable is reused or not. This can be; ||     however; read from the status code: if it is smaller than the specified ||     ``create_new_on``; the variable is reused; otherwise a new descriptor has been constructed. ||  ||     The exception to the rule is when ``create_new_on`` is OK. In this case; the  ||     function does not search through the existing variables and cannot know the  ||     status; so the returned status in this case is always :obj:`~Orange.data.variable.MakeStatus.OK`. ||  ||     :param name: Variable name ||     :param type: Variable type ||     :type type: Orange.data.variable.Type ||     :param ordered_values: a list of ordered values ||     :param unordered_values: a list of values; for which the order does not ||         matter ||     :param create_new_on: gives the condition for constructing a new variable instead ||         of using the new one ||      ||     :return_type: a tuple (:class:`Orange.data.variable.Variable`; int) ||      || .. function:: Orange.data.variable.retrieve(name; type; ordered_values; onordered_values[; create_new_on]) ||  ||     Find and return an existing variable; or :obj:`None` if no match is found. ||      ||     :param name: variable name. ||     :param type: variable type. ||     :type type: Orange.data.variable.Type ||     :param ordered_values: a list of ordered values ||     :param unordered_values: a list of values; for which the order does not ||         matter ||     :param create_new_on: gives the condition for constructing a new variable instead ||         of using the new one ||  ||     :return_type: :class:`Orange.data.variable.Variable` ||      || These following examples (from :download:`variable-reuse.py <code\/variable-reuse.py>`) give the shown results if || executed only once (in a Python session) and in this order. ||  || :func:`Orange.data.variable.make` can be used for the construction of new variables. :: ||      ||     >>> v1; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""a\""; \""b\""]) ||     >>> print s; v1.values ||     4 <a; b> ||  || No surprises here: a new variable is created and the status is :obj:`~Orange.data.variable.MakeStatus.NotFound`. :: ||  ||     >>> v2; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""a\""]; [\""c\""]) ||     >>> print s; v2 is v1; v1.values ||     1 True <a; b; c> ||  || The status is 1 (:obj:`~Orange.data.variable.MakeStatus.MissingValues`); yet the variable is reused (``v2 is v1``). || ``v1`` gets a new value; ``\""c\""``; which was given as an unordered value. It does || not matter that the new variable does not need the value ``b``. :: ||  ||     >>> v3; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""a\""; \""b\""; \""c\""; \""d\""]) ||     >>> print s; v3 is v1; v1.values ||     1 True <a; b; c; d> ||  || This is like before; except that the new value; ``d`` is not among the || ordered values. :: ||  ||     >>> v4; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""b\""]) ||     >>> print s; v4 is v1; v1.values; v4.values ||     3; False; <b>; <a; b; c; d> ||  || The new variable needs to have ``b`` as the first value; so it is incompatible  || with the existing variables. The status is thus 3 (:obj:`~Orange.data.variable.MakeStatus.Incompatible`); the two  || variables are not equal and have different lists of values. :: ||  ||     >>> v5; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; None; [\""c\""; \""a\""]) ||     >>> print s; v5 is v1; v1.values; v5.values ||     0 True <a; b; c; d> <a; b; c; d> ||  || The new variable has values ``c`` and ``a``; but the order is not important;  || so the existing attribute is :obj:`~Orange.data.variable.MakeStatus.OK`. :: ||  ||     >>> v6; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; None; [\""e\""]) \""a\""]) ||     >>> print s; v6 is v1; v1.values; v6.values ||     2 True <a; b; c; d; e> <a; b; c; d; e> ||  || The new variable has different values than the existing variable (status is 2; || :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`); but the existing one is nonetheless reused. Note that we || gave ``e`` in the list of unordered values. If it was among the ordered; the || reuse would fail. :: ||  ||     >>> v7; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; None; ||             [\""f\""]; Orange.data.variable.MakeStatus.NoRecognizedValues))) ||     >>> print s; v7 is v1; v1.values; v7.values ||     2 False <a; b; c; d; e> <f> ||  || This is the same as before; except that we prohibited reuse when there are no || recognized values. Hence a new variable is created; though the returned status is  || the same as before:: ||  ||     >>> v8; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; ||             [\""a\""; \""b\""; \""c\""; \""d\""; \""e\""]; None; Orange.data.variable.MakeStatus.OK) ||     >>> print s; v8 is v1; v1.values; v8.values ||     0 False <a; b; c; d; e> <a; b; c; d; e> ||  || Finally; this is a perfect match; but any reuse is prohibited; so a new  || variable is created. ||  || \""\""\""",https://github.com/biolab/orange3/commit/1c65f8552ef4a590cffa50cc43e5777708cc2509,Yes
1825,biolab/orange3,Orange/data/table.py,64cb7c5764587596e7821b53964e06480e403003,"TODO: unit tests do not pass because sort does not sort by \""header row\""; but rather sorts each row separately",https://github.com/biolab/orange3/commit/64cb7c5764587596e7821b53964e06480e403003,Yes
1826,biolab/orange3,Orange/widgets/model/tests/test_owsgd.py,78f51648eecdbe91df6a6c9b80c46228f9abeaf4,TODO Fix the base tests to support parameter delegation with,https://github.com/biolab/orange3/commit/78f51648eecdbe91df6a6c9b80c46228f9abeaf4,Yes
1827,biolab/orange3,Orange/widgets/model/tests/test_owadaboost.py,f74547687080f419319fec87d1f9b931fae6f71a,TODO Due to the way params are tested on the learner and the fact,https://github.com/biolab/orange3/commit/f74547687080f419319fec87d1f9b931fae6f71a,Yes
1828,biolab/orange3,Orange/base.py,cc7aa9d32f9c335befbe52515894c39ebd7cc9ad,TODO: Disallow (or mirror) __setattr__ for keys in params?,https://github.com/biolab/orange3/commit/cc7aa9d32f9c335befbe52515894c39ebd7cc9ad,Yes
1829,biolab/orange3,Orange/canvas/canvas/items/annotationitem.py,93bfdbdd35be263204152fdd106e53fe0f1fa772,TODO: Separate the editor from the view.,https://github.com/biolab/orange3/commit/93bfdbdd35be263204152fdd106e53fe0f1fa772,Yes
1830,biolab/orange3,Orange/projection/freeviz.py,daeb537764fc3cc47282ddd3e8079d0cbaa38cd9,TODO: center=False and scale=False  (widget calls with these two parameters!),https://github.com/biolab/orange3/commit/daeb537764fc3cc47282ddd3e8079d0cbaa38cd9,Yes
1831,biolab/orange3,Orange/canvas/application/canvasmain.py,ff96b9610b28649c4a407158e8989651005496f4,TODO: Help view and manager to separate singleton instance.,https://github.com/biolab/orange3/commit/ff96b9610b28649c4a407158e8989651005496f4,Yes
1832,biolab/orange3,Orange/canvas/application/canvasmain.py,ff96b9610b28649c4a407158e8989651005496f4,TODO: Log view to separate singleton instance.,https://github.com/biolab/orange3/commit/ff96b9610b28649c4a407158e8989651005496f4,Yes
1833,biolab/orange3,Orange/widgets/data/owcsvimport.py,30c4b437b1c622a93260e267e6df479f5113609f,TODO: use 'usecols' parameter in `read_csv` call to,https://github.com/biolab/orange3/commit/30c4b437b1c622a93260e267e6df479f5113609f,Yes
1834,biolab/orange3,Orange/tests/test_classification.py,98b0b521a2901a50b86e46f7bf208a7cf7a595fc,TODO: Softmax Regression will be fixed as a separate PR,https://github.com/biolab/orange3/commit/98b0b521a2901a50b86e46f7bf208a7cf7a595fc,Yes
1835,RasaHQ/rasa_core,rasa_core/policies/embedding_policy.py,e5c266edef1d4948b918c28a7ee37d3d22da748d,TODO we reload params again in train do we need it here?,https://github.com/RasaHQ/rasa_core/commit/e5c266edef1d4948b918c28a7ee37d3d22da748d,Yes
1836,dmlc/dgl,mx.py,62a39e5b37144c06eb11c5282b7844c71baf8dd3,TODO: 'sum' should be a separate function,https://github.com/dmlc/dgl/commit/62a39e5b37144c06eb11c5282b7844c71baf8dd3,Yes
1837,dmlc/dgl,mx_scalar.py,05547b37117ef164d2226e8fff136b018c2eb38d,TODO: 'sum' should be a separate function,https://github.com/dmlc/dgl/commit/05547b37117ef164d2226e8fff136b018c2eb38d,Yes
1838,dmlc/dgl,apps/kg/models/general_models.py,15b951d4c4d4a75cc30442d5dd72fb72ed110e09,TODO: only reg ent&rel embeddings. other params to be added.,https://github.com/dmlc/dgl/commit/15b951d4c4d4a75cc30442d5dd72fb72ed110e09,Yes
1839,iterative/dvc,dvc/dependency/__init__.py,88397ef43025ec90d6a82a624ab525aabc43e54e,FIXME PARAM_MD5; PARAM_ETAG,https://github.com/iterative/dvc/commit/88397ef43025ec90d6a82a624ab525aabc43e54e,Yes
1840,iterative/dvc,dvc/repo/experiments/__init__.py,958aa265e70670c49520e393d599fcdce3c122ac,TODO: parallelize this; currently --jobs for repro applies to,https://github.com/iterative/dvc/commit/958aa265e70670c49520e393d599fcdce3c122ac,Yes
1841,iterative/dvc,dvc/parsing/context.py,6a79111047a838b2e7daee0861570ffe01c38d02,"FIXME: after implementing of reading of \""params\"".",https://github.com/iterative/dvc/commit/6a79111047a838b2e7daee0861570ffe01c38d02,Yes
1842,iterative/dvc,tests/unit/test_context.py,b0ae5e2251112f8b3d19426c3f99cdea8394ff5b,FIXME: either support tracking list values in ParamsDependency,https://github.com/iterative/dvc/commit/b0ae5e2251112f8b3d19426c3f99cdea8394ff5b,Yes
1843,microsoft/nni,src/sdk/pynni/nni/feature_engineering/gradient_selector/learnability.py,8ac61b774419766dfd194fa7f1aba8b934763ac9,TODO: could make this a user given parameter,https://github.com/microsoft/nni/commit/8ac61b774419766dfd194fa7f1aba8b934763ac9,Yes
1844,microsoft/nni,src/sdk/pynni/nni/pbt_tuner/pbt_tuner.py,a82b4a3bf7c64e18172cc3a93de567ae6e607a76,TODO think about different type of hyperparameters for 1.perturbation 2.within search space,https://github.com/microsoft/nni/commit/a82b4a3bf7c64e18172cc3a93de567ae6e607a76,Yes
1845,microsoft/nni,src/sdk/pynni/nni/compression/torch/pruning/amc/channel_pruning_env.py,e9f3cddf95e58ca17641db793c420a1b6e5424c0,TODO replace this flops counter with nni.compression.torch.utils.counter.count_flops_params,https://github.com/microsoft/nni/commit/e9f3cddf95e58ca17641db793c420a1b6e5424c0,Yes
1846,microsoft/nni,test/convert_test/mutator.py,8af731463df46f6c73c933bdc45ed9a4ecd4e422,TODO: use formal method function to update parameters,https://github.com/microsoft/nni/commit/8af731463df46f6c73c933bdc45ed9a4ecd4e422,Yes
1847,microsoft/nni,nni/retiarii/nn/pytorch/nn.py,192a807b39bc029c273c2d1349fb73850ed9484c,TODO: 'ModuleDict'; 'ParameterList'; 'ParameterDict';,https://github.com/microsoft/nni/commit/192a807b39bc029c273c2d1349fb73850ed9484c,Yes
1848,OpenMined/PySyft,syft/core/frameworks/torch/utils.py,8cad60095841c6994aeb98bd480a8da362651bbd,TODO: should allow to mix Variable and Parameter in next_child_types,https://github.com/OpenMined/PySyft/commit/8cad60095841c6994aeb98bd480a8da362651bbd,Yes
1849,OpenMined/PySyft,syft/core/frameworks/torch/tensor.py,8d82fbf05553df57d7e14c70e9d4e1689738710a,TODO: @trask @theo could you take a look at this if you have better ideas on how to get these parameters,https://github.com/OpenMined/PySyft/commit/8d82fbf05553df57d7e14c70e9d4e1689738710a,Yes
1850,OpenMined/PySyft,syft/core/frameworks/torch/tensor.py,dc52872630ec3c7b8284948e74e19f5002d220f4,TODO: @trask @theo could you take a look at this if you have better ideas on how to get these parameters,https://github.com/OpenMined/PySyft/commit/dc52872630ec3c7b8284948e74e19f5002d220f4,Yes
1851,OpenMined/PySyft,syft/core/frameworks/torch/tensor.py,acc59e0abae60e79d6f4c1db016f2506239e850e,TODO: @trask @theo could you take a look at this if you have better ideas on how to get these parameters,https://github.com/OpenMined/PySyft/commit/acc59e0abae60e79d6f4c1db016f2506239e850e,Yes
1852,OpenMined/PySyft,syft/core/frameworks/torch/generalized_pointer.py,dae4a309df998c852c333c493fa4c4a3e111d2ad,TODO: @trask @theo could you take a look at this if you have better ideas on how to get these parameters,https://github.com/OpenMined/PySyft/commit/dae4a309df998c852c333c493fa4c4a3e111d2ad,Yes
1853,OpenMined/PySyft,syft/core/frameworks/torch/tensor/generalized_pointer_tensor.py,c832fe8fd8cbb0dfe4858b92f81210c17512ef49,TODO: @trask @theo could you take a look at this if you have better ideas on how to get these parameters,https://github.com/OpenMined/PySyft/commit/c832fe8fd8cbb0dfe4858b92f81210c17512ef49,Yes
1854,OpenMined/PySyft,syft/workers/websocket.py,23231b0b734277ebadcfb2e10e2e12821df3ab4a,TODO get angry when we have no connection params,https://github.com/OpenMined/PySyft/commit/23231b0b734277ebadcfb2e10e2e12821df3ab4a,Yes
1855,OpenMined/PySyft,syft/workers/websocket.py,3a0de19f843579ea28f9f3fbf04849fc4c30d11b,TODO get angry when we have no connection params,https://github.com/OpenMined/PySyft/commit/3a0de19f843579ea28f9f3fbf04849fc4c30d11b,Yes
1856,OpenMined/PySyft,syft/workers/websocket_client.py,6917846cf981485ca83f61142e073c9c343eba4c,TODO get angry when we have no connection params,https://github.com/OpenMined/PySyft/commit/6917846cf981485ca83f61142e073c9c343eba4c,Yes
1857,OpenMined/PySyft,syft/serde/msgpack/torch_serde.py,63dab685d71efac0bb0401165be46f7094aa0157,TODO: fix serialisation of parameters (check in particular .child & .data) See #3214,https://github.com/OpenMined/PySyft/commit/63dab685d71efac0bb0401165be46f7094aa0157,Yes
1858,OpenMined/PySyft,syft/frameworks/torch/he/fv/context.py,786bb4aec8413b95340b4befab9a85b789823af4,TODO: Check if the parameters are secure according to HomomorphicEncryption.org standards,https://github.com/OpenMined/PySyft/commit/786bb4aec8413b95340b4befab9a85b789823af4,Yes
1859,OpenMined/PySyft,src/syft/core/node/common/action/function_or_constructor_action.py,658f477c38d6ba0ceb08d13fec8337437a620208,TODO: eliminate this explicit parameter and just set the object,https://github.com/OpenMined/PySyft/commit/658f477c38d6ba0ceb08d13fec8337437a620208,Yes
1860,OpenMined/PySyft,src/syft/core/node/common/service/msg_forwarding_service.py,658f477c38d6ba0ceb08d13fec8337437a620208,TODO: don't return .data - instead have storableObject's parameters actually,https://github.com/OpenMined/PySyft/commit/658f477c38d6ba0ceb08d13fec8337437a620208,Yes
1861,OpenMined/PySyft,src/syft/core/node/common/action/function_or_constructor_action.py,953318be37b7d3d990ce99fae1f6c794008919c3,TODO: eliminate this explicit parameter and just set the object,https://github.com/OpenMined/PySyft/commit/953318be37b7d3d990ce99fae1f6c794008919c3,Yes
1862,allenai/allennlp,allennlp/data/data_iterator.py,6f415a2d96dfabb911b566eaa8076c9a7cc71081,TODO(joelgrus): implement `from_params` methods on subclasses + change,https://github.com/allenai/allennlp/commit/6f415a2d96dfabb911b566eaa8076c9a7cc71081,Yes
1863,allenai/allennlp,allennlp/commands/fine_tune.py,dc2f80cd721e1e6890669165e741f5cd193ffb03,TODO(mattg): pull this block out into a separate function (maybe just add this to,https://github.com/allenai/allennlp/commit/dc2f80cd721e1e6890669165e741f5cd193ffb03,Yes
1864,allenai/allennlp,allennlp/commands/train.py,dc2f80cd721e1e6890669165e741f5cd193ffb03,TODO(mattg): pull this block out into a separate function (maybe just add this to,https://github.com/allenai/allennlp/commit/dc2f80cd721e1e6890669165e741f5cd193ffb03,Yes
1865,allenai/allennlp,allennlp/models/semantic_parsing/atis/atis_semantic_parser.py,0459261c388bec72937e0268c4316b85279d0bac,TODO(kevin) Move some of this functionality to a separate method for computing validation outputs.,https://github.com/allenai/allennlp/commit/0459261c388bec72937e0268c4316b85279d0bac,Yes
1866,allenai/allennlp,allennlp/data/dataset_readers/semantic_parsing/quarel.py,8ff832471364b09278d5863f31c47fa32662dd08,TODO: Clarify this into an appropriate parameter,https://github.com/allenai/allennlp/commit/8ff832471364b09278d5863f31c47fa32662dd08,Yes
1867,ray-project/ray,rllib/contrib/bandits/agents/policy.py,6ddf84b019be0b6dee52c9699f7c6ebe416dc3d3,TODO: Have a separate model catalogue for bandits,https://github.com/ray-project/ray/commit/6ddf84b019be0b6dee52c9699f7c6ebe416dc3d3,Yes
1868,ray-project/ray,rllib/policy/torch_policy.py,f43d934817ae08119b38b59fb5dbcb68d2c8120d,TODO: (sven) Unify hyperparam annealing procedures across RLlib (tf\/torch),https://github.com/ray-project/ray/commit/f43d934817ae08119b38b59fb5dbcb68d2c8120d,Yes
1869,ray-project/ray,python/ray/autoscaler/aws/utils.py,908c0c630a6c7c6e35b7bb0e172d41372bfb309d,todo: any other errors that we should catch separately?,https://github.com/ray-project/ray/commit/908c0c630a6c7c6e35b7bb0e172d41372bfb309d,Yes
1870,ray-project/ray,python/ray/autoscaler/commands.py,908c0c630a6c7c6e35b7bb0e172d41372bfb309d,todo: add a separate flag?,https://github.com/ray-project/ray/commit/908c0c630a6c7c6e35b7bb0e172d41372bfb309d,Yes
1871,NervanaSystems/neon,neon/backends/layer_gpu.py,3497dcc8bb6200f2ce073b61bd6e5ad599f23141,"\""\""\"" || Definition of the GPU layers || These layers are mainly used for old benchmarking code; || but they also cache all the computed params for complex layers. || TODO: clean up merge with CPU layers || TODO: remove any non-param caching code; neon layers should replace benchmark code. || \""\""\""",https://github.com/NervanaSystems/neon/commit/3497dcc8bb6200f2ce073b61bd6e5ad599f23141,Yes
1872,NervanaSystems/neon,neon/callbacks/callbacks.py,3497dcc8bb6200f2ce073b61bd6e5ad599f23141,TODO can serialize be handled by regular data callback or should it be separate?,https://github.com/NervanaSystems/neon/commit/3497dcc8bb6200f2ce073b61bd6e5ad599f23141,Yes
1873,microsoft/MMdnn,mmdnn/conversion/caffe/mapper.py,a0e5dbeb927644467a908f6d2391e777ad9db8b4,TODO: The gamma parameter has to be set (in node.data?) and this should work.,https://github.com/microsoft/MMdnn/commit/a0e5dbeb927644467a908f6d2391e777ad9db8b4,Yes
1874,cleverhans-lab/cleverhans,cleverhans/attacks.py,4f1033788951f452736986645ff1d0dadb64e974,TODO: fix args parameter for Theano case,https://github.com/cleverhans-lab/cleverhans/commit/4f1033788951f452736986645ff1d0dadb64e974,Yes
1875,cleverhans-lab/cleverhans,cleverhans/attacks_tf.py,929386afb1b8a4afc8bb6a8462755336f46e7276,TODO: make use of 'ord' parameter,https://github.com/cleverhans-lab/cleverhans/commit/929386afb1b8a4afc8bb6a8462755336f46e7276,Yes
1876,cleverhans-lab/cleverhans,cleverhans/attacks_th.py,3369da498e5754d552207d0f40aded8c631a0912,TODO: make use of 'ord' parameter,https://github.com/cleverhans-lab/cleverhans/commit/3369da498e5754d552207d0f40aded8c631a0912,Yes
1877,cleverhans-lab/cleverhans,cleverhans/future/torch/attacks/projected_gradient_descent.py,eda18047272338f55a9d61eb557be7e9fdb644dd,TODO: deprecate the rand_minmax param? (as the original,https://github.com/cleverhans-lab/cleverhans/commit/eda18047272338f55a9d61eb557be7e9fdb644dd,Yes
1878,chainer/chainer,chainer/functions/ctc.py,854fcd3ca2ccf59b0776ba820215e0049b977e40,Todo: need to parameterize;,https://github.com/chainer/chainer/commit/854fcd3ca2ccf59b0776ba820215e0049b977e40,Yes
1879,chainer/chainer,chainer/graph_optimizations/static_graph_utilities.py,e5f605d06db1b31bcb82991f5656c173c35522a1,todo: consider only marking a variable if it is a parameter.,https://github.com/chainer/chainer/commit/e5f605d06db1b31bcb82991f5656c173c35522a1,Yes
1880,chainer/chainer,chainer/graph_optimizations/static_graph.py,aef64159d655c7196f8227f20afb1d1a8b4598b7,todo: Add a debug mode (perhaps enabled by defaut) that checks that the parameter references,https://github.com/chainer/chainer/commit/aef64159d655c7196f8227f20afb1d1a8b4598b7,Yes
1890,PPPLDeepLearning/plasma-python,plasma/models/mpi_runner.py,6edc92d58b011bb10ec3aab686de6153679cdd1c,FIXME make it return some stats over epoch,https://github.com/PPPLDeepLearning/plasma-python/commit/6edc92d58b011bb10ec3aab686de6153679cdd1c,Yes
1891,PPPLDeepLearning/plasma-python,plasma/models/mpi_runner.py,a7eb75d4b03ed1406b0eb4c2bac519d9344b00d3,FIXME make it return some stats over epoch,https://github.com/PPPLDeepLearning/plasma-python/commit/a7eb75d4b03ed1406b0eb4c2bac519d9344b00d3,Yes
1897,lopuhin/transformer-lm,lm/gpt_2_tf2/main2.py,083a91bd97125f5ae1e0a64ead08cc839a72a031,TODO: re-create this each epoch,https://github.com/lopuhin/transformer-lm/commit/083a91bd97125f5ae1e0a64ead08cc839a72a031,Yes
1902,ECRL/ECNet,ecnet/model.py,cad9a7ddbdc2d301d0abdd75a55fe49c6af0a357,Every 250 epochs (TODO: make this a variable?):,https://github.com/ECRL/ECNet/commit/cad9a7ddbdc2d301d0abdd75a55fe49c6af0a357,Yes
1903,IBM/mi-prometheus,workers/trainer.py,9318b6c5eec9c5172484b7227de7806157b5b9ff,early_stopping(index=epoch; avg_loss_valid). (todo: coming in next release),https://github.com/IBM/mi-prometheus/commit/9318b6c5eec9c5172484b7227de7806157b5b9ff,Yes
1904,IBM/mi-prometheus,workers/trainer.py,6eb0f92aa40659aaec95a3adedc5f6c273e3f58f,early_stopping(index=epoch; avg_loss_valid). (todo: coming in next release),https://github.com/IBM/mi-prometheus/commit/6eb0f92aa40659aaec95a3adedc5f6c273e3f58f,Yes
1905,IBM/mi-prometheus,workers/classic_trainer.py,d08f53cd37acdf8721ab196292df2bc6651e5ba9,early_stopping(index=epoch; avg_valid_loss). (TODO: coming in next release),https://github.com/IBM/mi-prometheus/commit/d08f53cd37acdf8721ab196292df2bc6651e5ba9,Yes
1906,IBM/mi-prometheus,workers/flexible_trainer.py,d08f53cd37acdf8721ab196292df2bc6651e5ba9,early_stopping(index=epoch; avg_valid_loss). (TODO: coming in next release),https://github.com/IBM/mi-prometheus/commit/d08f53cd37acdf8721ab196292df2bc6651e5ba9,Yes
1907,IBM/mi-prometheus,workers/offline_trainer.py,892379a07ac8c410748fbbbcd7509cdc55493b05,early_stopping(index=epoch; avg_valid_loss). (TODO: coming in next release),https://github.com/IBM/mi-prometheus/commit/892379a07ac8c410748fbbbcd7509cdc55493b05,Yes
1908,IBM/mi-prometheus,workers/online_trainer.py,892379a07ac8c410748fbbbcd7509cdc55493b05,early_stopping(index=epoch; avg_valid_loss). (TODO: coming in next release),https://github.com/IBM/mi-prometheus/commit/892379a07ac8c410748fbbbcd7509cdc55493b05,Yes
1920,pyannote/pyannote-audio,pyannote/audio/train.py,259502e343a60a55d2473ad0cec987cf95b100f8,TODO check in dlib's code whether patience * batches_per_epoch + 1,https://github.com/pyannote/pyannote-audio/commit/259502e343a60a55d2473ad0cec987cf95b100f8,Yes
1921,pyannote/pyannote-audio,pyannote/audio/train/schedulers.py,f6afd48f856e611a3ff55adae199f5470c2ac9b9,TODO check in dlib's code whether patience * batches_per_epoch + 1,https://github.com/pyannote/pyannote-audio/commit/f6afd48f856e611a3ff55adae199f5470c2ac9b9,Yes
1928,feedly/transfer-nlp,transfer_nlp/runners/runnersABC.py,f5c28a8d1d295f0674897b05d24fb0f63ac32c57,sample_probability = (20 + self.epoch_index) \/ self.config_args['num_epochs']  # TODO: include this into the NMT training part,https://github.com/feedly/transfer-nlp/commit/f5c28a8d1d295f0674897b05d24fb0f63ac32c57,Yes
1933,williamSYSU/TextGAN-PyTorch,instructor/oracle_data/evocatgan_instructor.py,d30235e7b9f8029ad3a739ac707a740817ed061c,self.update_temperature(adv_epoch; cfg.ADV_train_epoch)   # TODO: update parents temperature,https://github.com/williamSYSU/TextGAN-PyTorch/commit/d30235e7b9f8029ad3a739ac707a740817ed061c,Yes
1934,williamSYSU/TextGAN-PyTorch,instructor/oracle_data/evogan_instructor.py,d30235e7b9f8029ad3a739ac707a740817ed061c,self.update_temperature(adv_epoch; cfg.ADV_train_epoch)   # TODO: update parents temperature,https://github.com/williamSYSU/TextGAN-PyTorch/commit/d30235e7b9f8029ad3a739ac707a740817ed061c,Yes
1935,williamSYSU/TextGAN-PyTorch,instructor/oracle_data/evogan_instructor.py,02cada9ce2f974590568db33b1bc3b0729c9f2b3,self.update_temperature(adv_epoch; cfg.ADV_train_epoch)   # TODO: update parents temperature,https://github.com/williamSYSU/TextGAN-PyTorch/commit/02cada9ce2f974590568db33b1bc3b0729c9f2b3,Yes
2006,Erotemic/netharn,netharn/examples/cifar.py,5f79282c043a0d58be110ce7719c72f3402fbd58,"\""\""\"" || The examples\/cifar.py is probably the most clear example of what netharn is and || what it's trying to do \/ not do. ||  || The basic idea is make an object that inherits from nh.FitHarn. This is our || harness object. It will contain the hyperparameters as well as the learning || state. All the training loop boilerplate has already been written in the parent || class; so all our child class needs to do is: define `prepare_batch` (not || usually needed) and `run_batch`. Code to measure and record performance should || be placed in `on_batch` and `on_epoch`. ||  || The `train` function is our main entry point. It reads parameters from the || command line to override defaults. It then consructs the `HyperParams` object || and constructs an instance of `CIFAR_FitHarn` and calls `harn.run()`. ||  || This begins the training process. At a high level the harness will load the || data using torch DataLoaders; and call `run_batch` when it needs to compute the || model outputs and loss based on the input data. The returned loss is used to || update the model weights if `harn.tag === 'train'`; for validation; test; and || calibration (todo) datasets the loss is simply recorded. ||  || After `run_batch` finishes the `on_batch` function is called; where you can || optionally return a dict of scalars to log as measurements for this batch (note || loss is always recorded; so we need not return it here; but loss components may || be useful). A similar thing happens in `on_epoch`; where you should return || metrics about the entire dataset. ||  || The training harness manages the fit directory structure based on a hash of the || hyperparameters; the creation of algorithm component instance (e.g. model; || optimizer); initializing model weights; restarting from the most recent epoch; || updating the learning rates; various training loop boilerplate details; || checking divergence; reporting progress; handling differences between train; || validation; and test sets. In short; netharn handles the necessary parts and || let the developer focus on the important parts. || \""\""\""",https://github.com/Erotemic/netharn/commit/5f79282c043a0d58be110ce7719c72f3402fbd58,Yes
2007,simonfqy/PADME,dcCustom/models/tensor_graph.py,4f3610b7ee05d2bd755fa73ffdd2afdb154f1421,TODO: the following two variables are temporary; for hyperparameter tuning purpose.,https://github.com/simonfqy/PADME/commit/4f3610b7ee05d2bd755fa73ffdd2afdb154f1421,No
2008,simonfqy/PADME,dcCustom/models/tensorgraph/tensor_graph.py,2cb677a6ee205673b6e17454b979c3a2cce991a5,TODO: the following two variables are temporary; for hyperparameter tuning purpose.,https://github.com/simonfqy/PADME/commit/2cb677a6ee205673b6e17454b979c3a2cce991a5,No
2009,simonfqy/PADME,dcCustom/models/tensorgraph/tensor_graph.py,90a881f4afc0e536b8fc242c9f6646c6cce09b0a,TODO: the following two variables are temporary; for hyperparameter tuning purpose.,https://github.com/simonfqy/PADME/commit/90a881f4afc0e536b8fc242c9f6646c6cce09b0a,No
2010,bareblackfoot/lddp-tf-faster-rcnn,lib/model/train_val.py,ee512c4533be6d7c26a1f8ebbc1c373114b5bc9f,Needs to restore the other hyperparameters\/states for training; (TODO xinlei) I have,https://github.com/bareblackfoot/lddp-tf-faster-rcnn/commit/ee512c4533be6d7c26a1f8ebbc1c373114b5bc9f,Yes
2011,bareblackfoot/lddp-tf-faster-rcnn,lib/model/train_val_vgg16.py,f230e29fe94ccffa151a4a4406005c5be280192e,Needs to restore the other hyperparameters\/states for training; (TODO xinlei) I have,https://github.com/bareblackfoot/lddp-tf-faster-rcnn/commit/f230e29fe94ccffa151a4a4406005c5be280192e,Yes
2012,dimimal/semantics_segmentation_of_urban_environments,lib/buildModels.py,f1564a4d137fe5ef49eef41e660179c765d3de46,TODO: Pass CRF Hyperparameters,https://github.com/dimimal/semantics_segmentation_of_urban_environments/commit/f1564a4d137fe5ef49eef41e660179c765d3de46,No
2013,dtrckd/pymake,pymake/models/igmm/cgs.py,c40b412aac7c40865a09b38a8baf8ab1584e4899,todo: add in the likelihood for K and hyperparameter,https://github.com/dtrckd/pymake/commit/c40b412aac7c40865a09b38a8baf8ab1584e4899,Yes
2014,aws-samples/deep-learning-models,models/nlp/electra/run_pretraining.py,70db58fa60894e90f90c9f398c333bfa36da683a,TODO: Make temperature a hyperparameter,https://github.com/aws-samples/deep-learning-models/commit/70db58fa60894e90f90c9f398c333bfa36da683a,Yes
2015,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,f20c0f0542911575e6eac34dc1106638c21a6a84,"TODO: Add default acceptable selections\/ranges for algorithms' hyperparameters and add \""default\"" kwargs to \""add\"" methods",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
2016,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Check :attr:`module_name`'s library_helper for :attr:`model_initializer` for a default `hyperparameter` list,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
2017,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,f20c0f0542911575e6eac34dc1106638c21a6a84,# TODO: Supply callables declaring valid\/invalid relationships between hyperparameters to filter the total choices,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
2018,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,a7a9e62101defb39f2d0ea36a8abc9621f53f97f,TODO: :attr:`current_hyperparameters_list` only exists in Informed Protocols,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a7a9e62101defb39f2d0ea36a8abc9621f53f97f,Yes
2019,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,from hyperparameter_hunter.tracers import TranslateTrace  # TODO: Add when tested with `Mirror`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,No
2020,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/data/data_core.py,aaea5bb6c0452f694e9da3206836d0152b490bb9,"\""\""\""This module defines mechanisms for managing an experiment's various datasets; and each datasets's || inputs; targets; and predictions. ||  || **Important Contents** ||  || In order to maintain the states of different datasets across all divisions of an experiment and || amid transformations that may be applied to the data via || :mod:`~hyperparameter_hunter.feature_engineering`; two main classes are defined herein: ||  || 1. :class:`BaseDataChunk`: ||  ||     * Logical separations between \""columns\"" of data for a given :class:`BaseDataset` ||     * Held and maintained by :class:`BaseDataset` and its descendants ||     * Three primary descendants of :class:`BaseDataChunk`: ||  ||         1. :class:`InputChunk`: Maintains a dataset's input data (and transformations) ||         2. :class:`TargetChunk`: Maintains a dataset's target data (and transformations) ||         3. :class:`PredictionChunk`: Maintains a dataset's predictions (and transformations) ||  ||     * Descendants of :class:`BaseDataChunk` should implement the eight \""on_<division>_<point>\"" ||       callback methods defined by :class:`~hyperparameter_hunter.callbacks.bases.BaseCallback` ||  ||         * Because :class:`BaseDataChunk` subclasses are isolated from the experiment; these methods ||           need not invoke their `super` methods; although they are allowed to if necessary ||  ||     * :class:`NullDataChunk` does nothing but mimic the normal :class:`BaseDataChunk` child structure ||  ||         * Used for :class:`BaseDataset` subclasses lacking a particular data chunk; such as: ||  ||             1) `TestDataset`'s `TargetChunk`; because the targets for a test dataset are unknown; or ||             2) `TrainDataset`'s `PredictionChunk`; because predictions are not made on training data ||  || 2. :class:`BaseDataset`: ||  ||     # TODO: ... ||  || **Dataset Attribute Syntax** ||  || The intricate subclass network bolstering the module's predominant :class:`BaseDataset` subclasses || may be intimidating at first; but don't worry; there's a shortcut. Follow these steps to ensure || proper syntax and a valid result when accessing data from a || :class:`~hyperparameter_hunter.experiments.CVExperiment`: ||  || 1. {`data_train`; `data_oof`; `data_holdout`; `data_test`} - Dataset attribute || 2. {`input`; `target`; `prediction`} - Data chunk || 3. [`T`] - Optional transformation || 4. {`d`; `run`; `fold`; `rep`; `final`} - Division; initial (`d`) or `final` data ||  || By stacking three values (four if following optional step \""3\"") from the above formula; you can || access all of the interesting stuff stored in the datasets from the comfort of your experiment or || :func:`~hyperparameter_hunter.callbacks.bases.lambda_callback`. ||  || Related || ------- || :mod:`hyperparameter_hunter.callbacks.bases` ||     This module defines the core callback method structure mirrored by :class:`BaseDataCore`. ||     Despite the strong logical connection to this module; it is important to remember that the only ||     actual connection between the two modules is in :mod:`hyperparameter_hunter.callbacks.wranglers` || :mod:`hyperparameter_hunter.callbacks.wranglers` ||     # TODO: ... Handlers for the `Dataset`s to invoke callback methods with required parameters ||     This module defines the callback classes that act as handlers for the descendants of ||     :class:`BaseDataset` || :mod:`hyperparameter_hunter.experiments` ||     # TODO: ... || \""\""\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/aaea5bb6c0452f694e9da3206836d0152b490bb9,Yes
2021,Neuraxio/Neuraxle,neuraxle/metaopt/tpe.py,ddf7dd7511d981e8b87442f6fd0b97cd0ff03317,TODO: Maybe they use the likelyhood to sum over all possible parameters to find the max so it become a join distribution of all hyperparameters; would make sense.,https://github.com/Neuraxio/Neuraxle/commit/ddf7dd7511d981e8b87442f6fd0b97cd0ff03317,No
2022,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_4.py,ae7b50a959c1432ce14e2a984d0cf7e3dddbc786,TODO: once SKLL hyperparameters can be passed; replace this code,https://github.com/EducationalTestingService/rsmtool/commit/ae7b50a959c1432ce14e2a984d0cf7e3dddbc786,Yes
2023,hoya012/shake-shake-tensorflow,test.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Test hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,Yes
2024,hoya012/shake-shake-tensorflow,train.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Training hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,Yes
2025,hoya012/shake-shake-tensorflow,train.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Regularization hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,Yes
2026,hoya012/shake-shake-tensorflow,train.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Evaluation hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,No
2027,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,c462b034b0765b50bd7c0b2d83256c05f4bd9f8a,TODO sensible default hyperparameter grid,https://github.com/HealthCatalyst/healthcareai-py/commit/c462b034b0765b50bd7c0b2d83256c05f4bd9f8a,Yes
2028,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,58175722ed069a677c04034d139a216bc66641ff,TODO add sensible SGD classifier hyperparameter grid,https://github.com/HealthCatalyst/healthcareai-py/commit/58175722ed069a677c04034d139a216bc66641ff,Yes
2029,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,58175722ed069a677c04034d139a216bc66641ff,TODO add sensible hyperparameter grid,https://github.com/HealthCatalyst/healthcareai-py/commit/58175722ed069a677c04034d139a216bc66641ff,Yes
2030,HealthCatalyst/healthcareai-py,healthcareai/advanced_trainer.py,21c2ac2bd22098b16e2e0fab91e0be6ce4c9bae9,TODO add sensible KNN hyperparameter grid,https://github.com/HealthCatalyst/healthcareai-py/commit/21c2ac2bd22098b16e2e0fab91e0be6ce4c9bae9,Yes
2031,mme/vergeml,vergeml/env.py,b5cffa8a8e532d64bc38e58baa51ec38bf30285e,TODO reserved: hyperparameters and results,https://github.com/mme/vergeml/commit/b5cffa8a8e532d64bc38e58baa51ec38bf30285e,No
2032,deepchem/deepchem,deepchem/molnet/load_function/pdbbind_datasets.py,215e2f118ca8394ad3ce68a7bcd032f8844cffaf,TODO: This is not the correct setting. Set hyperparameters correctly,https://github.com/deepchem/deepchem/commit/215e2f118ca8394ad3ce68a7bcd032f8844cffaf,Yes
2033,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/triplet_loss.py,4cf3746c401f53e222acdd42fd737e31157de8cb,"TODO. tune this \""10\"" hyperparameter",https://github.com/pyannote/pyannote-audio/commit/4cf3746c401f53e222acdd42fd737e31157de8cb,Yes
2034,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/centroid_loss.py,36985df5599ff54855549689f6a6bf79f2b7969f,"TODO. tune this \""10\"" hyperparameter",https://github.com/pyannote/pyannote-audio/commit/36985df5599ff54855549689f6a6bf79f2b7969f,Yes
2035,autonomio/talos,talos/commands/deploy.py,b7646a7a66a1bffdec72179d0c9b59d795f90e05,TODO: needs to also deploy the hyperparameter configuration,https://github.com/autonomio/talos/commit/b7646a7a66a1bffdec72179d0c9b59d795f90e05,Yes
2036,PetrochukM/PyTorch-NLP,examples/end_to_end/main.py,c6c351965bf22965765f837c4b83bfdb02cb4743,TODO: Return the best loss if hyperparameter tunning.,https://github.com/PetrochukM/PyTorch-NLP/commit/c6c351965bf22965765f837c4b83bfdb02cb4743,Yes
2037,lai-bluejay/diego,diego/preprocessor/auto_binning.py,33ea32953c6219d750942a0ebae7a81ec7247b62,TODO add hyperparameter to gbdt binning,https://github.com/lai-bluejay/diego/commit/33ea32953c6219d750942a0ebae7a81ec7247b62,No
2038,lai-bluejay/diego,diego/preprocessor/local_uncertainty_sampling.py,33ea32953c6219d750942a0ebae7a81ec7247b62,TODO add hyperparameter to gbdt binning,https://github.com/lai-bluejay/diego/commit/33ea32953c6219d750942a0ebae7a81ec7247b62,Yes
2039,lai-bluejay/diego,diego/classifier.py,12e33a920ac8d0cb28d2717fe03d26d42dcf74b8,TODO add hyperparameter to gbdt binning,https://github.com/lai-bluejay/diego/commit/12e33a920ac8d0cb28d2717fe03d26d42dcf74b8,Yes
2040,lzfelix/minuet,minuet/minuet.py,98b90c10f7b2b5d2ccbd6bce3fd2eaaec2213ff1,TODO: fix hyperparameters reloading,https://github.com/lzfelix/minuet/commit/98b90c10f7b2b5d2ccbd6bce3fd2eaaec2213ff1,No
2041,microsoft/nni,src/sdk/pynni/nni/pbt_tuner/pbt_tuner.py,a82b4a3bf7c64e18172cc3a93de567ae6e607a76,TODO think about different type of hyperparameters for 1.perturbation 2.within search space,https://github.com/microsoft/nni/commit/a82b4a3bf7c64e18172cc3a93de567ae6e607a76,Yes
2042,omegaml/omegaml,omegaml/mixins/mdf/apply.py,fa4a8267a2ec050bc8a15d6a15289ac81e43e92a,TODO write a unit test for this condition,https://github.com/omegaml/omegaml/commit/fa4a8267a2ec050bc8a15d6a15289ac81e43e92a,No
2043,omegaml/omegaml,omegaml/tests/features/environment.py,bacd5dff3831d0feaa3f49b15328a416e7888adf,FIXME we do this because context.feature is set dynamically in EE testing,https://github.com/omegaml/omegaml/commit/bacd5dff3831d0feaa3f49b15328a416e7888adf,No
2044,omegaml/omegaml,omegaml/backends/restapi/asyncrest.py,65669112df0861b07c796bc5c0aaaa8f12b5e699,hack to allow local testing,https://github.com/omegaml/omegaml/commit/65669112df0861b07c796bc5c0aaaa8f12b5e699,No
2045,ssydasheng/GPflow-Slim,examples/gpr.py,5d83d31893746a9e768ce94a1e90874eff0ce09a,TODO: test when ARD=FALSE,https://github.com/ssydasheng/GPflow-Slim/commit/5d83d31893746a9e768ce94a1e90874eff0ce09a,Yes
2046,tritemio/FRETBursts,fretbursts/tests/test_burstlib.py,9a0d27d0641794ed9a22a63460051b80ce2a3617,TODO add all the ph_sel combinations like in test_bg_from(),https://github.com/tritemio/FRETBursts/commit/9a0d27d0641794ed9a22a63460051b80ce2a3617,No
2047,vaibhavshukla182/Brain-MRI-Segmentation,evaluation.py,f613cc9aa3a6ea0eaa134a142c4aa514bc9f0aa3,Get meta data from the test-image; needed for some sitk methods that check this,https://github.com/vaibhavshukla182/Brain-MRI-Segmentation/commit/f613cc9aa3a6ea0eaa134a142c4aa514bc9f0aa3,No
2048,rstemmer/musicdb,mdbapi/extern.py,b88a22c4d2ecbe4c09517384815a8831fbbfca9a,"\""\""\"" || This class handles the upload of music files to external storages. || Before most of the methods can be used; the mountpoint of the external storage must be set using :meth:`~mdbapi.extern.MusicDBExtern.SetMountpoint`. || The default mount poiunt is ``\""\/mnt\""``. || There are three main tasks this class implements: ||  || * `Initializing a Storage`_ || * `Updating a Storage`_  || * `Handling Toxic Environments`_ ||  ||  || Initializing a Storage || ---------------------- ||  || Each storage must have a directory configured in the main MusicDB-Configuration; that holds some states. || This directory can be created and initialized using the :meth:`~mdbapi.extern.MusicDBExtern.InitializeStorage`. || It is possible to check if the storage was already initialized using the method :meth:`~mdbapi.extern.MusicDBExtern.IsStorageInitialized`. ||  || Example: ||  ||     .. code-block:: python ||  ||         database = MusicDatabase(\"".\/music.db\"") ||         config   = MusicDBConfig(\"".\/musicdb.ini\"") ||         extern   = MusicDBExtern(config; database) ||         extern.SetMountpoint(\""\/mnt\"") ||  ||         # Initialize mounted storage if not done yet ||         if not extern.IsStorageInitialized(): ||             extern.InitializeStorage() ||  || After initializing; a directory is created and a bare *config.ini* is copied from the *MusicDB share directory*. || The name of the directory; the config-source-file and the state-file-names can be configured in the MusicDB-Config file. || It is recommended to use a hidden directory for the states on the external storage. || MusicDB must have write access to the directory and its files. ||  || Storage Configuration || ^^^^^^^^^^^^^^^^^^^^^ ||  || The storage configuration can be used to adapt to the environment other software or devices require to use the music that will be stored on it. || More details about `Handling Toxic Environments`_ can be found in the related subsection. ||  || Template for such a storage configuration: ||  ||     .. literalinclude:: ..\/..\/..\/share\/extconfig.ini ||         :language: ini ||  || The State-File || ^^^^^^^^^^^^^^ ||  || The state-file mostly called *songmap* is a Comma Separated Value (csv) file. || It contains a row for each song on the storage and is used to identify the song. || This is mandatory because the path on the external storage may differ from the paths of the music collection.  || This can happen for example by transcoding or renaming to a FAT compatible path. ||  || Each row has the following columns: ||  ||     * Original file path (relative) ||     * File path on the external storage (relative) ||  || The csv file must have the following dialect: ||  ||     * delimiter:  ``;``  ||     * escapechar: ``\\`` ||     * quotechar:  ``\""`` ||     * quoting:    ``csv.QUOTE_NONNUMERIC`` ||  || Example: ||  ||     .. code-block:: python ||  ||        \""Rammstein\/2001 - Mutter\/03 Sonne.m4a\"";\""Rammstein\/2001 - Mutter\/03 Sonne.mp3\""  ||  || This file gets generated by the method :meth:`~mdbapi.extern.MusicDBExtern.WriteSongmap`  || and can be read by :meth:`~mdbapi.extern.MusicDBExtern.ReadSongmap`. || Usually the user should never touch this file. ||  ||  || Updating a Storage || ------------------ ||  || Updating an external storage device like a mp3-player or a SD-Card; the :meth:`~mdbapi.extern.MusicDBExtern.UpdateStorage` method can be used. ||  || Example: ||  ||     .. code-block:: python ||  ||         database = MusicDatabase(\"".\/music.db\"") ||         config   = MusicDBConfig(\"".\/musicdb.ini\"") ||         extern   = MusicDBExtern(config; database) ||         extern.SetMountpoint(\""\/mnt\"") ||  ||         # Update storage if it is valid ||         if extern.IsStorageInitialized(): ||             extern.UpdateStorage() ||  ||  || Handling Toxic Environments || --------------------------- ||  || A *toxic environment* is a device that has some limitations and constraints the exported music has to fulfill. || For example; my car can only read mp3 files; has a path length limit of 256 characters and can only access a FAT filesystem. || My mp3-player does not have that many constraints; but slows down if the album covers are too large and have to be scaled down the mp3 players screen resolution. || Those limitations can be handled by MusicDBExtern. ||  || There are several methods to handle toxic environments. || They can be activated in the config file that will be generated when the storage gets initialized. ||  || The following methods will be applied if activated in the config: ||  ||     * :meth:`~mdbapi.extern.MusicDBExtern.ReducePathLength` if the pathlength is limited ||     * :meth:`~mdbapi.extern.MusicDBExtern.ConvertToMP3` if only mp3-files are allowed ||     * :meth:`~mdbapi.extern.MusicDBExtern.OptimizeMP3Tags` to scale artwork and make proper ID3 tags ||     * :meth:`~mdbapi.extern.MusicDBExtern.OptimizeM4ATags` make proper meta tags for m4a files ||     * :meth:`~mdbapi.extern.MusicDBExtern.FixPath` to handle unicode in paths ||  ||     .. warning:: ||  ||         When optimizing M4A-Tags; the album artwork gets lost. ||         This is a `bug <https:\/\/trac.ffmpeg.org\/ticket\/2798>`_ in ``ffmpeg``. I did not find any good workarounds yet. ||  || \""\""\""",https://github.com/rstemmer/musicdb/commit/b88a22c4d2ecbe4c09517384815a8831fbbfca9a,No
2049,yonkshi/text2imageNet,lenet/tensorcv/models/layers.py,28cf797566877a80db00e33e4311f70ab7b241b2,TODO need test,https://github.com/yonkshi/text2imageNet/commit/28cf797566877a80db00e33e4311f70ab7b241b2,No
2050,yonkshi/text2imageNet,train.py,322f5f8f7ca018c076e5fa1d2bb4a127dc9d4335,TODO TESTING ACCURAYC; MERGE BACK INTO FUNCTION,https://github.com/yonkshi/text2imageNet/commit/322f5f8f7ca018c076e5fa1d2bb4a127dc9d4335,No
2051,yonkshi/text2imageNet,dataloader.py,b7578edd5c413cd7a29fd66023715e258531e16c,if test_count > 100: continue # TODO Delete me; this is to limit test set for testing purpose,https://github.com/yonkshi/text2imageNet/commit/b7578edd5c413cd7a29fd66023715e258531e16c,Yes
2052,jawahar273/practNLPTools-lite,setup.py,a520b476be7ffcb09808d9fff869f82978d31238,TODO: put package test requirements here,https://github.com/jawahar273/practNLPTools-lite/commit/a520b476be7ffcb09808d9fff869f82978d31238,No
2053,Seanforfun/GMAN_Net_Haze_Removal,DehazeNet/dehazenet_eval.py,ab7d09ca84b7bba44924e1db3bdd5071aa693393,TODO STEP2:Use inference to create a test operation,https://github.com/Seanforfun/GMAN_Net_Haze_Removal/commit/ab7d09ca84b7bba44924e1db3bdd5071aa693393,Yes
2054,Seanforfun/GMAN_Net_Haze_Removal,DehazeNet/dehazenet_eval.py,ab7d09ca84b7bba44924e1db3bdd5071aa693393,TODO STEP3:Call _evaluate_single_batch() to run the test program,https://github.com/Seanforfun/GMAN_Net_Haze_Removal/commit/ab7d09ca84b7bba44924e1db3bdd5071aa693393,Yes
2055,thuijskens/scikit-hyperband,hyperband/tests/test_hyperband.py,89398bae46fe4c5607ab63c87c1ccfd4c2b50fd4,"\""\""\"" || TODO: This test fails due to the random state not being properly fixed ||  || def test_hyperband(): ||     model; param_dist; X; y; rng = setup() ||     search = HyperbandSearchCV(model; param_dist; random_state=rng) ||     search.fit(X; y) ||  ||     # results = pd.DataFrame(search.cv_results_) ||     expected_params = { ||         'bootstrap': False; ||         'criterion': 'entropy'; ||         'max_depth': None; ||         'max_features': 7; ||         'min_samples_leaf': 2; ||         'min_samples_split': 2; ||         'n_estimators': 81 ||     } ||  ||     # assert(results.shape[0] == 186) TODO: sort out what the expected n_i and r_i values are ||     assert(search.best_params_ == expected_params) || \""\""\""",https://github.com/thuijskens/scikit-hyperband/commit/89398bae46fe4c5607ab63c87c1ccfd4c2b50fd4,No
2056,jinfagang/tfboys,tf_codes/base_nn/seg_kitti_road.py,c43f18e793efebd2f79705cb2ea2f6a98a1e1a41,TODO: predict the testing data and save the augmented images,https://github.com/jinfagang/tfboys/commit/c43f18e793efebd2f79705cb2ea2f6a98a1e1a41,No
2057,sshleifer/object_detection_kitti,swivel/wordsim.py,f3a2f63b734e73dfb72e5d82719b03a0e93baba8,"\""\""\""Computes Spearman's rho with respect to human judgements. ||  || Given a set of row (and potentially column) embeddings; this computes Spearman's || rho between the rank ordering of predicted word similarity and human judgements. ||  || Usage: ||  ||   wordim.py --embeddings=<binvecs> --vocab=<vocab> eval1.tab eval2.tab ... ||  || Options: ||  ||   --embeddings=<filename>: the vectors to test ||   --vocab=<filename>: the vocabulary file ||  || Evaluation files are assumed to be tab-separated files with exactly three || columns.  The first two columns contain the words; and the third column contains || the scored human judgement. ||  || \""\""\""",https://github.com/sshleifer/object_detection_kitti/commit/f3a2f63b734e73dfb72e5d82719b03a0e93baba8,No
2058,sshleifer/object_detection_kitti,privacy/train_student.py,c711dc707e87a26504529031c1acde6283513f78,Store unused part of test set for use as a test set after student training,https://github.com/sshleifer/object_detection_kitti/commit/c711dc707e87a26504529031c1acde6283513f78,No
2059,sshleifer/object_detection_kitti,differential_privacy/multiple_teachers/train_student.py,ac0829fa2b94336a79af962c9cffbf283a81b6c3,Store unused part of test set for use as a test set after student training,https://github.com/sshleifer/object_detection_kitti/commit/ac0829fa2b94336a79af962c9cffbf283a81b6c3,No
2060,sshleifer/object_detection_kitti,cognitive_mapping_and_planning/datasets/nav_env.py,5b9d9097cc255becef4b5460c4b951c143d7a380,"r\""\""\""Navidation Environment. Includes the following classes along with some || helper functions. ||   Building: Loads buildings; computes traversibility; exposes functionality for ||     rendering images. ||    ||   GridWorld: Base class which implements functionality for moving an agent on a ||     grid world. ||    ||   NavigationEnv: Base class which generates navigation problems on a grid world. ||    ||   VisualNavigationEnv: Builds upon NavigationEnv and Building to provide ||     interface that is used externally to train the agent.  ||    ||   MeshMapper: Class used for distilling the model; testing the mapper. ||    ||   BuildingMultiplexer: Wrapper class that instantiates a VisualNavigationEnv for ||     each building and multiplexes between them as needed. || \""\""\""",https://github.com/sshleifer/object_detection_kitti/commit/5b9d9097cc255becef4b5460c4b951c143d7a380,Yes
2061,sshleifer/object_detection_kitti,object_detection/create_pet_tf_record.py,a4944a57ad2811e1f6a7a87589a9fc8a776e8d3c,TODO: Add test for pet\/PASCAL main files.,https://github.com/sshleifer/object_detection_kitti/commit/a4944a57ad2811e1f6a7a87589a9fc8a776e8d3c,Yes
2062,sshleifer/object_detection_kitti,object_detection/models/feature_map_generators_test.py,a4944a57ad2811e1f6a7a87589a9fc8a776e8d3c,TODO: add tests with different anchor strides.,https://github.com/sshleifer/object_detection_kitti/commit/a4944a57ad2811e1f6a7a87589a9fc8a776e8d3c,Yes
2063,sshleifer/object_detection_kitti,object_detection/vod_converter/kitti_tracking.py,5805950bb9c035818062efc2709925f85d27fca0,"\""\""\"" || Ingestor for KITTI tracking formats. ||  || http:\/\/www.cvlibs.net\/datasets\/kitti\/eval_tracking.php ||  || Note: even though this is for tracking instead of object detection; sometime it's helpful to convert || data from this for object detection training. This reads in the left color labels. ||  || Per devkit docs: ||  || The data for training and testing can be found in the corresponding folders. || The sub-folders are structured as follows: ||  ||   - image_02\/%04d\/ contains the left color camera sequence images (png) ||   - image_03\/%04d\/ contains the right color camera sequence images  (png) ||   - label_02\/ contains the left color camera label files (plain text files) ||   - calib\/ contains the calibration for all four cameras (plain text files) ||  || The label files contain the following information; which can be read and || written using the matlab tools (readLabels.m) provided within this devkit. || All values (numerical or strings) are separated via spaces; each row || corresponds to one object. The 17 columns represent: ||  || #Values    Name      Description || ---------------------------------------------------------------------------- ||    1    frame        Frame within the sequence where the object appearers ||    1    track id     Unique tracking id of this object within this sequence ||    1    type         Describes the type of object: 'Car'; 'Van'; 'Truck'; ||                      'Pedestrian'; 'Person_sitting'; 'Cyclist'; 'Tram'; ||                      'Misc' or 'DontCare' ||    1    truncated    Float from 0 (non-truncated) to 1 (truncated); where ||                      truncated refers to the object leaving image boundaries. || \t\t     Truncation 2 indicates an ignored object (in particular || \t\t     in the beginning or end of a track) introduced by manual || \t\t     labeling. ||    1    occluded     Integer (0;1;2;3) indicating occlusion state: ||                      0 = fully visible; 1 = partly occluded ||                      2 = largely occluded; 3 = unknown ||    1    alpha        Observation angle of object; ranging [-pi..pi] ||    4    bbox         2D bounding box of object in the image (0-based index): ||                      contains left; top; right; bottom pixel coordinates ||    3    dimensions   3D object dimensions: height; width; length (in meters) ||    3    location     3D object location x;y;z in camera coordinates (in meters) ||    1    rotation_y   Rotation ry around Y-axis in camera coordinates [-pi..pi] ||    1    score        Only for results: Float; indicating confidence in ||                      detection; needed for p\/r curves; higher is better. ||  ||  || \""\""\""",https://github.com/sshleifer/object_detection_kitti/commit/5805950bb9c035818062efc2709925f85d27fca0,Yes
2064,roscisz/TensorHive,tensorhive/core_anew/managers/InfrastructureManager.py,b2c0406aa5242857be47741e465f5c86400eb8d3,FIXME Only for testing purposes,https://github.com/roscisz/TensorHive/commit/b2c0406aa5242857be47741e465f5c86400eb8d3,No
2065,roscisz/TensorHive,tests/unit/models/test_user_model.py,00981d3c2f4e8d123bdfc068872800b42e7f0a5a,TODO Move to test_role_model.py,https://github.com/roscisz/TensorHive/commit/00981d3c2f4e8d123bdfc068872800b42e7f0a5a,Yes
2066,roscisz/TensorHive,tensorhive/controllers/task.py,ea42ca17cc28c6d2c8234efe71a9732ba97f716d,"\""\""\"" || TODO Group business functions into one place || TODO May want to have 1 function per endpoint.  || My goal was to separate authorization from controllers' logic; so that || manual and automatic testing don't require patching Flask context (@jwt_required breaks things) ||  || \""\""\""",https://github.com/roscisz/TensorHive/commit/ea42ca17cc28c6d2c8234efe71a9732ba97f716d,Yes
2067,roscisz/TensorHive,tensorhive/core/task_nursery.py,6b614e20ca55c58c254f1b9b9ef535376d9326c9,"\""\""\"" || This module provides functionality for spawning commands on host machines via ssh. || It's divided into 3 parts: ||     1) command builder(s): ||         Classes that hold a set of commands (as strings) which can be launched by task executor ||         in order to achieve some result; e.g. `spawn` and `terminate` ||         You can implement your own command builder that uses different backend; currently it's `screen` program. ||  ||     2) task executor: ||         Launches commands provided by command builder.  ||         It contains core logic for e.g. spawning; terminating processes on remote hosts via ssh.  ||  ||     3) stateless API functions: ||         Provides high-level interface for operations on remote host; like: ||         * spawning; terminating processes ||         * getting info about running processes ||         * log fetching ||  ||  || Author's note: ||     1) The ONLY recommended way to use this module from outside is by using ||     exposed API functions (see __all__) ||     # TODO Write tests || \""\""\""",https://github.com/roscisz/TensorHive/commit/6b614e20ca55c58c254f1b9b9ef535376d9326c9,Yes
2068,IlyaGusev/summarus,external/bart_scripts/trim_fairseq_model.py,cf23341258c0f41eeacd4c705d19b09276dafce1,"\""\""\"" || This is code to take a trained Fairseq model and discard the ADAM optimizer state; || which is not needed at test time. It can reduce a model size by ~70%. ||  || Original author: Brian Thompson || \""\""\""",https://github.com/IlyaGusev/summarus/commit/cf23341258c0f41eeacd4c705d19b09276dafce1,No
2069,spallas/time-attention,data_loader.py,7801b7353a4aa12f424b959009e9cd47946bb5c9,TODO(xhebraj) implement return of train\/test\/validation,https://github.com/spallas/time-attention/commit/7801b7353a4aa12f424b959009e9cd47946bb5c9,Yes
2070,EugeneLoy/coq_jupyter,test/kernel_test.py,490e0d990b780d31566f269cfeebf3684bbab287,TODO add test for different message severities,https://github.com/EugeneLoy/coq_jupyter/commit/490e0d990b780d31566f269cfeebf3684bbab287,Yes
2071,gmontamat/gentun,gentun/models/keras_models.py,9df97b50e01a7207fc0d08f2175fa5099654f45f,TODO: cross-validations or at least train\/test split,https://github.com/gmontamat/gentun/commit/9df97b50e01a7207fc0d08f2175fa5099654f45f,Yes
2072,gregversteeg/LinearCorex,linear_corex.py,a3a3065461ed9d2b8442c549c4305c882b811daa,TODO: test good IC,https://github.com/gregversteeg/LinearCorex/commit/a3a3065461ed9d2b8442c549c4305c882b811daa,Yes
2073,iamvon/viBlind,AI_Team/API/Blind_Vision_Backend/env-Backend/lib/python3.6/_dummy_thread.py,bc7d672875dcce2cc6f2729ffea044e3d2b80567,XXX Perhaps shouldn't actually bother to test?  Could lead,https://github.com/iamvon/viBlind/commit/bc7d672875dcce2cc6f2729ffea044e3d2b80567,Yes
2074,iamvon/viBlind,AI_Team/API/Blind_Vision_Backend/env-Backend/lib/python3.6/site.py,bc7d672875dcce2cc6f2729ffea044e3d2b80567,encoding after initialization.  The test for presence is needed when,https://github.com/iamvon/viBlind/commit/bc7d672875dcce2cc6f2729ffea044e3d2b80567,Yes
2075,1adrianb/pytorch-estimate-flops,test/test_ops.py,51d3ce3ce320217440c6ed25f6d7e7af3426fb27,TODO: Add test for every op,https://github.com/1adrianb/pytorch-estimate-flops/commit/51d3ce3ce320217440c6ed25f6d7e7af3426fb27,Yes
2076,erp12/pyshgp,tests/push/test_interpreter.py,2d48116876ca1d4290c977ef238165c278420721,TODO: WRITE _handle_?_instruction TESTS,https://github.com/erp12/pyshgp/commit/2d48116876ca1d4290c977ef238165c278420721,Yes
2077,erp12/pyshgp,tests/push/test_interpreter.py,2d48116876ca1d4290c977ef238165c278420721,TODO: WRITE INTERPRETER TESTS,https://github.com/erp12/pyshgp/commit/2d48116876ca1d4290c977ef238165c278420721,Yes
2078,erp12/pyshgp,tests/gp/test_search.py,571e081116a407f45ee820d620f1114ce5f65fdf,@TODO: Test with custom PushTypeLibrary and custom instructions.,https://github.com/erp12/pyshgp/commit/571e081116a407f45ee820d620f1114ce5f65fdf,No
2079,erp12/pyshgp,tests/push/test_interpreter.py,571e081116a407f45ee820d620f1114ce5f65fdf,@TODO: Interpreter test,https://github.com/erp12/pyshgp/commit/571e081116a407f45ee820d620f1114ce5f65fdf,Yes
2080,erp12/pyshgp,tests/push/instruction_test_specs.py,d61d2df2a98444d510a8b841be4fca86cc03553c,@todo test logical instructions,https://github.com/erp12/pyshgp/commit/d61d2df2a98444d510a8b841be4fca86cc03553c,Yes
2081,leopepe/GOApy,venv_old/share/doc/networkx-1.11/examples/advanced/iterated_dynamical_systems.py,515e6b53fa4f51a70cf79f57368db294bb607479,"\""\""\"" || Digraphs from Integer-valued Iterated Functions || =============================================== ||  ||  || Sums of cubes on 3N || ------------------- ||  || The number 153 has a curious property. ||  || Let 3N={3;6;9;12;...} be the set of positive multiples of 3.  Define an || iterative process f:3N->3N as follows: for a given n; take each digit || of n (in base 10); cube it and then sum the cubes to obtain f(n). ||  || When this process is repeated; the resulting series n; f(n); f(f(n));... || terminate in 153 after a finite number of iterations (the process ends || because 153 = 1**3 + 5**3 + 3**3). ||  || In the language of discrete dynamical systems; 153 is the global || attractor for the iterated map f restricted to the set 3N. ||  || For example: take the number 108 ||  || f(108) = 1**3 + 0**3 + 8**3 = 513 ||  || and ||  || f(513) = 5**3 + 1**3 + 3**3 = 153 ||  || So; starting at 108 we reach 153 in two iterations; || represented as: ||  || 108->513->153 ||  || Computing all orbits of 3N up to 10**5 reveals that the attractor || 153 is reached in a maximum of 14 iterations. In this code we || show that 13 cycles is the maximum required for all integers (in 3N) || less than 10;000. ||  || The smallest number that requires 13 iterations to reach 153; is 177; i.e.; ||  || 177->687->1071->345->216->225->141->66->432->99->1458->702->351->153 ||  || The resulting large digraphs are useful for testing network software.  ||  || The general problem || ------------------- ||  || Given numbers n; a power p and base b; define F(n; p; b) as the sum of || the digits of n (in base b) raised to the power p. The above example || corresponds to f(n)=F(n; 3;10); and below F(n; p; b) is implemented as || the function powersum(n;p;b). The iterative dynamical system defined by || the mapping n:->f(n) above (over 3N) converges to a single fixed point; || 153. Applying the map to all positive integers N; leads to a discrete || dynamical process with 5 fixed points: 1; 153; 370; 371; 407. Modulo 3 || those numbers are 1; 0; 1; 2; 2. The function f above has the added || property that it maps a multiple of 3 to another multiple of 3; i.e. it || is invariant on the subset 3N. ||  ||  || The squaring of digits (in base 10) result in cycles and the || single fixed point 1. I.e.; from a certain point on; the process || starts repeating itself. ||  || keywords: \""Recurring Digital Invariant\""; \""Narcissistic Number\""; || \""Happy Number\"" ||  || The 3n+1 problem || ---------------- ||  || There is a rich history of mathematical recreations || associated with discrete dynamical systems.  The most famous  || is the Collatz 3n+1 problem. See the function  || collatz_problem_digraph below. The Collatz conjecture || --- that every orbit returrns to the fixed point 1 in finite time  || --- is still unproven. Even the great Paul Erdos said \""Mathematics || is not yet ready for such problems\""; and offered $500  || for its solution.  ||  || keywords: \""3n+1\""; \""3x+1\""; \""Collatz problem\""; \""Thwaite's conjecture\"" ||  ||  || \""\""\""",https://github.com/leopepe/GOApy/commit/515e6b53fa4f51a70cf79f57368db294bb607479,Yes
2082,ildoonet/kaggle-human-protein-atlas-image-classification,main.py,7eb2a058c7f6fe71ee9c39003808d33134290fb7,TODO : test-time augmentation,https://github.com/ildoonet/kaggle-human-protein-atlas-image-classification/commit/7eb2a058c7f6fe71ee9c39003808d33134290fb7,Yes
2083,phohenecker/pytorch-transformer,examples/overfitting_test.py,e80dad100f507b1dcb056e719391408ca433ec32,"\""\""\""An implementation of the overfitting test for the Transformer model. ||  || A simple test; which often signifies bugs in the implementation of a model; is the overfitting test. To that end; the || considered model is trained and evaluated on the same tiny dataset; which it should be able to overfit easily. || Therefore; the final model should yield very high probabilities for the desired target values. If this is not the case; || however; then there is probably something wrong with the tested model and\/or its implementation. ||  || TODO: explain a bit more || \""\""\""",https://github.com/phohenecker/pytorch-transformer/commit/e80dad100f507b1dcb056e719391408ca433ec32,No
2084,andreasvc/disco-dop,tests/unittests.py,82b69825286cd7da6ea1da55274e176fd809e179,FIXME only import for relevant test:,https://github.com/andreasvc/disco-dop/commit/82b69825286cd7da6ea1da55274e176fd809e179,No
2085,andreasvc/disco-dop,discodop/treebank.py,62969c8f601554807881b4ff9785c858e7d860ca,hack to ensure test\/dev set as the first 1235 + 1235 sentences,https://github.com/andreasvc/disco-dop/commit/62969c8f601554807881b4ff9785c858e7d860ca,Yes
2086,Angzz/panoptic-fpn-gluon,docs/tutorials/classification/demo_imagenet.py,7caee8e1cb994730efa849cd10603c1294e2b759,"\""\""\""3. Getting Started with Pre-trained Models on ImageNet || =========================================================== ||  || `ImageNet <http:\/\/www.image-net.org\/>`__ is a || large labeled dataset of real-world images. It is one of the most || widely used dataset in latest computer vision research. ||  || |imagenet| ||  || In this tutorial; we will show how a pre-trained neural network || classifies real world images. ||  || For your convenience; we provide a script that loads a pre-trained ``ResNet50_v2`` model; || and classifies an input image. || For a list of all models we have; please visit `Gluon Model Zoo <..\/..\/model_zoo\/index.html>`__. ||  || Demo || ------------------ ||  || A model trained on ImageNet can classify images into 1000 classes; this makes it || much more powerful than the one we showed in the `CIFAR10 demo <demo_cifar10.html>`__. ||  || :download:`Download demo_imagenet.py<..\/..\/..\/scripts\/classification\/imagenet\/demo_imagenet.py>` ||  || With this script; you can load a pre-trained model and classify any image you have. ||  || Let's test with the photo of Mt. Baker again. ||  || |image0| ||  || :: ||  ||     python demo_imagenet.py --model ResNet50_v2 --input-pic mt_baker.jpg ||  || And the model predicts that ||  || :: ||  ||     The input picture is classified to be ||     \t[volcano]; with probability 0.558. ||     \t[alp]; with probability 0.398. ||     \t[valley]; with probability 0.018. ||     \t[lakeside]; with probability 0.006. ||     \t[mountain_tent]; with probability 0.006. ||  || This time it does a good job. Note that we have listed the top five || most probable classes; because with 1000 classes the model may not always rank the || correct answer highest. Besides top-1 accuracy; we often also || consider top-5 accuracy as a measurement of how well a model can predict. ||  || Next Step || --------- ||  || If you would like to dive deeper into ``ImageNet`` training; || feel free to read the next tutorial on `ImageNet Training <dive_deep_imagenet.html>`__. ||  || Or; if you would like to know how to train a powerful model tailored to your own data; || please go ahead and read the tutorial on `Transfer learning <transfer_learning_minc.html>`__. ||  || .. |imagenet| image:: https:\/\/raw.githubusercontent.com\/dmlc\/web-data\/master\/gluoncv\/datasets\/imagenet_mosaic.jpg || .. |image0| image:: https:\/\/raw.githubusercontent.com\/dmlc\/web-data\/master\/gluoncv\/classification\/mt_baker.jpg ||  || \""\""\""",https://github.com/Angzz/panoptic-fpn-gluon/commit/7caee8e1cb994730efa849cd10603c1294e2b759,Yes
2087,Angzz/panoptic-fpn-gluon,docs/tutorials/classification/dive_deep_imagenet.py,7caee8e1cb994730efa849cd10603c1294e2b759,"\""\""\""5. Train Your Own Model on ImageNet || ========================================== ||  || ``ImageNet`` is the most well-known dataset for image classification. || Since it was published; most of the research that advances the state-of-the-art || of image classification was based on this dataset. ||  || Although there are a lot of available models; it is still a non-trivial task to || train a state-of-the-art model on ``ImageNet`` from scratch. || In this tutorial; we will smoothly walk || you through the process of training a model on ``ImageNet``. ||  || .. note:: ||  ||     The rest of the tutorial walks you through the details of ``ImageNet`` training. ||     If you want a quick start without knowing the details; try downloading ||     this script and start training with just one command. ||  ||     :download:`Download train_imagenet.py<..\/..\/..\/scripts\/classification\/imagenet\/train_imagenet.py>` ||  ||     The commands used to reproduce results from papers are given in our ||     `Model Zoo <..\/..\/model_zoo\/index.html>`__. ||  || .. note:: ||  ||     Since real training is extremely resource consuming; we don't actually ||     execute code blocks in this tutorial. ||  ||  || Prerequisites || ------------- ||  || **Expertise** ||  || We assume readers have a basic understanding of ``Gluon``; we suggest || you start with `Gluon Crash Course <http:\/\/gluon-crash-course.mxnet.io\/index.html>`__ . ||  || Also; we assume that readers have gone through previous tutorials on || `CIFAR10 Training <dive_deep_cifar10.html>`_ and `ImageNet Demo <demo_imagenet.html>`_ . ||  || **Data Preparation** ||  || Unlike ``CIFAR10``; we need to prepare the data manually. || If you haven't done so; please go through our tutorial on || `Prepare ImageNet Data <..\/examples_datasets\/imagenet.html>`_ . ||  || **Hardware** ||  || Training deep learning models on a dataset of over one million images is || very resource demanding. || Two main bottlenecks are tensor computation and data IO. ||  || For tensor computation; it is recommended to use a GPU; preferably a high-end || one. || Using multiple GPUs together will further reduce training time. ||  || For data IO; we recommend a fast CPU and a SSD disk. Data loading can greatly benefit || from multiple CPU threads; and a fast SSD disk. Note that in total the compressed || and extracted ``ImageNet`` data could occupy around 300GB disk space; thus a SSD with || at least 300GB is required. ||  || Network structure || ----------------- ||  || Finished preparation? Let's get started! ||  || First; import the necessary libraries into python. ||  || .. code-block:: python ||  ||     import argparse; time ||  ||     import numpy as np ||     import mxnet as mx ||  ||     from mxnet import gluon; nd ||     from mxnet import autograd as ag ||     from mxnet.gluon import nn ||  ||     from gluoncv.model_zoo import get_model ||     from gluoncv.utils import makedirs; TrainingHistory ||  || In this tutorial we use ``ResNet50_v2``; a network with balanced prediction || accuracy and computational cost. ||  || .. code-block:: python ||  ||     # number of GPUs to use ||     num_gpus = 4 ||     ctx = [mx.gpu(i) for i in range(num_gpus)] ||  ||     # Get the model ResNet50_v2; with 10 output classes ||     net = get_model('ResNet50_v2'; classes=1000) ||     net.initialize(mx.init.MSRAPrelu(); ctx = ctx) ||  ||  || Note that the ResNet model we use here for ``ImageNet`` is different in structure from || the one we used to train ``CIFAR10``. Please refer to the original paper or || GluonCV codebase for details. ||  || Data Augmentation with ImageRecordIter || --------------------------------- ||  || When training a small network with multiple GPUs; data IO could be a bottleneck for the performance. || Besides data loader from gluon; we recommend to use the `ImageRecordIter` interface to load and || process data from record files. || For more information on record files; please refer to `our tutorial <..\/examples_datasets\/recordio.html>`_. ||  || Data augmentation is essential for a good result. || We can set related parameters in the `ImageRecordIter`. ||  || .. code-block:: python ||  ||     jitter_param = 0.4 ||     lighting_param = 0.1 ||     mean_rgb = [123.68; 116.779; 103.939] ||     std_rgb = [58.393; 57.12; 57.375] ||  ||     train_data = mx.io.ImageRecordIter( ||         path_imgrec         = '~\/.mxnet\/datasets\/imagenet\/rec\/train.rec'; ||         path_imgidx         = '~\/.mxnet\/datasets\/imagenet\/rec\/train.idx'; ||         preprocess_threads  = 32; ||         shuffle             = True; ||         batch_size          = 256; ||  ||         data_shape          = (3; 224; 224); ||         mean_r              = mean_rgb[0]; ||         mean_g              = mean_rgb[1]; ||         mean_b              = mean_rgb[2]; ||         std_r               = std_rgb[0]; ||         std_g               = std_rgb[1]; ||         std_b               = std_rgb[2]; ||         rand_mirror         = True; ||         random_resized_crop = True; ||         max_aspect_ratio    = 4. \/ 3.; ||         min_aspect_ratio    = 3. \/ 4.; ||         max_random_area     = 1; ||         min_random_area     = 0.08; ||         brightness          = jitter_param; ||         saturation          = jitter_param; ||         contrast            = jitter_param; ||         pca_noise           = lighting_param; ||     ) ||  ||  || Since ``ImageNet`` images have much higher resolution and quality than || ``CIFAR10``; we can crop a larger image (224x224) as input to the model. ||  || For prediction; we still need deterministic results. The function to read is: ||  || .. code-block:: python ||  ||     val_data = mx.io.ImageRecordIter( ||         path_imgrec         = '~\/.mxnet\/datasets\/imagenet\/rec\/val.rec'; ||         path_imgidx         = '~\/.mxnet\/datasets\/imagenet\/rec\/val.idx'; ||         preprocess_threads  = 32; ||         shuffle             = False; ||         batch_size          = 256; ||  ||         resize              = 256; ||         data_shape          = (3; 224; 224); ||         mean_r              = mean_rgb[0]; ||         mean_g              = mean_rgb[1]; ||         mean_b              = mean_rgb[2]; ||         std_r               = std_rgb[0]; ||         std_g               = std_rgb[1]; ||         std_b               = std_rgb[2]; ||     ) ||  || It is important to keep the normalization consistent; since trained || model only works well on test data from the same distribution. ||  || The above code works as data loader; thus we can later directly plug them into || the training loop. ||  || Note that we set `batch_size=256` as the total batch size on 4 GPUs. || It may not suit GPUs with memory smaller than 12GB. Please tune the value according to your specific configuration. ||  || Path ``'~\/.mxnet\/datasets\/imagenet\/rec'`` is the default path if you || prepared the data `with our script <..\/examples_datasets\/imagenet.html>`_. ||  || Optimizer; Loss and Metric || -------------------------- ||  || Optimizer is what improves the model during training. We use the popular || Nesterov accelerated gradient descent algorithm. ||  || .. code-block:: python ||  ||     # Learning rate decay factor ||     lr_decay = 0.1 ||     # Epochs where learning rate decays ||     lr_decay_epoch = [30; 60; 90; np.inf] ||  ||     # Nesterov accelerated gradient descent ||     optimizer = 'nag' ||     # Set parameters ||     optimizer_params = {'learning_rate': 0.1; 'wd': 0.0001; 'momentum': 0.9} ||  ||     # Define our trainer for net ||     trainer = gluon.Trainer(net.collect_params(); optimizer; optimizer_params) ||  ||  || For classification tasks; we usually use softmax cross entropy as the || loss function. ||  || .. code-block:: python ||  ||     loss_fn = gluon.loss.SoftmaxCrossEntropyLoss() ||  ||  || With 1000 classes the model may not always rate the correct answer with the highest || rank. Besides top-1 accuracy; we also consider top-5 accuracy as a measurement || of how well the model is doing. ||  || At the end of every epoch; we record and print the metric scores. ||  || .. code-block:: python ||  ||     acc_top1 = mx.metric.Accuracy() ||     acc_top5 = mx.metric.TopKAccuracy(5) ||     train_history = TrainingHistory(['training-top1-err'; 'training-top5-err'; ||                                      'validation-top1-err'; 'validation-top5-err']) ||  || Validation || ---------- ||  || At the end of every training epoch; we evaluate it on the validation data set; || and report the top-1 and top-5 error rate. ||  || .. code-block:: python ||  ||     def test(ctx; val_data): ||         acc_top1_val = mx.metric.Accuracy() ||         acc_top5_val = mx.metric.TopKAccuracy(5) ||         for i; batch in enumerate(val_data): ||             data = gluon.utils.split_and_load(batch.data[0]; ctx_list=ctx; batch_axis=0) ||             label = gluon.utils.split_and_load(batch.label[0]; ctx_list=ctx; batch_axis=0) ||             outputs = [net(X) for X in data] ||             acc_top1_val.update(label; outputs) ||             acc_top5_val.update(label; outputs) ||  ||         _; top1 = acc_top1_val.get() ||         _; top5 = acc_top5_val.get() ||         return (1 - top1; 1 - top5) ||  || Training || -------- ||  || After all these preparation; we can finally start training! || Following is the main training loop: ||  || .. code-block:: python ||  ||     epochs = 120 ||     lr_decay_count = 0 ||     log_interval = 50 ||  ||     for epoch in range(epochs): ||         tic = time.time() ||         btic = time.time() ||         acc_top1.reset() ||         acc_top5.reset() ||  ||         if lr_decay_period == 0 and epoch == lr_decay_epoch[lr_decay_count]: ||             trainer.set_learning_rate(trainer.learning_rate*lr_decay) ||             lr_decay_count += 1 ||  ||         for i; batch in enumerate(train_data): ||             data = gluon.utils.split_and_load(batch.data[0]; ctx_list=ctx; batch_axis=0) ||             label = gluon.utils.split_and_load(batch.label[0]; ctx_list=ctx; batch_axis=0) ||             with ag.record(): ||                 outputs = [net(X) for X in data] ||                 loss = [L(yhat; y) for yhat; y in zip(outputs; label)] ||             ag.backward(loss) ||             trainer.step(batch_size) ||             acc_top1.update(label; outputs) ||             acc_top5.update(label; outputs) ||             if log_interval and not (i + 1) % log_interval: ||                 _; top1 = acc_top1.get() ||                 _; top5 = acc_top5.get() ||                 err_top1; err_top5 = (1-top1; 1-top5) ||                 print('Epoch[%d] Batch [%d]\\tSpeed: %f samples\/sec\\ttop1-err=%f\\ttop5-err=%f'%( ||                           epoch; i; batch_size*opt.log_interval\/(time.time()-btic); err_top1; err_top5)) ||                 btic = time.time() ||  ||         _; top1 = acc_top1.get() ||         _; top5 = acc_top5.get() ||         err_top1; err_top5 = (1-top1; 1-top5) ||  ||         err_top1_val; err_top5_val = test(ctx; val_data) ||         train_history.update([err_top1; err_top5; err_top1_val; err_top5_val]) ||  ||         print('[Epoch %d] training: err-top1=%f err-top5=%f'%(epoch; err_top1; err_top5)) ||         print('[Epoch %d] time cost: %f'%(epoch; time.time()-tic)) ||         print('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch; err_top1_val; err_top5_val)) ||  ||  || We can plot the top-1 error rates with: ||  || .. code-block:: python ||  ||     train_history.plot(['training-top1-err'; 'validation-top1-err']) ||  || If you train the model with ``epochs=120``; the plot may look like: ||  || |image-imagenet-curve| ||  || Next Step || --------- ||  || `Model Zoo <..\/..\/model_zoo\/index.html>`_ provides scripts and commands for || training models on ``ImageNet``. ||  || If you want to know what you can do with the model you just || trained; please read the tutorial on `Transfer learning <transfer_learning_minc.html>`__. ||  || Besides classification; deep learning models nowadays can do other exciting tasks || like `object detection <..\/examples_detection\/index.html>`_ and || `semantic segmentation <..\/examples_segmentation\/index.html>`_. ||  || .. |image-imagenet-curve| image:: https:\/\/raw.githubusercontent.com\/dmlc\/web-data\/master\/gluoncv\/classification\/resnet50_v2_top1.png || \""\""\""",https://github.com/Angzz/panoptic-fpn-gluon/commit/7caee8e1cb994730efa849cd10603c1294e2b759,Yes
2088,Angzz/panoptic-fpn-gluon,docs/tutorials/detection/demo_webcam.py,7caee8e1cb994730efa849cd10603c1294e2b759,"\""\""\""09. Run an object detection model on your webcam || ================================================== ||  || This article will shows how to play with pre-trained object detection models by running || them directly on your webcam video stream. ||  || .. note:: ||  ||     - This tutorial has only been tested in a MacOS environment ||     - Python packages required: cv2; matplotlib ||     - You need a webcam :) ||     - Python compatible with matplotlib rendering; installed as a framework in MacOS see guide `here <https:\/\/matplotlib.org\/faq\/osx_framework.html>`__ ||  ||  || Loading the model and webcam || ---------------------------- || Finished preparation? Let's get started! || First; import the necessary libraries into python. ||  || .. code-block:: python ||  ||     import time ||  ||     import cv2 ||     import gluoncv as gcv ||     import matplotlib.pyplot as plt ||     import mxnet as mx ||  ||  || In this tutorial we use ``ssd_512_mobilenet1.0_voc``; a snappy network with good accuracy that should be || well above 1 frame per second on most laptops. Feel free to try a different model from || the `Gluon Model Zoo <..\/..\/model_zoo\/detection.html>`__ ! ||  || .. code-block:: python ||  ||     # Load the model ||     net = gcv.model_zoo.get_model('ssd_512_mobilenet1.0_voc'; pretrained=True) ||  ||  || We create the webcam handler in opencv to be able to acquire the frames: ||  || .. code-block:: python ||  ||     # Load the webcam handler ||     cap = cv2.VideoCapture(0) ||     time.sleep(1) ### letting the camera autofocus ||  ||  || Detection loop || -------------- ||  || The detection loop consists of four phases: ||  || * loading the webcam frame ||  || * pre-processing the image ||  || * running the image through the network ||  || * updating the output with the resulting predictions ||  ||  || .. code-block:: python ||  ||     axes = None ||     NUM_FRAMES = 200 # you can change this ||     for i in range(NUM_FRAMES): ||         # Load frame from the camera ||         ret; frame = cap.read() ||  ||         # Image pre-processing ||         frame = mx.nd.array(cv2.cvtColor(frame; cv2.COLOR_BGR2RGB)).astype('uint8') ||         rgb_nd; frame = gcv.data.transforms.presets.ssd.transform_test(frame; short=512; max_size=700) ||  ||         # Run frame through network ||         class_IDs; scores; bounding_boxes = net(rgb_nd) ||  ||         # Display the result ||         plt.cla() ||         axes = gcv.utils.viz.plot_bbox(frame; bounding_boxes[0]; scores[0]; class_IDs[0]; class_names=net.classes; ax=axes) ||         plt.draw() ||         plt.pause(0.001) ||  ||  || We release the webcam before exiting the script ||  || .. code-block:: python ||  ||     cap.release() ||  || Results || --------- ||  || Download the script to run the demo: ||  || :download:`Download demo_webcam_run.py<..\/..\/..\/scripts\/detection\/demo_webcam_run.py>` ||  ||  || Run the script using `pythonw` on MacOS: ||  || .. code-block:: bash ||  ||     pythonw demo_webcam_run.py --num-frames 200 ||  ||  || .. note:: ||  ||     On MacOS; to enable matplotlib rendering you need python installed as a framework; ||     see guide `here <https:\/\/matplotlib.org\/faq\/osx_framework.html>`__ ||  ||  || If all goes well you should be able to detect objects from the available || classes of the VOC dataset. That includes persons; chairs and TV Screens! ||  || .. image:: https:\/\/media.giphy.com\/media\/9JvoKeUeCt4bdRf3Cv\/giphy.gif ||  ||  || \""\""\""",https://github.com/Angzz/panoptic-fpn-gluon/commit/7caee8e1cb994730efa849cd10603c1294e2b759,No
2089,Erotemic/netharn,netharn/examples/tests/voc_eval_orig.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,hack so forward call behaves like it does in test,https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,Yes
2090,Erotemic/netharn,netharn/fit_harn.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,"\""\""\"" || Notes: ||     when profiling ensure CUDA_LAUNCH_BLOCKING=1 ||  || Notes: ||     to use; your training session must have the concept of: ||         * epochs ||         * batch_size ||         * xpu ||         * train \/ validation datasets ||  ||     or better yet: ||         * a model ||         * a criterion ||         * an optimizer ||  || TODO: ||     [ ] - output \""glance\"" curves to disk ||     [x] - move logs to a logs folder. Keep a single master log in the root ||     [ ] - Why didnt the best_snapshot.pt get saved in the most recent yolo run? ||  || Notes: ||     In the following example we demonstrate how to use netharn to train a model ||     to solve a toy problem. ||  ||     In this toy problem; we do not extend the nh.FitHarn object; so we are ||     using the default behavior of ``run_batch``. The default ``on_batch``; and ||     ``on_epoch`` do nothing; so only loss will be the only measurement of ||     performance. ||  ||     For further examples please see the examples directory. These example show ||     how to extend nh.FitHarn to measure performance wrt a particular problem. ||     The MNIST and CIFAR examples are the most simple. The YOLO example is more ||     complex.  The IBEIS example depends on non-public data \/ software; but can ||     still be useful to look at.  Its complexity is more than CIFAR but less ||     than YOLO. ||  || CommandLine: ||     xdoctest netharn.fit_harn __doc__:0 ||     xdoctest netharn.fit_harn __doc__:0 --progiter ||  || Example: ||     >>> import netharn as nh ||     >>> hyper = nh.HyperParams(**{ ||     >>>     # ================ ||     >>>     # Environment Components ||     >>>     'workdir'     : ub.ensure_app_cache_dir('netharn\/tests\/demo'); ||     >>>     'nice'        : 'demo'; ||     >>>     'xpu'         : nh.XPU.cast('auto'); ||     >>>     # workdir is a directory where intermediate results can be saved ||     >>>     # nice symlinks <workdir>\/fit\/nice\/<nice> -> ..\/runs\/<hashid> ||     >>>     # XPU auto select a gpu if idle and VRAM>6GB else a cpu ||     >>>     # ================ ||     >>>     # Data Components ||     >>>     'datasets'    : {  # dict of plain ol torch.data.Dataset instances ||     >>>         'train': nh.data.ToyData2d(size=3; border=1; n=256; rng=0); ||     >>>         'vali': nh.data.ToyData2d(size=3; border=1; n=128; rng=1); ||     >>>         'test': nh.data.ToyData2d(size=3; border=1; n=128; rng=1); ||     >>>     }; ||     >>>     'loaders'     : {'batch_size': 64}; # DataLoader instances or kw ||     >>>     # ================ ||     >>>     # Algorithm Components ||     >>>     # Note the (cls; kw) tuple formatting ||     >>>     'model'       : (nh.models.ToyNet2d; {}); ||     >>>     'optimizer'   : (nh.optimizers.SGD; { ||     >>>         'lr': 0.0001 ||     >>>     }); ||     >>>     # focal loss is usually better than nh.criterions.CrossEntropyLoss ||     >>>     'criterion'   : (nh.criterions.FocalLoss; {}); ||     >>>     'initializer' : (nh.initializers.KaimingNormal; { ||     >>>         'param': 0; ||     >>>     }); ||     >>>     # these may receive an overhaul soon ||     >>>     'scheduler'   : (nh.schedulers.ListedLR; { ||     >>>         'points': {0: .0001; 2: .01; 5: .015; 6: .005; 9: .001}; ||     >>>         'interpolate': True; ||     >>>     }); ||     >>>     'monitor'     : (nh.Monitor; { ||     >>>         'max_epoch': 10; ||     >>>     }); ||     >>>     # dynamics are a config option that modify the behavior of the main ||     >>>     # training loop. These parameters effect the learned model. ||     >>>     'dynamics'   : {'batch_step': 4}; ||     >>> }) ||     >>> harn = nh.FitHarn(hyper) ||     >>> # non-algorithmic behavior configs (do not change learned models) ||     >>> harn.config['prog_backend'] = 'tqdm' ||     >>> if ub.argflag('--progiter'):  # I prefer progiter (I may be biased) ||     ...     harn.config['prog_backend'] = 'progiter' ||     >>> # start training. ||     >>> harn.initialize(reset='delete') ||     >>> harn.run()  # note: run calls initialize it hasn't already been called. ||     >>> # xdoc: +IGNORE_WANT ||     RESET HARNESS BY DELETING EVERYTHING IN TRAINING DIR ||     Symlink: ...tests\/demo\/fit\/runs\/demo\/keyeewlr -> ...tests\/demo\/fit\/nice\/demo ||     .... already exists ||     .... and points to the right place ||     Initializing tensorboard (dont forget to start the tensorboard server) ||     Model has 824 parameters ||     Mounting ToyNet2d model on CPU ||     Initializing model weights ||      * harn.train_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr' ||      * harn.nice_dpath  = '...tests\/demo\/fit\/nice\/demo' ||     Snapshots will save to harn.snapshot_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr\/torch_snapshots' ||     dont forget to start: ||         tensorboard --logdir ...tests\/demo\/fit\/nice ||     === begin training === ||     epoch lr:0.001 \u2502 vloss: 0.1409 (n_bad_epochs=00; best=0.1409): 100%|\u2588| 10\/10 [00:01<00:00;  9.95it\/s]  0:00<?; ?it\/s] ||     train x64 \u2502 loss:0.147 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8\/8 [00:00<00:00; 130.56it\/s] ||     vali x64 \u2502 loss:0.140 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4\/4 [00:00<00:00; 342.04it\/s] ||     test x64 \u2502 loss:0.140 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4\/4 [00:00<00:00; 342.92it\/s] ||     <BLANKLINE> ||     Maximum harn.epoch reached; terminating ... ||     <BLANKLINE> ||     training completed ||     current lrs: [0.001] ||     harn.train_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr' ||     harn.nice_dpath  = '...tests\/demo\/fit\/nice\/demo' ||     view tensorboard results for this run via: ||         tensorboard --logdir ...tests\/demo\/fit\/nice ||     exiting fit harness. || \""\""\""",https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,No
2091,Erotemic/netharn,netharn/examples/cifar.py,5f79282c043a0d58be110ce7719c72f3402fbd58,"\""\""\"" || The examples\/cifar.py is probably the most clear example of what netharn is and || what it's trying to do \/ not do. ||  || The basic idea is make an object that inherits from nh.FitHarn. This is our || harness object. It will contain the hyperparameters as well as the learning || state. All the training loop boilerplate has already been written in the parent || class; so all our child class needs to do is: define `prepare_batch` (not || usually needed) and `run_batch`. Code to measure and record performance should || be placed in `on_batch` and `on_epoch`. ||  || The `train` function is our main entry point. It reads parameters from the || command line to override defaults. It then consructs the `HyperParams` object || and constructs an instance of `CIFAR_FitHarn` and calls `harn.run()`. ||  || This begins the training process. At a high level the harness will load the || data using torch DataLoaders; and call `run_batch` when it needs to compute the || model outputs and loss based on the input data. The returned loss is used to || update the model weights if `harn.tag === 'train'`; for validation; test; and || calibration (todo) datasets the loss is simply recorded. ||  || After `run_batch` finishes the `on_batch` function is called; where you can || optionally return a dict of scalars to log as measurements for this batch (note || loss is always recorded; so we need not return it here; but loss components may || be useful). A similar thing happens in `on_epoch`; where you should return || metrics about the entire dataset. ||  || The training harness manages the fit directory structure based on a hash of the || hyperparameters; the creation of algorithm component instance (e.g. model; || optimizer); initializing model weights; restarting from the most recent epoch; || updating the learning rates; various training loop boilerplate details; || checking divergence; reporting progress; handling differences between train; || validation; and test sets. In short; netharn handles the necessary parts and || let the developer focus on the important parts. || \""\""\""",https://github.com/Erotemic/netharn/commit/5f79282c043a0d58be110ce7719c72f3402fbd58,Yes
2092,Erotemic/netharn,netharn/metrics/detect_metrics.py,e16c3da37b8806b1dbd0f936a2392bb496de76d5,HACK! All that nice work we did is too slow for doctests,https://github.com/Erotemic/netharn/commit/e16c3da37b8806b1dbd0f936a2392bb496de76d5,Yes
2093,Erotemic/netharn,netharn/initializers/_nx_extensions.py,39b5429b273284f0a4aeeed0c6f50c3401c271e7,TODO: it may be the case that some of these tests are redundant; in,https://github.com/Erotemic/netharn/commit/39b5429b273284f0a4aeeed0c6f50c3401c271e7,No
2094,aldro61/kover,src/kover/core/learning/set_covering_machine/scm.py,caaef576bffbd477e194311af375e843fd9983b1,XXX: This includes the testing examples; but we don't consider them. Otherwise the idx would be invalid.,https://github.com/aldro61/kover/commit/caaef576bffbd477e194311af375e843fd9983b1,Yes
2095,jungtaekkim/bayeso,tests/test_bo.py,a892a6d4cc6a8adf67f217c081e66d3cc1a01638,TODO: add DIRECT test; now it causes an error.,https://github.com/jungtaekkim/bayeso/commit/a892a6d4cc6a8adf67f217c081e66d3cc1a01638,No
2096,jungtaekkim/bayeso,bayeso/bo.py,2316e9536fbec0709187e0be4c3195def45db847,TODO: Can we test this else statement?,https://github.com/jungtaekkim/bayeso/commit/2316e9536fbec0709187e0be4c3195def45db847,No
2097,oduwa/Pic-Numero,tqdm/tests/tests_tqdm.py,781f5997802c6007f915257fd70fb8096f9e3e3e,TODO: test degradation on windows without colorama?,https://github.com/oduwa/Pic-Numero/commit/781f5997802c6007f915257fd70fb8096f9e3e3e,Yes
2098,felixchenfy/Realtime-Action-Recognition,src/s5_test.py,4e61c338786efb12c9593bfc9f7f0bc45b40481b,''' || Test action recognition on || (1) a video; (2) a folder of images; (3) or web camera. ||  || Input: ||     classes: data_proc\/classes.csv # TODO: change this to a config file ||     model: model\/trained_classifier.pickle ||  || Output: ||     result video:    output\/${video_name}\/video.avi ||     result skeleton: output\/${video_name}\/skeleton_res\/XXXXX.txt ||     visualization by cv2.imshow() in img_displayer || ''',https://github.com/felixchenfy/Realtime-Action-Recognition/commit/4e61c338786efb12c9593bfc9f7f0bc45b40481b,Yes
2099,yao23/Machine_Learning_Playground,TaoBaoClothesMatching/util.py,a3ffd32aa232fee633b86bafdd23f48849784fd2,TODO: write some codes to test each function or write independent test class,https://github.com/yao23/Machine_Learning_Playground/commit/a3ffd32aa232fee633b86bafdd23f48849784fd2,No
2100,Barbany/Multi-speaker-Neural-Vocoder,venv/Lib/_dummy_thread.py,ee1bdc1381fb5452374bf104b50694e85eec42fe,XXX Perhaps shouldn't actually bother to test?  Could lead,https://github.com/Barbany/Multi-speaker-Neural-Vocoder/commit/ee1bdc1381fb5452374bf104b50694e85eec42fe,Yes
2101,Barbany/Multi-speaker-Neural-Vocoder,venv/Lib/site.py,ee1bdc1381fb5452374bf104b50694e85eec42fe,encoding after initialization.  The test for presence is needed when,https://github.com/Barbany/Multi-speaker-Neural-Vocoder/commit/ee1bdc1381fb5452374bf104b50694e85eec42fe,Yes
2102,PratikBarhate/question-classification,qc/dataprep/feature_stack.py,c4d372e6eac489dd4c94853105b292d83f6b0cda,TODO check results after standardizing num ft once a ml algorithm is ready to test,https://github.com/PratikBarhate/question-classification/commit/c4d372e6eac489dd4c94853105b292d83f6b0cda,Yes
2103,priyankshah7/hypers,tests/test_process.py,00c5db31fa141d065fef8a9226b39b4351acd5e2,TODO Added tests for preprocessing,https://github.com/priyankshah7/hypers/commit/00c5db31fa141d065fef8a9226b39b4351acd5e2,No
2104,richemslie/galvanise_zero,src/cpp/test_cpp.py,a565f081eb1a29ce610e6f37907fb6b51e8104b5,XXX this is not a valid test anymore???,https://github.com/richemslie/galvanise_zero/commit/a565f081eb1a29ce610e6f37907fb6b51e8104b5,Yes
2105,richemslie/galvanise_zero,src/test/test_nn_train.py,bdcd5f0ceddf07db604715f26e822af1341151aa,''' XXX this is not a test  More like a script? ''',https://github.com/richemslie/galvanise_zero/commit/bdcd5f0ceddf07db604715f26e822af1341151aa,No
2106,richemslie/galvanise_zero,src/test/cpp/test_interface.py,12eda9d52abe43022d116b935ee95a3fcc9a0ce0,''' note these tests really need a GPU.  XXX add a skip; or CPU versions of test. ''',https://github.com/richemslie/galvanise_zero/commit/12eda9d52abe43022d116b935ee95a3fcc9a0ce0,Yes
2107,richemslie/galvanise_zero,src/ggpzero/nn/datacache.py,3e59b1bdd0f6299bdf4190840c261e69330b6bd2,''' || * todo || * bcolz.set_nthreads(nthreads) || * bucket sampling ||  ||  || --- ||  || same bucket algorithm; with indexing: ||  || figure out the sizes required from each generation based on buckets (easy; any rounding issues; || drop from oldest generation) ||  || create a range(n) where n is the size of a generation.  shuffle.  remove head or tail until size. || [old version removed tail; but it doesn't matter] ||  || combine all (need to offset start index of each generation data] ||  || shuffle. ||  || --- ||  || old algo for validation set: || * no shuffling done on generation.  ie always the same last n of the generation data. || * any buckets < 1.0; threw away tail || * was shuffled before training ||  || --- ||  ||  || # XXX call samples -> observations (from self play). ||  ||  || # steps: ||  || init || ---- || * read summary file (gendata_summary.json); and validate against cache || * if no cache \/ summary file.  delete any spurious files.  Create. || * Validate existing files (md5sum). ||  ||  || sync || ---- || * check directory for any recent files. || * read new data; and preprocessed into numpy arrays (as generator) || * append numpy data to cache || * update the summary ||  ||  || create an indexer || ----------------- || * specify from buckets || * init: how to do this?  Should be a bunch of ranges; I guess. || * resample_data\/validation_data() both create a chunk indexer || * what about weightings?  Future step. ||  ||  || generator || --------- || * bcolz generator.  yields batches. ||  || * inputs: ||   * cache & chunk indexer ||  || * evaluation speed tests ||  ||  || callbacks || --------- || * before each epoch.  Idea is to keep epochs small (1 million) ||  ||  ||  ||  ||  || missing from train.py: ||  ||  ||     def verify_samples(self; sm): ||         # create a basestate ||         basestate = sm.new_base_state() ||  ||         counters = [Counter(); Counter()] ||         max_values = [{}; {}] ||         min_values = [{}; {}] ||         for s in self.samples: ||             basestate.from_list(decode_state(s.state)) ||             sm.update_bases(basestate) ||  ||             # get legals... ||             for ri in range(2): ||                 ls = sm.get_legal_state(ri) ||                 policy = s.policies[ri] ||                 for legal in ls.to_list(): ||                     found = False ||                     for ll; pp in policy: ||                         if ll == legal: ||                             max_values[ri][legal] = max(max_values[ri].get(legal; -1); pp) ||                             min_values[ri][legal] = min(max_values[ri].get(legal; 2); pp) ||                             found = True ||                             break ||                     assert found ||                     counters[ri][legal] += 1 ||  ||  ||  ||  ||  ||     def debug(self): ||         # good to see some outputs ||         for x in (10; 420; 42): ||             log.info('train input; shape: %s.  Example: %s' % (self.inputs.shape; self.inputs[x])) ||             for o in self.outputs: ||                 log.info('train output; shape: %s.  Example: %s' % (o.shape; o[x])) ||  ||  || # XXX add tests || class Buckets(object): ||     def __init__(self; bucket_def): ||         self.bucket_def = bucket_def ||  ||     def get(self; depth; max_depth): ||         if not self.bucket_def: ||             return 1.0 ||  ||         for idx; (cut_off; pct) in enumerate(self.bucket_def): ||             if cut_off <= 0: ||                 return self.get2(depth; max_depth; self.bucket_def[idx:]) ||  ||             if depth < cut_off: ||                 return pct ||  ||     def get2(self; depth; max_depth; stripped_def): ||         assert len(stripped_def) == 1 ||         return stripped_def[0][1] ||  ||  || ''',https://github.com/richemslie/galvanise_zero/commit/3e59b1bdd0f6299bdf4190840c261e69330b6bd2,No
2108,richemslie/galvanise_zero,src/ggpzero/nn/model.py,e856f688b45ff53e0f9747bc16b0acfe6a4034ad,XXX this hasn't been tested,https://github.com/richemslie/galvanise_zero/commit/e856f688b45ff53e0f9747bc16b0acfe6a4034ad,Yes
2109,TyJK/EchoBurst,Doc2Vec Model Creation and Testing/testModel.py,eeaf6ed86ab122a4f9766394f102a0a47e8a5316,strings within the comments. Better testing data will come along with scraped data in the coming weeks,https://github.com/TyJK/EchoBurst/commit/eeaf6ed86ab122a4f9766394f102a0a47e8a5316,No
2110,Sanqiang/text_simplification,trans_data_processor/en/parser/nltk_lite/parse/chunk.py,aa326601488d19ed8b6209d53c1179632b99a25e,"\""\""\"" || Classes and interfaces for identifying non-overlapping linguistic || groups (such as base noun phrases) in unrestricted text.  This task is || called X{chunk parsing} or X{chunking}; and the identified groups are || called X{chunks}.  The chunked text is represented using a shallow || tree called a \""chunk structure.\""  A X{chunk structure} is a tree || containing tokens and chunks; where each chunk is a subtree containing || only tokens.  For example; the chunk structure for base noun phrase || chunks in the sentence \""I saw the big dog on the hill\"" is:: ||  ||   (SENTENCE: ||     (NP: <I>) ||     <saw> ||     (NP: <the> <big> <dog>) ||     <on> ||     (NP: <the> <hill>)) ||  || To convert a chunk structure back to a list of tokens; simply use the || chunk structure's L{leaves<Tree.leaves>} method. ||  || The C{parser.chunk} module defines L{ChunkI}; a standard interface for || chunking texts; and L{RegexpChunk}; a regular-expression based || implementation of that interface.  It uses the L{tree.chunk} and || L{tree.conll_chunk} methods; which tokenize strings containing chunked || and tagged texts.  It defines L{ChunkScore}; a utility class for || scoring chunk parsers. ||  || RegexpChunk || =========== ||  ||   C{parse.RegexpChunk} is an implementation of the chunk parser interface ||   that uses regular-expressions over tags to chunk a text.  Its ||   C{parse} method first constructs a C{ChunkString}; which encodes a ||   particular chunking of the input text.  Initially; nothing is ||   chunked.  C{parse.RegexpChunk} then applies a sequence of ||   C{RegexpChunkRule}s to the C{ChunkString}; each of which modifies ||   the chunking that it encodes.  Finally; the C{ChunkString} is ||   transformed back into a chunk structure; which is returned. ||  ||   C{RegexpChunk} can only be used to chunk a single kind of phrase. ||   For example; you can use an C{RegexpChunk} to chunk the noun ||   phrases in a text; or the verb phrases in a text; but you can not ||   use it to simultaneously chunk both noun phrases and verb phrases in ||   the same text.  (This is a limitation of C{RegexpChunk}; not of ||   chunk parsers in general.) ||  ||   RegexpChunkRules ||   ------------------ ||     C{RegexpChunkRule}s are transformational rules that update the ||     chunking of a text by modifying its C{ChunkString}.  Each ||     C{RegexpChunkRule} defines the C{apply} method; which modifies ||     the chunking encoded by a C{ChunkString}.  The ||     L{RegexpChunkRule} class itself can be used to implement any ||     transformational rule based on regular expressions.  There are ||     also a number of subclasses; which can be used to implement ||     simpler types of rules: ||  ||       - L{ChunkRule} chunks anything that matches a given regular ||         expression. ||       - L{ChinkRule} chinks anything that matches a given regular ||         expression. ||       - L{UnChunkRule} will un-chunk any chunk that matches a given ||         regular expression. ||       - L{MergeRule} can be used to merge two contiguous chunks. ||       - L{SplitRule} can be used to split a single chunk into two ||         smaller chunks. ||       - L{ExpandLeftRule} will expand a chunk to incorporate new ||         unchunked material on the left. ||       - L{ExpandRightRule} will expand a chunk to incorporate new ||         unchunked material on the right. ||  ||     Tag Patterns ||     ~~~~~~~~~~~~ ||       C{RegexpChunkRule}s use a modified version of regular ||       expression patterns; called X{tag patterns}.  Tag patterns are ||       used to match sequences of tags.  Examples of tag patterns are:: ||  ||          r'(<DT>|<JJ>|<NN>)+' ||          r'<NN>+' ||          r'<NN.*>' ||  ||       The differences between regular expression patterns and tag ||       patterns are: ||  ||         - In tag patterns; C{'<'} and C{'>'} act as parenthases; so ||           C{'<NN>+'} matches one or more repetitions of C{'<NN>'}; not ||           C{'<NN'} followed by one or more repetitions of C{'>'}. ||         - Whitespace in tag patterns is ignored.  So ||           C{'<DT> | <NN>'} is equivalant to C{'<DT>|<NN>'} ||         - In tag patterns; C{'.'} is equivalant to C{'[^{}<>]'}; so ||           C{'<NN.*>'} matches any single tag starting with C{'NN'}. ||  ||       The function L{tag_pattern2re_pattern} can be used to transform ||       a tag pattern to an equivalent regular expression pattern. ||  ||   Efficiency ||   ---------- ||     Preliminary tests indicate that C{RegexpChunk} can chunk at a ||     rate of about 300 tokens\/second; with a moderately complex rule ||     set. ||  ||     There may be problems if C{RegexpChunk} is used with more than ||     5;000 tokens at a time.  In particular; evaluation of some regular ||     expressions may cause the Python regular expression engine to ||     exceed its maximum recursion depth.  We have attempted to minimize ||     these problems; but it is impossible to avoid them completely.  We ||     therefore recommend that you apply the chunk parser to a single ||     sentence at a time. ||  ||   Emacs Tip ||   --------- ||     If you evaluate the following elisp expression in emacs; it will ||     colorize C{ChunkString}s when you use an interactive python shell ||     with emacs or xemacs (\""C-c !\""):: ||  ||       (let () ||         (defconst comint-mode-font-lock-keywords  ||           '((\""<[^>]+>\"" 0 'font-lock-reference-face) ||             (\""[{}]\"" 0 'font-lock-function-name-face))) ||         (add-hook 'comint-mode-hook (lambda () (turn-on-font-lock)))) ||  ||     You can evaluate this code by copying it to a temporary buffer; ||     placing the cursor after the last close parenthasis; and typing ||     \""C{C-x C-e}\"".  You should evaluate it before running the interactive ||     session.  The change will last until you close emacs. ||  ||   Unresolved Issues ||   ----------------- ||     If we use the C{re} module for regular expressions; Python's ||     regular expression engine generates \""maximum recursion depth ||     exceeded\"" errors when processing very large texts; even for ||     regular expressions that should not require any recursion.  We ||     therefore use the C{pre} module instead.  But note that C{pre} ||     does not include Unicode support; so this module will not work ||     with unicode strings.  Note also that C{pre} regular expressions ||     are not quite as advanced as C{re} ones (e.g.; no leftward ||     zero-length assertions). ||  || @type _VALID_CHUNK_STRING: C{regexp} || @var _VALID_CHUNK_STRING: A regular expression to test whether a chunk ||      string is valid. || @type _VALID_TAG_PATTERN: C{regexp} || @var _VALID_TAG_PATTERN: A regular expression to test whether a tag ||      pattern is valid. || \""\""\""",https://github.com/Sanqiang/text_simplification/commit/aa326601488d19ed8b6209d53c1179632b99a25e,No
2111,jason718/game-feature-learning,src/networks.py,99835f70d7cbb2c81ccaf8632ee5ad9cb5233ee4,TODO: test,https://github.com/jason718/game-feature-learning/commit/99835f70d7cbb2c81ccaf8632ee5ad9cb5233ee4,No
2112,prajjwal1/person-reid-incremental,fastai/conv_learner.py,6e13c80861809cbc1615494b5fe5bb9167950468,TODO: Somehow check that directory names haven't changed (e.g. added test set),https://github.com/prajjwal1/person-reid-incremental/commit/6e13c80861809cbc1615494b5fe5bb9167950468,Yes
2113,alexarnimueller/som,som.py,453527db7afe79ea528acb2f68ffa414122a032d,TODO: test method!,https://github.com/alexarnimueller/som/commit/453527db7afe79ea528acb2f68ffa414122a032d,No
2114,ealcobaca/pymfe,examples/dev_examples/plot_dev.py,55369cb3d5181db2f25bf53e72645e08418284a8,"\""\""\""A developer sample class for Metafeature groups. || =================================================== ||  || This class was built to give a model of how you should write a || metafeature group class as a Pymfe developer. Please read this || entire guide with attention before programming your own class. ||  || In the end of this reading; you will know ||     * How to register your class as a valid MFE metafeature class ||     * What are the special method name prefixes and how to use them ||       properly ||     * What are the game rules involving precomputation; metafeature ||       extraction and post-computation methods ||     * What are the coding practices usually adopted in this library ||  || Also; feel free to copy this file to use as template for your own || class. ||  || First; some tips and tricks which may help you follow the code || standards stabilished in this library. ||  || 1. Use type annotations as much as possible. || -------------------------------------------- ||  || Always run `mypy` to check if the variable types was specified correctly. || You can install it with pip using the following code: ||  || >>> pip install -U mypy ||  || Use the following command before pushing your modifications to the remote || repository: ||  || >>> mypy pymfe --ignore-missing-imports ||  || The expected output for this command is no output. ||  || Note that all warnings must be fixed to your modifications be accepted in || the master branch; so take your time to fix your variable types carefully. ||  ||  || 2. Use `pylint` to check your code style and auto-formatters such as `yapf` || --------------------------------------------------------------------------- ||  || Pylint can be used to check if your code follow some coding practices || adopted by the python community. You can install with with pip using the || following command: ||  || >>> pip install -U pylint ||  || It can be harsh sometimes; so we have decided to disable some of the || verifications. You can use the following command to check if your code || met the standards stabilished in this library; ||  || >>> pylint pymfe -d 'C0103; R0913; R0902; R0914; C0302; R0904; R0801; E1101' ||  || The expected output is something like ||  || >>> Your code has been rated at 10.00\/10 (previous run: x\/10; y) ||  || Your code will not be accepted in the master branch unless it gets the || maximum pylint score. ||  || Yapf is a code auto-formatter which usually solves a large amount of || coding style related issues automatically. ||  || >>> pip install -U yapf ||  || If you use the flag ``-i``; Yapf changes your code in-place. ||  || >>> yapf -i yourModulename.py ||  ||  || 3. Make all verifications with the provided Makefile. || ----------------------------------------------------- ||  || You can use the Makefile provided in the root directory to run mypy; || pylint; and also pytest. Obviously; all tests (both for coding style || and programming logic) must pass in order to your modifications be || accepted. ||  || You can use the tag ``test-cov`` for make test and get the coverage: ||  || >>> make test-cov ||  || You can use the tag ``test`` for make only tests: ||  || >>> make test ||  || You can use the tag ``code-check`` for chack the `mypy`; `pylint` and || `pep8`: ||  || >>> make code-check ||  || .. note:: ||     You should not forget to create tests for all new functionalities that ||     you implemented. The test can be found in `.\/tests\/` fold. ||  || .. note:: ||     This class is being actualized in GitHub; check this ||     `link <https:\/\/github.com\/ealcobaca\/pymfe\/blob\/master\/pymfe\/dev.py>`_ ||     to see the current version. ||  || \""\""\""",https://github.com/ealcobaca/pymfe/commit/55369cb3d5181db2f25bf53e72645e08418284a8,Yes
2115,peterwhycs/T-BEAR,env/lib/python3.5/site.py,fcf9664766de1c67c77a0bcd0f1bcb10c2176004,encoding after initialization.  The test for presence is needed when,https://github.com/peterwhycs/T-BEAR/commit/fcf9664766de1c67c77a0bcd0f1bcb10c2176004,Yes
2116,arose13/rosey,tests.py,824575239581e936d5bd069f98fa84b93b1db62c,TODO 11\/3\/2018 check that this test runs as expected,https://github.com/arose13/rosey/commit/824575239581e936d5bd069f98fa84b93b1db62c,Yes
2117,arose13/rosey,tests/test_models.py,a9b0435b7b6e03acaae16ffd147617b7ed7a4bbf,TODO 11\/11\/2018 finish writing this test!,https://github.com/arose13/rosey/commit/a9b0435b7b6e03acaae16ffd147617b7ed7a4bbf,Yes
2118,linbo0518/BLSeg,blseg/backbone/base.py,9ea9b65abfb1d437876b31a9bb28567a7bfd5eae,TODO: waiting for test,https://github.com/linbo0518/BLSeg/commit/9ea9b65abfb1d437876b31a9bb28567a7bfd5eae,No
2119,takahi-i/pfm,setup.py,441d03faf39a783972b1cd34af1579f29fa1a9fb,TODO: put package test requirements here,https://github.com/takahi-i/pfm/commit/441d03faf39a783972b1cd34af1579f29fa1a9fb,No
2120,VSainteuf/izitorch,izitorch/trainRack.py,0876ea7d96f6d1b626e2217a1a27fef37b9f8aa2,"\""\""\"" || Main script of the package. It implements an abstract training rack class which manages common mechanisms of || training models on pytorch: training iterations; back-propagation; training scheme (e.g. k-fold or simple training; || with or without a validation set; testing); dataset splitting; and checkpoints. || The idea is to manage all these aspects under the hood; with a fair degree of flexibility; and to reduce the user's || scope to providing the dataset to be used; and defining the models that are to be trained. || The package was mainly developed for image classification problems but should easily be applied to other types of || problems. || \""\""\""",https://github.com/VSainteuf/izitorch/commit/0876ea7d96f6d1b626e2217a1a27fef37b9f8aa2,Yes
2121,921kiyo/3d-dl,keras/keras-official/retrain_unittest.py,122e1d417b31a0c54799e18056982ce917f17be9,better than random on a test set after 15 minutes of training,https://github.com/921kiyo/3d-dl/commit/122e1d417b31a0c54799e18056982ce917f17be9,No
2122,921kiyo/3d-dl,keras/keras-official/retrain_unittest.py,0450c3391b2abe7e8a74fac78499315f12765c47,TODO: write test that loss is never zero,https://github.com/921kiyo/3d-dl/commit/0450c3391b2abe7e8a74fac78499315f12765c47,Yes
2123,amir-jafari/Deep-Learning,Caffe_/1-Simple_Example/PIL/doctest.py,1bc09ffc7eb3e5452577f3f8efe5f3efeae3c136,"\""\""\""Module doctest -- a framework for running examples in docstrings. ||  || NORMAL USAGE ||  || In normal use; end each module M with: ||  || def _test(): ||     import doctest; M           # replace M with your module's name ||     return doctest.testmod(M)   # ditto ||  || if __name__ == \""__main__\"": ||     _test() ||  || Then running the module as a script will cause the examples in the || docstrings to get executed and verified: ||  || python M.py ||  || This won't display anything unless an example fails; in which case the || failing example(s) and the cause(s) of the failure(s) are printed to stdout || (why not stderr? because stderr is a lame hack <0.2 wink>); and the final || line of output is \""Test failed.\"". ||  || Run it with the -v switch instead: ||  || python M.py -v ||  || and a detailed report of all examples tried is printed to stdout; along || with assorted summaries at the end. ||  || You can force verbose mode by passing \""verbose=1\"" to testmod; or prohibit || it by passing \""verbose=0\"".  In either of those cases; sys.argv is not || examined by testmod. ||  || In any case; testmod returns a 2-tuple of ints (f; t); where f is the || number of docstring examples that failed and t is the total number of || docstring examples attempted. ||  ||  || WHICH DOCSTRINGS ARE EXAMINED? ||  || + M.__doc__. ||  || + f.__doc__ for all functions f in M.__dict__.values(); except those ||   with private names. ||  || + C.__doc__ for all classes C in M.__dict__.values(); except those with ||   private names. ||  || + If M.__test__ exists and \""is true\""; it must be a dict; and ||   each entry maps a (string) name to a function object; class object; or ||   string.  Function and class object docstrings found from M.__test__ ||   are searched even if the name is private; and strings are searched ||   directly as if they were docstrings.  In output; a key K in M.__test__ ||   appears with name ||       <name of M>.__test__.K ||  || Any classes found are recursively searched similarly; to test docstrings in || their contained methods and nested classes.  Private names reached from M's || globals are skipped; but all names reached from M.__test__ are searched. ||  || By default; a name is considered to be private if it begins with an || underscore (like \""_my_func\"") but doesn't both begin and end with (at least) || two underscores (like \""__init__\"").  You can change the default by passing || your own \""isprivate\"" function to testmod. ||  || If you want to test docstrings in objects with private names too; stuff || them into an M.__test__ dict; or see ADVANCED USAGE below (e.g.; pass your || own isprivate function to Tester's constructor; or call the rundoc method || of a Tester instance). ||  || Warning:  imports can cause trouble; e.g.; if you do ||  || from XYZ import XYZclass ||  || then XYZclass is a name in M.__dict__ too; and doctest has no way to know || that XYZclass wasn't *defined* in M.  So it may try to execute the examples || in XYZclass's docstring; and those in turn may require a different set of || globals to work correctly.  I prefer to do \""import *\""- friendly imports; || a la ||  || import XYY || _XYZclass = XYZ.XYZclass || del XYZ ||  || or (Python 2.0) ||  || from XYZ import XYZclass as _XYZclass ||  || and then the leading underscore stops testmod from going nuts.  You may || prefer the method in the next section. ||  ||  || WHAT'S THE EXECUTION CONTEXT? ||  || By default; each time testmod finds a docstring to test; it uses a *copy* || of M's globals (so that running tests on a module doesn't change the || module's real globals; and so that one test in M can't leave behind crumbs || that accidentally allow another test to work).  This means examples can || freely use any names defined at top-level in M.  It also means that sloppy || imports (see above) can cause examples in external docstrings to use || globals inappropriate for them. ||  || You can force use of your own dict as the execution context by passing || \""globs=your_dict\"" to testmod instead.  Presumably this would be a copy of || M.__dict__ merged with the globals from other imported modules. ||  ||  || WHAT IF I WANT TO TEST A WHOLE PACKAGE? ||  || Piece o' cake; provided the modules do their testing from docstrings. || Here's the test.py I use for the world's most elaborate Rational\/ || floating-base-conversion pkg (which I'll distribute some day): ||  || from Rational import Cvt || from Rational import Format || from Rational import machprec || from Rational import Rat || from Rational import Round || from Rational import utils ||  || modules = (Cvt; ||            Format; ||            machprec; ||            Rat; ||            Round; ||            utils) ||  || def _test(): ||     import doctest ||     import sys ||     verbose = \""-v\"" in sys.argv ||     for mod in modules: ||         doctest.testmod(mod; verbose=verbose; report=0) ||     doctest.master.summarize() ||  || if __name__ == \""__main__\"": ||     _test() ||  || IOW; it just runs testmod on all the pkg modules.  testmod remembers the || names and outcomes (# of failures; # of tries) for each item it's seen; and || passing \""report=0\"" prevents it from printing a summary in verbose mode. || Instead; the summary is delayed until all modules have been tested; and || then \""doctest.master.summarize()\"" forces the summary at the end. ||  || So this is very nice in practice:  each module can be tested individually || with almost no work beyond writing up docstring examples; and collections || of modules can be tested too as a unit with no more work than the above. ||  ||  || WHAT ABOUT EXCEPTIONS? ||  || No problem; as long as the only output generated by the example is the || traceback itself.  For example: ||  ||     >>> a = [None] ||     >>> a[1] ||     Traceback (innermost last): ||       File \""<stdin>\""; line 1; in ? ||     IndexError: list index out of range ||     >>> ||  || Note that only the exception type and value are compared (specifically; || only the last line in the traceback). ||  ||  || ADVANCED USAGE ||  || doctest.testmod() captures the testing policy I find most useful most || often.  You may want other policies. ||  || testmod() actually creates a local instance of class doctest.Tester; runs || appropriate methods of that class; and merges the results into global || Tester instance doctest.master. ||  || You can create your own instances of doctest.Tester; and so build your own || policies; or even run methods of doctest.master directly.  See || doctest.Tester.__doc__ for details. ||  ||  || SO WHAT DOES A DOCSTRING EXAMPLE LOOK LIKE ALREADY!? ||  || Oh ya.  It's easy!  In most cases a copy-and-paste of an interactive || console session works fine -- just make sure the leading whitespace is || rigidly consistent (you can mix tabs and spaces if you're too lazy to do it || right; but doctest is not in the business of guessing what you think a tab || means). ||  ||     >>> # comments are ignored ||     >>> x = 12 ||     >>> x ||     12 ||     >>> if x == 13: ||     ...     print \""yes\"" ||     ... else: ||     ...     print \""no\"" ||     ...     print \""NO\"" ||     ...     print \""NO!!!\"" ||     ... ||     no ||     NO ||     NO!!! ||     >>> ||  || Any expected output must immediately follow the final \"">>>\"" or \""...\"" line || containing the code; and the expected output (if any) extends to the next || \"">>>\"" or all-whitespace line.  That's it. ||  || Bummers: ||  || + Expected output cannot contain an all-whitespace line; since such a line ||   is taken to signal the end of expected output. ||  || + Output to stdout is captured; but not output to stderr (exception ||   tracebacks are captured via a different means). ||  || + If you continue a line via backslashing in an interactive session; or for ||   any other reason use a backslash; you need to double the backslash in the ||   docstring version.  This is simply because you're in a string; and so the ||   backslash must be escaped for it to survive intact.  Like: ||  || >>> if \""yes\"" == \\\\ || ...     \""y\"" +   \\\\ || ...     \""es\"":   # in the source code you'll see the doubled backslashes || ...     print 'yes' || yes ||  || The starting column doesn't matter: ||  || >>> assert \""Easy!\"" ||      >>> import math ||             >>> math.floor(1.9) ||             1.0 ||  || and as many leading whitespace characters are stripped from the expected || output as appeared in the initial \"">>>\"" line that triggered it. ||  || If you execute this very file; the examples above will be found and || executed; leading to this output in verbose mode: ||  || Running doctest.__doc__ || Trying: a = [None] || Expecting: nothing || ok || Trying: a[1] || Expecting: || Traceback (innermost last): ||   File \""<stdin>\""; line 1; in ? || IndexError: list index out of range || ok || Trying: x = 12 || Expecting: nothing || ok || Trying: x || Expecting: 12 || ok || Trying: || if x == 13: ||     print \""yes\"" || else: ||     print \""no\"" ||     print \""NO\"" ||     print \""NO!!!\"" || Expecting: || no || NO || NO!!! || ok || ... and a bunch more like that; with this summary at the end: ||  || 5 items had no tests: ||     doctest.Tester.__init__ ||     doctest.Tester.run__test__ ||     doctest.Tester.summarize ||     doctest.run_docstring_examples ||     doctest.testmod || 12 items passed all tests: ||    9 tests in doctest ||    6 tests in doctest.Tester ||   10 tests in doctest.Tester.merge ||    7 tests in doctest.Tester.rundict ||    3 tests in doctest.Tester.rundoc ||    3 tests in doctest.Tester.runstring ||    2 tests in doctest.__test__._TestClass ||    2 tests in doctest.__test__._TestClass.__init__ ||    2 tests in doctest.__test__._TestClass.get ||    1 tests in doctest.__test__._TestClass.square ||    2 tests in doctest.__test__.string ||    7 tests in doctest.is_private || 54 tests in 17 items. || 54 passed and 0 failed. || Test passed. || \""\""\""",https://github.com/amir-jafari/Deep-Learning/commit/1bc09ffc7eb3e5452577f3f8efe5f3efeae3c136,Yes
2124,jerrygaoLondon/jgtextrank,resource/example_utility.py,5487de6cba46dc962f26961f9e7eea4ee12118cf,test_set =  ['natural killer cells activates endothelial cells'; 'il 6 induced cells resemble plasma cells'; 'human myeloid cell nuclear differentiation antigen gene promoter'; 'lipopolysaccharide induced transcription factor regulating tumor necrosis factor alpha gene expression'; 'responsive cells blocked il 1 induced gene transcription'; 'cell receptor positive cells'; 'multipotent eml cells harbor substantial nuclear hormone receptor coactivator activity'; 'human gm csf receptor alpha promoter directs reporter gene activity'; 'cells contained nuclear stat protein'; 'human t cell leukemia line jurkat cells'; 'peroxisome proliferator activated receptor activators target human endothelial cells'; 'thp 1 cells c jun mrna expression increased'; 'protein kinase c depleted endothelial cells'; 'human t cell leukemia virus type 1 transformed mt 2 cells'; 'cells protein kinase'; 'human t cell leukemia virus type i infected cells'; 'class ii transactivator independent endothelial cell mhc class ii gene activation induced'; 'jurkat t cells induced dramatic cell aggregation'; 'cells induces nuclear expression'; 'human monocytic m csf receptor promoter directs reporter gene activity'; 'human myeloid leukemia cells induced'; 'pu 1 promoter directs cell type specific reporter gene expression'; 'cells inhibits human immunodeficiency virus replication'; 'human myeloid selective ccaat enhancer binding protein gene'; 'human il 2 activated nk cells'; 'cells expressing macrophage cell surface ags'; 'normal human peripheral blood mononuclear cells'; 'cell precursor acute lymphoblastic leukemia cells'; 'endothelial cells increased expression'; 'cell leukemia jurkat cells'; 'cells expressing cell surface glycophorin'; 'human immunodeficiency virus type 1 infected cells'; 'cell prolymphocytic leukemia cells mediated'; 'cells demonstrate accelerated cell cycle progression'; 'il 1 activated human umbilical vein endothelial cells'; 'normal bone marrow cells documents expression'; 'cells enhances transcription factor'; 'mature cells underwent premature cell death'; 'primary human peripheral blood mononuclear cells'; 'cells transcription factor'; 'mature normal human myeloid cells'; 'activated human nk cells'; 'human histiocytic u937 cells mrna'; 'normal immature human myeloid cells'; 'cell transcription factor gata 3 stimulates hiv 1 expression'; 'cell lineage cells arrested'; 'cells expressing high v abl kinase activity'; 'cells selectively enhances il 4 expression relative'; 'normal human hematopoietic cells'; 'transcription factor nf kappa b endothelial cell activation'; 'duffy gene promoter abolishes erythroid gene expression'; 'cell hybridoma hs 72 cells'; 'transcription factor ccaat enhancer binding protein alpha'; 'human peripheral blood nk cells'; 'primary human blood mononuclear cells'; 'cell surface protein expression'; 'human peripheral blood mononuclear cells'; 'human hl 60 myeloid leukemia cells differentiate'; 'normal human cells'; 'human endothelial cells demonstrated'; 'stimulated human endothelial cells'; 'human thp 1 monocytic leukemia cells cultured'; 'transcription factor nf kappab regulates inducible oct 2 gene expression'; 'human peripheral blood cells'; 'human nk cells activate porcine ec'; 'primary human erythroid cells'; 'chinese hamster ovary cells expressing human recombinant alphaiibbeta3'; 'human blood mononuclear cells cultured'; 'cultured human blood mononuclear cells'; 'activate human umbilical vein endothelial cells'; 'cells decreased pkr expression'; 'human cd34 hematopoietic progenitor cells isolated'; 'human kg 1 myeloid leukemia cells'; 'human monocytic cells results'; 'human u 937 leukemia cells differentiate'; 'human myeloid leukemia cells'; 'treated cytokine stimulated human saphenous vein endothelial cells'; 'irf family transcription factor gene expression'; 'human cd3 cd16 natural killer cells express'; 'human promyelocytic leukemia hl 60 cells'; 'human monoblastic leukemia u937 cells'; 'cells activates expression'; 'human blood mononuclear cells'; 'human leukemia u937 cells'; 'activate human natural killer cells'; 'purified human hematopoietic cells'; 'human myeloblastic leukemia hl 60 cells'; 'human leukemia hl60 cells respond'; 'u 937 human promonocytic leukemia cells'; 'human monocytic cells express interleukin 1beta'; 'human nk cells provide'; 'human u 937 leukemia cells'; 'primary human cd34 hemopoietic progenitor cells'; 'transfected human monocytic thp 1 cells'; 'human red blood cells'; 'human hl60 leukemia cells'; 'transfected human cells suggests'; 'hl60 human leukemia cells'; 'human jurkat lymphoblastoid cells'; 'cultured human umbilical vein endothelial cells'; 'human mononuclear cells isolated'; 'blast cells express lineage specific transcription factors'; 'human promyelocytic leukemia cells'; 'human breast cancer mcf 7 cells'; 'cultured human dermal endothelial cells'; 'human peripheral mononuclear cells'; 'human cd34 erythroid progenitor cells'; 'peripheral human mononuclear cells'; 'lipopolysaccharide stimulated human monocytic cells treated'; 'human leukemic cells studied'; 'human myeloblastic leukemia ml 1 cells'; 'ml 1 human myeloblastic leukemia cells'; 'cultured human endothelial cells'; 'resting human umbilical vein endothelial cells'; 'human promyeloid leukemia cells'; 'human myeloid leukemic cells'; 'human leukemia cells'; 'human primary haemopoietic cells'; 'e1a expression marks cells'; 'human purified cd34 cells'; '1a9 m cells expressing human bcl2'; 'human plasma cells'; 'cytokine stimulated human umbilical vein endothelial cells'; 'human umbilical arterial endothelial cells'; 'tnf treated human umbilical vein endothelial cells'; 'long term human lymphoid cells'; 'human umbilical vein endothelial cells'; 'lncap human prostate cancer cells'; 'human myeloid u 937 cells'; 'human dermal microvessel endothelial cells'; 'human leukemic k562 cells'; 'human prostatic cancer lncap cells'; 'human cd34 hematopoietic progenitor cells'; 'human cd34 hematopoietic stem cells'; 'human aortic endothelial cells'; 'bipotent human myeloid progenitor cells'; 'mature human monocytic cells'; 'human endothelial cells'; 'human hematopoietic progenitor cells'; 'human cancer lncap cells'; 'transfected human erythroleukemia cells'; 'human monocytic thp 1 cells'; 'human thp 1 monocytic cells'; 'cells infiltrating human genital herpes lesions'; 'hematopoietic human erythroleukemia cells'; 'human lymphoid cells'; 'human mammary epithelial cells'; 'human myeloid cell nuclear differentiation antigen promoter'; 'human thp 1 macrophage cells'; 'undifferentiated human monocytic cells'; 'human hematopoietic cells'; 'human natural killer cells'; 'human myeloid cells'; 'ifn gamma treated human cells infected'; 'human intestinal epithelial cells'; 'human lymphoma cells'; 'human promonocytic u937 cells'; 'human u937 promonocytic cells'; 'human nk cells'; 'human bronchial epithelial cells'; 'u937 human monoblastic cells'; 'porphyromonas gingivalis lipopolysaccharide stimulated human monocytic cells'; 'human leukaemia cells carrying'; 'human leukemic cells'; 'lipopolysaccharide stimulated human monocytic cells'; 'hiv infected human monocytic cells'; 'k562 human erythroleukemia cells'; 'thp 1 human monocytoid cells'; 'transfected human colonic carcinoma cell line ht29 activates transcription'; 'cells transcriptional activation'; 'human prostatic epithelial cells'; 'human monocytic cells'; 'human thp 1 promonocytic cells'; 'human allergen specific th2 cells'; 'human tonsillar mononuclear cells'; 'human k562 cells'; 'human b lymphocyte precursor cells'; 'u 937 human promonocytic cells'; 'human promonocytic u 937 cells'; 'human mononuclear cells'; 'human epithelial cells'; 'human th2 cells'; 'human naive cells'; 'human differentiated cells'; 'murine baf3 cells involves activation'; 'human lymphoblastoid cells'; 'human dendritic cells'; 'transcription factor ap 2 activates gene expression'; 'human glomerular mesangial cells'; 'human glial cells'; 'human erythroleukemia cells'; 'cotransfected human cells'; 'human accessory cells'; 'human jurkat t cells'; 'ad transformed human cells'; 'transcription factor activation protein'; 'human nk t cells'; 'human t lymphoblastoid cells'; 'human monoblastic cells'; 'human erythroleukemic cells'],https://github.com/jerrygaoLondon/jgtextrank/commit/5487de6cba46dc962f26961f9e7eea4ee12118cf,Yes
2125,bankroll-py/bankroll,tests/test_fidelity.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test short sale and cover trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,No
2126,bankroll-py/bankroll,tests/test_fidelity.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test exercised option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2127,bankroll-py/bankroll,tests/test_fidelity.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test assigned option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2128,bankroll-py/bankroll,tests/test_fidelity.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test expired short option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,No
2129,bankroll-py/bankroll,tests/test_fidelity.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test buy to close option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2130,bankroll-py/bankroll,tests/test_fidelity.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test sell to open option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2131,bankroll-py/bankroll,tests/test_fidelity.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test security transfer trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,No
2132,bankroll-py/bankroll,tests/test_vanguard.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test short sale and cover trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,No
2133,bankroll-py/bankroll,tests/test_vanguard.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test buy to open trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2134,bankroll-py/bankroll,tests/test_vanguard.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test sell to close trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2135,bankroll-py/bankroll,tests/test_vanguard.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test exercised option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2136,bankroll-py/bankroll,tests/test_vanguard.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test assigned option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2137,bankroll-py/bankroll,tests/test_vanguard.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test expired short option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,No
2138,bankroll-py/bankroll,tests/test_vanguard.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test buy to close option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2139,bankroll-py/bankroll,tests/test_vanguard.py,772718dac4f9d163df1731d912a628ebffb9c53e,TODO: Test sell to open option trades,https://github.com/bankroll-py/bankroll/commit/772718dac4f9d163df1731d912a628ebffb9c53e,Yes
2140,nicolay-r/sentiment-pcnn,networks/pcnn_input.py,8bbd218843932fadd9758351f96bc80ad2170ea9,TODO. For neutral labels (in case of test collection) we take,https://github.com/nicolay-r/sentiment-pcnn/commit/8bbd218843932fadd9758351f96bc80ad2170ea9,Yes
2141,nicolay-r/sentiment-pcnn,networks/core/relations.py,29691b71e0bcf9fca9971d80194986427a762e34,TODO. For neutral labels (in case of test collection) we take,https://github.com/nicolay-r/sentiment-pcnn/commit/29691b71e0bcf9fca9971d80194986427a762e34,Yes
2142,proceduralia/pytorch-neural-enhance,main.py,34bb2863520360f99a4f1ca16c7b1cab73414525,FIXME test_loss is full of nans idky,https://github.com/proceduralia/pytorch-neural-enhance/commit/34bb2863520360f99a4f1ca16c7b1cab73414525,Yes
2143,willingc/circuitpython_kernel,setup.py,379a9f8d7a904e35c7605348b80fcbf4b5310a6d,TODO: put package test requirements here,https://github.com/willingc/circuitpython_kernel/commit/379a9f8d7a904e35c7605348b80fcbf4b5310a6d,No
2144,korenyoni/opus-api,setup.py,d616f61c199167eae6ffe6dc68cb40e6b9ebb60e,TODO: put package test requirements here,https://github.com/korenyoni/opus-api/commit/d616f61c199167eae6ffe6dc68cb40e6b9ebb60e,No
2145,annomator/annomator_1.0,annotate/annotate_text_tables.py,042cc36cba515855a4cbc963df4328123d624c9f,Considering it would take a few days to go through every picture I have ever taken; how much power do you need?  You won't be able to take photos fast enough to keep up with it.  I have also tested all the other models (the resnet 50; resnet 101 and inception resnet V2) and found little difference.  In fact; the inceptionV2 often scored better in some situations.  It also runs at least times 4 faster (tf says 8x) than any other.,https://github.com/annomator/annomator_1.0/commit/042cc36cba515855a4cbc963df4328123d624c9f,No
2146,lukasc-ch/CBinfer,pycbinfer/conv2d.py,f91585c8f68874932f3f5493fe2361f995dcde4d,"\""\""\"" || TODO: ||  1) matrix multiplication... ||  2) make overall test ||  3) package ||  4) re-benchmark\/test ||  5) create automatic converter ||  6) try more test cases\/nets ||  7) implement improvements\/alternatives || \""\""\""",https://github.com/lukasc-ch/CBinfer/commit/f91585c8f68874932f3f5493fe2361f995dcde4d,No
2147,jcreinhold/synthtorch,synthnn/exec/nn_predict.py,27fbcdb5a4df0e9c89a308658496cad11ece7cba,TODO: Break this piece off and make it unittest-able!,https://github.com/jcreinhold/synthtorch/commit/27fbcdb5a4df0e9c89a308658496cad11ece7cba,Yes
2148,jcreinhold/synthtorch,tests/test_exec.py,32220c6cb05a59527912a762568f519c93b7a4b9,TODO: cannot test 2d prediction here because nii needs to be same size as tiff; fix,https://github.com/jcreinhold/synthtorch/commit/32220c6cb05a59527912a762568f519c93b7a4b9,No
2149,jcreinhold/synthtorch,tests/test_exec.py,b4b79f27d3910cfcb0c4249244e994afb87150b0,TODO: cannot test 2d prediction here because nii needs to be same size as tiff; fix,https://github.com/jcreinhold/synthtorch/commit/b4b79f27d3910cfcb0c4249244e994afb87150b0,No
2150,jcreinhold/synthtorch,tests/test_exec.py,10340bf6aecbfec8e9bab24d9e98a1e3fe7c8610,TODO: cannot test 2d prediction here because nii needs to be same size as tiff; fix,https://github.com/jcreinhold/synthtorch/commit/10340bf6aecbfec8e9bab24d9e98a1e3fe7c8610,No
2151,KMouratidis/EDA_miner,apps/exploration/Exploration.py,55ca5bddb5152ba934efac66a989a73b07f99797,# To test the right variables; we need to see if yvars is needed,https://github.com/KMouratidis/EDA_miner/commit/55ca5bddb5152ba934efac66a989a73b07f99797,No
2152,KMouratidis/EDA_miner,tests/test_utils.py,4a777096f4bfe1af75191ed676dd4691d43a3483,TODO: correctly cleanup after tests,https://github.com/KMouratidis/EDA_miner/commit/4a777096f4bfe1af75191ed676dd4691d43a3483,Yes
2153,KMouratidis/EDA_miner,EDA_miner/modeling/__init__.py,d0505ee0ec2d00f12832966f9b313f83aa4a9a58,"\""\""\"" || Developer notes: ||     Some suggestions on what could be done: ||         - Better reporting on fitting results. ||         - Download a model. ||         - Predict a test set. ||         - Prettier interfaces. ||         - Implement Econometrics functionality. || \""\""\""",https://github.com/KMouratidis/EDA_miner/commit/d0505ee0ec2d00f12832966f9b313f83aa4a9a58,Yes
2154,KMouratidis/EDA_miner,tests/test_utils.py,d0505ee0ec2d00f12832966f9b313f83aa4a9a58,TODO: correctly cleanup after tests,https://github.com/KMouratidis/EDA_miner/commit/d0505ee0ec2d00f12832966f9b313f83aa4a9a58,Yes
2155,mesnico/learning-relationship-aware-visual-features,networkx/examples/advanced/iterated_dynamical_systems.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,"\""\""\"" || ========================== || Iterated Dynamical Systems || ========================== ||  || Digraphs from Integer-valued Iterated Functions ||  || Sums of cubes on 3N || ------------------- ||  || The number 153 has a curious property. ||  || Let 3N={3;6;9;12;...} be the set of positive multiples of 3.  Define an || iterative process f:3N->3N as follows: for a given n; take each digit || of n (in base 10); cube it and then sum the cubes to obtain f(n). ||  || When this process is repeated; the resulting series n; f(n); f(f(n));... || terminate in 153 after a finite number of iterations (the process ends || because 153 = 1**3 + 5**3 + 3**3). ||  || In the language of discrete dynamical systems; 153 is the global || attractor for the iterated map f restricted to the set 3N. ||  || For example: take the number 108 ||  || f(108) = 1**3 + 0**3 + 8**3 = 513 ||  || and ||  || f(513) = 5**3 + 1**3 + 3**3 = 153 ||  || So; starting at 108 we reach 153 in two iterations; || represented as: ||  || 108->513->153 ||  || Computing all orbits of 3N up to 10**5 reveals that the attractor || 153 is reached in a maximum of 14 iterations. In this code we || show that 13 cycles is the maximum required for all integers (in 3N) || less than 10;000. ||  || The smallest number that requires 13 iterations to reach 153; is 177; i.e.; ||  || 177->687->1071->345->216->225->141->66->432->99->1458->702->351->153 ||  || The resulting large digraphs are useful for testing network software. ||  || The general problem || ------------------- ||  || Given numbers n; a power p and base b; define F(n; p; b) as the sum of || the digits of n (in base b) raised to the power p. The above example || corresponds to f(n)=F(n; 3;10); and below F(n; p; b) is implemented as || the function powersum(n;p;b). The iterative dynamical system defined by || the mapping n:->f(n) above (over 3N) converges to a single fixed point; || 153. Applying the map to all positive integers N; leads to a discrete || dynamical process with 5 fixed points: 1; 153; 370; 371; 407. Modulo 3 || those numbers are 1; 0; 1; 2; 2. The function f above has the added || property that it maps a multiple of 3 to another multiple of 3; i.e. it || is invariant on the subset 3N. ||  ||  || The squaring of digits (in base 10) result in cycles and the || single fixed point 1. I.e.; from a certain point on; the process || starts repeating itself. ||  || keywords: \""Recurring Digital Invariant\""; \""Narcissistic Number\""; || \""Happy Number\"" ||  || The 3n+1 problem || ---------------- ||  || There is a rich history of mathematical recreations || associated with discrete dynamical systems.  The most famous || is the Collatz 3n+1 problem. See the function || collatz_problem_digraph below. The Collatz conjecture || --- that every orbit returrns to the fixed point 1 in finite time || --- is still unproven. Even the great Paul Erdos said \""Mathematics || is not yet ready for such problems\""; and offered $500 || for its solution. ||  || keywords: \""3n+1\""; \""3x+1\""; \""Collatz problem\""; \""Thwaite's conjecture\"" || \""\""\""",https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,No
2156,mesnico/learning-relationship-aware-visual-features,networkx/networkx/algorithms/efficiency.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,TODO This can be made more efficient by computing all pairs shortest,https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,No
2157,mesnico/learning-relationship-aware-visual-features,networkx/networkx/algorithms/tests/test_cycles.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,TODO What does this test do?  das 6\/2013,https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,No
2158,mesnico/learning-relationship-aware-visual-features,networkx/networkx/algorithms/tree/recognition.py,f7d9014a2009b22ab6a75ca44ac7822c0099ce09,"\""\""\"" || Recognition Tests || ================= ||  || A *forest* is an acyclic; undirected graph; and a *tree* is a connected forest. || Depending on the subfield; there are various conventions for generalizing these || definitions to directed graphs. ||  || In one convention; directed variants of forest and tree are defined in an || identical manner; except that the direction of the edges is ignored. In effect; || each directed edge is treated as a single undirected edge. Then; additional || restrictions are imposed to define *branchings* and *arborescences*. ||  || In another convention; directed variants of forest and tree correspond to || the previous convention's branchings and arborescences; respectively. Then two || new terms; *polyforest* and *polytree*; are defined to correspond to the other || convention's forest and tree. ||  || Summarizing:: ||  ||    +-----------------------------+ ||    | Convention A | Convention B | ||    +=============================+ ||    | forest       | polyforest   | ||    | tree         | polytree     | ||    | branching    | forest       | ||    | arborescence | tree         | ||    +-----------------------------+ ||  || Each convention has its reasons. The first convention emphasizes definitional || similarity in that directed forests and trees are only concerned with || acyclicity and do not have an in-degree constraint; just as their undirected || counterparts do not. The second convention emphasizes functional similarity || in the sense that the directed analog of a spanning tree is a spanning || arborescence. That is; take any spanning tree and choose one node as the root. || Then every edge is assigned a direction such there is a directed path from the || root to every other node. The result is a spanning arborescence. ||  || NetworkX follows convention \""A\"". Explicitly; these are: ||  || undirected forest ||    An undirected graph with no undirected cycles. ||  || undirected tree ||    A connected; undirected forest. ||  || directed forest ||    A directed graph with no undirected cycles. Equivalently; the underlying ||    graph structure (which ignores edge orientations) is an undirected forest. ||    In convention B; this is known as a polyforest. ||  || directed tree ||    A weakly connected; directed forest. Equivalently; the underlying graph ||    structure (which ignores edge orientations) is an undirected tree. In ||    convention B; this is known as a polytree. ||  || branching ||    A directed forest with each node having; at most; one parent. So the maximum ||    in-degree is equal to 1. In convention B; this is known as a forest. ||  || arborescence ||    A directed tree with each node having; at most; one parent. So the maximum ||    in-degree is equal to 1. In convention B; this is known as a tree. ||  || For trees and arborescences; the adjective \""spanning\"" may be added to designate || that the graph; when considered as a forest\/branching; consists of a single || tree\/arborescence that includes all nodes in the graph. It is true; by || definition; that every tree\/arborescence is spanning with respect to the nodes || that define the tree\/arborescence and so; it might seem redundant to introduce || the notion of \""spanning\"". However; the nodes may represent a subset of || nodes from a larger graph; and it is in this context that the term \""spanning\"" || becomes a useful notion. ||  || \""\""\""",https://github.com/mesnico/learning-relationship-aware-visual-features/commit/f7d9014a2009b22ab6a75ca44ac7822c0099ce09,Yes
2159,mesnico/learning-relationship-aware-visual-features,build_stats.py,80202476ad14e11b5822f0789a40bbb4583fb50d,TODO: check arguments to all functions in test,https://github.com/mesnico/learning-relationship-aware-visual-features/commit/80202476ad14e11b5822f0789a40bbb4583fb50d,Yes
2160,permfl/dictlearn,dictlearn/filters.py,fa800d2a083afc2413b491ffe308c16f8ea8e0a7,todo test,https://github.com/permfl/dictlearn/commit/fa800d2a083afc2413b491ffe308c16f8ea8e0a7,No
2161,Stable-Baselines-Team/stable-baselines,tests/test_identity.py,c1bb5c30b5a5494861b17a3bbbeb19dcd0bbf8b3,FIXME: this test fail for now,https://github.com/Stable-Baselines-Team/stable-baselines/commit/c1bb5c30b5a5494861b17a3bbbeb19dcd0bbf8b3,Yes
2162,Stable-Baselines-Team/stable-baselines,tests/test_identity.py,5ca954734375bd2c96d3d0f32b40ab4bbf6e29b4,FIXME: this test fail for now,https://github.com/Stable-Baselines-Team/stable-baselines/commit/5ca954734375bd2c96d3d0f32b40ab4bbf6e29b4,Yes
2163,Stable-Baselines-Team/stable-baselines,stable_baselines/common/identity_env.py,f7ff5c04a088360858fd88926fa949c15c9b13e6,TODO: test with epsilon instead of just pos\/neg actions,https://github.com/Stable-Baselines-Team/stable-baselines/commit/f7ff5c04a088360858fd88926fa949c15c9b13e6,Yes
2164,Stable-Baselines-Team/stable-baselines,stable_baselines/sac/sac.py,c4d41d31fed18023a21d011b36b161bcac3bd386,TODO: test with huber loss (it would avoid too high values),https://github.com/Stable-Baselines-Team/stable-baselines/commit/c4d41d31fed18023a21d011b36b161bcac3bd386,No
2165,comtravo/ctparse,ctparse/ctparse.py,ca4f11da5ee5b14da53caa03c51aaaa1e14f8199,TODO: this way of testing a failure to find a match is a bit clunky with types,https://github.com/comtravo/ctparse/commit/ca4f11da5ee5b14da53caa03c51aaaa1e14f8199,Yes
2166,PPPLDeepLearning/plasma-python,examples/dynamic_lstm.py,30e04c3d375e91567e26284ca18f740056e1b9ba,FIXME to be tested,https://github.com/PPPLDeepLearning/plasma-python/commit/30e04c3d375e91567e26284ca18f740056e1b9ba,No
2167,PPPLDeepLearning/plasma-python,data/get_mdsplus_data.py,c1c7fa2f6b21af32ed22c438a2604acd85d6167d,'''TODO || - mapping to flux surfaces: its not always [0;1]! || - handling of 1D signals during preprocessing & normalization || - handling of 1D signals for feeding into RNN (convolutional layers) || - handling of missing data in shots? || - TEST || ''',https://github.com/PPPLDeepLearning/plasma-python/commit/c1c7fa2f6b21af32ed22c438a2604acd85d6167d,No
2168,PPPLDeepLearning/plasma-python,plasma/utils/downloading.py,3eb3a3e68b2d9b6c22edafd136fdb67ed9cb8031,'''TODO || - mapping to flux surfaces: its not always [0;1]! || - handling of 1D signals during preprocessing & normalization || - handling of 1D signals for feeding into RNN (convolutional layers) || - handling of missing data in shots? || - TEST || ''',https://github.com/PPPLDeepLearning/plasma-python/commit/3eb3a3e68b2d9b6c22edafd136fdb67ed9cb8031,No
2169,nanoporetech/taiyaki,test/unit/test_iterate_fast5_reads.py,6833944dd61c6aadc29a56b264c3a609de0bf9ac,TODO add recursive test (requires adding recursive data dir,https://github.com/nanoporetech/taiyaki/commit/6833944dd61c6aadc29a56b264c3a609de0bf9ac,Yes
2170,sbl-sdsc/mmtf-pyspark,filters.py,751ed720ba9780200ee3479774584991e8511bf6,"''' || filters.py: || This file contains all the filter functions that can be used for Spark's filter operation ||  || Authorship information: ||     __author__ = \""Peter Rose\"" ||     __maintainer__ = \""Mars Huang\"" ||     __email__ = \""marshuang80@gmai.com: ||     __status__ = \""debug\"" || TODO: ||     Debug and Test || '''",https://github.com/sbl-sdsc/mmtf-pyspark/commit/751ed720ba9780200ee3479774584991e8511bf6,No
2171,sbl-sdsc/mmtf-pyspark,test.py,751ed720ba9780200ee3479774584991e8511bf6,"''' || test.py: Testing mmtf_spark ||  || Authorship information: ||     __author__ = \""Peter Rose\"" ||     __maintainer__ = \""Mars Huang\"" ||     __email__ = \""marshuang80@gmai.com: ||     __status__ = \""debug\"" || TODO:  ||     Change to main funciton instead of test.py || '''",https://github.com/sbl-sdsc/mmtf-pyspark/commit/751ed720ba9780200ee3479774584991e8511bf6,Yes
2172,sbl-sdsc/mmtf-pyspark,filters.py,62ddb863ee412e4353dca6560f55d4d816a0fe73,TODO NEED Debug\/Testing,https://github.com/sbl-sdsc/mmtf-pyspark/commit/62ddb863ee412e4353dca6560f55d4d816a0fe73,No
2173,sbl-sdsc/mmtf-pyspark,src/main/filters.py,e39246a5a640249b4e5611a25c91c929cd3a9cc8,"''' || filters.py: || This file contains all the filter functions that can be used for Spark's filter operation ||  || Authorship information: ||     __author__ = \""Peter Rose\"" ||     __maintainer__ = \""Mars Huang\"" ||     __email__ = \""marshuang80@gmai.com: ||     __status__ = \""debug\"" || TODO: ||     Debug and Test ||     Doc String || '''",https://github.com/sbl-sdsc/mmtf-pyspark/commit/e39246a5a640249b4e5611a25c91c929cd3a9cc8,Yes
2174,sbl-sdsc/mmtf-pyspark,src/main/filters.py,e39246a5a640249b4e5611a25c91c929cd3a9cc8,TODO NEED Debug\/Testing; Not Sure if it is Working,https://github.com/sbl-sdsc/mmtf-pyspark/commit/e39246a5a640249b4e5611a25c91c929cd3a9cc8,Yes
2175,sbl-sdsc/mmtf-pyspark,src/main/test.py,e39246a5a640249b4e5611a25c91c929cd3a9cc8,"''' || test.py: Testing mmtf_spark ||  || Authorship information: ||     __author__ = \""Peter Rose\"" ||     __maintainer__ = \""Mars Huang\"" ||     __email__ = \""marshuang80@gmai.com: ||     __status__ = \""debug\"" || TODO: ||     Change to main funciton instead of test.py || '''",https://github.com/sbl-sdsc/mmtf-pyspark/commit/e39246a5a640249b4e5611a25c91c929cd3a9cc8,Yes
2176,sbl-sdsc/mmtf-pyspark,src/tests/Demo1a.py,e39246a5a640249b4e5611a25c91c929cd3a9cc8,TODO No actual value for unit test,https://github.com/sbl-sdsc/mmtf-pyspark/commit/e39246a5a640249b4e5611a25c91c929cd3a9cc8,No
2177,sbl-sdsc/mmtf-pyspark,src/tests/rFreeFilterTest.py,1bf99657605a7f521a30ce7c7ddf401e9b5ffe38,TODO No actual value for unit test,https://github.com/sbl-sdsc/mmtf-pyspark/commit/1bf99657605a7f521a30ce7c7ddf401e9b5ffe38,No
2178,sbl-sdsc/mmtf-pyspark,src/tests/depositionDateFilterTest.py,53e3695d595ac06479389537975b374df17a6388,TODO No actual value for unit test,https://github.com/sbl-sdsc/mmtf-pyspark/commit/53e3695d595ac06479389537975b374df17a6388,No
2179,sbl-sdsc/mmtf-pyspark,src/tests/orFilterTest.py,53e3695d595ac06479389537975b374df17a6388,TODO No actual value for unit test,https://github.com/sbl-sdsc/mmtf-pyspark/commit/53e3695d595ac06479389537975b374df17a6388,No
2180,sbl-sdsc/mmtf-pyspark,src/tests/orFilterTest.py,53e3695d595ac06479389537975b374df17a6388,TODO Need mappers for this unit test to work,https://github.com/sbl-sdsc/mmtf-pyspark/commit/53e3695d595ac06479389537975b374df17a6388,Yes
2181,sbl-sdsc/mmtf-pyspark,src/tests/polymerCompositionTest.py,53e3695d595ac06479389537975b374df17a6388,TODO test3 needs mapper,https://github.com/sbl-sdsc/mmtf-pyspark/commit/53e3695d595ac06479389537975b374df17a6388,No
2182,sbl-sdsc/mmtf-pyspark,src/tests/polymerCompositionTest.py,53e3695d595ac06479389537975b374df17a6388,TODO test4 needs mapper,https://github.com/sbl-sdsc/mmtf-pyspark/commit/53e3695d595ac06479389537975b374df17a6388,Yes
2183,sbl-sdsc/mmtf-pyspark,src/tests/releaseDateFilterTest.py,53e3695d595ac06479389537975b374df17a6388,TODO No actual value for unit test,https://github.com/sbl-sdsc/mmtf-pyspark/commit/53e3695d595ac06479389537975b374df17a6388,No
2184,sbl-sdsc/mmtf-pyspark,src/tests/secondaryStructureTest.py,53e3695d595ac06479389537975b374df17a6388,TODO No actual value for unit test,https://github.com/sbl-sdsc/mmtf-pyspark/commit/53e3695d595ac06479389537975b374df17a6388,No
2185,sbl-sdsc/mmtf-pyspark,mmtfPyspark/src/main/datasets/UniProt.py,09c5b7ce1489e639a95fc16df7df243b1c5eda90,TODO Uniprot Links are down; can't test,https://github.com/sbl-sdsc/mmtf-pyspark/commit/09c5b7ce1489e639a95fc16df7df243b1c5eda90,Yes
2186,sbl-sdsc/mmtf-pyspark,mmtfPyspark/datasets/UniProt.py,c9dbf60a303441e4244d5b92bec5ebb65986fdab,TODO: Testing ftplib,https://github.com/sbl-sdsc/mmtf-pyspark/commit/c9dbf60a303441e4244d5b92bec5ebb65986fdab,Yes
2187,sbl-sdsc/mmtf-pyspark,mmtfPyspark/dev/atomInteraction.py,160b83e74423f445657ff8834584512b0bf8b883,TODO: testing list,https://github.com/sbl-sdsc/mmtf-pyspark/commit/160b83e74423f445657ff8834584512b0bf8b883,No
2188,sbl-sdsc/mmtf-pyspark,mmtfPyspark/dev/atomInteraction.py,160b83e74423f445657ff8834584512b0bf8b883,TODO test list,https://github.com/sbl-sdsc/mmtf-pyspark/commit/160b83e74423f445657ff8834584512b0bf8b883,Yes
2189,SINGROUP/dscribe,regtests/soap.py,60f9d7126ae3434fc03eada4fb2eb0916d55cb44,Fix random seed for tests,https://github.com/SINGROUP/dscribe/commit/60f9d7126ae3434fc03eada4fb2eb0916d55cb44,Yes
2190,SINGROUP/dscribe,regtests/soap.py,e671119a18e920d7791a9efeead2f76db3715e95,Fix random seed for tests,https://github.com/SINGROUP/dscribe/commit/e671119a18e920d7791a9efeead2f76db3715e95,Yes
2191,SINGROUP/dscribe,regtests/soap.py,8ab3d5e355087967fbb9e256c94a9f923382314d,# Fix random seed for tests,https://github.com/SINGROUP/dscribe/commit/8ab3d5e355087967fbb9e256c94a9f923382314d,Yes
2192,SINGROUP/dscribe,regtests/soap.py,0f84ed05eef3e66eead240dedab17d153bbb3c4a,Fix random seed for tests,https://github.com/SINGROUP/dscribe/commit/0f84ed05eef3e66eead240dedab17d153bbb3c4a,Yes
2193,SINGROUP/dscribe,regtests/soap.py,75bbea353c6e6e51166f4a51350ca6a35bd31040,# Fix random seed for tests,https://github.com/SINGROUP/dscribe/commit/75bbea353c6e6e51166f4a51350ca6a35bd31040,Yes
2194,SINGROUP/dscribe,regtests/soap.py,b24ab8cc9448c26c59a73cbce016405e1f2c8d3c,Fix random seed for tests,https://github.com/SINGROUP/dscribe/commit/b24ab8cc9448c26c59a73cbce016405e1f2c8d3c,Yes
2195,SINGROUP/dscribe,regtests/soap.py,71b17d50d5e1ec7f43d2eace0c41a936ac3d3fbe,# Fix random seed for tests,https://github.com/SINGROUP/dscribe/commit/71b17d50d5e1ec7f43d2eace0c41a936ac3d3fbe,Yes
2196,SINGROUP/dscribe,regtests/soap.py,38ff7dde713f5b369082dea0a5172a14de437ee4,Fix random seed for tests,https://github.com/SINGROUP/dscribe/commit/38ff7dde713f5b369082dea0a5172a14de437ee4,Yes
2197,SINGROUP/dscribe,regtests/soap.py,694538e9a2e10a8b8a0593c8d487fc740290792c,Fix random seed for tests,https://github.com/SINGROUP/dscribe/commit/694538e9a2e10a8b8a0593c8d487fc740290792c,Yes
2198,FreeDiscovery/FreeDiscovery,freediscovery/externals/pathlib2.py,8371b0342777e166ca170088e363fdda55588bba,XXX untested yet!,https://github.com/FreeDiscovery/FreeDiscovery/commit/8371b0342777e166ca170088e363fdda55588bba,Yes
2199,uw-biorobotics/IKBT,ikbtleaves/sincos_solver.py,289e874533bd87f3335790626e8e016d9724be82,TODO: make tests work for the new version (use assigner),https://github.com/uw-biorobotics/IKBT/commit/289e874533bd87f3335790626e8e016d9724be82,No
2200,uw-biorobotics/IKBT,ikbtleaves/tan_solver.py,289e874533bd87f3335790626e8e016d9724be82,needed for test results,https://github.com/uw-biorobotics/IKBT/commit/289e874533bd87f3335790626e8e016d9724be82,No
2201,uw-biorobotics/IKBT,ikbtleaves/x2y2_solver.py,289e874533bd87f3335790626e8e016d9724be82,uth23.set_solved(R;variables)  # needed for this test,https://github.com/uw-biorobotics/IKBT/commit/289e874533bd87f3335790626e8e016d9724be82,Yes
2202,uw-biorobotics/IKBT,tests/HTMLTestRunner.py,289e874533bd87f3335790626e8e016d9724be82,"\""\""\"" || Change History ||  || Version 0.8.2 || * Show output inline instead of popup window (Viorel Lupu). ||  || Version in 0.8.1 || * Validated XHTML (Wolfgang Borgert). || * Added description of test classes and test cases. ||  || Version in 0.8.0 || * Define Template_mixin class for customization. || * Workaround a IE 6 bug that it does not treat <script> block as CDATA. ||  || Version in 0.7.1 || * Back port to Python 2.3 (Frank Horowitz). || * Fix missing scroll bars in detail log (Podi). || \""\""\""",https://github.com/uw-biorobotics/IKBT/commit/289e874533bd87f3335790626e8e016d9724be82,No
2203,pyro-ppl/funsor,docs/source/conf.py,fd0dfe4b2f75c64076e9eb62c02cdceed441fae3,@jpchen's hack to get rtd builder to install latest pytorch,https://github.com/pyro-ppl/funsor/commit/fd0dfe4b2f75c64076e9eb62c02cdceed441fae3,No
2204,pyro-ppl/funsor,funsor/interpreter.py,7e456e60fbcabae1db262787119489f603e7a1d5,TODO remove this; used temporarily for testing,https://github.com/pyro-ppl/funsor/commit/7e456e60fbcabae1db262787119489f603e7a1d5,Yes
2205,pyro-ppl/funsor,test/test_sum_product.py,b2e116910c9e193de86d1808e870ad382eb0fc0c,TODO test with Gaussians.,https://github.com/pyro-ppl/funsor/commit/b2e116910c9e193de86d1808e870ad382eb0fc0c,Yes
2206,pyro-ppl/funsor,funsor/affine.py,6cc0a38cdfd6430234aab80598df7c2752185c70,FIXME change this to a sound but incomplete test using pattern matching.,https://github.com/pyro-ppl/funsor/commit/6cc0a38cdfd6430234aab80598df7c2752185c70,Yes
2207,pyro-ppl/funsor,funsor/tensor.py,a29a5d6a1d52ce9cfb96df33cab690c96886acba,XXX: memoize tests fail for np.generic because those scalar values are hashable?,https://github.com/pyro-ppl/funsor/commit/a29a5d6a1d52ce9cfb96df33cab690c96886acba,Yes
2208,pyro-ppl/funsor,test/test_transpose.py,3fff907773fd73e301b5d7156a8668b4c92ee14e,"\""\""\"" || TODO tests the following ideas: ||  ||   i:Bint[3] |- x:Real || ----------------------- || |- x.reduce(op; i):Real ||  ||            i:Bint[3] |- x:Real || ---------------------------------------------------- || i2:Bint[3] |- transpose(x.reduce(op; i; i2))[x]:Real ||  ||    G1 |- x1:t1   ...   Gn |- xn:tn ||       G0 |- f(x1; ...; xn):Real ||         f structurally linear || -------------------------------------- || Gi |- transpose(F(x1; ...; xn))[xi]:ti ||  || G1 |- x:t1 || G2 |- y = f(x) : t2 || f structurally linear || -------------------------------------- || G2 |- transpose(transpose(y)) = y : t2 || \""\""\""",https://github.com/pyro-ppl/funsor/commit/3fff907773fd73e301b5d7156a8668b4c92ee14e,Yes
2209,aws-samples/deep-learning-models,utils/tensorflow/tensorflow_image_resizer.py,8f37d810df7fa035e98084b6fe948c0ca6a99ae7,HACK TESTING upscaling small images to 320,https://github.com/aws-samples/deep-learning-models/commit/8f37d810df7fa035e98084b6fe948c0ca6a99ae7,No
2210,aws-samples/deep-learning-models,models/nlp/albert/run_pretraining.py,f768c761dc7d248e52f408f765f87ddd81a39ded,SageMaker may have some extra strings. TODO: Test this on SM.,https://github.com/aws-samples/deep-learning-models/commit/f768c761dc7d248e52f408f765f87ddd81a39ded,Yes
2211,aws-samples/deep-learning-models,models/nlp/electra/run_pretraining.py,bda976272e0db701032048f99a378bb897603dc8,SageMaker may have some extra strings. TODO: Test this on SM.,https://github.com/aws-samples/deep-learning-models/commit/bda976272e0db701032048f99a378bb897603dc8,Yes
2212,estnltk/estnltk,estnltk/wikiXmlParser/externalLink.py,a4defe62574448223cdbde5babc3e396ee51a209,TODO: test,https://github.com/estnltk/estnltk/commit/a4defe62574448223cdbde5babc3e396ee51a209,No
2213,estnltk/estnltk,estnltk/tests/test_disambiguator.py,d6447888aafa0c2265948279ff9fd3d51e6b8e61,todo: test ei t\u00F6\u00F6ta; sest ma pole kindel; kuidas tulemust tuleks kontrollida,https://github.com/estnltk/estnltk/commit/d6447888aafa0c2265948279ff9fd3d51e6b8e61,No
2214,estnltk/estnltk,estnltk/prettyprinter/prettyprinter.py,80e727676f849992d0fab766a8b64786b891372c,TODO: m\u00E4rkus. testimiseks ei pea faili kirjutamist tegema otse koodis; vaid v\u00F5ib ka lihtsalt tr\u00FCkkide ekraanile,https://github.com/estnltk/estnltk/commit/80e727676f849992d0fab766a8b64786b891372c,No
2215,estnltk/estnltk,estnltk/prettyprinter/prettyprinter.py,cf5f38373ec0a08518dae7051e53e44c316c166c,TODO: m\u00E4rkus. testimiseks ei pea faili kirjutamist tegema otse koodis; vaid v\u00F5ib ka lihtsalt tr\u00FCkkide ekraanile s\u00F5ne; ning k\u00E4surealt see suunata m\u00F5nda faili.,https://github.com/estnltk/estnltk/commit/cf5f38373ec0a08518dae7051e53e44c316c166c,Yes
2216,estnltk/estnltk,estnltk/database/tests/test_insert.py,3d7e92a6260a5e4a82291ffc023b5ff323ef298c,TODO: add a test that uses layer argument,https://github.com/estnltk/estnltk/commit/3d7e92a6260a5e4a82291ffc023b5ff323ef298c,No
2217,estnltk/estnltk,estnltk/syntax/tests/test_tagger.py,c8161d85557ab2ead2e6d55a4b3c3b53f9d5b70d,"\""\""\"" || TODO: syntax tagger module needs tons of testing; because there are a lot of things that can break. || \""\""\""",https://github.com/estnltk/estnltk/commit/c8161d85557ab2ead2e6d55a4b3c3b53f9d5b70d,Yes
2218,NLPatVCU/medaCy,medacy/tests/learn/test_feature_extractor.py,c69888184b3d0ce3c639414d186b71e1bd7001fd,TODO write tests for feature extractor once class is written,https://github.com/NLPatVCU/medaCy/commit/c69888184b3d0ce3c639414d186b71e1bd7001fd,Yes
2219,NLPatVCU/medaCy,medacy/learn/learn.py,9f353dc535951af4a94fd99ec62501532f4306fb,TODO untested,https://github.com/NLPatVCU/medaCy/commit/9f353dc535951af4a94fd99ec62501532f4306fb,Yes
2220,NLPatVCU/medaCy,medacy/models/crf_model.py,41f0c32bb257f4f16025f11050def5b5ee53b673,TODO untested after transfer from experimental codebase; should work though.,https://github.com/NLPatVCU/medaCy/commit/41f0c32bb257f4f16025f11050def5b5ee53b673,Yes
2221,NLPatVCU/medaCy,medacy/tools/annotations.py,7b28a81acdf102983202a1fca3051bb9a9f3d514,TODO TEST THIS,https://github.com/NLPatVCU/medaCy/commit/7b28a81acdf102983202a1fca3051bb9a9f3d514,No
2222,NLPatVCU/medaCy,medacy/tests/pipeline_components/metamap/test_metamap.py,bc4791a27642367af3651bd3330c72369aa5726c,TODO Cannot test metamap due to issue with processes communication during unit tests - we could place pre-metamapped,https://github.com/NLPatVCU/medaCy/commit/bc4791a27642367af3651bd3330c72369aa5726c,Yes
2223,NLPatVCU/medaCy,medacy/tests/tools/test_data_manager.py,bc4791a27642367af3651bd3330c72369aa5726c,Metamapping does not work in unit tests due to issues with streaming between processes - maybe address later,https://github.com/NLPatVCU/medaCy/commit/bc4791a27642367af3651bd3330c72369aa5726c,Yes
2224,NLPatVCU/medaCy,medacy/tests/tools/test_annotation.py,428e313601444930a7270c56992d349c7efa0702,"\""\""\"" || Unit tests for annotations.py; contains hard-coded test data for ann files. || :author: Andriy Mulyar; Steele W. Farnsworth || :date: 12 January; 2019 || \""\""\""",https://github.com/NLPatVCU/medaCy/commit/428e313601444930a7270c56992d349c7efa0702,Yes
2225,NLPatVCU/medaCy,medacy/ner/learners/bilstm_crf.py,24083b66080ce69f522952bfb5db38dbba15da61,TODO Only here for testing until we switch to word embeddings,https://github.com/NLPatVCU/medaCy/commit/24083b66080ce69f522952bfb5db38dbba15da61,No
2226,sorgerlab/indra,indra/tests/test_pysb_assembler.py,eafeeec297aecdd2acc94989a31eb183265f7613,TODO: Test for one or more statement-specific policies,https://github.com/sorgerlab/indra/commit/eafeeec297aecdd2acc94989a31eb183265f7613,Yes
2227,sorgerlab/indra,indra/tests/test_pysb_assembler.py,eafeeec297aecdd2acc94989a31eb183265f7613,TODO: Test for setting of default policy using a dict,https://github.com/sorgerlab/indra/commit/eafeeec297aecdd2acc94989a31eb183265f7613,No
2228,sorgerlab/indra,indra/tests/test_statements.py,f3af3dbc2339fd4276a20ad904af848e5d58750a,TODO Expand match tests to,https://github.com/sorgerlab/indra/commit/f3af3dbc2339fd4276a20ad904af848e5d58750a,No
2229,sorgerlab/indra,indra/tests/test_statements.py,ec7cc076f59112ad165acc5a1d4bf0dda2137369,TODO: Fill in tests below!,https://github.com/sorgerlab/indra/commit/ec7cc076f59112ad165acc5a1d4bf0dda2137369,No
2230,sorgerlab/indra,indra/reach/reach_api.py,eb8a250ee66be02e5d691ce1a1bcfd9abae3b1d4,TODO: Test if UTF-8 files are parsed correctly here,https://github.com/sorgerlab/indra/commit/eb8a250ee66be02e5d691ce1a1bcfd9abae3b1d4,No
2231,sorgerlab/indra,indra/tests/test_trips_ekbs.py,7c9a4c198888fc3210ebf130f380e02245f5ef5d,''' || TODO: put back when this is implemented || def test_33(): ||     sentence = 'Stimulation by EGF activates MAPK1.' ||     tp = process_sentence_xml(sentence) ||     assert_onestmt(tp) ||     st = tp.statements[0] ||     assert(isinstance(st; Activation)) ||     assert(st.obj is not None) ||     assert(st.subj is not None) ||     assert(st.obj_activity == 'activity') ||     assert(st.subj_activity == 'activity') ||     assert(st.is_activation) ||     assert(not st.evidence[0].epistemics['direct']) ||     assert_evidence(st) || ''',https://github.com/sorgerlab/indra/commit/7c9a4c198888fc3210ebf130f380e02245f5ef5d,No
2232,sorgerlab/indra,indra/tests/test_pysb_assembler.py,0ac6d8ad43dbef254bd570ac203837d1d2e8712b,TODO Add test for unmodified agent!,https://github.com/sorgerlab/indra/commit/0ac6d8ad43dbef254bd570ac203837d1d2e8712b,No
2233,sorgerlab/indra,indra/tests/test_pysb_assembler.py,0ac6d8ad43dbef254bd570ac203837d1d2e8712b,TODO Add test involving multiple (possibly degenerate) modifications!,https://github.com/sorgerlab/indra/commit/0ac6d8ad43dbef254bd570ac203837d1d2e8712b,No
2234,sorgerlab/indra,indra/tests/test_pysb_assembler.py,cad8a5800da27e7c87c01fdd92887effcb86ae7a,TODO Add test for generic double phosphorylation,https://github.com/sorgerlab/indra/commit/cad8a5800da27e7c87c01fdd92887effcb86ae7a,No
2235,sorgerlab/indra,indra/tests/test_pysb_assembler.py,efeab090c82d509c108ee032b03ebfd0330a247e,TODO test grounded assembly where the modification is on the agent; not,https://github.com/sorgerlab/indra/commit/efeab090c82d509c108ee032b03ebfd0330a247e,Yes
2236,sorgerlab/indra,indra/tests/test_model_checker.py,1042d4ee0953c47716455fd251082a82fb1b9142,TODO Add tests for autophosphorylation,https://github.com/sorgerlab/indra/commit/1042d4ee0953c47716455fd251082a82fb1b9142,No
2237,sorgerlab/indra,indra/tests/test_model_checker.py,1042d4ee0953c47716455fd251082a82fb1b9142,TODO Add test for transphosphorylation,https://github.com/sorgerlab/indra/commit/1042d4ee0953c47716455fd251082a82fb1b9142,Yes
2238,sorgerlab/indra,indra/tests/test_model_checker.py,1042d4ee0953c47716455fd251082a82fb1b9142,TODO Add test for dephosphorylation,https://github.com/sorgerlab/indra/commit/1042d4ee0953c47716455fd251082a82fb1b9142,No
2239,sorgerlab/indra,indra/tests/test_model_checker.py,b4a9193780b22e60e186c5b49d1e91f9c5696fbb,FIXME: This test fails--file as an issue. The problem is that the pysb,https://github.com/sorgerlab/indra/commit/b4a9193780b22e60e186c5b49d1e91f9c5696fbb,No
2240,sorgerlab/indra,indra/tests/test_s3_client.py,b8a5e2a07264a992fd2296fe50628422d5ebfaa4,TODO: verify that this was what was intended. Was `test_check_key',https://github.com/sorgerlab/indra/commit/b8a5e2a07264a992fd2296fe50628422d5ebfaa4,No
2241,sorgerlab/indra,indra/tests/test_pybel_assembler.py,a453c86d123bc009ff1d4bd84f3d7faa1b18efc7,TODO: Add tests for specific edge content,https://github.com/sorgerlab/indra/commit/a453c86d123bc009ff1d4bd84f3d7faa1b18efc7,No
2242,sorgerlab/indra,indra/tests/test_pybel_assembler.py,2fe70920bcae7e162a0e5e8d4e870c9c1c01b8a8,TODO: Add tests for evidence,https://github.com/sorgerlab/indra/commit/2fe70920bcae7e162a0e5e8d4e870c9c1c01b8a8,No
2243,sorgerlab/indra,indra/tests/test_pybel_assembler.py,2fe70920bcae7e162a0e5e8d4e870c9c1c01b8a8,TODO: Add tests for different groundings,https://github.com/sorgerlab/indra/commit/2fe70920bcae7e162a0e5e8d4e870c9c1c01b8a8,No
2244,sorgerlab/indra,indra/tests/test_springer.py,273b53d6327800bd7645c3251768ff8b09ab0c71,TODO: Test a doi that returns no other ids,https://github.com/sorgerlab/indra/commit/273b53d6327800bd7645c3251768ff8b09ab0c71,No
2245,sorgerlab/indra,indra/tests/test_springer.py,886e62bba438982d4dcb5ccbb07fb0c4bdb536d8,TODO: Setup db for testing.,https://github.com/sorgerlab/indra/commit/886e62bba438982d4dcb5ccbb07fb0c4bdb536d8,Yes
2246,sorgerlab/indra,indra/tests/test_springer.py,d6d20d0981f693c219e5815f247249dc262d72d0,TODO: Do some setup in the database; to test whether we handle,https://github.com/sorgerlab/indra/commit/d6d20d0981f693c219e5815f247249dc262d72d0,Yes
2247,sorgerlab/indra,indra/db/build_sample_set.py,625b7f055d6942efd59af0c526ad1f83d96723da,TODO: Add test case to touch this.,https://github.com/sorgerlab/indra/commit/625b7f055d6942efd59af0c526ad1f83d96723da,Yes
2248,sorgerlab/indra,indra/tests/test_db.py,625b7f055d6942efd59af0c526ad1f83d96723da,TODO: test the download.,https://github.com/sorgerlab/indra/commit/625b7f055d6942efd59af0c526ad1f83d96723da,Yes
2249,sorgerlab/indra,indra/tests/test_statements.py,3b7cc7d2a2c9f5c32dc6fc2390b98df751b30ca0,TODO: add tests with Agent refinement,https://github.com/sorgerlab/indra/commit/3b7cc7d2a2c9f5c32dc6fc2390b98df751b30ca0,Yes
2250,sorgerlab/indra,indra/tests/test_statements.py,3b7cc7d2a2c9f5c32dc6fc2390b98df751b30ca0,TODO: add tests with other Modification types,https://github.com/sorgerlab/indra/commit/3b7cc7d2a2c9f5c32dc6fc2390b98df751b30ca0,Yes
2251,sorgerlab/indra,indra/tests/test_db_preassembly.py,38e99bb2760e3b77337a34615fbb91b97c4f06ea,TODO: test more thoroughly.,https://github.com/sorgerlab/indra/commit/38e99bb2760e3b77337a34615fbb91b97c4f06ea,No
2252,sorgerlab/indra,db_rest_api/test_api.py,59b2867bdfa28388c82edb6d23c3f658010cf785,longer than needed; given that the quality is tested in other tests.,https://github.com/sorgerlab/indra/commit/59b2867bdfa28388c82edb6d23c3f658010cf785,Yes
2253,sorgerlab/indra,indra/tests/test_db_client.py,8e081a534e848667ddf239e36d5b4ca51c402bec,TODO: don't rely on the primary database; because that's scary in a test.,https://github.com/sorgerlab/indra/commit/8e081a534e848667ddf239e36d5b4ca51c402bec,Yes
2254,sorgerlab/indra,indra/tests/test_live_curation.py,aaa460dfdaed378c6880ca9ffb16c2a64f4df481,"\""\""\"" || FIXME: this Flask-app test suite is not working || class LiveCurationTestCase(unittest.TestCase): ||     def setUp(self): ||         _make_corpus() ||         app.testing = True ||         self.app = app.test_client() ||  ||     def _send_request(self; endpoint; req_dict): ||         resp = self.app.post(endpoint; ||                              data=json.dumps(req_dict); ||                              headers={'Content-Type': 'application\/json'}) ||         return resp ||  ||     # Tests ================== ||     def test_alive(self): ||         resp = self._send_request('submit_curation'; {'corpus_id': '1'}) ||         assert resp.status_code == 200; resp ||  ||     def test_bad_corpus(self): ||         resp = self._send_request('submit_curation'; {'corpus_id': '2'}) ||         assert resp.status_code == 400; resp ||  ||     def test_no_curation(self): ||         self._send_request('submit_curation'; {'corpus_id': '1'}) ||         resp = self._send_request('update_beliefs'; {'corpus_id': '1'}) ||         res = json.loads(resp.data.decode('utf-8')) ||         expected = {'1': 0.9167547741034001; ||                     '2': 0.8968421052631579; ||                     '3': 0.957125; ||                     '4': 0.65; ||                     '5': 0.65} ||         assert close_enough(res; expected); (res; expected) ||  ||     def test_eid_rule1_incorrect(self): ||         self._send_request('submit_curation'; {'corpus_id': '1'; ||                                                'curations': {'1': 0}}) ||         resp = self._send_request('update_beliefs'; {'corpus_id': '1'}) ||         assert resp.status_code == 200 ||         res = json.loads(resp.data.decode('utf-8')) ||         expected = {'1': 0; ||                     '2': 0.8917525773195876; ||                     '3': 0.957125; ||                     '4': 0.65; ||                     '5': 0.65} ||         assert close_enough(res; expected); (res; expected) ||  ||     def test_eid_rule1_incorrect_again(self): ||         self._send_request('submit_curation'; {'corpus_id': '1'; ||                                                'curations': {'1': 0}}) ||         resp = self._send_request('update_beliefs'; {'corpus_id': '1'}) ||         assert resp.status_code == 200; resp ||         res = json.loads(resp.data.decode('utf-8')) ||         expected = {'1': 0; ||                     '2': 0.8917525773195876; ||                     '3': 0.957125; ||                     '4': 0.65; ||                     '5': 0.65} ||         assert close_enough(res; expected); (res; expected) ||  ||     def test_eid_rule1_correct(self): ||         resp = self._send_request('submit_curation'; {'corpus_id': '1'; ||                                    'curations': {'1': 1}}) ||         resp = self._send_request('update_beliefs'; {'corpus_id': '1'}) ||         assert resp.status_code == 200 ||         res = json.loads(resp.data.decode('utf-8')) ||         expected = {'1': 1; ||                     '2': 0.8979166666666667; ||                     '3': 0.957125; ||                     '4': 0.65; ||                     '5': 0.65} ||         assert close_enough(res; expected); (res; expected) ||  ||     def test_eid_rule2_correct(self): ||         resp = self._send_request('submit_curation'; {'corpus_id': '1'; ||                                    'curations': {'2': 1}}) ||         resp = self._send_request('update_beliefs'; {'corpus_id': '1'}) ||         assert resp.status_code == 200 ||         res = json.loads(resp.data.decode('utf-8')) ||         assert res == {'1': 0.9171718289085546; ||                        '2': 1; ||                        '3': 0.9591666666666667; ||                        '4': 0.65; ||                        '5': 0.65}; res ||  ||     def test_hume_correct(self): ||         resp = self._send_request('submit_curation'; {'corpus_id': '1'; ||                                    'curations': {'3': 0}}) ||         resp = self._send_request('update_beliefs'; {'corpus_id': '1'}) ||         assert resp.status_code == 200 ||         res = json.loads(resp.data.decode('utf-8')) ||         assert close_enough(res; {'1': 0.9167547741034001; ||                                   '2': 0.887719298245614; ||                                   '3': 0; ||                                   '4': 0.6190476190476191; ||                                   '5': 0.6190476190476191}); res ||  ||     def test_sofia_incorrect(self): ||         resp = self._send_request('submit_curation'; {'corpus_id': '1'; ||                                    'curations': {'4': 0}}) ||         resp = self._send_request('update_beliefs'; {'corpus_id': '1'}) ||         assert resp.status_code == 200 ||         res = json.loads(resp.data.decode('utf-8')) ||         assert res == {'1': 0.9167547741034001; ||                        '2': 0.8968421052631579; ||                        '3': 0.9533333333333334; ||                        '4': 0; ||                        '5': 0.6190476190476191}; res ||         resp = self._send_request('submit_curation'; {'corpus_id': '1'; ||                                    'curations': {'5': 0}}) ||         resp = self._send_request('update_beliefs'; {'corpus_id': '1'}) ||         assert resp.status_code == 200 ||         res = json.loads(resp.data.decode('utf-8')) ||         assert close_enough(res; {'1': 0.9167547741034001; ||                                   '2': 0.8968421052631579; ||                                   '3': 0.9498863636363637; ||                                   '4': 0; ||                                   '5': 0}); res || \""\""\""",https://github.com/sorgerlab/indra/commit/aaa460dfdaed378c6880ca9ffb16c2a64f4df481,No
2255,sorgerlab/indra,indra/tests/test_hprd.py,31442bf2babd17a20651db74bc9eb9ba5cbf8c29,TODO: Add further tests here,https://github.com/sorgerlab/indra/commit/31442bf2babd17a20651db74bc9eb9ba5cbf8c29,Yes
2256,sorgerlab/indra,indra/tests/test_rest_api.py,3938d75d05bb96dbbeac2da31e0d4e15d98ac2a0,TODO: Add test of reach process json,https://github.com/sorgerlab/indra/commit/3938d75d05bb96dbbeac2da31e0d4e15d98ac2a0,No
2257,sorgerlab/indra,indra/tests/test_flask_api.py,7993d0c7f3b287f4669e2df0d80b22c7de58a4ff,TODO: Add test of reach process json,https://github.com/sorgerlab/indra/commit/7993d0c7f3b287f4669e2df0d80b22c7de58a4ff,No
2258,pescadores/pescador,tests/test_streams.py,c5fd8315289478c68f12f69908ae9aa40f556cfa,TODO minimize copypasta from above test.,https://github.com/pescadores/pescador/commit/c5fd8315289478c68f12f69908ae9aa40f556cfa,No
2259,pescadores/pescador,tests/test_mux.py,f88d552fa1e82d0b6c959b7056a04bf61d886597,TODO: write test that checks the stats - that there's,https://github.com/pescadores/pescador/commit/f88d552fa1e82d0b6c959b7056a04bf61d886597,No
2260,pescadores/pescador,tests/test_mux.py,2227e6fef0c7f5850fec11044db5807261b7972a,TODO: redesign this test so,https://github.com/pescadores/pescador/commit/2227e6fef0c7f5850fec11044db5807261b7972a,Yes
2261,ROCmSoftwarePlatform/Tensile,CobaltGen/SolutionCandidateGenerator.py,656e90253b821bf7590e39d566fba2e6cabc130d,TODO: haven't tested > 1024 threshold,https://github.com/ROCmSoftwarePlatform/Tensile/commit/656e90253b821bf7590e39d566fba2e6cabc130d,Yes
2262,ROCmSoftwarePlatform/Tensile,CobaltGen/SolutionCandidateGenerator.py,13b45e9b2786b3b285d1f108f592de2677d0184d,"\""\""\"" || 3 levels ||  2 - (e)xhaustive (everything that's supported; for validation) ||  1 - (t)horough (for benchmarking new project; for research) ||  0 - (f)ast (smallest subset \""guaranteed\"" to find fastest solution; for production) ||  || work-groups (threshold = 256) ||  (e) all m*n <= threshold ||  (t) all m*n multiple of 64 and <= threshold; exact matches for small dimensions ||  (f) 8x8; 16x16; exact matches for small dimensions ||  || micro-tiles (threshold = 8*8) ||  (e) all m*n <= threshold ||  (t) m=n; m;n=[2;sqrt(threshold)]; if skinny m;n within factor of 2 ||  (f) subset of (t): based on free index size; small tiles if problem is small; large tiles if problem is large ||  || unrolls (threshold = 16) ||  (e) all u <= threshold ||  (t) 4; 8; 16; if sum < threshold; exact match also ||  (f) same as (t) ||   || loads ||  (e) all ||  (t) only 1 or N depending on transpose ||  (f) same as (t) ||  || preprocessor defines || [0] leading strides; [1] offsets; [2] everything ||  (e) ||      [ 0; 0; 0]; \\ ||      [ 1; 0; 0]; \\ ||      [ 0; 1; 0]; \\ ||      [ 1; 1; 0]; \\ ||      [ 1; 1; 1]; \\ ||  (t) [ 1; 0; 0]; \\ ||      [ 1; 1; 1]; \\ ||  (f) [ 1; 0; 0]; \\ ||  ||  ||  || \""\""\""",https://github.com/ROCmSoftwarePlatform/Tensile/commit/13b45e9b2786b3b285d1f108f592de2677d0184d,Yes
2263,ROCmSoftwarePlatform/Tensile,CobaltGen/SolutionSelectionWriter.py,cfaea1e35564d74869b7b405da619d0a6d9f4b0e,todo; choose fastest no just idx=0,https://github.com/ROCmSoftwarePlatform/Tensile/commit/cfaea1e35564d74869b7b405da619d0a6d9f4b0e,Yes
2264,ROCmSoftwarePlatform/Tensile,Scripts/BenchmarkProblems.py,2e1690ee36e3de5b93dd31eb696e8431635b68b1,TODO when new list is joining; we need to choose the fastest,https://github.com/ROCmSoftwarePlatform/Tensile/commit/2e1690ee36e3de5b93dd31eb696e8431635b68b1,Yes
2265,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,1b27709a8c7cbf9e3585da380e839beacb3d201d,HACK just hard coding to verify it works for the case I am testing,https://github.com/ROCmSoftwarePlatform/Tensile/commit/1b27709a8c7cbf9e3585da380e839beacb3d201d,Yes
2266,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,94902fc71c98856e9fc7b3c36e68c9c50d22162c,HACK just hard coding to verify it works for the case I am testing,https://github.com/ROCmSoftwarePlatform/Tensile/commit/94902fc71c98856e9fc7b3c36e68c9c50d22162c,Yes
2267,Epistimio/orion,tests/functional/commands/test_db_rm.py,b4a5cf0226b72510c42472650e48976fb7616075,TODO: Test that the correct trials were deleted,https://github.com/Epistimio/orion/commit/b4a5cf0226b72510c42472650e48976fb7616075,Yes
2268,Epistimio/orion,tests/functional/configuration/test_all_options.py,83e980a02e2ceff73c2b99aa4fd56bd28282e543,TODO: Anything to test still???,https://github.com/Epistimio/orion/commit/83e980a02e2ceff73c2b99aa4fd56bd28282e543,Yes
2269,TwentyBN/GulpIO,src/unittest/python/loader_tests.py,caa366abb620198cf0230449328120a2a6b086c6,TODO: add sample gulpio files for testing,https://github.com/TwentyBN/GulpIO/commit/caa366abb620198cf0230449328120a2a6b086c6,Yes
2270,TwentyBN/GulpIO,src/unittest/python/loader_tests.py,cc36d320cfdd3690091da62bbc3436ab6308864f,TODO: add sample gulpio files for testing,https://github.com/TwentyBN/GulpIO/commit/cc36d320cfdd3690091da62bbc3436ab6308864f,Yes
2271,castorini/castor,sm_model/main.py,a67e2d12c4d79287de28b41e436fe6c4d9a108ef,TODO: remember to update args.* in testing loop below,https://github.com/castorini/castor/commit/a67e2d12c4d79287de28b41e436fe6c4d9a108ef,No
2272,Speedml/speedml,tests/test_feature.py,452abcdc60e13acf775304aff0d809a5b40960c2,TODO: What is the order of executin of test_classes?,https://github.com/Speedml/speedml/commit/452abcdc60e13acf775304aff0d809a5b40960c2,Yes
2273,glm-tools/pyglmnet,pyglmnet/mini_sklearn.py,ba512c00ab7788ca40cddc7319dae9264cc44f94,XXX: should we rather test if instance of estimator?,https://github.com/glm-tools/pyglmnet/commit/ba512c00ab7788ca40cddc7319dae9264cc44f94,No
2274,glm-tools/pyglmnet,tests/test_pyglmnet.py,b64e022a9ae62562a0cdf3a939338fec7a73c86e,XXX: don't use scikit-learn for tests.,https://github.com/glm-tools/pyglmnet/commit/b64e022a9ae62562a0cdf3a939338fec7a73c86e,Yes
2275,glm-tools/pyglmnet,tests/test_pyglmnet.py,a9b50ab29d42b71571c0e42faada35c6c54da24b,XXX: don't use scikit-learn for tests.,https://github.com/glm-tools/pyglmnet/commit/a9b50ab29d42b71571c0e42faada35c6c54da24b,Yes
2276,glm-tools/pyglmnet,pyglmnet/base.py,4dc03426487168eff54c87ec369085efff3621d2,XXX: should we rather test if instance of estimator?,https://github.com/glm-tools/pyglmnet/commit/4dc03426487168eff54c87ec369085efff3621d2,No
2277,uchicago-cs/deepdish,deepdish/tools/caffe/maker.py,1b1e36a4c89ef27bce2f56be839f8bc265b14c29,Test model (TODO: this is an ugly and brittle line),https://github.com/uchicago-cs/deepdish/commit/1b1e36a4c89ef27bce2f56be839f8bc265b14c29,Yes
2278,IQTLabs/poseidon,poseidon/test_poseidon.py,52ad2cd5d2a244245ee13dcadafa28ac01982b11,!! TODO write a test to ensure the version matches,https://github.com/IQTLabs/poseidon/commit/52ad2cd5d2a244245ee13dcadafa28ac01982b11,Yes
2279,IQTLabs/poseidon,tests/test_bcf.py,b006594989cd2e40a37a835e681f502d33e85a3e,TODO this test needs to actually do something with the switch\/interface,https://github.com/IQTLabs/poseidon/commit/b006594989cd2e40a37a835e681f502d33e85a3e,Yes
2280,IntelAI/models,models/object_detection/tensorflow/ssd-resnet34/inference/fp32/preprocessing.py,eb793bcc2429f0dbdbb7ae1cc745ff72490c8aee,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/IntelAI/models/commit/eb793bcc2429f0dbdbb7ae1cc745ff72490c8aee,No
2281,IntelAI/models,models/image_recognition/tensorflow/densenet169/inference/fp32/image_preprocessing.py,c827585ea8087ad8f5428e513d00014dceef911c,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/IntelAI/models/commit/c827585ea8087ad8f5428e513d00014dceef911c,No
2282,IntelAI/models,models/image_recognition/tensorflow/densenet169/inference/fp32/image_preprocessing.py,c827585ea8087ad8f5428e513d00014dceef911c,dynamic_pad=True) # HACK TESTING dynamic_pad=True,https://github.com/IntelAI/models/commit/c827585ea8087ad8f5428e513d00014dceef911c,No
2283,IntelAI/models,models/image_recognition/tensorflow/resnet50v1_5/int8/preprocessing.py,1ecd87bebcfdcbd7e5522e112aba7f0643cb6cfa,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/IntelAI/models/commit/1ecd87bebcfdcbd7e5522e112aba7f0643cb6cfa,No
2284,IntelAI/models,models/image_recognition/tensorflow/resnet50v1_5/int8/preprocessing.py,1ecd87bebcfdcbd7e5522e112aba7f0643cb6cfa,dynamic_pad=True) # HACK TESTING dynamic_pad=True,https://github.com/IntelAI/models/commit/1ecd87bebcfdcbd7e5522e112aba7f0643cb6cfa,No
2285,IntelAI/models,benchmarks/language_translation/tensorflow/bert/inference/fp32/model_init.py,3072d2efbe656c6a47f32f32691e0a61839758bf,Test accuracy if needed,https://github.com/IntelAI/models/commit/3072d2efbe656c6a47f32f32691e0a61839758bf,No
2286,IntelAI/models,benchmarks/language_translation/tensorflow/bert/inference/fp32/model_init.py,47d13bcf70133a496d2f1d67ffd261493be575dd,Test accuracy if needed,https://github.com/IntelAI/models/commit/47d13bcf70133a496d2f1d67ffd261493be575dd,No
2287,castorini/hedwig,sm_model/main.py,a67e2d12c4d79287de28b41e436fe6c4d9a108ef,TODO: remember to update args.* in testing loop below,https://github.com/castorini/hedwig/commit/a67e2d12c4d79287de28b41e436fe6c4d9a108ef,No
2288,analysiscenter/batchflow,batchflow/tests/dataformat_test.py,235fa793b48219130a898a5f389c3015b15586e2,"\""\""\"" Tests for data_d_f functionality for models with multiple inputs. \""\""\""",https://github.com/analysiscenter/batchflow/commit/235fa793b48219130a898a5f389c3015b15586e2,Yes
2289,analysiscenter/batchflow,batchflow/tests/fixtures/dataformat.py,62ca4d03f437d05cdd471b6cd33483729167d5c4,"\""\""\"" Fixtures for 'data_format' tests. || General idea is to create such combination of neural network architecture || and input data; so that passing wrong 'data_format' would inevitably raise ValueError. ||  || For example; 'single_setup' and 'get_single_pipeline' fixtures are creating input data of || 32x32x4 size. That array is passed through convolutional layer with 'kernel_size' of 11; 'valid' padding; || and that is possible only in 'channels_last' mode. Switching it to 'channels_first' would result || in an atempt of creating array with negative dimension sinse 4-11 is less than 0. ||  || For even more confidence in that test; we use multiple layers; and each of them follows || the same pattern: if the 'data_format' is wrong; it would raise ValueError. ||  || Last correctness check is performed at the loss evaluation step: we use l1 loss; || which is possible to calculate only for equal-sized arrays. Since we know all the dimensions || and their transforms layer-to-layer; we can use 'ground_truth' labels || with desired dimensions as one of the testing methods. ||  || Model with multiple imputs is tested in the same way; except for the fact that || 'tf.concat' is used as additional measure of correct performance. ||  || For predefined models like VGG7; ResNet18 and so on it is necessary to change || padding to 'valid'. ||  || For every type of tested models (e.g. 'single_input'; 'vgg') there are two || corresponding fixtures: '<prefix>_setup' and 'get_<prefix>_pipeline'. It is || necessary to split them as it allows us to modify 'config' between creating || it and actually using for 'train_model' action. That is exactly how testing is performed. ||  || There are documentation strings for 'single_setup' and 'get_single_pipeline' fixtures. || Others follow same structure and have same parameters and returns. ||  || There are also convinient 'setup' and 'get_pipeline' fixtures; that allow || to pass 'mode' argument to choose exact type of architecture to use. That allows || to parametrize tests. || \""\""\""",https://github.com/analysiscenter/batchflow/commit/62ca4d03f437d05cdd471b6cd33483729167d5c4,Yes
2290,analysiscenter/batchflow,batchflow/tests/temporal.py,90cfedece8631c85f6f6d2f29e55c2f76d5f2a5a,"\""\""\""Tests for DatasetIndex class. || If needed and possible; type of DatasetIndex is specified at || the very first line of each test. || Tests that have '_baseset_' in the name use methods; inherited from Baseset class. || \""\""\""",https://github.com/analysiscenter/batchflow/commit/90cfedece8631c85f6f6d2f29e55c2f76d5f2a5a,Yes
2291,thinkingmachines/geomancer,tests/conftest.py,439a7877d0cf78a7f24b2fec67b9f10ceccf9e8b,TODO: Use a testing service account for this,https://github.com/thinkingmachines/geomancer/commit/439a7877d0cf78a7f24b2fec67b9f10ceccf9e8b,No
2292,thinkingmachines/geomancer,tests/backend/cores/conftest.py,8e500aff19a91b439aeeeb9b60f91e48d82e48dd,TODO: Use a testing service account for this,https://github.com/thinkingmachines/geomancer/commit/8e500aff19a91b439aeeeb9b60f91e48d82e48dd,No
2293,EmuKit/emukit,tests/emukit/conftest.py,33d930033dd1b380aefdc4ded2d4102469615887,following decorator: @pytest.mark.parametrize('n_dims'; [xxx]) where xxx is the number of desired dimensions,https://github.com/EmuKit/emukit/commit/33d930033dd1b380aefdc4ded2d4102469615887,Yes
2294,dirty-cat/dirty_cat,dirty_cat/test/test_string_distances.py,35a47f8ec63dd08af2c12938fe847c96f9c56386,TODO: pytest patch to check which is called,https://github.com/dirty-cat/dirty_cat/commit/35a47f8ec63dd08af2c12938fe847c96f9c56386,No
2295,CartoDB/cartoframes,test/test_context.py,6be87e0362f24c8858273c2bf1a3ce091bfc477c,TODO: how to test instances of a class?,https://github.com/CartoDB/cartoframes/commit/6be87e0362f24c8858273c2bf1a3ce091bfc477c,Yes
2296,CartoDB/cartoframes,cartoframes/context.py,1274070f422406d5c5c87d1606efed8e5434bbbd,TODO: remove this after testing,https://github.com/CartoDB/cartoframes/commit/1274070f422406d5c5c87d1606efed8e5434bbbd,Yes
2297,CartoDB/cartoframes,cartoframes/context.py,c93afdc4853a2b1dde0ac3a28ed5d9791a1e75d5,TODO: remove this after testing,https://github.com/CartoDB/cartoframes/commit/c93afdc4853a2b1dde0ac3a28ed5d9791a1e75d5,Yes
2298,CartoDB/cartoframes,cartoframes/context.py,61e8d46b4024475c90a256fc7c0b5d69eb9c6b93,TODO: remove this after testing,https://github.com/CartoDB/cartoframes/commit/61e8d46b4024475c90a256fc7c0b5d69eb9c6b93,Yes
2299,CartoDB/cartoframes,cartoframes/context.py,e9f143d18c532ef1a44f3ef7ba4badc16be47868,TODO: remove this after testing,https://github.com/CartoDB/cartoframes/commit/e9f143d18c532ef1a44f3ef7ba4badc16be47868,Yes
2300,CartoDB/cartoframes,test/test_context.py,bf375fa106284a82faa34c01421cc640e1446194,TODO: how to test instances of a class?,https://github.com/CartoDB/cartoframes/commit/bf375fa106284a82faa34c01421cc640e1446194,Yes
2301,CartoDB/cartoframes,test/test_context.py,957051ddf348d1906f47b0b98017f7a4c1719a08,origcols = set(cc.read(self.test_data_table; limit=1).columns),https://github.com/CartoDB/cartoframes/commit/957051ddf348d1906f47b0b98017f7a4c1719a08,No
2302,CartoDB/cartoframes,cartoframes/context.py,b33d7f9d8d0427a1783f3337abed62ce0c40f10c,TODO: remove this after testing,https://github.com/CartoDB/cartoframes/commit/b33d7f9d8d0427a1783f3337abed62ce0c40f10c,Yes
2303,textpipe/textpipe,tests/test_doc.py,799ea921f82f2614d26a4bca6fe713e8b87a7eaf,TODO: finish this test,https://github.com/textpipe/textpipe/commit/799ea921f82f2614d26a4bca6fe713e8b87a7eaf,No
2304,mozilla/bugbug,scripts/commit_classifier.py,db318babcdb0b915b9206e647b309995654629b8,TODO: Classify multiple commit\/test at the same time.,https://github.com/mozilla/bugbug/commit/db318babcdb0b915b9206e647b309995654629b8,No
2305,mozilla/bugbug,scripts/bug_retriever.py,54f9d3ae8c2de90e84310d3ad2ad3fb18686b7f1,XXX: Temporarily avoid downloading the commits DB when a limit is set; to avoid the integration test fail when the commits DB is bumped.,https://github.com/mozilla/bugbug/commit/54f9d3ae8c2de90e84310d3ad2ad3fb18686b7f1,No
2306,mozilla/bugbug,scripts/commit_classifier.py,d5dc544f61c0d5debf43f5154576021d15ea5554,XXX: For now; only restrict to test-linux64 tasks.,https://github.com/mozilla/bugbug/commit/d5dc544f61c0d5debf43f5154576021d15ea5554,No
2307,mozilla/bugbug,scripts/commit_classifier.py,92be9cc3a6bd4157c235a034cd4b122954148623,XXX: For now; only restrict to test-linux64 tasks.,https://github.com/mozilla/bugbug/commit/92be9cc3a6bd4157c235a034cd4b122954148623,No
2308,mozilla/bugbug,scripts/commit_classifier.py,fdc74c8e3bfb426ebb472c91b5843f7795392078,XXX: For now; only restrict to linux64 test tasks.,https://github.com/mozilla/bugbug/commit/fdc74c8e3bfb426ebb472c91b5843f7795392078,Yes
2309,mozilla/bugbug,scripts/generate_landings_risk_report.py,9b171c5cd672d168a45c9c6a987d6cfc3cc5eec6,Retrieve components of test failures that occurred when landing patches to fix bugs in specific components.,https://github.com/mozilla/bugbug/commit/9b171c5cd672d168a45c9c6a987d6cfc3cc5eec6,Yes
2310,pymc-learn/pymc-learn,pmlearn/tests/test_logistic.py,450b5c6e922d9874cc6a67a88b1bd6f5f673809f,TODO: Figure out best way to test,https://github.com/pymc-learn/pymc-learn/commit/450b5c6e922d9874cc6a67a88b1bd6f5f673809f,Yes
2311,Epistimio/orion,src/metaopt/algo/space.py,66f5422dd656c04d5da6882678e1f251cba2b019,TODO TEST,https://github.com/Epistimio/orion/commit/66f5422dd656c04d5da6882678e1f251cba2b019,No
2312,Epistimio/orion,tests/functional/demo/test_demo.py,66a18aa921baf750234ff82b74c9ee136387c524,XXX: return and complete test when there is a way to control random,https://github.com/Epistimio/orion/commit/66a18aa921baf750234ff82b74c9ee136387c524,No
2313,Epistimio/orion,tests/unittests/core/evc/test_conflicts.py,004e3b8899927767f2706a5de4389dba8497c321,TODO : Add tests for ScriptConfigConflict and CommandLineConflict,https://github.com/Epistimio/orion/commit/004e3b8899927767f2706a5de4389dba8497c321,No
2314,Epistimio/orion,tests/unittests/core/test_producer.py,ffaed0081cefed3a75b5e4309def50ba60eb50ca,TODO: Remove this commented out if test pass,https://github.com/Epistimio/orion/commit/ffaed0081cefed3a75b5e4309def50ba60eb50ca,Yes
2315,ROCmSoftwarePlatform/Tensile,Tensile/Tests/nightly/convolution_config/test_conv_vs_contraction.py,38e722cba20e44f2133f847da98bc92ca0f2787c,TODO - re-enable this test,https://github.com/ROCmSoftwarePlatform/Tensile/commit/38e722cba20e44f2133f847da98bc92ca0f2787c,No
2316,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,0a4c61a71c2ea107bc7f3188940c82f0b87ac614,TODO remove debug stuff after testing,https://github.com/ROCmSoftwarePlatform/Tensile/commit/0a4c61a71c2ea107bc7f3188940c82f0b87ac614,No
2317,ROCmSoftwarePlatform/Tensile,Tensile/Tests/nightly/convolution_config/test_conv_vs_contraction.py,e2f3dbe05b627e8b493220abfb1a90222aeab4b4,TODO - re-enable this test,https://github.com/ROCmSoftwarePlatform/Tensile/commit/e2f3dbe05b627e8b493220abfb1a90222aeab4b4,No
2318,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,1ae540b7170ef971abdecd1ab0b4fc3e5f927dc1,TODO HACK to support latest,https://github.com/ROCmSoftwarePlatform/Tensile/commit/1ae540b7170ef971abdecd1ab0b4fc3e5f927dc1,No
2319,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,5b5d264f5892c1c3474d6644fcfc94c5ee532439,TODO HACK to support latest,https://github.com/ROCmSoftwarePlatform/Tensile/commit/5b5d264f5892c1c3474d6644fcfc94c5ee532439,No
2320,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,79c57ef8c7d78723b30f9a037b513d2a4a70e24b,TODO HACK to support latest,https://github.com/ROCmSoftwarePlatform/Tensile/commit/79c57ef8c7d78723b30f9a037b513d2a4a70e24b,No
2321,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,5df49889d2bca2a0b30504671c97f54660c66abc,TODO HACK to support latest,https://github.com/ROCmSoftwarePlatform/Tensile/commit/5df49889d2bca2a0b30504671c97f54660c66abc,No
2322,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,328f73eed67c2ccfc8e8dc257aee4a567914b4bc,TODO : re-enable later after running testlists,https://github.com/ROCmSoftwarePlatform/Tensile/commit/328f73eed67c2ccfc8e8dc257aee4a567914b4bc,No
2323,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,abdd0e760757d62a469cf138767d8a4a7dd74e58,TODO HACK to support latest,https://github.com/ROCmSoftwarePlatform/Tensile/commit/abdd0e760757d62a469cf138767d8a4a7dd74e58,No
2324,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,4dc1e0650fa4acf8e3eef9965bda972e9a0eeb47,TODO HACK to support latest,https://github.com/ROCmSoftwarePlatform/Tensile/commit/4dc1e0650fa4acf8e3eef9965bda972e9a0eeb47,No
2325,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,4dc1e0650fa4acf8e3eef9965bda972e9a0eeb47,TODO : re-enable later after running testlists,https://github.com/ROCmSoftwarePlatform/Tensile/commit/4dc1e0650fa4acf8e3eef9965bda972e9a0eeb47,No
2326,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,ecdb9bc2197985678e9a4e002c106fa442edf5d0,TODO HACK to support latest,https://github.com/ROCmSoftwarePlatform/Tensile/commit/ecdb9bc2197985678e9a4e002c106fa442edf5d0,No
2327,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,ecdb9bc2197985678e9a4e002c106fa442edf5d0,TODO : re-enable later after running testlists,https://github.com/ROCmSoftwarePlatform/Tensile/commit/ecdb9bc2197985678e9a4e002c106fa442edf5d0,No
2328,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,3f3bff6640c4fde0b0e0281c38d9c75a5a430689,not tested; would maybe need to restore base too if limit 0,https://github.com/ROCmSoftwarePlatform/Tensile/commit/3f3bff6640c4fde0b0e0281c38d9c75a5a430689,Yes
2329,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,66487a1e7756bc2a5a9005cea0f903308e63f248,not tested; would maybe need to restore base too if limit 0,https://github.com/ROCmSoftwarePlatform/Tensile/commit/66487a1e7756bc2a5a9005cea0f903308e63f248,Yes
2330,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,65c134cac482840e4bf2e9b0bdae3d9c98f92f12,not tested; would maybe need to restore base too if limit 0,https://github.com/ROCmSoftwarePlatform/Tensile/commit/65c134cac482840e4bf2e9b0bdae3d9c98f92f12,Yes
2331,ROCmSoftwarePlatform/Tensile,Tensile/KernelWriterAssembly.py,ddd237185f3fb378a198b3199eb755556958c806,TODO HACK to support latest,https://github.com/ROCmSoftwarePlatform/Tensile/commit/ddd237185f3fb378a198b3199eb755556958c806,No
2332,ROCmSoftwarePlatform/Tensile,Tensile/Contractions.py,7d178aacbebe02a5935e1db3befbf9d2ca1e307c,TODO - haven't been fully tested for FP16 and BF16; so we consider single and double only so far.,https://github.com/ROCmSoftwarePlatform/Tensile/commit/7d178aacbebe02a5935e1db3befbf9d2ca1e307c,No
2333,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,2f9a6df2669751243c1f7c4b3fd69436650294f0,TODO- Test and migrate ([H\/H\/H]+HPA) to ([H\/H\/S]+HPA),https://github.com/ROCmSoftwarePlatform/Tensile/commit/2f9a6df2669751243c1f7c4b3fd69436650294f0,Yes
2334,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,5cd4b9757459ff9e6dd12c0482ccb3e6759e38ea,TODO- Test and migrate ([H\/H\/H]+HPA) to ([H\/H\/S]+HPA),https://github.com/ROCmSoftwarePlatform/Tensile/commit/5cd4b9757459ff9e6dd12c0482ccb3e6759e38ea,Yes
2335,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,b659353d01ecb5b833e4d2b4f6c8df92a4c56378,TODO- less restriction? Haven't tested for not BufferLoad,https://github.com/ROCmSoftwarePlatform/Tensile/commit/b659353d01ecb5b833e4d2b4f6c8df92a4c56378,No
2336,ROCmSoftwarePlatform/Tensile,Tensile/SolutionStructs.py,0ad3cdceacf620ac4908e3754ab9bacefdabd854,TODO- Remove this DataType test condition;,https://github.com/ROCmSoftwarePlatform/Tensile/commit/0ad3cdceacf620ac4908e3754ab9bacefdabd854,Yes
2337,ROCmSoftwarePlatform/Tensile,tuning/automation/ExtractSizes.py,f3125674dad7aa1739e688ddd9c36e2265c04838,TODO: this logic needs to be tested more extensively,https://github.com/ROCmSoftwarePlatform/Tensile/commit/f3125674dad7aa1739e688ddd9c36e2265c04838,No
2338,persephone-tools/persephone,src/datasets/chatino.py,b9528a88a5b73823bf3b1f72e4ff858be18fa10a,TODO Reconsider the place of these splits. Perhaps train\/dev\/test,https://github.com/persephone-tools/persephone/commit/b9528a88a5b73823bf3b1f72e4ff858be18fa10a,Yes
2339,persephone-tools/persephone,src/datasets/na.py,d0b0e2be7dd864810568be4004b94fba68d59d7a,TODO Probably should be hardcoding the list of train\/dev\/test utterances,https://github.com/persephone-tools/persephone/commit/d0b0e2be7dd864810568be4004b94fba68d59d7a,Yes
2340,persephone-tools/persephone,src/datasets/na.py,588fe2472c158f240e1268426e796e689d178d86,TODO Probably should be hardcoding the list of train\/dev\/test utterances,https://github.com/persephone-tools/persephone/commit/588fe2472c158f240e1268426e796e689d178d86,Yes
2341,persephone-tools/persephone,persephone/tests/test_integration.py,85bd2fbed487931809423527080c0247efdf7cf1,TODO uncomment after test function is complete,https://github.com/persephone-tools/persephone/commit/85bd2fbed487931809423527080c0247efdf7cf1,No
2342,persephone-tools/persephone,persephone/tests/test_integration.py,d5b7e51391938ef7eb6f89c9633611b4f207bfc3,TODO Only test_tutorial really needs to actually pull the data each time.,https://github.com/persephone-tools/persephone/commit/d5b7e51391938ef7eb6f89c9633611b4f207bfc3,Yes
2343,persephone-tools/persephone,persephone/tests/test_integration.py,d5b7e51391938ef7eb6f89c9633611b4f207bfc3,the (normal) data dir and only pull it if needed. Currently this test,https://github.com/persephone-tools/persephone/commit/d5b7e51391938ef7eb6f89c9633611b4f207bfc3,Yes
2344,persephone-tools/persephone,persephone/tests/test_bkw.py,166479e794f51aeb90e76a07763bd6e42db98340,TODO This sort of test; and others don't really rely on the BKW data,https://github.com/persephone-tools/persephone/commit/166479e794f51aeb90e76a07763bd6e42db98340,Yes
2345,persephone-tools/persephone,persephone/tests/test_results.py,9308c2c3ba8da6d17c9af79a4e79605d0f6b6b1a,TODO This should become testable on Travis using mock data.,https://github.com/persephone-tools/persephone/commit/9308c2c3ba8da6d17c9af79a4e79605d0f6b6b1a,Yes
2346,persephone-tools/persephone,persephone/tests/test_rnn_ctc_model.py,90dc954cf7cb6d75579f5e2bf4bcff2d437283ec,TODO Fix this test so that we actually confirm decent decoding output,https://github.com/persephone-tools/persephone/commit/90dc954cf7cb6d75579f5e2bf4bcff2d437283ec,No
2347,Calysto/conx,conx/network.py,f8a217037617ac60c75fc5fa28bae0d8b4e7ae86,# FIXME: need standard format for train\/test\/inputs\/targets,https://github.com/Calysto/conx/commit/f8a217037617ac60c75fc5fa28bae0d8b4e7ae86,No
2348,Calysto/conx,conx/network.py,f8a217037617ac60c75fc5fa28bae0d8b4e7ae86,# FIXME: Define when we have a specific file to test on:,https://github.com/Calysto/conx/commit/f8a217037617ac60c75fc5fa28bae0d8b4e7ae86,Yes
2349,Calysto/conx,conx/dataset.py,9a06508b5b5e0d3cf8a87617d6341bae7947b228,# FIXME: Define when we have a specific file to test on:,https://github.com/Calysto/conx/commit/9a06508b5b5e0d3cf8a87617d6341bae7947b228,Yes
2350,Calysto/conx,conx/dataset.py,1f4f0c1cc16481cedcaf97d6b0dac59368985f84,# FIXME: handle test lengths based on split,https://github.com/Calysto/conx/commit/1f4f0c1cc16481cedcaf97d6b0dac59368985f84,Yes
2351,Calysto/conx,conx/network.py,714b30e25858a6cb8586814dcf36bc5e003be0d6,# FIXME: combine results from train and test,https://github.com/Calysto/conx/commit/714b30e25858a6cb8586814dcf36bc5e003be0d6,No
2352,onnx/sklearn-onnx,docs/tutorial/plot_dbegin_options.py,6f077f60d208a064bc76228a89be5e88dced4bbc,"\""\""\"" || One model; many possible conversions with options || ================================================= ||  || .. index:: options ||  || There is not one way to convert a model. A new operator || might have been added in a newer version of :epkg:`ONNX` || and that speeds up the converted model. The rational choice || would be to use this new operator but what means the associated || runtime has an implementation for it. What if two different || users needs two different conversion for the same model? || Let's see how this may be done. ||  || .. contents:: ||     :local: ||  ||  || Option *zipmap* || +++++++++++++++ ||  || Every classifier is by design converted into an ONNX graph which outputs || two results: the predicted label and the prediction probabilites || for every label. By default; the labels are integers and the || probabilites are stored in dictionaries. That's the purpose || of operator *ZipMap* added at the end of the following graph. ||  || .. gdot:: ||     :script: DOT-SECTION ||  ||     import numpy ||     from sklearn.datasets import load_iris ||     from sklearn.model_selection import train_test_split ||     from sklearn.linear_model import LogisticRegression ||     from skl2onnx import to_onnx ||     from mlprodict.onnxrt import OnnxInference ||  ||     iris = load_iris() ||     X; y = iris.data; iris.target ||     X_train; _; y_train; __ = train_test_split(X; y; random_state=11) ||     clr = LogisticRegression() ||     clr.fit(X_train; y_train) ||  ||     model_def = to_onnx(clr; X_train.astype(numpy.float32)) ||     oinf = OnnxInference(model_def) ||     print(\""DOT-SECTION\""; oinf.to_dot()) ||  || This operator is not really efficient as it copies every probabilies and || labels in a different container. This time is usually significant for || small classifiers. Then it makes sense to remove it. ||  || .. gdot:: ||     :script: DOT-SECTION ||  ||     import numpy ||     from sklearn.datasets import load_iris ||     from sklearn.model_selection import train_test_split ||     from sklearn.linear_model import LogisticRegression ||     from skl2onnx import to_onnx ||     from mlprodict.onnxrt import OnnxInference ||  ||     iris = load_iris() ||     X; y = iris.data; iris.target ||     X_train; _; y_train; __ = train_test_split(X; y; random_state=11) ||     clr = LogisticRegression() ||     clr.fit(X_train; y_train) ||  ||     model_def = to_onnx(clr; X_train.astype(numpy.float32); ||                         options={LogisticRegression: {'zipmap': False}}) ||     oinf = OnnxInference(model_def) ||     print(\""DOT-SECTION\""; oinf.to_dot()) ||  || There might be in the graph many classifiers; it is important to have || a way to specify which classifier should keep its *ZipMap* || and which is not. So it is possible to specify options by id. || \""\""\""",https://github.com/onnx/sklearn-onnx/commit/6f077f60d208a064bc76228a89be5e88dced4bbc,Yes
2353,onnx/sklearn-onnx,docs/tutorial/plot_pextend_python_runtime.py,6f077f60d208a064bc76228a89be5e88dced4bbc,"\""\""\"" ||  || .. _l-extend-python-runtime: ||  || Fast design with a python runtime || ================================= ||  || .. index:: custom python runtime ||  || :epkg:`ONNX operators` do not contain all operators || from :epkg:`numpy`. There is no operator for || `solve <https:\/\/numpy.org\/doc\/stable\/reference\/ || generated\/numpy.linalg.solve.html>`_ but this one || is needed to implement the prediction function || of model :epkg:`NMF`. The converter can be written || including a new ONNX operator but then it requires a || runtime for it to be tested. This example shows how || to do that with the python runtime implemented in || :epkg:`mlprodict`. It may not be :epkg:`onnxruntime` || but that speeds up the implementation of the converter. ||  || The example changes the transformer from || :ref:`l-plot-custom-converter`; the method *predict* || decorrelates the variables by computing the eigen || values. Method *fit* does not do anything anymore. ||  || .. contents:: ||     :local: ||  || A transformer which decorrelates variables || ++++++++++++++++++++++++++++++++++++++++++ ||  || This time; the eigen values are not estimated at || training time but at prediction time. || \""\""\""",https://github.com/onnx/sklearn-onnx/commit/6f077f60d208a064bc76228a89be5e88dced4bbc,Yes
2354,onnx/sklearn-onnx,docs/tutorial/plot_qextend_onnxruntime.py,6f077f60d208a064bc76228a89be5e88dced4bbc,"\""\""\"" || Fast runtime with onnxruntime || ============================= ||  || :epkg:`ONNX operators` does not contain operator || from :epkg:`numpy`. There is no operator for || `solve <https:\/\/numpy.org\/doc\/stable\/reference\/ || generated\/numpy.linalg.solve.html>`_ but this one || is needed to implement the prediction function || of model :epkg:`NMF`. The converter can be written || including a new ONNX operator but then it requires a || runtime for it to be tested. Example || :ref:`l-extend-python-runtime` shows how to do that || with :epkg:`mlprodict`. Doing the same with || :epkg:`onnxruntime` is more ambitious as it requires || C++... ||  || *to be continued* || \""\""\""",https://github.com/onnx/sklearn-onnx/commit/6f077f60d208a064bc76228a89be5e88dced4bbc,No
2355,SMTorg/smt,smt/surrogate_models/neural_net/tests/test_neural_net.py,b172c21ffcdb7c0c5cd81d0c2776d69573904b69,TODO: make test output more meaningful (e.g. print mini-batches --> mini-batch i; example j: X = ...; Y = ...),https://github.com/SMTorg/smt/commit/b172c21ffcdb7c0c5cd81d0c2776d69573904b69,Yes
2356,SMTorg/smt,smt/surrogate_models/neural_net/tests/test_neural_net.py,b172c21ffcdb7c0c5cd81d0c2776d69573904b69,"TODO: make test output more meaningful (e.g. print(\""expected LSE = X.XXX; computed X.XXX; error = X.XXX\"")",https://github.com/SMTorg/smt/commit/b172c21ffcdb7c0c5cd81d0c2776d69573904b69,No
2357,SMTorg/smt,smt/surrogate_models/neural_net/tests/test_neural_net.py,b172c21ffcdb7c0c5cd81d0c2776d69573904b69,TODO: add tests for fwd_prop; bwd_prop; and metrics,https://github.com/SMTorg/smt/commit/b172c21ffcdb7c0c5cd81d0c2776d69573904b69,No
2358,HoloClean/holoclean,detect/nulldetector.py,31a19d7ab39d0cb0db8665528cec0b8b16621cc4,TODO: test if isnull() resolve null issue,https://github.com/HoloClean/holoclean/commit/31a19d7ab39d0cb0db8665528cec0b8b16621cc4,No
2359,AcutronicRobotics/ros2learn,optimization/Spearmint/spearmint/tests/models/in_progress/gp.py,ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,TODO: The tests below should be converted into proper nosetests.,https://github.com/AcutronicRobotics/ros2learn/commit/ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,Yes
2360,AcutronicRobotics/ros2learn,optimization/hyperopt/hyperopt/pyll/base.py,ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,XXX: THIS IS NOT BEING TESTED AND IS OBVIOUSLY BROKEN,https://github.com/AcutronicRobotics/ros2learn/commit/ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,No
2361,AcutronicRobotics/ros2learn,optimization/hyperopt/hyperopt/tests/test_fmin.py,ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,XXX also test domain.exceptions mechanism that actually catches this,https://github.com/AcutronicRobotics/ros2learn/commit/ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,Yes
2362,AcutronicRobotics/ros2learn,optimization/hyperopt/hyperopt/tests/test_rand.py,ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,-- TODO: put in a test that guarantees that,https://github.com/AcutronicRobotics/ros2learn/commit/ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,No
2363,AcutronicRobotics/ros2learn,optimization/hyperopt/hyperopt/tests/test_vectorize.py,ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,XXX: test more distributions,https://github.com/AcutronicRobotics/ros2learn/commit/ea33ae0715bdd70f93be2d3913fdb9d6f578ee53,Yes
2364,snuspl/parallax,parallax/parallax/examples/tf_cnn_benchmarks/preprocessing.py,8af2efbb22242f811672f9bba7ad240e4a00e675,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/snuspl/parallax/commit/8af2efbb22242f811672f9bba7ad240e4a00e675,No
2365,chainer/onnx-chainer,tests/functions_tests/test_arrays.py,0022d493fd07be81e58b7e64f5db8333c167389d,FIXME(syoyo): Currently the test will fail due to different behavior,https://github.com/chainer/onnx-chainer/commit/0022d493fd07be81e58b7e64f5db8333c167389d,No
2366,chainer/onnx-chainer,tests/functions_tests/test_arrays.py,98f4faebf6d1aec00fc22a76387c92f17e832b02,FIXME(syoyo): Currently the test will fail due to the different,https://github.com/chainer/onnx-chainer/commit/98f4faebf6d1aec00fc22a76387c92f17e832b02,No
2367,metabrainz/acousticbrainz-server,acousticbrainz/views/api_test.py,73591e63a31afc7487f2af2b4fb1ab3abac29bcd,TODO: Test in get_high_level.,https://github.com/metabrainz/acousticbrainz-server/commit/73591e63a31afc7487f2af2b4fb1ab3abac29bcd,Yes
2368,metabrainz/acousticbrainz-server,acousticbrainz/__init__.py,7cb76e039b407afb0518918c34d4b8bd7cbeb606,FIXME(roman): This fails during testing if database referenced in PG_CONNECT doesn't exist.,https://github.com/metabrainz/acousticbrainz-server/commit/7cb76e039b407afb0518918c34d4b8bd7cbeb606,Yes
2369,metabrainz/acousticbrainz-server,webserver/views/test/api_v1.py,298d88e1e2c239e7936f243cecc5919e1d2d7935,TODO: Test in get_high_level.,https://github.com/metabrainz/acousticbrainz-server/commit/298d88e1e2c239e7936f243cecc5919e1d2d7935,Yes
2370,HewlettPackard/dlcookbook-dlbs,python/tf_cnn_benchmarks/preprocessing.py,9c3c64967b385a215cd7964630d43160f61e8998,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/HewlettPackard/dlcookbook-dlbs/commit/9c3c64967b385a215cd7964630d43160f61e8998,No
2371,HewlettPackard/dlcookbook-dlbs,python/tf_cnn_benchmarks/preprocessing.py,9c3c64967b385a215cd7964630d43160f61e8998,dynamic_pad=True) # HACK TESTING dynamic_pad=True,https://github.com/HewlettPackard/dlcookbook-dlbs/commit/9c3c64967b385a215cd7964630d43160f61e8998,No
2372,HewlettPackard/dlcookbook-dlbs,python/tf_cnn_benchmarks/tf_cnn_benchmarks.py,9c3c64967b385a215cd7964630d43160f61e8998,global cross_entropy # HACK TESTING,https://github.com/HewlettPackard/dlcookbook-dlbs/commit/9c3c64967b385a215cd7964630d43160f61e8998,No
2373,HewlettPackard/dlcookbook-dlbs,python/tf_cnn_benchmarks/benchmark_cnn_distributed_test.py,cec704bd54458bb5b86ca7db4061a5c597fac85e,on every test. See the TODO in benchmark_cnn_test.py.,https://github.com/HewlettPackard/dlcookbook-dlbs/commit/cec704bd54458bb5b86ca7db4061a5c597fac85e,Yes
2374,HewlettPackard/dlcookbook-dlbs,python/tf_cnn_benchmarks/benchmark_cnn_distributed_test_runner.py,cec704bd54458bb5b86ca7db4061a5c597fac85e,"\""\""\""Used to run benchmark_cnn for distributed tests. ||  || In distributed tests; we spawn processes to run tf_cnn_benchmark tasks. We could || directly spawn tf_cnn_benchmark processes; but we want some added functionality; || such as being able to inject custom images during training. So instead; this || file is spawned as a Python process; which supports the added functionality. || \""\""\""",https://github.com/HewlettPackard/dlcookbook-dlbs/commit/cec704bd54458bb5b86ca7db4061a5c597fac85e,Yes
2375,HewlettPackard/dlcookbook-dlbs,python/mxnet_benchmarks/benchmarks.py,31ff5f8d1f12710c3af1d410de224946c5978f25,TODO: Is this implemented in NGC 19-05 in kv_store? There's a test below. I test it here because,https://github.com/HewlettPackard/dlcookbook-dlbs/commit/31ff5f8d1f12710c3af1d410de224946c5978f25,No
2376,alan-turing-institute/sktime,sktime/utils/estimator_checks.py,601dcf5cac7cb07b66c6d473d05084a75345d2bd,TODO: remove\/add appropriate tests (now many tests are removed),https://github.com/alan-turing-institute/sktime/commit/601dcf5cac7cb07b66c6d473d05084a75345d2bd,Yes
2377,alan-turing-institute/sktime,sktime/utils/estimator_checks.py,601dcf5cac7cb07b66c6d473d05084a75345d2bd,TODO: add test for xpandas dtypes,https://github.com/alan-turing-institute/sktime/commit/601dcf5cac7cb07b66c6d473d05084a75345d2bd,No
2378,alan-turing-institute/sktime,sktime/tests/test_common.py,1c5393427b3184f37c216d0e89da024864d18379,TODO: these tests should be performed after writing TS and pandas friendly tests,https://github.com/alan-turing-institute/sktime/commit/1c5393427b3184f37c216d0e89da024864d18379,No
2379,alan-turing-institute/sktime,heavy.py,dc28cc62fc11b04dcc7b4225e27ad814800be55c,TODO: (discuss) shall we putestimator type in meta?,https://github.com/alan-turing-institute/sktime/commit/dc28cc62fc11b04dcc7b4225e27ad814800be55c,Yes
2380,alan-turing-institute/sktime,sktime/tests/test_utils_transformations.py,a31aeaba89a884944b101596af8e5eceb2c8c6e9,Test dataframe input with columns having series of different length.,https://github.com/alan-turing-institute/sktime/commit/a31aeaba89a884944b101596af8e5eceb2c8c6e9,No
2381,alan-turing-institute/sktime,sktime/analyze_results/analyze_results.py,265d41c64eb8872ab18cb517144e844cbe7de93c,TODO: uncomment and test,https://github.com/alan-turing-institute/sktime/commit/265d41c64eb8872ab18cb517144e844cbe7de93c,No
2382,alan-turing-institute/sktime,sktime/classifiers/proximity_forest/proximity_forest.py,3c3e92a09a888856c4ee20301e5f030f68001cf6,todo deep copy; test,https://github.com/alan-turing-institute/sktime/commit/3c3e92a09a888856c4ee20301e5f030f68001cf6,No
2383,alan-turing-institute/sktime,sktime/classifiers/proximity_forest/proximity_forest.py,3c3e92a09a888856c4ee20301e5f030f68001cf6,todo test,https://github.com/alan-turing-institute/sktime/commit/3c3e92a09a888856c4ee20301e5f030f68001cf6,No
2384,alan-turing-institute/sktime,sktime/classifiers/proximity_forest/proximity_forest.py,6a2813f1357d5a894e5803c661e69837bbb30306,todo deep copy; test,https://github.com/alan-turing-institute/sktime/commit/6a2813f1357d5a894e5803c661e69837bbb30306,No
2385,alan-turing-institute/sktime,sktime/classifiers/proximity_forest/proximity_forest.py,6a2813f1357d5a894e5803c661e69837bbb30306,todo test,https://github.com/alan-turing-institute/sktime/commit/6a2813f1357d5a894e5803c661e69837bbb30306,No
2386,alan-turing-institute/sktime,sktime/forecasting/tests/test_DummyForecaster.py,d1a1906a917f2102f3e0b7dc56904db0bbe2f454,TODO add test for linear strategy,https://github.com/alan-turing-institute/sktime/commit/d1a1906a917f2102f3e0b7dc56904db0bbe2f454,Yes
2387,alan-turing-institute/sktime,sktime/classifiers/proximity.py,56cb698fbbc0e5ac293aef8bd896a78aefb4f848,todo unit tests,https://github.com/alan-turing-institute/sktime/commit/56cb698fbbc0e5ac293aef8bd896a78aefb4f848,No
2388,alan-turing-institute/sktime,sktime/forecasting/tests/test_ARIMA.py,2bb99c51adbebeaa2c143440a2c8cb8fda1ae1ae,TODO currently tests only run fit\/predict and compare length of predicted series,https://github.com/alan-turing-institute/sktime/commit/2bb99c51adbebeaa2c143440a2c8cb8fda1ae1ae,Yes
2389,alan-turing-institute/sktime,sktime/transformers/shapelets-workshop/shapelets_v3.py,3dbfbb052243f78bf278091f6966e72b10f639d9,Cython dist to-do however; and timing experiments necessary to settle on which approach is fastest,https://github.com/alan-turing-institute/sktime/commit/3dbfbb052243f78bf278091f6966e72b10f639d9,Yes
2390,alan-turing-institute/sktime,sktime/transformers/shapelets-workshop/shapelets_v3.py,3dbfbb052243f78bf278091f6966e72b10f639d9,TODO: We have to think about how to work when we have shapelets with length higher than min(lengths_test_set),https://github.com/alan-turing-institute/sktime/commit/3dbfbb052243f78bf278091f6966e72b10f639d9,No
2391,alan-turing-institute/sktime,sktime/contrib/workshop_shapelets/shapelets_v3.py,7b35447bf4955f0c8f2194880d0bc980cf3c501a,TO-DO: add CI tests; comments; documentation; etc.,https://github.com/alan-turing-institute/sktime/commit/7b35447bf4955f0c8f2194880d0bc980cf3c501a,Yes
2392,alan-turing-institute/sktime,sktime/utils/seasonality.py,cab589787b00e46a5849472ca57033ccf630c097,TODO: extend to handle multiple series (rows) more efficiently; requires to change seasonality testing too;,https://github.com/alan-turing-institute/sktime/commit/cab589787b00e46a5849472ca57033ccf630c097,No
2393,alan-turing-institute/sktime,sktime/classifiers/proximity.py,68a9aac1ab0b7e1db824450aa11914fed7964149,todo unit tests \/ sort out current unit tests,https://github.com/alan-turing-institute/sktime/commit/68a9aac1ab0b7e1db824450aa11914fed7964149,No
2394,alan-turing-institute/sktime,sktime/transformers/shapelets.py,5edfae90aa154abcea39f5b43d62cd112640721d,TO-DO: add CI tests; comments; documentation; etc.,https://github.com/alan-turing-institute/sktime/commit/5edfae90aa154abcea39f5b43d62cd112640721d,Yes
2395,alan-turing-institute/sktime,sktime/classifiers/proximity.py,80df7efaa34a05db561ce362bd5072b2e45e9fbe,todo unit tests,https://github.com/alan-turing-institute/sktime/commit/80df7efaa34a05db561ce362bd5072b2e45e9fbe,No
2396,alan-turing-institute/sktime,sktime/classifiers/proximity.py,80df7efaa34a05db561ce362bd5072b2e45e9fbe,todo unit tests \/ sort out current unit tests,https://github.com/alan-turing-institute/sktime/commit/80df7efaa34a05db561ce362bd5072b2e45e9fbe,No
2397,alan-turing-institute/sktime,sktime/contrib/distance_based/proximity.py,7f328f3f8e8b11058f0b27875cbf0a7f61742aa0,todo unit tests \/ sort out current unit tests,https://github.com/alan-turing-institute/sktime/commit/7f328f3f8e8b11058f0b27875cbf0a7f61742aa0,No
2398,alan-turing-institute/sktime,sktime/classifiers/proximity.py,8bf2f5717fdf58710cf32828060d0721013747c8,todo unit tests \/ sort out current unit tests,https://github.com/alan-turing-institute/sktime/commit/8bf2f5717fdf58710cf32828060d0721013747c8,No
2399,alan-turing-institute/sktime,sktime/tests/test_all_estimators.py,07bc8569342e0bba152c7f65218539f9b23596dc,TODO fix estimators to pass all tests,https://github.com/alan-turing-institute/sktime/commit/07bc8569342e0bba152c7f65218539f9b23596dc,No
2400,alan-turing-institute/sktime,sktime/tests/config.py,4a378eea3d63fd7ff547a6a85effbe1cce25267a,TODO fix estimators to pass all tests,https://github.com/alan-turing-institute/sktime/commit/4a378eea3d63fd7ff547a6a85effbe1cce25267a,No
2401,alan-turing-institute/sktime,sktime/transformers/series_as_features/tests/test_matrix_profile_transformer.py,697099857da5bbfad2b768f49afeb78ca66df385,TODO remove this test as this is covered in,https://github.com/alan-turing-institute/sktime/commit/697099857da5bbfad2b768f49afeb78ca66df385,Yes
2402,NeuroTechX/moabb,moabb/contexts/base.py,3273554153b1f2661a0c52064c04d64f9a97cb1f,TODO: letter case test,https://github.com/NeuroTechX/moabb/commit/3273554153b1f2661a0c52064c04d64f9a97cb1f,Yes
2403,KhronosGroup/NNEF-Tools,nnef_tools/conversion/tensorflow/tf_pb_all_in_one.py,45b9bf8aa646b75d1fbfda9f29ea6598432f5177,TODO Not final api (used for testing but probably it will be removed),https://github.com/KhronosGroup/NNEF-Tools/commit/45b9bf8aa646b75d1fbfda9f29ea6598432f5177,No
2404,CogComp/cogcomp-nlpy,tests/test_pipeline_config.py,fa5e76a40934d2770b1110626ba3c7977680899e,This test is no longer needed because the config file in root directory will be generated when user download the models,https://github.com/CogComp/cogcomp-nlpy/commit/fa5e76a40934d2770b1110626ba3c7977680899e,Yes
2405,IntelAI/models,models/image_recognition/tensorflow/resnet50/int8/preprocessing.py,8481f8228e8611580b50cff8f849730e82f077e1,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/IntelAI/models/commit/8481f8228e8611580b50cff8f849730e82f077e1,No
2406,IntelAI/models,models/image_recognition/tensorflow/resnet50/int8/preprocessing.py,8481f8228e8611580b50cff8f849730e82f077e1,dynamic_pad=True) # HACK TESTING dynamic_pad=True,https://github.com/IntelAI/models/commit/8481f8228e8611580b50cff8f849730e82f077e1,No
2407,IntelAI/models,models/image_recognition/tensorflow/resnet50/fp32/preprocessing.py,b99c70569321ca28a12a872f568287d1252dd60f,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/IntelAI/models/commit/b99c70569321ca28a12a872f568287d1252dd60f,No
2408,IntelAI/models,models/image_recognition/tensorflow/resnet50/fp32/preprocessing.py,b99c70569321ca28a12a872f568287d1252dd60f,dynamic_pad=True) # HACK TESTING dynamic_pad=True,https://github.com/IntelAI/models/commit/b99c70569321ca28a12a872f568287d1252dd60f,No
2409,IntelAI/models,models/image_recognition/tensorflow/resnet101/fp32/preprocessing.py,a632eaf51393c67693255a5367d95503c06c3140,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/IntelAI/models/commit/a632eaf51393c67693255a5367d95503c06c3140,No
2410,IntelAI/models,models/image_recognition/tensorflow/resnet101/fp32/preprocessing.py,a632eaf51393c67693255a5367d95503c06c3140,dynamic_pad=True) # HACK TESTING dynamic_pad=True,https://github.com/IntelAI/models/commit/a632eaf51393c67693255a5367d95503c06c3140,No
2411,IntelAI/models,models/image_recognition/tensorflow/resnet101/int8/preprocessing.py,c1e472bb4b13b1baa944716dd728255ad6c007f1,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/IntelAI/models/commit/c1e472bb4b13b1baa944716dd728255ad6c007f1,No
2412,IntelAI/models,models/image_recognition/tensorflow/resnet101/int8/preprocessing.py,c1e472bb4b13b1baa944716dd728255ad6c007f1,dynamic_pad=True) # HACK TESTING dynamic_pad=True,https://github.com/IntelAI/models/commit/c1e472bb4b13b1baa944716dd728255ad6c007f1,No
2413,darwinex/DarwinexLabs,tools/Python/MetaTrader_Helpers/Report_Conversion/MT_TO_PYTHON/DWX_MT_TO_PYTHON_v2_RC4.py,8b84d6f8816b09e5a6350db5114cff7351417d7b,"\""\""\""\r || Created on Mon Feb 04 12:01:07 2019\r || @author: Darwinex Labs (www.darwinex.com)\r || \r || DWX_MT_TO_PYTHON_v2_RC4.py\r || \r || Purpose:\r ||     \r ||     This script enables traders knowledgable in Python to import \r ||     their Account History and Strategy Tester reports directly into \r ||     pandas dataframes.\r ||     \r ||     Leverage the capabilities of Python to conduct more meaninful;\r ||     sophisitcated analyses of your track records and backtests.\r || \r || Dependencies:\r || \r ||     - Python 3.6+\r ||     - BeautifulSoup 4 (bs4)\r ||     - pandas (Python Data Analysis Library)\r ||     - numpy (Scientific Computing with Python)\r || \r || Notes:\r ||     \r ||     The script isolates certain structural nuances of MetaTrader's HTML report;\r ||     specified in the __init__() function.\r ||     \r ||     These are at the mercy of MetaTrader; and should these change in future; the\r ||     corresponding variables in the script will require adjustments accordingly.\r ||     \r || Tested with:\r ||     \r ||     MetaTrader 4 Build 1170 (20 Dec 2018)\r ||     \r || Usage:\r ||     \r ||     1) Set _type = 'normal' for Account Histories saves as \""Normal\"" Reports\r ||     2) Set _type = 'detailed' for Account Histories saves as \""Detailed\"" Reports\r ||     3) Set _type = 'backtest' for Strategy Tester reports\r ||     \r ||     By default; dataframes generated are stored inside a class variable\r ||     called '_statement_df'. Set _verbose to True to print this upon generation.\r ||     \r || \""\""\""",https://github.com/darwinex/DarwinexLabs/commit/8b84d6f8816b09e5a6350db5114cff7351417d7b,No
2414,jasonlaska/spherecluster,spherecluster/von_mises_fisher_mixture.py,39ab416844e01941986f89254e9a49cc7436b560,XXX This skips _check_test_data; which may change the dtype;,https://github.com/jasonlaska/spherecluster/commit/39ab416844e01941986f89254e9a49cc7436b560,No
2415,umbertogriffo/Predictive-Maintenance-using-LSTM,src/lstm/binary_classification.py,619e6907bb392eb3e128064eb906711a68bb846b,generate label columns w0 and w1 for test data,https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM/commit/619e6907bb392eb3e128064eb906711a68bb846b,Yes
2416,umbertogriffo/Predictive-Maintenance-using-LSTM,src/lstm/regression.py,619e6907bb392eb3e128064eb906711a68bb846b,generate label columns w0 and w1 for test data,https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM/commit/619e6907bb392eb3e128064eb906711a68bb846b,Yes
2417,mondejar/ecg-classification,python/load_MITBIH.py,5e2b68124276a7bb483dac41a5ed826bd855b058,TODO test with np.sqrt(np.dot(x_diff; y_diff)),https://github.com/mondejar/ecg-classification/commit/5e2b68124276a7bb483dac41a5ed826bd855b058,Yes
2418,mondejar/ecg-classification,python/features_ECG.py,ad7ef2cd12b64034b12a46355fce8a74c36367eb,TODO test with np.sqrt(np.dot(x_diff; y_diff)),https://github.com/mondejar/ecg-classification/commit/ad7ef2cd12b64034b12a46355fce8a74c36367eb,Yes
2419,mondejar/ecg-classification,python/features_ECG.py,938aff733db5165d8d1f88379b88449cf0537cbe,TODO test with np.sqrt(np.dot(x_diff; y_diff)),https://github.com/mondejar/ecg-classification/commit/938aff733db5165d8d1f88379b88449cf0537cbe,Yes
2420,vc1492a/PyNomaly,PyNomaly/loop.py,63250372a283d877513eeaf6f8792ccaa7e44e02,"\""\""\"" || NOTES: When specifying testing data; we want to use the training data as the \""normal\"" distribution of observations; and || \""compare\"" the testing distribution to the norm. One potential concern: in the case where the entire set of testing data || is abnormal; and both the train and test data are the same length; then the test data would mask the distribution of || the normal data (worded horribly but will fix later). Solution: set a ratio of some kind; say 80% training data and 20% || testing data. ||  || Problem. Impractical in practice. Our passes sometimes include 40k points; which means we would need way more points || for training and loop is fairly computationally expensive. Using a sliding window of continuous computation results in || the same concerns. ||  ||  ||  ||  || \""\""\""",https://github.com/vc1492a/PyNomaly/commit/63250372a283d877513eeaf6f8792ccaa7e44e02,Yes
2421,vc1492a/PyNomaly,PyNomaly/loop_dev.py,63250372a283d877513eeaf6f8792ccaa7e44e02,"\""\""\"" || NOTES: When specifying testing data; we want to use the training data as the \""normal\"" distribution of observations; and || \""compare\"" the testing distribution to the norm. One potential concern: in the case where the entire set of testing data || is abnormal; and both the train and test data are the same length; then the test data would mask the distribution of || the normal data (worded horribly but will fix later). Solution: set a ratio of some kind; say 80% training data and 20% || testing data. ||  || Problem. Impractical in practice. Our passes sometimes include 40k points; which means we would need way more points || for training and loop is fairly computationally expensive. Using a sliding window of continuous computation results in || the same concerns. ||  || NOTES 2: The other concern I have with LoOP in its current form is that euclidean distance is not necessarily the || best method of calculating similarity across time series data. There are a variety of options here; like using || another metric measure such as Mahalanobis Distance or using something along the like of Dynamic Time Warping (DTW). || Utilizing the Euclidean distance assumes that clusters have identity covariances i.e. all dimensions are statistically || independent and the variance of along each of the dimensions (columns) is equal to one. The Mahalanobis distance is an || euclidian distance that considers the covariance of the data by down-weighting the axis with higher variance. ||  ||  ||  || REF: http:\/\/kldavenport.com\/mahalanobis-distance-and-outliers\/ ||  || \""\""\""",https://github.com/vc1492a/PyNomaly/commit/63250372a283d877513eeaf6f8792ccaa7e44e02,Yes
2422,vc1492a/PyNomaly,tests/test_loop.py,cba4cb6ded41c9b24a10cc7421155b171f1a3991,TODO: pytest fixtures and type hints to clean up unit testing,https://github.com/vc1492a/PyNomaly/commit/cba4cb6ded41c9b24a10cc7421155b171f1a3991,Yes
2423,vc1492a/PyNomaly,PyNomaly/loop.py,e60d39d79f7998290d9470194fd565f00987c88e,TODO: write unit test for the below,https://github.com/vc1492a/PyNomaly/commit/e60d39d79f7998290d9470194fd565f00987c88e,Yes
2424,polyrabbit/hacker-news-digest,page_content_extractor.py,bcbb1c2ab307d0790b42b1e40d537ef95311f8c4,TODO test when html_doc is empty,https://github.com/polyrabbit/hacker-news-digest/commit/bcbb1c2ab307d0790b42b1e40d537ef95311f8c4,Yes
2425,polyrabbit/hacker-news-digest,test/test_html_parser.py,5da671cc02600aafa626e2ae4bb46ccda8aabd36,def test_common_sites_xxx(self):,https://github.com/polyrabbit/hacker-news-digest/commit/5da671cc02600aafa626e2ae4bb46ccda8aabd36,No
2426,polyrabbit/hacker-news-digest,test/test_html_parser.py,e3a8ec87fb547bb6d1c2a555c359eb54d150cee5,TODO test longer html,https://github.com/polyrabbit/hacker-news-digest/commit/e3a8ec87fb547bb6d1c2a555c359eb54d150cee5,Yes
2427,loli/medpy,src/medpy/application/evaluate_labels.py,63d0245d412f5204efe84b712b60c16fa52cc3ec,!TODO: Volume apparently needs some more unittests to figure everything out,https://github.com/loli/medpy/commit/63d0245d412f5204efe84b712b60c16fa52cc3ec,Yes
2428,loli/medpy,src/medpy/filter/FitLabelsToMask.py,63d0245d412f5204efe84b712b60c16fa52cc3ec,!TODO: Write a unittest for this,https://github.com/loli/medpy/commit/63d0245d412f5204efe84b712b60c16fa52cc3ec,No
2429,loli/medpy,src/medpy/tests/metric/Surface.py,63d0245d412f5204efe84b712b60c16fa52cc3ec,!TODO: This is ran before each test case!,https://github.com/loli/medpy/commit/63d0245d412f5204efe84b712b60c16fa52cc3ec,Yes
2430,loli/medpy,src/medpy/filter/LabelImageStatistics.py,639f578913e355532cb95108d362821a703645a5,!TODO: Write a unittest for this,https://github.com/loli/medpy/commit/639f578913e355532cb95108d362821a703645a5,No
2431,loli/medpy,src/medpy/filter/MinimaExtraction.py,639f578913e355532cb95108d362821a703645a5,@TODO: Write a unittest for this.,https://github.com/loli/medpy/commit/639f578913e355532cb95108d362821a703645a5,Yes
2432,loli/medpy,src/medpy/unittests/metric/surface.py,639f578913e355532cb95108d362821a703645a5,!TODO: This is ran before each test case!,https://github.com/loli/medpy/commit/639f578913e355532cb95108d362821a703645a5,Yes
2433,loli/medpy,src/medpy/unittests/graphcut/graph.py,00fb39bc02ce45567575b80a6f4a9973f498312a,"\""\""\"" || Unittest for the medpy.graphcut.graph methods. ||  || !TODO: || - This is more a test for the medpy.graphcut.generate methods than for the Graph objects! ||   Change this by writing new tests. Export the old to medpy.unittests.graphcut.generate || - Update and run ||  || @author Oskar Maier || @version d0.2.0 || @since 2011-01-19 || @status Development || \""\""\""",https://github.com/loli/medpy/commit/00fb39bc02ce45567575b80a6f4a9973f498312a,Yes
2434,loli/medpy,src/medpy/application/testbed_watershed.py,77263060b08102c1ac0e13b07357e9ed6c6ec172,!TODO: Test if input image is of size 0,https://github.com/loli/medpy/commit/77263060b08102c1ac0e13b07357e9ed6c6ec172,No
2435,loli/medpy,src/medpy/filter/_FitLabelsToMask.py,77263060b08102c1ac0e13b07357e9ed6c6ec172,!TODO: Write a unittest for this,https://github.com/loli/medpy/commit/77263060b08102c1ac0e13b07357e9ed6c6ec172,No
2436,loli/medpy,src/medpy/graphcut/energy_voxel.py,77263060b08102c1ac0e13b07357e9ed6c6ec172,!TODO: Implement a test for the voxel based graph cut. The expected cut is to separate the left 3\/5 of the array,https://github.com/loli/medpy/commit/77263060b08102c1ac0e13b07357e9ed6c6ec172,Yes
2437,loli/medpy,src/medpy/graphcut/energy_voxel.py,77263060b08102c1ac0e13b07357e9ed6c6ec172,An additional good test would be a very short example of an image where 0 weights could occur (this should never happen).,https://github.com/loli/medpy/commit/77263060b08102c1ac0e13b07357e9ed6c6ec172,Yes
2438,loli/medpy,src/medpy/unittests/graphcut/cut.py,77263060b08102c1ac0e13b07357e9ed6c6ec172,"\""\""\"" || Unittest for the medpy.graphcut 's graph cut algorithm approach. || Essentially executes the whole pipeline from a supplied label image; foreground markers || and background markers over graph construction; cut execution until the final || re-labelling of the original label image. || One test if performed with the region bases; and one with the voxel based term. || The test is conducted with an artificial boundary term. ||  || !TODO: || - Update the test for the region based term to include boundary conditions || - Implement the test for the voxel based term ||  || @author Oskar Maier || @version d0.1.1 || @since 2011-01-29 || @status Development || \""\""\""",https://github.com/loli/medpy/commit/77263060b08102c1ac0e13b07357e9ed6c6ec172,Yes
2439,loli/medpy,src/medpy/unittests/graphcut/energy_voxel.py,77263060b08102c1ac0e13b07357e9ed6c6ec172,"\""\""\"" || Unittest for the medpy.graphcut.energy_voxel methods. ||  || !TODO: || - Write the test according to the example of the energy_label test ||   (All existing code is from there and has to be strongly adapted.) || - Update the description in medpy.graphcut.energy_voxel ||  || @author Oskar Maier || @version d0.0.1 || @since 2011-04-11 || @status Development || \""\""\""",https://github.com/loli/medpy/commit/77263060b08102c1ac0e13b07357e9ed6c6ec172,Yes
2440,loli/medpy,src/medpy/unittests/graphcut/generate.py,77263060b08102c1ac0e13b07357e9ed6c6ec172,"\""\""\"" || Unittest for the medpy.graphcut.generate methods. ||  || !TODO: || - Update and run ||  || @author Oskar Maier || @version d0.1.0 || @since 2011-01-26 || @status Development || \""\""\""",https://github.com/loli/medpy/commit/77263060b08102c1ac0e13b07357e9ed6c6ec172,Yes
2441,loli/medpy,src/medpy/unittests/graphcut/generate.py,3b85bceac01134b0cc5745420c745629af047a46,"\""\""\"" || Unittest for the medpy.graphcut.generate methods. ||  || !TODO: || - Create and implement tests for the generate_from_label and generate_from_voxel methods. ||  || @author Oskar Maier || @version d0.1.0 || @since 2011-01-26 || @status Development || \""\""\""",https://github.com/loli/medpy/commit/3b85bceac01134b0cc5745420c745629af047a46,No
2442,loli/medpy,tests/graphcut_/energy_voxel.py,919d2f830729b01c9300b615aa0f1c8ffa5ad596,"\""\""\"" || Unittest for the medpy.graphcut.energy_voxel methods. ||  || !TODO: || - Write tests for the other boundary difference term || - Write tests for the boundary maximum terms ||  ||  || @author Oskar Maier || @version r0.1.4 || @since 2011-04-11 || @status Release || \""\""\""",https://github.com/loli/medpy/commit/919d2f830729b01c9300b615aa0f1c8ffa5ad596,No
2443,loli/medpy,tests/graphcut_/generate.py,919d2f830729b01c9300b615aa0f1c8ffa5ad596,"\""\""\"" || Unittest for the medpy.graphcut.generate methods. ||  || !TODO: || - Create and implement tests for the generate_from_label and generate_from_voxel methods. ||  || @author Oskar Maier || @version d0.1.0 || @since 2011-01-26 || @status Development || \""\""\""",https://github.com/loli/medpy/commit/919d2f830729b01c9300b615aa0f1c8ffa5ad596,No
2444,loli/medpy,tests/graphcut_/graph.py,919d2f830729b01c9300b615aa0f1c8ffa5ad596,"\""\""\"" || Unittest for the medpy.graphcut.graph classes. ||  || !TODO: || - Implement the test_Graph() test for the Graph class. Follow the line along test_GCGraph. ||  || @author Oskar Maier || @version d0.2.1 || @since 2011-01-19 || @status Development || \""\""\""",https://github.com/loli/medpy/commit/919d2f830729b01c9300b615aa0f1c8ffa5ad596,Yes
2445,loli/medpy,tests/metric_/surface.py,919d2f830729b01c9300b615aa0f1c8ffa5ad596,!TODO: This is ran before each test case!,https://github.com/loli/medpy/commit/919d2f830729b01c9300b615aa0f1c8ffa5ad596,Yes
2446,loli/medpy,tests/run.py,919d2f830729b01c9300b615aa0f1c8ffa5ad596,!TODO: Fix tests for graphcut functionality,https://github.com/loli/medpy/commit/919d2f830729b01c9300b615aa0f1c8ffa5ad596,Yes
2447,loli/medpy,tests/run.py,919d2f830729b01c9300b615aa0f1c8ffa5ad596,!TODO: Fix tests for LabelImageStatistic functionality,https://github.com/loli/medpy/commit/919d2f830729b01c9300b615aa0f1c8ffa5ad596,No
2448,loli/medpy,medpy/filter/image.py,f438bdc55fb8b2f24b682ba7bca7d84bfe394209,@TODO: Write a unittest for this.,https://github.com/loli/medpy/commit/f438bdc55fb8b2f24b682ba7bca7d84bfe394209,Yes
2449,MaxHalford/xam,tests/test_check_estimator.py,c3673d1ad150965fcec8da0408f921ab7c7f17b1,This is needed for going through the same testing hoops as scikit-learn's Imputer,https://github.com/MaxHalford/xam/commit/c3673d1ad150965fcec8da0408f921ab7c7f17b1,No
2450,medipixel/rl_algorithms,tests/integration/test_run_distillation_agent.py,8b90ba85162ed0d69cf060770bfbaed92ee0f8bc,TODO: Add student training test code.,https://github.com/medipixel/rl_algorithms/commit/8b90ba85162ed0d69cf060770bfbaed92ee0f8bc,No
2451,brightmart/ai_law,predictor/HAN_model.py,1ddc6853194f18da0b2ada4afbdaf08662abc786,conv3=conv3+conv2[:;0:(self.total_sequence_length - filter_size*3+3);:;:] #TODO TEST IT; OTHERWISE NEED TO REMOVE ...........................,https://github.com/brightmart/ai_law/commit/1ddc6853194f18da0b2ada4afbdaf08662abc786,No
2452,brightmart/ai_law,data_util_hdf5.py,2e0cdd872df063b637f52e3cc7cfe0ddc12c9e8d,if not os.path.exists(cache_file): #TODO TODO TODO test 2018.07.05,https://github.com/brightmart/ai_law/commit/2e0cdd872df063b637f52e3cc7cfe0ddc12c9e8d,Yes
2453,brightmart/ai_law,predictor/model_single_task_1.py,0d6b73bc00e48981c53b36a740f45c7298b72e9c,conv3=conv3+conv2[:;0:(self.total_sequence_length - filter_size*3+3);:;:] #TODO TEST IT; OTHERWISE NEED TO REMOVE ...........................,https://github.com/brightmart/ai_law/commit/0d6b73bc00e48981c53b36a740f45c7298b72e9c,No
2454,dmbee/seglearn,seglearn/split.py,1b342820a03d7b822ad8e40b9248256235086dda,''' || This module has two classes for splitting time series data temporally - where train\/test or fold splits are created within each of the time series' in the time series data. This splitting approach is for evaluating how well the algorithm performs on segments drawn from the same time series but excluded from the training set. The performance from this splitting approach should be similar to performance on the training data so long as the data in each series is relatively uniform. || ''',https://github.com/dmbee/seglearn/commit/1b342820a03d7b822ad8e40b9248256235086dda,No
2455,inejc/paragraph-vectors,tests/test_loss.py,78bf1255c0feb6aa58752a629df0571b745b464e,todo: test actual value,https://github.com/inejc/paragraph-vectors/commit/78bf1255c0feb6aa58752a629df0571b745b464e,Yes
2456,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/callbacks/aggregators.py,f20c0f0542911575e6eac34dc1106638c21a6a84,"TODO: Record \""full_test_predictions\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
2457,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/environment.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Allow providing separate test_input; test_target dataframes; or the full df,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
2458,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/tests/experiments_tests.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Other tests for errors thrown by initialization of StandardCVExperiment,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
2459,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,from hyperparameter_hunter.tracers import TranslateTrace  # TODO: Add when tested with `Mirror`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,Yes
2460,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,"@TranslateTrace(\""model\""; (\""model_initializer\""; \""model_init_params\""))  # TODO: Add when tested with `Mirror`",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,Yes
2461,HunterMcGushion/hyperparameter_hunter,tests/test_metrics.py,7941a9fc4771da69066be17ef608a80757bf3474,TODO: Add test to verify `Metric.__call__` calls `Metric.metric_function` with expected inputs,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/7941a9fc4771da69066be17ef608a80757bf3474,No
2462,HunterMcGushion/hyperparameter_hunter,tests/test_workflows/test_general_workflows.py,657f3b5aaa0d735aa2dfccfab608774bda4b8707,TODO: 1-18-19 - Add test scenarios below that use `sentinels`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/657f3b5aaa0d735aa2dfccfab608774bda4b8707,Yes
2463,HunterMcGushion/hyperparameter_hunter,tests/test_optimization_utils.py,3cba8e3a42e9138ba41ca5cacfc373380f22112f,TODO: Add more tests dealing with Keras-specific issues like layers; callbacks and initializers,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/3cba8e3a42e9138ba41ca5cacfc373380f22112f,Yes
2464,HunterMcGushion/hyperparameter_hunter,tests/integration_tests/feature_engineering/test_both_stages_transform.py,128979b7df521d6cd6cf5639c2f4fb7ace220640,TODO: Basically identical to `test_intra_cv_target_transform` except for repeated KFold,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/128979b7df521d6cd6cf5639c2f4fb7ace220640,Yes
2465,HunterMcGushion/hyperparameter_hunter,tests/integration_tests/feature_engineering/test_both_stages_transform.py,128979b7df521d6cd6cf5639c2f4fb7ace220640,TODO: Only differences from `test_intra_cv_target_transform` are reps and above `chunked`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/128979b7df521d6cd6cf5639c2f4fb7ace220640,No
2466,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/data/data_core.py,aaea5bb6c0452f694e9da3206836d0152b490bb9,"\""\""\""This module defines mechanisms for managing an experiment's various datasets; and each datasets's || inputs; targets; and predictions. ||  || **Important Contents** ||  || In order to maintain the states of different datasets across all divisions of an experiment and || amid transformations that may be applied to the data via || :mod:`~hyperparameter_hunter.feature_engineering`; two main classes are defined herein: ||  || 1. :class:`BaseDataChunk`: ||  ||     * Logical separations between \""columns\"" of data for a given :class:`BaseDataset` ||     * Held and maintained by :class:`BaseDataset` and its descendants ||     * Three primary descendants of :class:`BaseDataChunk`: ||  ||         1. :class:`InputChunk`: Maintains a dataset's input data (and transformations) ||         2. :class:`TargetChunk`: Maintains a dataset's target data (and transformations) ||         3. :class:`PredictionChunk`: Maintains a dataset's predictions (and transformations) ||  ||     * Descendants of :class:`BaseDataChunk` should implement the eight \""on_<division>_<point>\"" ||       callback methods defined by :class:`~hyperparameter_hunter.callbacks.bases.BaseCallback` ||  ||         * Because :class:`BaseDataChunk` subclasses are isolated from the experiment; these methods ||           need not invoke their `super` methods; although they are allowed to if necessary ||  ||     * :class:`NullDataChunk` does nothing but mimic the normal :class:`BaseDataChunk` child structure ||  ||         * Used for :class:`BaseDataset` subclasses lacking a particular data chunk; such as: ||  ||             1) `TestDataset`'s `TargetChunk`; because the targets for a test dataset are unknown; or ||             2) `TrainDataset`'s `PredictionChunk`; because predictions are not made on training data ||  || 2. :class:`BaseDataset`: ||  ||     # TODO: ... ||  || **Dataset Attribute Syntax** ||  || The intricate subclass network bolstering the module's predominant :class:`BaseDataset` subclasses || may be intimidating at first; but don't worry; there's a shortcut. Follow these steps to ensure || proper syntax and a valid result when accessing data from a || :class:`~hyperparameter_hunter.experiments.CVExperiment`: ||  || 1. {`data_train`; `data_oof`; `data_holdout`; `data_test`} - Dataset attribute || 2. {`input`; `target`; `prediction`} - Data chunk || 3. [`T`] - Optional transformation || 4. {`d`; `run`; `fold`; `rep`; `final`} - Division; initial (`d`) or `final` data ||  || By stacking three values (four if following optional step \""3\"") from the above formula; you can || access all of the interesting stuff stored in the datasets from the comfort of your experiment or || :func:`~hyperparameter_hunter.callbacks.bases.lambda_callback`. ||  || Related || ------- || :mod:`hyperparameter_hunter.callbacks.bases` ||     This module defines the core callback method structure mirrored by :class:`BaseDataCore`. ||     Despite the strong logical connection to this module; it is important to remember that the only ||     actual connection between the two modules is in :mod:`hyperparameter_hunter.callbacks.wranglers` || :mod:`hyperparameter_hunter.callbacks.wranglers` ||     # TODO: ... Handlers for the `Dataset`s to invoke callback methods with required parameters ||     This module defines the callback classes that act as handlers for the descendants of ||     :class:`BaseDataset` || :mod:`hyperparameter_hunter.experiments` ||     # TODO: ... || \""\""\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/aaea5bb6c0452f694e9da3206836d0152b490bb9,Yes
2467,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/callbacks/wranglers/__init__.py,0a7cf589b414e9d45c902ed9ee0144c05291a333,"\""\""\""This module contains the final implementations of the three types of || :class:`~hyperparameter_hunter.callbacks.bases.BaseWranglerCallback` descendants. The callbacks || defined herein act as liaisons between the experiment and its datasets (the datasets' data chunks). || Each callback in the module is expected to be responsible for a specific descendant of || :class:`hyperparameter_hunter.data.data_core.BaseDataChunk`; which can be seen from the type || annotation at the forefront of each callback class for its \""data\""-prefixed attribute ||  || Each callback in the module is actually pulling its dataset (and the appropriate data chunk) from || the experiment via its four dataset attributes: ||  || * `data_train`: :class:`~hyperparameter_hunter.data.datasets.TrainDataset` || * `data_oof`: :class:`~hyperparameter_hunter.data.datasets.OOFDataset` || * `data_holdout`: :class:`~hyperparameter_hunter.data.datasets.HoldoutDataset` || * `data_test`: :class:`~hyperparameter_hunter.data.datasets.TestDataset` ||  || Specifically; each callback herein is responsible for the data chunk denoted by the name of that || callback's immediate parent callback; which is one of the following: ||  || * :class:`~hyperparameter_hunter.callbacks.bases.BaseInputWranglerCallback` || * :class:`~hyperparameter_hunter.callbacks.bases.BaseTargetWranglerCallback` || * :class:`~hyperparameter_hunter.callbacks.bases.BasePredictorCallback` ||  || Related || ------- || :mod:`hyperparameter_hunter.data` ||     This module defines the data chunks (attributes of datasets); for which each callback defined in ||     :mod:`~hyperparameter_hunter.callbacks.wranglers` is responsible. This responsibility is usually ||     satisfied by simply invoking the correct callback method. However; occasionally a data chunk's ||     callback method will require additional inputs. In these cases; the wrangler callbacks must ||     ensure the proper arguments are provided\""\""\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/0a7cf589b414e9d45c902ed9ee0144c05291a333,Yes
2468,HunterMcGushion/hyperparameter_hunter,tests/integration_tests/test_data_wranglers.py,f8db154886f5922ffc9ea955702341d44efd1fc5,return all_targets; label_encoder  # TODO: Test support,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f8db154886f5922ffc9ea955702341d44efd1fc5,No
2469,HunterMcGushion/hyperparameter_hunter,tests/integration_tests/test_data_wranglers.py,fd23afa156e4e56cde6d72f8e016d184bef83f42,OOFTargetChunk;  # TODO: Add tests,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/fd23afa156e4e56cde6d72f8e016d184bef83f42,Yes
2470,HunterMcGushion/hyperparameter_hunter,tests/integration_tests/test_data_wranglers.py,fd23afa156e4e56cde6d72f8e016d184bef83f42,HoldoutTargetChunk;  # TODO: Add tests,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/fd23afa156e4e56cde6d72f8e016d184bef83f42,Yes
2471,HunterMcGushion/hyperparameter_hunter,tests/integration_tests/feature_engineering/test_feature_optimization.py,75377b8e990afbeb1bd70ecf18eef5b7a5e2d97a,TODO: Expand below tests to multiple steps; multiple `optional`s; multiple categories for options,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/75377b8e990afbeb1bd70ecf18eef5b7a5e2d97a,Yes
2472,HunterMcGushion/hyperparameter_hunter,tests/test_optimization/test_backends/test_skopt/test_engine.py,4561dd50c7c92f8a7f664515b48eec97a7e5c037,TODO: Refactor - Split into separate error test,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/4561dd50c7c92f8a7f664515b48eec97a7e5c037,Yes
2473,HunterMcGushion/hyperparameter_hunter,tests/test_space/test_skopt_space.py,4561dd50c7c92f8a7f664515b48eec97a7e5c037,TODO: Refactor - Use PyTest - Break this up into multiple tests,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/4561dd50c7c92f8a7f664515b48eec97a7e5c037,No
2474,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization/protocol_core.py,4b1c458151dccf6c3b2cefa1c40a68c3ffa63456,TODO: Should make it easier\/faster to test some OptPro stuff by skipping optimization part,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/4b1c458151dccf6c3b2cefa1c40a68c3ffa63456,No
2475,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/environment.py,df50741a02aa5f68ad59e82e5bbba6ac1bc3ba12,TODO: Make `cv_params` optional. If not given; use default `cv_type` parameters. Test,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/df50741a02aa5f68ad59e82e5bbba6ac1bc3ba12,Yes
2476,oscarknagg/few-shot,tests/test_utils.py,e9f5f817edf7b3f57c3eaa7dd52e965ee0d78afa,TODO: Less brittle test,https://github.com/oscarknagg/few-shot/commit/e9f5f817edf7b3f57c3eaa7dd52e965ee0d78afa,No
2477,BenWhetton/keras-surgeon,tests/prune_test.py,92245aeefa61a022ae2065d20c43b8dd1afcf598,TODO: Add tests for flatten layer,https://github.com/BenWhetton/keras-surgeon/commit/92245aeefa61a022ae2065d20c43b8dd1afcf598,Yes
2478,BenWhetton/keras-surgeon,tests/prune_test.py,423a58b3e733d3a5ac886e7ea41dc21a105f8c2b,TODO: Concatenate tests (test for batch axis?),https://github.com/BenWhetton/keras-surgeon/commit/423a58b3e733d3a5ac886e7ea41dc21a105f8c2b,No
2479,BenWhetton/keras-surgeon,src/kerasprune/prune.py,9eef78fdc339386f2a9859856c978f025ed0fd93,TODO: This breaks layer sharing. Write a test for this.,https://github.com/BenWhetton/keras-surgeon/commit/9eef78fdc339386f2a9859856c978f025ed0fd93,Yes
2480,edublancas/sklearn-evaluation,doc/sphinxext/ipython_sphinxext/ipython_directive.py,74238f95ebe8fbaa62becadef967c6a0ee7989e7,"\""\""\"" || Sphinx directive to support embedded IPython code. ||  || This directive allows pasting of entire interactive IPython sessions; prompts || and all; and their code will actually get re-executed at doc build time; with || all prompts renumbered sequentially. It also allows you to input code as a pure || python input by giving the argument python to the directive. The output looks || like an interactive ipython section. ||  || To enable this directive; simply list it in your Sphinx ``conf.py`` file || (making sure the directory where you placed it is visible to sphinx; as is || needed for all Sphinx directives). For example; to enable syntax highlighting || and the IPython directive:: ||  ||     extensions = ['IPython.sphinxext.ipython_console_highlighting'; ||                   'IPython.sphinxext.ipython_directive'] ||  || The IPython directive outputs code-blocks with the language 'ipython'. So || if you do not have the syntax highlighting extension enabled as well; then || all rendered code-blocks will be uncolored. By default this directive assumes || that your prompts are unchanged IPython ones; but this can be customized. || The configurable options that can be placed in conf.py are: ||  || ipython_savefig_dir: ||     The directory in which to save the figures. This is relative to the ||     Sphinx source directory. The default is `html_static_path`. || ipython_rgxin: ||     The compiled regular expression to denote the start of IPython input ||     lines. The default is re.compile('In \\[(\\d+)\\]:\\s?(.*)\\s*'). You ||     shouldn't need to change this. || ipython_rgxout: ||     The compiled regular expression to denote the start of IPython output ||     lines. The default is re.compile('Out\\[(\\d+)\\]:\\s?(.*)\\s*'). You ||     shouldn't need to change this. || ipython_promptin: ||     The string to represent the IPython input prompt in the generated ReST. ||     The default is 'In [%d]:'. This expects that the line numbers are used ||     in the prompt. || ipython_promptout: ||     The string to represent the IPython prompt in the generated ReST. The ||     default is 'Out [%d]:'. This expects that the line numbers are used ||     in the prompt. || ipython_mplbackend: ||     The string which specifies if the embedded Sphinx shell should import ||     Matplotlib and set the backend. The value specifies a backend that is ||     passed to `matplotlib.use()` before any lines in `ipython_execlines` are ||     executed. If not specified in conf.py; then the default value of 'agg' is ||     used. To use the IPython directive without matplotlib as a dependency; set ||     the value to `None`. It may end up that matplotlib is still imported ||     if the user specifies so in `ipython_execlines` or makes use of the ||     @savefig pseudo decorator. || ipython_execlines: ||     A list of strings to be exec'd in the embedded Sphinx shell. Typical ||     usage is to make certain packages always available. Set this to an empty ||     list if you wish to have no imports always available. If specified in ||     conf.py as `None`; then it has the effect of making no imports available. ||     If omitted from conf.py altogether; then the default value of ||     ['import numpy as np'; 'import matplotlib.pyplot as plt'] is used. || ipython_holdcount ||     When the @suppress pseudo-decorator is used; the execution count can be ||     incremented or not. The default behavior is to hold the execution count; ||     corresponding to a value of `True`. Set this to `False` to increment ||     the execution count after each suppressed command. ||  || As an example; to use the IPython directive when `matplotlib` is not available; || one sets the backend to `None`:: ||  ||     ipython_mplbackend = None ||  || An example usage of the directive is: ||  || .. code-block:: rst ||  ||     .. ipython:: ||  ||         In [1]: x = 1 ||  ||         In [2]: y = x**2 ||  ||         In [3]: print(y) ||  || See http:\/\/matplotlib.org\/sampledoc\/ipython_directive.html for additional || documentation. ||  || Pseudo-Decorators || ================= ||  || Note: Only one decorator is supported per input. If more than one decorator || is specified; then only the last one is used. ||  || In addition to the Pseudo-Decorators\/options described at the above link; || several enhancements have been made. The directive will emit a message to the || console at build-time if code-execution resulted in an exception or warning. || You can suppress these on a per-block basis by specifying the :okexcept: || or :okwarning: options: ||  || .. code-block:: rst ||  ||     .. ipython:: ||         :okexcept: ||         :okwarning: ||  ||         In [1]: 1\/0 ||         In [2]: # raise warning. ||  || ToDo || ---- ||  || - Turn the ad-hoc test() function into a real test suite. || - Break up ipython-specific functionality from matplotlib stuff into better ||   separated code. ||  || Authors || ------- ||  || - John D Hunter: orignal author. || - Fernando Perez: refactoring; documentation; cleanups; port to 0.11. || - V\u00E1clav\u0160milauer <eudoxos-AT-arcig.cz>: Prompt generalizations. || - Skipper Seabold; refactoring; cleanups; pure python addition || \""\""\""",https://github.com/edublancas/sklearn-evaluation/commit/74238f95ebe8fbaa62becadef967c6a0ee7989e7,No
2481,edublancas/sklearn-evaluation,src/sklearn_evaluation/manage/SQLiteTracker.py,fe7a234d85f046067fa3294c413cc07a60761080,TODO: implement a function to get the latest experiments,https://github.com/edublancas/sklearn-evaluation/commit/fe7a234d85f046067fa3294c413cc07a60761080,Yes
2482,edublancas/sklearn-evaluation,noxfile.py,8b6f4d9bca5d1ceb9acd285b6c8458ea5307c5e3,this is needed to run tests,https://github.com/edublancas/sklearn-evaluation/commit/8b6f4d9bca5d1ceb9acd285b6c8458ea5307c5e3,No
2483,XuezheMax/NeuroNLP2,examples/posTagger.py,aaa8d676fe5564a7bfdc5a3b52dfaaa0ef5e5c67,evaluate on test data when better performance detected,https://github.com/XuezheMax/NeuroNLP2/commit/aaa8d676fe5564a7bfdc5a3b52dfaaa0ef5e5c67,No
2484,XuezheMax/NeuroNLP2,examples/NER.py,78ca1e052eaabf43525b7c34b556dad6758d39ed,evaluate on test data when better performance detected,https://github.com/XuezheMax/NeuroNLP2/commit/78ca1e052eaabf43525b7c34b556dad6758d39ed,No
2485,XuezheMax/NeuroNLP2,examples/posCRFTagger.py,308d2914438088769ca344429f360501147ed055,evaluate on test data when better performance detected,https://github.com/XuezheMax/NeuroNLP2/commit/308d2914438088769ca344429f360501147ed055,No
2486,XuezheMax/NeuroNLP2,examples/NERCRF.py,e7de35fbe97cba477b1b42dbc40647f853cb2d55,evaluate on test data when better performance detected,https://github.com/XuezheMax/NeuroNLP2/commit/e7de35fbe97cba477b1b42dbc40647f853cb2d55,No
2487,XuezheMax/NeuroNLP2,neuronlp2/nn/modules/attention.py,b7a553bac1857886a8a4f70d786c062a5af66750,TODO test it!,https://github.com/XuezheMax/NeuroNLP2/commit/b7a553bac1857886a8a4f70d786c062a5af66750,Yes
2488,XuezheMax/NeuroNLP2,experiments/pos_tagging.py,35cfce1ceacc59367eaed85e511a7e77e219cf5c,evaluate on test data when better performance detected,https://github.com/XuezheMax/NeuroNLP2/commit/35cfce1ceacc59367eaed85e511a7e77e219cf5c,No
2489,BloodAxe/pytorch-toolbelt,pytorch_toolbelt/losses/soft_f1.py,c59646a1f128e6f7e014952dbff3ea53188f93df,TODO: Test,https://github.com/BloodAxe/pytorch-toolbelt/commit/c59646a1f128e6f7e014952dbff3ea53188f93df,Yes
2490,ironman5366/W.I.L.L,will/nlp.py,2cf1fe3c9b6d9d113bd71976d8e89ccec1a8d167,"''' || plugins_left structure: || [ || {\""name\"": \""test\""; || \""ents_needed\"" : [\""PERSON\""; \""PHONE\""]; || \""structure\"" : {\""needed\"":[\""VERB\""]}; || \""questions_needed\"" : True} || ] || '''",https://github.com/ironman5366/W.I.L.L/commit/2cf1fe3c9b6d9d113bd71976d8e89ccec1a8d167,No
2491,ironman5366/W.I.L.L,will/plugins/jsonplugins.py,e967959e52921f0e8f30c0c348322c7b75a92f80,TODO: test this and make sure I can access it properly,https://github.com/ironman5366/W.I.L.L/commit/e967959e52921f0e8f30c0c348322c7b75a92f80,No
2492,ironman5366/W.I.L.L,plugins/chromecast.py,6fe01b4de6dd42c91223865db589238ceec5c3e1,TODO: change this to zope.testbrowser once it's working in the frontend,https://github.com/ironman5366/W.I.L.L/commit/6fe01b4de6dd42c91223865db589238ceec5c3e1,Yes
2493,voxelmorph/voxelmorph,ext/pytools-lib/pytools/iniparse.py,862b1426f153279b93a59487266d7aa56e07f90b,"\""\""\"" || very simple ini parser and tools ||  || tested on python 3.6 ||  || contact: adalca at csail.mit.edu ||  || TODO: see  ||   from collections import namedtuple ||   instead of Struct || \""\""\""",https://github.com/voxelmorph/voxelmorph/commit/862b1426f153279b93a59487266d7aa56e07f90b,Yes
2494,ResidentMario/py_d3,tests/test_services.py,df56442754e74897a060a9cab576d8c495360724,TODO: implement and test Blockbuilder API layer.,https://github.com/ResidentMario/py_d3/commit/df56442754e74897a060a9cab576d8c495360724,Yes
2495,robertmartin8/MachineLearningStocks,current_data.py,cbe5e181b980d2f3a75d8f5547b4012f444d4005,TODO:  testing? parsing?,https://github.com/robertmartin8/MachineLearningStocks/commit/cbe5e181b980d2f3a75d8f5547b4012f444d4005,No
2496,philipperemy/keras-tcn,mnist_pixel/tcn_mnist.py,eeda6a096375b151da3c8bd2fb913b77eac92260,TODO: remove this zero. just for testing.,https://github.com/philipperemy/keras-tcn/commit/eeda6a096375b151da3c8bd2fb913b77eac92260,No
2497,yaserkl/RLSeq2Seq,run_summarization.py,7d8ac8c8fe2b5733ba27e06044afadfaa8bdeb7a,hps_dict.update({'dqn_input_feature_len':(FLAGS.dec_hidden_dim+FLAGS.max_dec_steps)}) # TODO: more test on this; if wanna use time as a categorical feature,https://github.com/yaserkl/RLSeq2Seq/commit/7d8ac8c8fe2b5733ba27e06044afadfaa8bdeb7a,Yes
2498,xvjiarui/GCNet,mmdet/models/detectors/detector.py,088919f6cf930875589967f1e412e93dafc50258,TODO aug test haven't been verified,https://github.com/xvjiarui/GCNet/commit/088919f6cf930875589967f1e412e93dafc50258,No
2499,xvjiarui/GCNet,tools/test.py,088919f6cf930875589967f1e412e93dafc50258,TODO write single_test,https://github.com/xvjiarui/GCNet/commit/088919f6cf930875589967f1e412e93dafc50258,Yes
2500,xvjiarui/GCNet,mmdet/models/backbones/resnet.py,8500c14e41c85bea91b58622eaa8c96e4372f5e7,TODO can be removed after tested,https://github.com/xvjiarui/GCNet/commit/8500c14e41c85bea91b58622eaa8c96e4372f5e7,No
2501,xvjiarui/GCNet,mmdet/models/backbones/resnet.py,8a086f02106b63e4ea27c36028d72542d9822d12,TODO can be removed after tested,https://github.com/xvjiarui/GCNet/commit/8a086f02106b63e4ea27c36028d72542d9822d12,No
2502,deepmedic/deepmedic,deepmedic/frontEnd/configParsing/modelParams.py,07663bf1b5a9ae61cc93533a8f2f0c586832a1a9,TODO: Move config in train\/test cfg.,https://github.com/deepmedic/deepmedic/commit/07663bf1b5a9ae61cc93533a8f2f0c586832a1a9,No
2503,deepmedic/deepmedic,deepmedic/frontEnd/configParsing/trainSessionParams.py,a84f61744c77e8310dddab69f473211ffd0ced5f,TODO: Merge with same in testSessionParams,https://github.com/deepmedic/deepmedic/commit/a84f61744c77e8310dddab69f473211ffd0ced5f,Yes
2504,trevorstephens/gplearn,gplearn/skutils/tests/test_validation.py,8253491ac815c780aa43fb1af7a8ba59efa304a8,XXX: We should have a test with a string; but what is correct behaviour?,https://github.com/trevorstephens/gplearn/commit/8253491ac815c780aa43fb1af7a8ba59efa304a8,Yes
2505,trevorstephens/gplearn,gplearn/tests/test_common.py,8253491ac815c780aa43fb1af7a8ba59efa304a8,TODO: test with intercept,https://github.com/trevorstephens/gplearn/commit/8253491ac815c780aa43fb1af7a8ba59efa304a8,Yes
2506,trevorstephens/gplearn,gplearn/tests/test_common.py,8253491ac815c780aa43fb1af7a8ba59efa304a8,TODO: test with multiple responses,https://github.com/trevorstephens/gplearn/commit/8253491ac815c780aa43fb1af7a8ba59efa304a8,Yes
2507,brightmart/bert_language_understanding,train_bert_fine_tuning.py,29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,from model.bert_model import BertModel # TODO TODO TODO test whether pretrain can boost perofrmance with other model,https://github.com/brightmart/bert_language_understanding/commit/29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,Yes
2508,brightmart/bert_language_understanding,train_bert_lm.py,29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,from model.bert_model import BertModel # TODO TODO TODO test whether pretrain can boost perofrmance with other model,https://github.com/brightmart/bert_language_understanding/commit/29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,Yes
2509,namisan/mt-dnn,mt_dnn/model.py,e48e3c7a9c554c029244e5681978ab8d8d606183,TODO: updated the package to support the latest PyTorch (xiaodl),https://github.com/namisan/mt-dnn/commit/e48e3c7a9c554c029244e5681978ab8d8d606183,No
2510,DeepLabCut/DeepLabCut,deeplabcut/utils/conversioncode.py,e0bc796bd118057ea33b3793ae27d0686afddb5c,TODO: test len of images vs. len of imagenames for another sanity check,https://github.com/DeepLabCut/DeepLabCut/commit/e0bc796bd118057ea33b3793ae27d0686afddb5c,Yes
2511,JasonKessler/scattertext,.eggs/nose-1.3.7-py2.7.egg/nose/core.py,5a493ab05db91050e782242ab4d3c667bb0cb560,FIXME if self.module is a string; add it to self.testNames? not sure,https://github.com/JasonKessler/scattertext/commit/5a493ab05db91050e782242ab4d3c667bb0cb560,Yes
2512,JasonKessler/scattertext,.eggs/nose-1.3.7-py2.7.egg/nose/exc.py,5a493ab05db91050e782242ab4d3c667bb0cb560,"\""\""\""Exceptions for marking tests as skipped or deprecated. ||  || This module exists to provide backwards compatibility with previous || versions of nose where skipped and deprecated tests were core || functionality; rather than being provided by plugins. It may be || removed in a future release. || \""\""\""",https://github.com/JasonKessler/scattertext/commit/5a493ab05db91050e782242ab4d3c667bb0cb560,Yes
2513,JasonKessler/scattertext,.eggs/nose-1.3.7-py2.7.egg/nose/ext/dtcompat.py,5a493ab05db91050e782242ab4d3c667bb0cb560,"r\""\""\""Module doctest -- a framework for running examples in docstrings. ||  || In simplest use; end each module M to be tested with: ||  || def _test(): ||     import doctest ||     doctest.testmod() ||  || if __name__ == \""__main__\"": ||     _test() ||  || Then running the module as a script will cause the examples in the || docstrings to get executed and verified: ||  || python M.py ||  || This won't display anything unless an example fails; in which case the || failing example(s) and the cause(s) of the failure(s) are printed to stdout || (why not stderr? because stderr is a lame hack <0.2 wink>); and the final || line of output is \""Test failed.\"". ||  || Run it with the -v switch instead: ||  || python M.py -v ||  || and a detailed report of all examples tried is printed to stdout; along || with assorted summaries at the end. ||  || You can force verbose mode by passing \""verbose=True\"" to testmod; or prohibit || it by passing \""verbose=False\"".  In either of those cases; sys.argv is not || examined by testmod. ||  || There are a variety of other ways to run doctests; including integration || with the unittest framework; and support for running non-Python text || files containing doctests.  There are also many ways to override parts || of doctest's default behaviors.  See the Library Reference Manual for || details. || \""\""\""",https://github.com/JasonKessler/scattertext/commit/5a493ab05db91050e782242ab4d3c667bb0cb560,No
2514,JasonKessler/scattertext,.eggs/nose-1.3.7-py2.7.egg/nose/plugins/__init__.py,5a493ab05db91050e782242ab4d3c667bb0cb560,"\""\""\"" || Writing Plugins || --------------- ||  || nose supports plugins for test collection; selection; observation and || reporting. There are two basic rules for plugins: ||  || * Plugin classes should subclass :class:`nose.plugins.Plugin`. ||  || * Plugins may implement any of the methods described in the class ||   :doc:`IPluginInterface <interface>` in nose.plugins.base. Please note that ||   this class is for documentary purposes only; plugins may not subclass ||   IPluginInterface. ||  || Hello World || =========== ||  || Here's a basic plugin.  It doesn't do much so read on for more ideas or dive || into the :doc:`IPluginInterface <interface>` to see all available hooks. ||  || .. code-block:: python ||  ||     import logging ||     import os ||  ||     from nose.plugins import Plugin ||  ||     log = logging.getLogger('nose.plugins.helloworld') ||  ||     class HelloWorld(Plugin): ||         name = 'helloworld' ||  ||         def options(self; parser; env=os.environ): ||             super(HelloWorld; self).options(parser; env=env) ||  ||         def configure(self; options; conf): ||             super(HelloWorld; self).configure(options; conf) ||             if not self.enabled: ||                 return ||  ||         def finalize(self; result): ||             log.info('Hello pluginized world!') ||  || Registering || =========== ||  || .. Note:: ||   Important note: the following applies only to the default ||   plugin manager. Other plugin managers may use different means to ||   locate and load plugins. ||  || For nose to find a plugin; it must be part of a package that uses || setuptools_; and the plugin must be included in the entry points defined || in the setup.py for the package: ||  || .. code-block:: python ||  ||     setup(name='Some plugin'; ||         # ... ||         entry_points = { ||             'nose.plugins.0.10': [ ||                 'someplugin = someplugin:SomePlugin' ||                 ] ||             }; ||         # ... ||         ) ||  || Once the package is installed with install or develop; nose will be able || to load the plugin. ||  || .. _setuptools: http:\/\/peak.telecommunity.com\/DevCenter\/setuptools ||  || Registering a plugin without setuptools || ======================================= ||  || It is currently possible to register a plugin programmatically by || creating a custom nose runner like this : ||  || .. code-block:: python ||  ||     import nose ||     from yourplugin import YourPlugin ||  ||     if __name__ == '__main__': ||         nose.main(addplugins=[YourPlugin()]) ||  || Defining options || ================ ||  || All plugins must implement the methods ``options(self; parser; env)`` || and ``configure(self; options; conf)``. Subclasses of nose.plugins.Plugin || that want the standard options should call the superclass methods. ||  || nose uses optparse.OptionParser from the standard library to parse || arguments. A plugin's ``options()`` method receives a parser || instance. It's good form for a plugin to use that instance only to add || additional arguments that take only long arguments (--like-this). Most || of nose's built-in arguments get their default value from an environment || variable. ||  || A plugin's ``configure()`` method receives the parsed ``OptionParser`` options || object; as well as the current config object. Plugins should configure their || behavior based on the user-selected settings; and may raise exceptions || if the configured behavior is nonsensical. ||  || Logging || ======= ||  || nose uses the logging classes from the standard library. To enable users || to view debug messages easily; plugins should use ``logging.getLogger()`` to || acquire a logger in the ``nose.plugins`` namespace. ||  || Recipes || ======= ||  || * Writing a plugin that monitors or controls test result output ||  ||   Implement any or all of ``addError``; ``addFailure``; etc.; to monitor test ||   results. If you also want to monitor output; implement ||   ``setOutputStream`` and keep a reference to the output stream. If you ||   want to prevent the builtin ``TextTestResult`` output; implement ||   ``setOutputSteam`` and *return a dummy stream*. The default output will go ||   to the dummy stream; while you send your desired output to the real stream. ||  ||   Example: `examples\/html_plugin\/htmlplug.py`_ ||  || * Writing a plugin that handles exceptions ||  ||   Subclass :doc:`ErrorClassPlugin <errorclasses>`. ||  ||   Examples: :doc:`nose.plugins.deprecated <deprecated>`; ||   :doc:`nose.plugins.skip <skip>` ||  || * Writing a plugin that adds detail to error reports ||  ||   Implement ``formatError`` and\/or ``formatFailure``. The error tuple ||   you return (error class; error message; traceback) will replace the ||   original error tuple. ||  ||   Examples: :doc:`nose.plugins.capture <capture>`; ||   :doc:`nose.plugins.failuredetail <failuredetail>` ||  || * Writing a plugin that loads tests from files other than python modules ||  ||   Implement ``wantFile`` and ``loadTestsFromFile``. In ``wantFile``; ||   return True for files that you want to examine for tests. In ||   ``loadTestsFromFile``; for those files; return an iterable ||   containing TestCases (or yield them as you find them; ||   ``loadTestsFromFile`` may also be a generator). ||  ||   Example: :doc:`nose.plugins.doctests <doctests>` ||  || * Writing a plugin that prints a report ||  ||   Implement ``begin`` if you need to perform setup before testing ||   begins. Implement ``report`` and output your report to the provided stream. ||  ||   Examples: :doc:`nose.plugins.cover <cover>`; :doc:`nose.plugins.prof <prof>` ||  || * Writing a plugin that selects or rejects tests ||  ||   Implement any or all ``want*``  methods. Return False to reject the test ||   candidate; True to accept it -- which  means that the test candidate ||   will pass through the rest of the system; so you must be prepared to ||   load tests from it if tests can't be loaded by the core loader or ||   another plugin -- and None if you don't care. ||  ||   Examples: :doc:`nose.plugins.attrib <attrib>`; ||   :doc:`nose.plugins.doctests <doctests>`; :doc:`nose.plugins.testid <testid>` ||  ||  || More Examples || ============= ||  || See any builtin plugin or example plugin in the examples_ directory in || the nose source distribution. There is a list of third-party plugins || `on jottit`_. ||  || .. _examples\/html_plugin\/htmlplug.py: http:\/\/python-nose.googlecode.com\/svn\/trunk\/examples\/html_plugin\/htmlplug.py || .. _examples: http:\/\/python-nose.googlecode.com\/svn\/trunk\/examples || .. _on jottit: http:\/\/nose-plugins.jottit.com\/ ||  || \""\""\""",https://github.com/JasonKessler/scattertext/commit/5a493ab05db91050e782242ab4d3c667bb0cb560,Yes
2515,JasonKessler/scattertext,.eggs/nose-1.3.7-py2.7.egg/nose/plugins/collect.py,5a493ab05db91050e782242ab4d3c667bb0cb560,"\""\""\"" || This plugin bypasses the actual execution of tests; and instead just collects || test names. Fixtures are also bypassed; so running nosetests with the  || collection plugin enabled should be very quick. ||  || This plugin is useful in combination with the testid plugin (``--with-id``). || Run both together to get an indexed list of all tests; which will enable you to || run individual tests by index number. ||  || This plugin is also useful for counting tests in a test suite; and making || people watching your demo think all of your tests pass. || \""\""\""",https://github.com/JasonKessler/scattertext/commit/5a493ab05db91050e782242ab4d3c667bb0cb560,No
2516,JasonKessler/scattertext,.eggs/nose-1.3.7-py2.7.egg/nose/plugins/doctests.py,5a493ab05db91050e782242ab4d3c667bb0cb560,FIXME this breaks the id plugin somehow (tests probably don't,https://github.com/JasonKessler/scattertext/commit/5a493ab05db91050e782242ab4d3c667bb0cb560,Yes
2517,JasonKessler/scattertext,.eggs/nose-1.3.7-py2.7.egg/nose/plugins/multiprocess.py,5a493ab05db91050e782242ab4d3c667bb0cb560,"\""\""\"" || Overview || ======== ||  || The multiprocess plugin enables you to distribute your test run among a set of || worker processes that run tests in parallel. This can speed up CPU-bound test || runs (as long as the number of work processeses is around the number of || processors or cores available); but is mainly useful for IO-bound tests that || spend most of their time waiting for data to arrive from someplace else. ||  || .. note :: ||  ||    See :doc:`..\/doc_tests\/test_multiprocess\/multiprocess` for ||    additional documentation and examples. Use of this plugin on python ||    2.5 or earlier requires the multiprocessing_ module; also available ||    from PyPI. ||  || .. _multiprocessing : http:\/\/code.google.com\/p\/python-multiprocessing\/ ||  || How tests are distributed || ========================= ||  || The ideal case would be to dispatch each test to a worker process || separately. This ideal is not attainable in all cases; however; because many || test suites depend on context (class; module or package) fixtures. ||  || The plugin can't know (unless you tell it -- see below!) if a context fixture || can be called many times concurrently (is re-entrant); or if it can be shared || among tests running in different processes. Therefore; if a context has || fixtures; the default behavior is to dispatch the entire suite to a worker as || a unit. ||  || Controlling distribution || ^^^^^^^^^^^^^^^^^^^^^^^^ ||  || There are two context-level variables that you can use to control this default || behavior. ||  || If a context's fixtures are re-entrant; set ``_multiprocess_can_split_ = True`` || in the context; and the plugin will dispatch tests in suites bound to that || context as if the context had no fixtures. This means that the fixtures will || execute concurrently and multiple times; typically once per test. ||  || If a context's fixtures can be shared by tests running in different processes || -- such as a package-level fixture that starts an external http server or || initializes a shared database -- then set ``_multiprocess_shared_ = True`` in || the context. These fixtures will then execute in the primary nose process; and || tests in those contexts will be individually dispatched to run in parallel. ||  || How results are collected and reported || ====================================== ||  || As each test or suite executes in a worker process; results (failures; errors; || and specially handled exceptions like SkipTest) are collected in that || process. When the worker process finishes; it returns results to the main || nose process. There; any progress output is printed (dots!); and the || results from the test run are combined into a consolidated result || set. When results have been received for all dispatched tests; or all || workers have died; the result summary is output as normal. ||  || Beware! || ======= ||  || Not all test suites will benefit from; or even operate correctly using; this || plugin. For example; CPU-bound tests will run more slowly if you don't have || multiple processors. There are also some differences in plugin || interactions and behaviors due to the way in which tests are dispatched and || loaded. In general; test loading under this plugin operates as if it were || always in directed mode instead of discovered mode. For instance; doctests || in test modules will always be found when using this plugin with the doctest || plugin. ||  || But the biggest issue you will face is probably concurrency. Unless you || have kept your tests as religiously pure unit tests; with no side-effects; no || ordering issues; and no external dependencies; chances are you will experience || odd; intermittent and unexplainable failures and errors when using this || plugin. This doesn't necessarily mean the plugin is broken; it may mean that || your test suite is not safe for concurrency. ||  || New Features in 1.1.0 || ===================== ||  || * functions generated by test generators are now added to the worker queue ||   making them multi-threaded. || * fixed timeout functionality; now functions will be terminated with a ||   TimedOutException exception when they exceed their execution time. The ||   worker processes are not terminated. || * added ``--process-restartworker`` option to restart workers once they are ||   done; this helps control memory usage. Sometimes memory leaks can accumulate ||   making long runs very difficult. || * added global _instantiate_plugins to configure which plugins are started ||   on the worker processes. ||  || \""\""\""",https://github.com/JasonKessler/scattertext/commit/5a493ab05db91050e782242ab4d3c667bb0cb560,Yes
2518,JasonKessler/scattertext,.eggs/nose-1.3.7-py2.7.egg/nose/twistedtools.py,5a493ab05db91050e782242ab4d3c667bb0cb560,"\""\""\"" || Twisted integration || ------------------- ||  || This module provides a very simple way to integrate your tests with the || Twisted_ event loop. ||  || You must import this module *before* importing anything from Twisted itself! ||  || Example:: ||  ||   from nose.twistedtools import reactor; deferred ||    ||   @deferred() ||   def test_resolve(): ||       return reactor.resolve(\""www.python.org\"") ||  || Or; more realistically:: ||  ||   @deferred(timeout=5.0) ||   def test_resolve(): ||       d = reactor.resolve(\""www.python.org\"") ||       def check_ip(ip): ||           assert ip == \""67.15.36.43\"" ||       d.addCallback(check_ip) ||       return d ||  || .. _Twisted: http:\/\/twistedmatrix.com\/trac\/ || \""\""\""",https://github.com/JasonKessler/scattertext/commit/5a493ab05db91050e782242ab4d3c667bb0cb560,Yes
2519,JasonKessler/scattertext,python27/lib/python2.7/site.py,5a493ab05db91050e782242ab4d3c667bb0cb560,encoding after initialization.  The test for presence is needed when,https://github.com/JasonKessler/scattertext/commit/5a493ab05db91050e782242ab4d3c667bb0cb560,Yes
2520,maxpumperla/elephas,tests/mllib/test_adapter.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test mllib adapter\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,Yes
2521,maxpumperla/elephas,tests/parameter/test_client.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test clients\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,Yes
2522,maxpumperla/elephas,tests/parameter/test_server.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test servers\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,Yes
2523,maxpumperla/elephas,tests/test_ml_model.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test basic ml model\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,No
2524,maxpumperla/elephas,tests/test_optimizers.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO: test optimizers\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,No
2525,maxpumperla/elephas,tests/test_spark_model.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test basic spark model\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,No
2526,maxpumperla/elephas,tests/test_worker.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test workers\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,Yes
2527,maxpumperla/elephas,tests/utils/test_rwlock.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test lock\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,No
2528,maxpumperla/elephas,tests/utils/test_sockets.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test sockets\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,Yes
2529,shenweichen/DeepCTR,tests/models/FNN_test.py,8182ea386e6529a1a2294d8e2d33fc040d0cbfb2,x; y; feature_columns = get_test_data(sample_size; sparse_feature_num; dense_feature_num; sequence_feature=()),https://github.com/shenweichen/DeepCTR/commit/8182ea386e6529a1a2294d8e2d33fc040d0cbfb2,No
2530,shenweichen/DeepCTR,tests/models/DIN_test.py,db229dc31f0d4c79c0de2ece0bb919b35258d6b2,todo test dice,https://github.com/shenweichen/DeepCTR/commit/db229dc31f0d4c79c0de2ece0bb919b35258d6b2,Yes
2531,shenweichen/DeepCTR,tests/models/FGCNN_test.py,2d720403e43ccbf2286c99876ce3bb8a9286f5c6,x; y; feature_columns = get_test_data(sample_size; sparse_feature_num=sparse_feature_num;,https://github.com/shenweichen/DeepCTR/commit/2d720403e43ccbf2286c99876ce3bb8a9286f5c6,Yes
2532,AKSHAYUBHAT/DeepVideoAnalytics,repos/object_detection/create_pet_tf_record.py,e1068503267ee475d33e77523b049165a6548614,TODO: Add test for pet\/PASCAL main files.,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/e1068503267ee475d33e77523b049165a6548614,Yes
2533,AKSHAYUBHAT/DeepVideoAnalytics,repos/object_detection/models/feature_map_generators_test.py,e1068503267ee475d33e77523b049165a6548614,TODO: add tests with different anchor strides.,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/e1068503267ee475d33e77523b049165a6548614,Yes
2534,AKSHAYUBHAT/DeepVideoAnalytics,repos/tfdetection/object_detection/create_pet_tf_record.py,54370fcfcd1b6fe237fe004045103fd12da756f1,TODO: Add test for pet\/PASCAL main files.,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/54370fcfcd1b6fe237fe004045103fd12da756f1,Yes
2535,AKSHAYUBHAT/DeepVideoAnalytics,repos/tfdetection/object_detection/models/feature_map_generators_test.py,54370fcfcd1b6fe237fe004045103fd12da756f1,TODO: add tests with different anchor strides.,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/54370fcfcd1b6fe237fe004045103fd12da756f1,Yes
2536,AKSHAYUBHAT/DeepVideoAnalytics,repos/object_detection/models/feature_map_generators_test.py,19b2103de8a502a6569c6a1dce0d75c5465d66dd,TODO: add tests with different anchor strides.,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/19b2103de8a502a6569c6a1dce0d75c5465d66dd,Yes
2537,lmcinnes/umap,examples/plot_feature_extraction_classification.py,7963e426e2d1f58105d8712c0379114d93d32b07,"\""\""\"" || UMAP as a Feature Extraction Technique for Classification || --------------------------------------------------------- ||  || The following script shows how UMAP can be used as a feature extraction || technique to improve the accuracy on a classification task. It also shows || how UMAP can be integrated in standard scikit-learn pipelines. ||  || The first step is to create a dataset for a classification task; which is || performed with the function ``sklearn.datasets.make_classification``. The || dataset is then split into a training set and a test set using the || ``sklearn.model_selection.train_test_split`` function. ||  || Second; a linear SVM is fitted on the training set. To choose the best || hyperparameters automatically; a gridsearch is performed on the training set. || The performance of the model is then evaluated on the test set with the || accuracy metric. ||  ||  Third; the previous step is repeated with a slight modification: UMAP is ||  used as a feature extraction technique. This small change results in a ||  substantial improvement compared to the model where raw data is used. || \""\""\""",https://github.com/lmcinnes/umap/commit/7963e426e2d1f58105d8712c0379114d93d32b07,Yes
2538,lmcinnes/umap,umap/tests/__init__.py,995dfe8464d3ac74c1587b28b82e40b6703792a3,"\""\""\"" || Tests for UMAP to ensure things are working as expected. ||  || The test suite has been refactored to support PyTest. || This allow to prefer test fixtures over global variable. || All the fixtures are defined in the `conftest.py` module. ||  || Moreover; test cases have been re-organised in different || sections (implemented as different modules) || according to the specific set of UMAP features under test. ||  || Each test module\/section include a set of utility functions - || defined on top of each section - which are meant to define the || core processing instructions required by (most of) the tests || so to avoid code clones (repetitions) as much as possible. || This is to make the testing code easier to maintain. ||  || Moreover; the multiple testing sections\/modules are well-integrated || each other; as data are now pytest fixtures and no more global variables. ||  || Therefore: || - easy to run specific (subsets) of tests || - easy to add additional tests to specific sections || - avoiding code repetitions and multiple dependencies ||     (pytest will handle fixtures deps auto-magically) ||  || \""\""\""",https://github.com/lmcinnes/umap/commit/995dfe8464d3ac74c1587b28b82e40b6703792a3,Yes
2539,junyanz/iGAN,lib/AlexNet.py,2fe23eeaeb18910693bea16d021e52da01787635,[hack] to-be-tested,https://github.com/junyanz/iGAN/commit/2fe23eeaeb18910693bea16d021e52da01787635,Yes
2540,lanpa/tensorboardX,tests/test_caffe2.py,3858986041e9f57ec2bfcbb7fba69d17a37655bb,TODO: Add test for show_simplified=True.,https://github.com/lanpa/tensorboardX/commit/3858986041e9f57ec2bfcbb7fba69d17a37655bb,Yes
2541,sloria/TextBlob,nltk-3.0a0/nltk/chunk/__init__.py,d539a164ed1cac8239b632a778946248a00c0c42,"\""\""\"" || Classes and interfaces for identifying non-overlapping linguistic || groups (such as base noun phrases) in unrestricted text.  This task is || called \""chunk parsing\"" or \""chunking\""; and the identified groups are || called \""chunks\"".  The chunked text is represented using a shallow || tree called a \""chunk structure.\""  A chunk structure is a tree || containing tokens and chunks; where each chunk is a subtree containing || only tokens.  For example; the chunk structure for base noun phrase || chunks in the sentence \""I saw the big dog on the hill\"" is:: ||  ||   (SENTENCE: ||     (NP: <I>) ||     <saw> ||     (NP: <the> <big> <dog>) ||     <on> ||     (NP: <the> <hill>)) ||  || To convert a chunk structure back to a list of tokens; simply use the || chunk structure's ``leaves()`` method. ||  || This module defines ``ChunkParserI``; a standard interface for || chunking texts; and ``RegexpChunkParser``; a regular-expression based || implementation of that interface. It also defines ``ChunkScore``; a || utility class for scoring chunk parsers. ||  || RegexpChunkParser || ================= ||  || ``RegexpChunkParser`` is an implementation of the chunk parser interface || that uses regular-expressions over tags to chunk a text.  Its || ``parse()`` method first constructs a ``ChunkString``; which encodes a || particular chunking of the input text.  Initially; nothing is || chunked.  ``parse.RegexpChunkParser`` then applies a sequence of || ``RegexpChunkRule`` rules to the ``ChunkString``; each of which modifies || the chunking that it encodes.  Finally; the ``ChunkString`` is || transformed back into a chunk structure; which is returned. ||  || ``RegexpChunkParser`` can only be used to chunk a single kind of phrase. || For example; you can use an ``RegexpChunkParser`` to chunk the noun || phrases in a text; or the verb phrases in a text; but you can not || use it to simultaneously chunk both noun phrases and verb phrases in || the same text.  (This is a limitation of ``RegexpChunkParser``; not of || chunk parsers in general.) ||  || RegexpChunkRules || ---------------- ||  || A ``RegexpChunkRule`` is a transformational rule that updates the || chunking of a text by modifying its ``ChunkString``.  Each || ``RegexpChunkRule`` defines the ``apply()`` method; which modifies || the chunking encoded by a ``ChunkString``.  The || ``RegexpChunkRule`` class itself can be used to implement any || transformational rule based on regular expressions.  There are || also a number of subclasses; which can be used to implement || simpler types of rules: ||  ||     - ``ChunkRule`` chunks anything that matches a given regular ||       expression. ||     - ``ChinkRule`` chinks anything that matches a given regular ||       expression. ||     - ``UnChunkRule`` will un-chunk any chunk that matches a given ||       regular expression. ||     - ``MergeRule`` can be used to merge two contiguous chunks. ||     - ``SplitRule`` can be used to split a single chunk into two ||       smaller chunks. ||     - ``ExpandLeftRule`` will expand a chunk to incorporate new ||       unchunked material on the left. ||     - ``ExpandRightRule`` will expand a chunk to incorporate new ||       unchunked material on the right. ||  || Tag Patterns || ~~~~~~~~~~~~ ||  || A ``RegexpChunkRule`` uses a modified version of regular || expression patterns; called \""tag patterns\"".  Tag patterns are || used to match sequences of tags.  Examples of tag patterns are:: ||  ||      r'(<DT>|<JJ>|<NN>)+' ||      r'<NN>+' ||      r'<NN.*>' ||  || The differences between regular expression patterns and tag || patterns are: ||  ||     - In tag patterns; ``'<'`` and ``'>'`` act as parentheses; so ||       ``'<NN>+'`` matches one or more repetitions of ``'<NN>'``; not ||       ``'<NN'`` followed by one or more repetitions of ``'>'``. ||     - Whitespace in tag patterns is ignored.  So ||       ``'<DT> | <NN>'`` is equivalant to ``'<DT>|<NN>'`` ||     - In tag patterns; ``'.'`` is equivalant to ``'[^{}<>]'``; so ||       ``'<NN.*>'`` matches any single tag starting with ``'NN'``. ||  || The function ``tag_pattern2re_pattern`` can be used to transform || a tag pattern to an equivalent regular expression pattern. ||  || Efficiency || ---------- ||  || Preliminary tests indicate that ``RegexpChunkParser`` can chunk at a || rate of about 300 tokens\/second; with a moderately complex rule set. ||  || There may be problems if ``RegexpChunkParser`` is used with more than || 5;000 tokens at a time.  In particular; evaluation of some regular || expressions may cause the Python regular expression engine to || exceed its maximum recursion depth.  We have attempted to minimize || these problems; but it is impossible to avoid them completely.  We || therefore recommend that you apply the chunk parser to a single || sentence at a time. ||  || Emacs Tip || --------- ||  || If you evaluate the following elisp expression in emacs; it will || colorize a ``ChunkString`` when you use an interactive python shell || with emacs or xemacs (\""C-c !\""):: ||  ||     (let () ||       (defconst comint-mode-font-lock-keywords ||         '((\""<[^>]+>\"" 0 'font-lock-reference-face) ||           (\""[{}]\"" 0 'font-lock-function-name-face))) ||       (add-hook 'comint-mode-hook (lambda () (turn-on-font-lock)))) ||  || You can evaluate this code by copying it to a temporary buffer; || placing the cursor after the last close parenthesis; and typing || \""``C-x C-e``\"".  You should evaluate it before running the interactive || session.  The change will last until you close emacs. ||  || Unresolved Issues || ----------------- ||  || If we use the ``re`` module for regular expressions; Python's || regular expression engine generates \""maximum recursion depth || exceeded\"" errors when processing very large texts; even for || regular expressions that should not require any recursion.  We || therefore use the ``pre`` module instead.  But note that ``pre`` || does not include Unicode support; so this module will not work || with unicode strings.  Note also that ``pre`` regular expressions || are not quite as advanced as ``re`` ones (e.g.; no leftward || zero-length assertions). ||  || :type CHUNK_TAG_PATTERN: regexp || :var CHUNK_TAG_PATTERN: A regular expression to test whether a tag ||      pattern is valid. || \""\""\""",https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
2542,sloria/TextBlob,nltk-3.0a0/nltk/classify/mallet.py,d539a164ed1cac8239b632a778946248a00c0c42,"\""\""\"" || A set of functions used to interface with the external Mallet_ machine learning || package. Before mallet can be used; you should tell NLTK where it can find || the mallet package; using the ``config_mallet()`` function. Typical usage: ||  ||     >>> from nltk.classify import mallet ||     >>> mallet.config_mallet() # pass path to mallet as argument if needed # doctest: +SKIP ||     [Found mallet: ...] ||  || .. _Mallet: http:\/\/mallet.cs.umass.edu\/ || \""\""\""",https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
2543,sloria/TextBlob,nltk-3.0a0/nltk/corpus/util.py,d539a164ed1cac8239b632a778946248a00c0c42,Without this fix tests may take extra 1.5GB RAM,https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,No
2544,sloria/TextBlob,nltk-3.0a0/nltk/data.py,d539a164ed1cac8239b632a778946248a00c0c42,XXX: ``path`` must be a bytestring under Python 2.x because,https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
2545,sloria/TextBlob,nltk-3.0a0/nltk/downloader.py,d539a164ed1cac8239b632a778946248a00c0c42,"\""\""\"" || The NLTK corpus and module downloader.  This module defines several || interfaces which can be used to download corpora; models; and other || data packages that can be used with NLTK. ||  || Downloading Packages || ==================== || If called with no arguments; ``download()`` will display an interactive || interface which can be used to download and install new packages. || If Tkinter is available; then a graphical interface will be shown; || otherwise a simple text interface will be provided. ||  || Individual packages can be downloaded by calling the ``download()`` || function with a single argument; giving the package identifier for the || package that should be downloaded: ||  ||     >>> download('treebank') # doctest: +SKIP ||     [nltk_data] Downloading package 'treebank'... ||     [nltk_data]   Unzipping corpora\/treebank.zip. ||  || NLTK also provides a number of \\\""package collections\\\""; consisting of || a group of related packages.  To download all packages in a || colleciton; simply call ``download()`` with the collection's || identifier: ||  ||     >>> download('all-corpora') # doctest: +SKIP ||     [nltk_data] Downloading package 'abc'... ||     [nltk_data]   Unzipping corpora\/abc.zip. ||     [nltk_data] Downloading package 'alpino'... ||     [nltk_data]   Unzipping corpora\/alpino.zip. ||       ... ||     [nltk_data] Downloading package 'words'... ||     [nltk_data]   Unzipping corpora\/words.zip. ||  || Download Directory || ================== || By default; packages are installed in either a system-wide directory || (if Python has sufficient access to write to it); or in the current || user's home directory.  However; the ``download_dir`` argument may be || used to specify a different installation target; if desired. ||  || See ``Downloader.default_download_dir()`` for more a detailed || description of how the default download directory is chosen. ||  || NLTK Download Server || ==================== || Before downloading any packages; the corpus and module downloader || contacts the NLTK download server; to retrieve an index file || describing the available packages.  By default; this index file is || loaded from ``http:\/\/nltk.googlecode.com\/svn\/trunk\/nltk_data\/index.xml``. || If necessary; it is possible to create a new ``Downloader`` object; || specifying a different URL for the package index file. ||  || Usage:: ||  ||     python nltk\/downloader.py [-d DATADIR] [-q] [-f] [-k] PACKAGE_IDS ||  || or:: ||  ||     python -m nltk.downloader [-d DATADIR] [-q] [-f] [-k] PACKAGE_IDS || \""\""\""",https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
2546,sloria/TextBlob,nltk-3.0a0/nltk/sem/boxer.py,d539a164ed1cac8239b632a778946248a00c0c42,"\""\""\"" || An interface to Boxer. ||  || This interface relies on the latest version of the development (subversion) version of || C&C and Boxer. ||  || Usage: ||   Set the environment variable CANDCHOME to the bin directory of your CandC installation. ||   The models directory should be in the CandC root directory. ||   For example: ||      \/path\/to\/candc\/ ||         bin\/ ||             candc ||             boxer ||         models\/ ||             boxer\/ || \""\""\""",https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
2547,sloria/TextBlob,nltk-3.0a0/nltk/test/discourse_fixt.py,d539a164ed1cac8239b632a778946248a00c0c42,FIXME: the entire discourse.doctest is skipped if Prover9\/Mace4 is,https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
2548,sloria/TextBlob,nltk/test/discourse_fixt.py,048a354a62524ee5b7501350c2a699cfd6d0d0f7,FIXME: the entire discourse.doctest is skipped if Prover9\/Mace4 is,https://github.com/sloria/TextBlob/commit/048a354a62524ee5b7501350c2a699cfd6d0d0f7,Yes
2549,aleju/imgaug,ImageAugmenter.py,b720dcb1c701241df36fbd716f568ccbd4fad6c8,TODO this is untested but is the same as in augment_batch,https://github.com/aleju/imgaug/commit/b720dcb1c701241df36fbd716f568ccbd4fad6c8,Yes
2550,aleju/imgaug,imgaug/augmenters.py,5bf4689eee28b032f48e659e710d80536bbe8658,TODO tests,https://github.com/aleju/imgaug/commit/5bf4689eee28b032f48e659e710d80536bbe8658,Yes
2551,aleju/imgaug,imgaug/augmenters.py,79396fd60be87a7c4a4d0078d3e7589efc2c9e9b,TODO tests,https://github.com/aleju/imgaug/commit/79396fd60be87a7c4a4d0078d3e7589efc2c9e9b,Yes
2552,aleju/imgaug,imgaug/augmenters.py,b64cb5a84e6d061b696eca7c54efee57a53e510f,TODO test for 2D images,https://github.com/aleju/imgaug/commit/b64cb5a84e6d061b696eca7c54efee57a53e510f,Yes
2553,aleju/imgaug,imgaug/augmenters.py,b64cb5a84e6d061b696eca7c54efee57a53e510f,TODO test with C = 1,https://github.com/aleju/imgaug/commit/b64cb5a84e6d061b696eca7c54efee57a53e510f,No
2554,aleju/imgaug,imgaug/augmenters/meta.py,ec46c7e629d9fc233f89f12cc51df00409697bd3,TODO test for 2D images,https://github.com/aleju/imgaug/commit/ec46c7e629d9fc233f89f12cc51df00409697bd3,Yes
2555,aleju/imgaug,imgaug/augmenters/meta.py,ec46c7e629d9fc233f89f12cc51df00409697bd3,TODO test with C = 1,https://github.com/aleju/imgaug/commit/ec46c7e629d9fc233f89f12cc51df00409697bd3,No
2556,aleju/imgaug,imgaug/augmenters/segmentation.py,ec46c7e629d9fc233f89f12cc51df00409697bd3,TODO tests,https://github.com/aleju/imgaug/commit/ec46c7e629d9fc233f89f12cc51df00409697bd3,Yes
2557,aleju/imgaug,imgaug/augmenters/overlay.py,603fad3ddb0df7519efb20993f6ed95c4367b175,TODO tests,https://github.com/aleju/imgaug/commit/603fad3ddb0df7519efb20993f6ed95c4367b175,Yes
2558,aleju/imgaug,tests/test.py,36031a108e0ab26da1920fb38ba14008e5a5582d,test is needed here,https://github.com/aleju/imgaug/commit/36031a108e0ab26da1920fb38ba14008e5a5582d,No
2559,aleju/imgaug,tests/test.py,b3234afaf7237fb88116b4e421cfa587effa66a7,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/b3234afaf7237fb88116b4e421cfa587effa66a7,No
2560,aleju/imgaug,tests/test.py,beabad25eb853a2960e86be0c60702b13621318c,TODO deviations of around 0.4-0.7 in this and the next test (between expected and observed,https://github.com/aleju/imgaug/commit/beabad25eb853a2960e86be0c60702b13621318c,No
2561,aleju/imgaug,tests/test.py,6da1af45453cef0784277792b540df94c4b0920c,TODO add test for keypoints once their handling was improved in Convolve,https://github.com/aleju/imgaug/commit/6da1af45453cef0784277792b540df94c4b0920c,Yes
2562,aleju/imgaug,tests/test.py,fc5d30af9356556edafae5fb3e8da7fdd13b0b4c,TODO these tests change the input type from list to array. Might be reasnoable to change,https://github.com/aleju/imgaug/commit/fc5d30af9356556edafae5fb3e8da7fdd13b0b4c,No
2563,aleju/imgaug,tests/test.py,3b04c24655c875be8cbcee5198c9f9aa285491b4,TODO incomplete tests; handle only cases that were missing in code coverage report,https://github.com/aleju/imgaug/commit/3b04c24655c875be8cbcee5198c9f9aa285491b4,Yes
2564,aleju/imgaug,tests/test.py,3b04c24655c875be8cbcee5198c9f9aa285491b4,TODO this is now already tested above via lamdba functions?,https://github.com/aleju/imgaug/commit/3b04c24655c875be8cbcee5198c9f9aa285491b4,Yes
2565,aleju/imgaug,tests/test.py,96f68b64d3e584e7d557272bb77e6cf93d536bef,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/96f68b64d3e584e7d557272bb77e6cf93d536bef,No
2566,aleju/imgaug,tests/test.py,96f68b64d3e584e7d557272bb77e6cf93d536bef,TODO test_CropAndPad(),https://github.com/aleju/imgaug/commit/96f68b64d3e584e7d557272bb77e6cf93d536bef,Yes
2567,aleju/imgaug,tests/test.py,96f68b64d3e584e7d557272bb77e6cf93d536bef,TODO add test for keypoints once their handling was improved in Convolve,https://github.com/aleju/imgaug/commit/96f68b64d3e584e7d557272bb77e6cf93d536bef,Yes
2568,aleju/imgaug,tests/test.py,96f68b64d3e584e7d557272bb77e6cf93d536bef,TODO deviations of around 0.4-0.7 in this and the next test (between expected and observed,https://github.com/aleju/imgaug/commit/96f68b64d3e584e7d557272bb77e6cf93d536bef,No
2569,aleju/imgaug,tests/test.py,96f68b64d3e584e7d557272bb77e6cf93d536bef,TODO these tests change the input type from list to array. Might be reasnoable to change,https://github.com/aleju/imgaug/commit/96f68b64d3e584e7d557272bb77e6cf93d536bef,No
2570,aleju/imgaug,tests/test.py,96f68b64d3e584e7d557272bb77e6cf93d536bef,TODO this is now already tested above via lamdba functions?,https://github.com/aleju/imgaug/commit/96f68b64d3e584e7d557272bb77e6cf93d536bef,Yes
2571,aleju/imgaug,tests/test.py,96f68b64d3e584e7d557272bb77e6cf93d536bef,test is needed here,https://github.com/aleju/imgaug/commit/96f68b64d3e584e7d557272bb77e6cf93d536bef,No
2572,aleju/imgaug,tests/test.py,9eb9158033ba24aaa8dd4a004f60c9b25e2ffd9c,TODO test_PadUptoFixedSize(),https://github.com/aleju/imgaug/commit/9eb9158033ba24aaa8dd4a004f60c9b25e2ffd9c,No
2573,aleju/imgaug,tests/test.py,9eb9158033ba24aaa8dd4a004f60c9b25e2ffd9c,TODO test_CropFixedSize(),https://github.com/aleju/imgaug/commit/9eb9158033ba24aaa8dd4a004f60c9b25e2ffd9c,Yes
2574,aleju/imgaug,tests/test.py,5020e636f6627f7b6036aa2a1332c33c9bbd3048,TODO test_PadUptoFixedSize(),https://github.com/aleju/imgaug/commit/5020e636f6627f7b6036aa2a1332c33c9bbd3048,No
2575,aleju/imgaug,tests/test.py,5020e636f6627f7b6036aa2a1332c33c9bbd3048,TODO test_CropFixedSize(),https://github.com/aleju/imgaug/commit/5020e636f6627f7b6036aa2a1332c33c9bbd3048,Yes
2576,aleju/imgaug,tests/test.py,bc8d053827f9cb708f719871c4ff32cf61bdc3ef,TODO test_PadToFixedSize(),https://github.com/aleju/imgaug/commit/bc8d053827f9cb708f719871c4ff32cf61bdc3ef,Yes
2577,aleju/imgaug,tests/test.py,bc8d053827f9cb708f719871c4ff32cf61bdc3ef,TODO test_CropToFixedSize(),https://github.com/aleju/imgaug/commit/bc8d053827f9cb708f719871c4ff32cf61bdc3ef,Yes
2578,aleju/imgaug,imgaug/augmenters/size.py,6e7a66e8cfc5fa701c6f7422e0062655383544a4,TODO add test,https://github.com/aleju/imgaug/commit/6e7a66e8cfc5fa701c6f7422e0062655383544a4,No
2579,aleju/imgaug,imgaug/imgaug.py,f5a67c5d793a4be5c9fd96fe05a994729d8e6719,TODO add test,https://github.com/aleju/imgaug/commit/f5a67c5d793a4be5c9fd96fe05a994729d8e6719,No
2580,aleju/imgaug,tests/test.py,bbc2f34a72fcc8ad4a5a4e6c095a5131b1d327f8,TODO test_PadToFixedSize(),https://github.com/aleju/imgaug/commit/bbc2f34a72fcc8ad4a5a4e6c095a5131b1d327f8,Yes
2581,aleju/imgaug,tests/test.py,bbc2f34a72fcc8ad4a5a4e6c095a5131b1d327f8,TODO test_CropToFixedSize(),https://github.com/aleju/imgaug/commit/bbc2f34a72fcc8ad4a5a4e6c095a5131b1d327f8,Yes
2582,aleju/imgaug,imgaug/augmenters/size.py,de4921f4439fd6d2f353d63eec131a656f3134ff,TODO add test for zero-size prevention,https://github.com/aleju/imgaug/commit/de4921f4439fd6d2f353d63eec131a656f3134ff,Yes
2583,aleju/imgaug,imgaug/imgaug.py,42ad7ba6df35cd38a013534a59fd5655b5ea6507,TODO add test,https://github.com/aleju/imgaug/commit/42ad7ba6df35cd38a013534a59fd5655b5ea6507,No
2584,aleju/imgaug,imgaug/imgaug.py,1fe0900fcc1bda11157980c10c39d5c98ac77c39,TODO add test,https://github.com/aleju/imgaug/commit/1fe0900fcc1bda11157980c10c39d5c98ac77c39,No
2585,aleju/imgaug,tests/test.py,00e8ae4aee6a9da5520798646762e2a24b02188e,TODO test drawing on float32; float64 image,https://github.com/aleju/imgaug/commit/00e8ae4aee6a9da5520798646762e2a24b02188e,Yes
2586,aleju/imgaug,imgaug/imgaug.py,1c8a7001bc70f1006e81099b02f58696bde6886b,TODO add test,https://github.com/aleju/imgaug/commit/1c8a7001bc70f1006e81099b02f58696bde6886b,No
2587,aleju/imgaug,tests/augmenters/test_arithmetic.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2588,aleju/imgaug,tests/augmenters/test_blur.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2589,aleju/imgaug,tests/augmenters/test_color.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2590,aleju/imgaug,tests/augmenters/test_contrast.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2591,aleju/imgaug,tests/augmenters/test_convolutional.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2592,aleju/imgaug,tests/augmenters/test_convolutional.py,fd9706106f80e05360969b8063f7a56ff2015378,TODO add test for keypoints once their handling was improved in Convolve,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,Yes
2593,aleju/imgaug,tests/augmenters/test_flip.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2594,aleju/imgaug,tests/augmenters/test_geometric.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2595,aleju/imgaug,tests/augmenters/test_geometric.py,fd9706106f80e05360969b8063f7a56ff2015378,TODO deviations of around 0.4-0.7 in this and the next test (between expected and observed,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2596,aleju/imgaug,tests/augmenters/test_meta.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2597,aleju/imgaug,tests/augmenters/test_meta.py,fd9706106f80e05360969b8063f7a56ff2015378,TODO these tests change the input type from list to array. Might be reasnoable to change,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2598,aleju/imgaug,tests/augmenters/test_meta.py,fd9706106f80e05360969b8063f7a56ff2015378,TODO this is now already tested above via lamdba functions?,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,Yes
2599,aleju/imgaug,tests/augmenters/test_mixed_files.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2600,aleju/imgaug,tests/augmenters/test_overlay.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2601,aleju/imgaug,tests/augmenters/test_segmentation.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2602,aleju/imgaug,tests/augmenters/test_size.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2603,aleju/imgaug,tests/augmenters/test_size.py,fd9706106f80e05360969b8063f7a56ff2015378,TODO test_CropAndPad(),https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,Yes
2604,aleju/imgaug,tests/test_imgaug.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2605,aleju/imgaug,tests/test_imgaug.py,fd9706106f80e05360969b8063f7a56ff2015378,TODO test drawing on float32; float64 image,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,Yes
2606,aleju/imgaug,tests/test_parameters.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2607,aleju/imgaug,tests/test_parameters.py,fd9706106f80e05360969b8063f7a56ff2015378,test is needed here,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2608,aleju/imgaug,tests/utils.py,fd9706106f80e05360969b8063f7a56ff2015378,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fd9706106f80e05360969b8063f7a56ff2015378,No
2609,aleju/imgaug,test/test_imgaug.py,1b03a78240331c7d82f7194dca4212ccdd872de9,TODO this function seems to already be covered completely by other tests; so add a proper test later,https://github.com/aleju/imgaug/commit/1b03a78240331c7d82f7194dca4212ccdd872de9,Yes
2610,aleju/imgaug,test/augmenters/test_weather.py,c9e6ef33a6b562a0d692fa0a4589105db5dbde54,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/c9e6ef33a6b562a0d692fa0a4589105db5dbde54,No
2611,aleju/imgaug,test/augmenters/test_geometric.py,57b650ea3ad929e20a6f56bc01c08a0127943feb,TODO switch all other tests from float(...) to np.float128(...) pattern; seems,https://github.com/aleju/imgaug/commit/57b650ea3ad929e20a6f56bc01c08a0127943feb,No
2612,aleju/imgaug,test/augmenters/test_meta.py,d5b7c8a5bec98e49302fc3e4a18e09a56cdbb4e5,TODO remove these tests once a similar test for restore_dtypes_() was added,https://github.com/aleju/imgaug/commit/d5b7c8a5bec98e49302fc3e4a18e09a56cdbb4e5,No
2613,aleju/imgaug,test/augmenters/test_overlay.py,e8e37deac1f10b50d916eb86d280108ff2b1a917,TODO this test breaks for numpy <1.15 -- why?,https://github.com/aleju/imgaug/commit/e8e37deac1f10b50d916eb86d280108ff2b1a917,No
2614,aleju/imgaug,test/test_multicore.py,a0079c274c8556813fb3186aa3eed6418b52b27a,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/a0079c274c8556813fb3186aa3eed6418b52b27a,No
2615,aleju/imgaug,imgaug/parameters.py,d7cfa903cc6892cd3ecb87dcde324b1468ce7029,TODO test if result[coinflips_mask] *= (-1) is faster  (with protection against mask being empty?),https://github.com/aleju/imgaug/commit/d7cfa903cc6892cd3ecb87dcde324b1468ce7029,Yes
2616,aleju/imgaug,test/test_dtypes.py,ac39bd6abfaf715ed30f5836cc3021fa87dccae7,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/ac39bd6abfaf715ed30f5836cc3021fa87dccae7,No
2617,aleju/imgaug,test/test_dtypes.py,ac39bd6abfaf715ed30f5836cc3021fa87dccae7,TODO remove these tests once a similar test for restore_dtypes_() was added,https://github.com/aleju/imgaug/commit/ac39bd6abfaf715ed30f5836cc3021fa87dccae7,No
2618,aleju/imgaug,test/augmenters/test_meta.py,08592186f62c725e003d6b05e8a6dc79c49247b6,"TODO test seems to be geared here towards original data; but variable is named as \""_aug\""",https://github.com/aleju/imgaug/commit/08592186f62c725e003d6b05e8a6dc79c49247b6,Yes
2619,aleju/imgaug,test/augmenters/test_meta.py,6876de2142a1532a5943619375952f68b2c09a3e,TODO add tests when funcs are not set in Lambda,https://github.com/aleju/imgaug/commit/6876de2142a1532a5943619375952f68b2c09a3e,Yes
2620,aleju/imgaug,imgaug/augmenters/geometric.py,b7f6958db512726aa26e25e66a84f2cf4f76025d,TODO add test for this,https://github.com/aleju/imgaug/commit/b7f6958db512726aa26e25e66a84f2cf4f76025d,Yes
2621,aleju/imgaug,imgaug/augmenters/geometric.py,c72e37662e37d97ccc0447260eeea1575f8861c8,TODO add test for this,https://github.com/aleju/imgaug/commit/c72e37662e37d97ccc0447260eeea1575f8861c8,Yes
2622,aleju/imgaug,imgaug/augmentables/kps.py,cb8cfe40ff1d006417ee7576dec0b00d0d2e9954,TODO add test,https://github.com/aleju/imgaug/commit/cb8cfe40ff1d006417ee7576dec0b00d0d2e9954,No
2623,aleju/imgaug,test/augmentables/test_kps.py,cb8cfe40ff1d006417ee7576dec0b00d0d2e9954,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/cb8cfe40ff1d006417ee7576dec0b00d0d2e9954,No
2624,aleju/imgaug,imgaug/augmentables/bbs.py,82febec2db268d6b237bcf84e517c3f0234a2cc4,TODO add explicit test for zero-sized BBs (worked when tested by hand),https://github.com/aleju/imgaug/commit/82febec2db268d6b237bcf84e517c3f0234a2cc4,Yes
2625,aleju/imgaug,imgaug/augmentables/bbs.py,82febec2db268d6b237bcf84e517c3f0234a2cc4,TODO add test,https://github.com/aleju/imgaug/commit/82febec2db268d6b237bcf84e517c3f0234a2cc4,No
2626,aleju/imgaug,test/augmentables/test_bbs.py,82febec2db268d6b237bcf84e517c3f0234a2cc4,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/82febec2db268d6b237bcf84e517c3f0234a2cc4,No
2627,aleju/imgaug,test/augmentables/test_polys.py,3a7cb068e0b17d20747e0d84b1dedd535898255d,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/3a7cb068e0b17d20747e0d84b1dedd535898255d,No
2628,aleju/imgaug,test/augmentables/test_polys.py,3a7cb068e0b17d20747e0d84b1dedd535898255d,TODO this function seems to already be covered completely by other tests; so add a proper test later,https://github.com/aleju/imgaug/commit/3a7cb068e0b17d20747e0d84b1dedd535898255d,Yes
2629,aleju/imgaug,test/augmentables/test_heatmaps.py,f6b0256f196cae78b0b38f01cd674cebf18a963b,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/f6b0256f196cae78b0b38f01cd674cebf18a963b,No
2630,aleju/imgaug,test/augmentables/test_segmaps.py,2e0e798d843f9db5a9b6721b9fdccf1d05afbd89,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/2e0e798d843f9db5a9b6721b9fdccf1d05afbd89,No
2631,aleju/imgaug,test/augmentables/test_batches.py,07d54c07d546e741defb40c6e17943771ca7ee80,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/07d54c07d546e741defb40c6e17943771ca7ee80,No
2632,aleju/imgaug,test/augmentables/test_normalization.py,02b6f21905f083a6d2488786d3ce12ed697b5563,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/02b6f21905f083a6d2488786d3ce12ed697b5563,No
2633,aleju/imgaug,test/test_dtypes.py,959a79ea9575d7b8c016c044b563943de6852a27,TODO extend tests towards all dtypes and actual minima\/maxima of value ranges,https://github.com/aleju/imgaug/commit/959a79ea9575d7b8c016c044b563943de6852a27,Yes
2634,aleju/imgaug,imgaug/augmentables/bbs.py,437306d52ffad3d00f576abaf785fe8782573223,TODO add test for tuple of number,https://github.com/aleju/imgaug/commit/437306d52ffad3d00f576abaf785fe8782573223,No
2635,aleju/imgaug,imgaug/augmentables/bbs.py,437306d52ffad3d00f576abaf785fe8782573223,TODO add tests for ndarray inputs,https://github.com/aleju/imgaug/commit/437306d52ffad3d00f576abaf785fe8782573223,No
2636,aleju/imgaug,test/augmenters/test_meta.py,ce4061128f92f339c93c1515fcd07ca258fe00ed,TODO merge this test with test_Augmenter_augment_polygons()?,https://github.com/aleju/imgaug/commit/ce4061128f92f339c93c1515fcd07ca258fe00ed,Yes
2637,aleju/imgaug,test/augmentables/test_lines.py,745e014161d758ec90ea9bb96b523520e311589d,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/745e014161d758ec90ea9bb96b523520e311589d,No
2638,aleju/imgaug,imgaug/augmentables/bbs.py,3156b5920201c5d9a5631e5604dfe7eab4146d0d,TODO add tests for pad and pad_max,https://github.com/aleju/imgaug/commit/3156b5920201c5d9a5631e5604dfe7eab4146d0d,Yes
2639,aleju/imgaug,imgaug/augmenters/blend.py,5e780ee6b741bfacf5d5669e54b50825c6fb03bd,TODO add test for this,https://github.com/aleju/imgaug/commit/5e780ee6b741bfacf5d5669e54b50825c6fb03bd,Yes
2640,aleju/imgaug,imgaug/augmentables/lines.py,1732173127ecce503d069300329727065e16e238,TODO Do this with edge-wise intersection tests,https://github.com/aleju/imgaug/commit/1732173127ecce503d069300329727065e16e238,Yes
2641,aleju/imgaug,imgaug/augmentables/lines.py,106027e3d6079cc7b01923dbb8802262a5b70b2c,TODO add tests for this,https://github.com/aleju/imgaug/commit/106027e3d6079cc7b01923dbb8802262a5b70b2c,Yes
2642,aleju/imgaug,imgaug/augmentables/polys.py,fb0cbc1f5b9bd9df45fc60dfe3f2c69db83700e2,TODO add tests for this,https://github.com/aleju/imgaug/commit/fb0cbc1f5b9bd9df45fc60dfe3f2c69db83700e2,Yes
2643,aleju/imgaug,test/augmentables/test_utils.py,ffb8fdf496355d753425bbd980d5f86d0342c9ae,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/ffb8fdf496355d753425bbd980d5f86d0342c9ae,No
2644,aleju/imgaug,imgaug/augmentables/lines.py,5d34bef15a2c5661561c08c8316bcdeaf45869e5,TODO add tests,https://github.com/aleju/imgaug/commit/5d34bef15a2c5661561c08c8316bcdeaf45869e5,Yes
2645,aleju/imgaug,test/augmenters/test_edges.py,d11ce12765559449a518ebef3b30f645b7e81d22,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/d11ce12765559449a518ebef3b30f645b7e81d22,No
2646,aleju/imgaug,test/augmenters/test_pool.py,b9959f2f43a68ffc67057ae24331396592377d8b,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/b9959f2f43a68ffc67057ae24331396592377d8b,No
2647,aleju/imgaug,test/augmenters/test_arithmetic.py,79fcdc7f4552782ce97937f74ac30050342814ad,TODO split into two tests,https://github.com/aleju/imgaug/commit/79fcdc7f4552782ce97937f74ac30050342814ad,No
2648,aleju/imgaug,test/augmenters/test_blur.py,d3752ba4dae574b890945ae63ff673bc9c05ceb5,TODO extend this to test sampled kernel sizes,https://github.com/aleju/imgaug/commit/d3752ba4dae574b890945ae63ff673bc9c05ceb5,No
2649,aleju/imgaug,imgaug/augmenters/segmentation.py,6c557fe5b6cf33fc082a292ee5e0e6b6784d3ad1,TODO add test for this,https://github.com/aleju/imgaug/commit/6c557fe5b6cf33fc082a292ee5e0e6b6784d3ad1,Yes
2650,aleju/imgaug,test/augmenters/test_convolutional.py,93e97989d118e54d590fa9827f5a536eb44c5609,TODO add test for keypoints once their handling was improved in Convolve,https://github.com/aleju/imgaug/commit/93e97989d118e54d590fa9827f5a536eb44c5609,Yes
2651,aleju/imgaug,test/test_parameters.py,7860ccff020ca20e06d932b22dae6824a7a9f286,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/7860ccff020ca20e06d932b22dae6824a7a9f286,No
2652,aleju/imgaug,test/test_parameters.py,7860ccff020ca20e06d932b22dae6824a7a9f286,TODO why are these tests applied to DiscreteUniform instead of Uniform?,https://github.com/aleju/imgaug/commit/7860ccff020ca20e06d932b22dae6824a7a9f286,No
2653,aleju/imgaug,test/augmentables/test_lines.py,f4f699b80960f5049c3c7b8a7eca395412d6efd0,TODO add antialiased=True test,https://github.com/aleju/imgaug/commit/f4f699b80960f5049c3c7b8a7eca395412d6efd0,Yes
2654,aleju/imgaug,test/augmenters/test_overlay.py,baea2392e345914ca0469080b316c14cbbd326d4,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/baea2392e345914ca0469080b316c14cbbd326d4,No
2655,aleju/imgaug,test/augmenters/test_pooling.py,479203412d14d172661a5ed5acd9d365070f4b83,TODO add test that checks the padding behaviour,https://github.com/aleju/imgaug/commit/479203412d14d172661a5ed5acd9d365070f4b83,No
2656,aleju/imgaug,test/augmenters/test_pooling.py,2f76fb179b706b7b293cad1843dbe529ff0d2d5c,TODO add test that checks the padding behaviour,https://github.com/aleju/imgaug/commit/2f76fb179b706b7b293cad1843dbe529ff0d2d5c,No
2657,aleju/imgaug,test/augmenters/test_pooling.py,e4a5159c0c82424bdfbe50f031f1fd5166c5c8d3,TODO add test that checks the padding behaviour,https://github.com/aleju/imgaug/commit/e4a5159c0c82424bdfbe50f031f1fd5166c5c8d3,No
2658,aleju/imgaug,test/augmenters/test_pooling.py,e7d11353a1764bf37750b36681ea26928991c0d4,TODO add test that checks the padding behaviour,https://github.com/aleju/imgaug/commit/e7d11353a1764bf37750b36681ea26928991c0d4,No
2659,aleju/imgaug,imgaug/imgaug.py,9b8fd7df45bf687308fb95124b6392f37c10c835,TODO add tests,https://github.com/aleju/imgaug/commit/9b8fd7df45bf687308fb95124b6392f37c10c835,Yes
2660,aleju/imgaug,imgaug/augmenters/size.py,3da21ac48193da3660eed47c488d2fa5bb8225d9,TODO add test for zero-size prevention,https://github.com/aleju/imgaug/commit/3da21ac48193da3660eed47c488d2fa5bb8225d9,Yes
2661,aleju/imgaug,test/augmenters/test_meta.py,8c21a9ff3a02bd5078dc6c4e3eabbc09d89d6bfd,TODO this is now already tested above via lamdba functions?,https://github.com/aleju/imgaug/commit/8c21a9ff3a02bd5078dc6c4e3eabbc09d89d6bfd,Yes
2662,aleju/imgaug,imgaug/augmenters/color.py,5c3263034b520f02c7d8e163285b6d1147580a3a,TODO Changing the dtype here to int8 makes gen test for this method,https://github.com/aleju/imgaug/commit/5c3263034b520f02c7d8e163285b6d1147580a3a,Yes
2663,aleju/imgaug,imgaug/random.py,ad144cf86aa080c30859551c5b80a1cb20ab092b,TODO add tests,https://github.com/aleju/imgaug/commit/ad144cf86aa080c30859551c5b80a1cb20ab092b,Yes
2664,aleju/imgaug,test/test_random.py,fa4ec0890a553789e5b8fb95f035f1c6e61cc977,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fa4ec0890a553789e5b8fb95f035f1c6e61cc977,No
2665,aleju/imgaug,imgaug/parameters.py,875770d071d1577801ac3b8cda8058d722942363,TODO test if,https://github.com/aleju/imgaug/commit/875770d071d1577801ac3b8cda8058d722942363,No
2666,aleju/imgaug,test/augmenters/test_arithmetic.py,488132b1356d7a9b678fa3a387303906aaf408ac,TODO use this in test_contrast.py or remove it?,https://github.com/aleju/imgaug/commit/488132b1356d7a9b678fa3a387303906aaf408ac,Yes
2667,aleju/imgaug,test/augmenters/test_blend.py,81923aaa8910e9e763df65d31392c1fe60834d3c,TODO split this up into multiple tests,https://github.com/aleju/imgaug/commit/81923aaa8910e9e763df65d31392c1fe60834d3c,Yes
2668,aleju/imgaug,test/augmenters/test_size.py,9db57673552fb8d989cb81a2a3472fed8df5b2bf,TODO add test for shorter side being tuple; list; stochastic parameter,https://github.com/aleju/imgaug/commit/9db57673552fb8d989cb81a2a3472fed8df5b2bf,Yes
2669,aleju/imgaug,test/augmenters/test_size.py,9db57673552fb8d989cb81a2a3472fed8df5b2bf,TODO add test for longer side being tuple; list; stochastic parameter,https://github.com/aleju/imgaug/commit/9db57673552fb8d989cb81a2a3472fed8df5b2bf,Yes
2670,aleju/imgaug,test/augmentables/test_heatmaps.py,ac5b28e1690bbf983853e2d57fc4ddfe60d8420b,TODO add tests for:,https://github.com/aleju/imgaug/commit/ac5b28e1690bbf983853e2d57fc4ddfe60d8420b,Yes
2671,aleju/imgaug,test/augmentables/test_heatmaps.py,ac5b28e1690bbf983853e2d57fc4ddfe60d8420b,TODO test other cmaps,https://github.com/aleju/imgaug/commit/ac5b28e1690bbf983853e2d57fc4ddfe60d8420b,Yes
2672,aleju/imgaug,test/augmentables/test_normalization.py,9cc2b74e21d548dd4db082401b46dc21530152e5,TODO split up tests here,https://github.com/aleju/imgaug/commit/9cc2b74e21d548dd4db082401b46dc21530152e5,No
2673,aleju/imgaug,test/augmenters/test_mixed_files.py,6d27324706e7faa19c4e91255dec6c99d9c8526d,TODO this should probably be tested just once for Augmenter,https://github.com/aleju/imgaug/commit/6d27324706e7faa19c4e91255dec6c99d9c8526d,Yes
2674,aleju/imgaug,test/augmenters/test_mixed_files.py,6d27324706e7faa19c4e91255dec6c99d9c8526d,TODO move these tests to the individual augmenters?,https://github.com/aleju/imgaug/commit/6d27324706e7faa19c4e91255dec6c99d9c8526d,No
2675,aleju/imgaug,test/augmenters/test_geometric.py,7f475f31414f70636c4fceb5c38b620bfe3b2646,TODO add more tests for Affine .mode,https://github.com/aleju/imgaug/commit/7f475f31414f70636c4fceb5c38b620bfe3b2646,No
2676,aleju/imgaug,test/augmenters/test_geometric.py,7f475f31414f70636c4fceb5c38b620bfe3b2646,TODO add more tests for Affine shear,https://github.com/aleju/imgaug/commit/7f475f31414f70636c4fceb5c38b620bfe3b2646,Yes
2677,aleju/imgaug,test/augmenters/test_geometric.py,7f475f31414f70636c4fceb5c38b620bfe3b2646,TODO merge these into TestAffine_rotate since they are rotations?,https://github.com/aleju/imgaug/commit/7f475f31414f70636c4fceb5c38b620bfe3b2646,No
2678,aleju/imgaug,test/augmenters/test_geometric.py,807808f4469ae04aaa1b291bf1a3ff33c23b6999,this test was apparently added later on (?) without noticing that,https://github.com/aleju/imgaug/commit/807808f4469ae04aaa1b291bf1a3ff33c23b6999,No
2679,aleju/imgaug,test/augmenters/test_geometric.py,807808f4469ae04aaa1b291bf1a3ff33c23b6999,TODO migrate to unittest and split up tests,https://github.com/aleju/imgaug/commit/807808f4469ae04aaa1b291bf1a3ff33c23b6999,Yes
2680,aleju/imgaug,test/augmenters/test_geometric.py,0da94e43d86fda16138b85195b1d6047a8741d59,TODO add tests for order,https://github.com/aleju/imgaug/commit/0da94e43d86fda16138b85195b1d6047a8741d59,Yes
2681,aleju/imgaug,test/augmenters/test_geometric.py,0da94e43d86fda16138b85195b1d6047a8741d59,TODO add tests for mode,https://github.com/aleju/imgaug/commit/0da94e43d86fda16138b85195b1d6047a8741d59,No
2682,aleju/imgaug,imgaug/augmenters/color.py,0004ab656077c6f3d51b65dcbc04cd62922deb2b,TODO add test for this,https://github.com/aleju/imgaug/commit/0004ab656077c6f3d51b65dcbc04cd62922deb2b,Yes
2683,aleju/imgaug,imgaug/augmenters/color.py,58deb45ce1af00811c32ca7dfd170c9205998e10,TODO add direct tests,https://github.com/aleju/imgaug/commit/58deb45ce1af00811c32ca7dfd170c9205998e10,Yes
2684,aleju/imgaug,imgaug/augmenters/color.py,db1d4b9e52d7496a9049424906b6980305926980,TODO test these pairs,https://github.com/aleju/imgaug/commit/db1d4b9e52d7496a9049424906b6980305926980,No
2685,aleju/imgaug,imgaug/augmentables/utils.py,5169c6cd27339a1e3ec365f5318de73cd07b1f55,TODO add tests,https://github.com/aleju/imgaug/commit/5169c6cd27339a1e3ec365f5318de73cd07b1f55,Yes
2686,aleju/imgaug,imgaug/augmenters/arithmetic.py,9ec1d0d295588cec34f42258339e9392184868bf,TODO add tests,https://github.com/aleju/imgaug/commit/9ec1d0d295588cec34f42258339e9392184868bf,Yes
2687,aleju/imgaug,imgaug/augmenters/geometric.py,344e83f11c531c63483f836052a79997f939a928,TODO add test for this,https://github.com/aleju/imgaug/commit/344e83f11c531c63483f836052a79997f939a928,Yes
2688,aleju/imgaug,imgaug/imgaug.py,b4d39888aff4a0a9a8c1b8437d758f49c4ea8151,TODO add direct test for this. indirectly tested via Pad,https://github.com/aleju/imgaug/commit/b4d39888aff4a0a9a8c1b8437d758f49c4ea8151,No
2689,aleju/imgaug,test/augmenters/test_blur.py,c0925c3b2980f0563460d81d1d60a6926db3b47a,TODO extend these tests,https://github.com/aleju/imgaug/commit/c0925c3b2980f0563460d81d1d60a6926db3b47a,No
2690,aleju/imgaug,checks/check_readme_examples.py,52d742b320d101d632a813b07a550245d641828f,"\""\""\"" || Script to verify all examples in the readme. || Simply execute ||     python test_readme_examples.py ||  ||  || The tests in this file are currently not unittests! || They do plot images. ||  || TODO move this to checks\/ ? ||  || \""\""\""",https://github.com/aleju/imgaug/commit/52d742b320d101d632a813b07a550245d641828f,No
2691,aleju/imgaug,test/augmenters/test_geometric.py,c298ef5e389eb92b59750646427a23b0f4bdaef8,TODO deviations of around 0.4-0.7 in this and the next test (between,https://github.com/aleju/imgaug/commit/c298ef5e389eb92b59750646427a23b0f4bdaef8,No
2692,aleju/imgaug,test/augmenters/test_geometric.py,0f062cc86828341a1c7e0d59bbe0c233141e2530,TODO add alignment tests for: BBs; Polys; LS,https://github.com/aleju/imgaug/commit/0f062cc86828341a1c7e0d59bbe0c233141e2530,Yes
2693,aleju/imgaug,imgaug/augmenters/geometric.py,a059ea8f1f60de1917f3566e6e523bc698777832,TODO add test for recoverer,https://github.com/aleju/imgaug/commit/a059ea8f1f60de1917f3566e6e523bc698777832,No
2694,aleju/imgaug,imgaug/augmenters/geometric.py,a059ea8f1f60de1917f3566e6e523bc698777832,TODO if left out; only one test failed -- should be more,https://github.com/aleju/imgaug/commit/a059ea8f1f60de1917f3566e6e523bc698777832,Yes
2695,aleju/imgaug,imgaug/augmenters/geometric.py,8213b5db448971319386954d846b1ff985196f11,TODO add test for recoverer,https://github.com/aleju/imgaug/commit/8213b5db448971319386954d846b1ff985196f11,No
2696,aleju/imgaug,test/augmentables/test_batches.py,5d9c6cc0903837bbb44ff7cc97ec3eca64b39d4f,TODO test __init__(),https://github.com/aleju/imgaug/commit/5d9c6cc0903837bbb44ff7cc97ec3eca64b39d4f,Yes
2697,aleju/imgaug,test/augmentables/test_batches.py,5d9c6cc0903837bbb44ff7cc97ec3eca64b39d4f,TODO test __init__,https://github.com/aleju/imgaug/commit/5d9c6cc0903837bbb44ff7cc97ec3eca64b39d4f,Yes
2698,aleju/imgaug,test/augmentables/test_lines.py,954dbd885fb1ab13bb0bcb8721d0e5ecd3c287be,TODO test to_keypoints_on_image(),https://github.com/aleju/imgaug/commit/954dbd885fb1ab13bb0bcb8721d0e5ecd3c287be,Yes
2699,aleju/imgaug,imgaug/augmenters/size.py,1228321efffe2fb6a2f9f51c62e26fbb9ca595bc,TODO add direct test for this. indirectly tested via Pad,https://github.com/aleju/imgaug/commit/1228321efffe2fb6a2f9f51c62e26fbb9ca595bc,No
2700,aleju/imgaug,test/augmenters/test_size.py,1228321efffe2fb6a2f9f51c62e26fbb9ca595bc,TODO add tests for return_pad_values=True,https://github.com/aleju/imgaug/commit/1228321efffe2fb6a2f9f51c62e26fbb9ca595bc,Yes
2701,aleju/imgaug,test/augmenters/test_color.py,f0912af0dc8000df5b7addecb59830b7e40ac833,TODO add tests for prop hooks,https://github.com/aleju/imgaug/commit/f0912af0dc8000df5b7addecb59830b7e40ac833,Yes
2702,aleju/imgaug,imgaug/augmenters/color.py,97210bb14facb542d03bf3520d0e56702cf696d2,TODO add tests,https://github.com/aleju/imgaug/commit/97210bb14facb542d03bf3520d0e56702cf696d2,Yes
2703,aleju/imgaug,test/augmenters/test_color.py,1088bab52bdc276372da301b36c11dc84f8f68bd,TODO add tests for prop hooks,https://github.com/aleju/imgaug/commit/1088bab52bdc276372da301b36c11dc84f8f68bd,Yes
2704,aleju/imgaug,imgaug/augmenters/color.py,90171e99fb1198bf3050faa0c183f8ff07e7ce63,TODO add tests,https://github.com/aleju/imgaug/commit/90171e99fb1198bf3050faa0c183f8ff07e7ce63,Yes
2705,aleju/imgaug,imgaug/augmenters/color.py,016d8b5dd0ce52c6df7b57b6b80bcaf4587486e3,TODO add tests,https://github.com/aleju/imgaug/commit/016d8b5dd0ce52c6df7b57b6b80bcaf4587486e3,Yes
2706,aleju/imgaug,test/augmenters/test_artistic.py,ec0300cd3c0c7502c52903fb9f74a108c952bacf,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/ec0300cd3c0c7502c52903fb9f74a108c952bacf,No
2707,aleju/imgaug,imgaug/augmenters/color.py,510c021bd28b4c70901421baaff6f2ee73165ce6,TODO add tests,https://github.com/aleju/imgaug/commit/510c021bd28b4c70901421baaff6f2ee73165ce6,Yes
2708,aleju/imgaug,imgaug/parameters.py,c0216586743cf3f95b5148f972d94bd3955ddbe4,TODO replace two-value parameters used in tests with this,https://github.com/aleju/imgaug/commit/c0216586743cf3f95b5148f972d94bd3955ddbe4,No
2709,aleju/imgaug,test/augmenters/test_debug.py,fc411a004e6dbdfbbb989254d32ea545ff7a0241,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/fc411a004e6dbdfbbb989254d32ea545ff7a0241,No
2710,aleju/imgaug,imgaug/augmentables/bbs.py,da27fd28ace4d7302017341aef36362c0c3f1f6f,TODO add explicit test for zero-sized BBs (worked when tested by hand),https://github.com/aleju/imgaug/commit/da27fd28ace4d7302017341aef36362c0c3f1f6f,Yes
2711,aleju/imgaug,test/augmenters/test_pillike.py,bf31f2336f0b7e437fcb482e02ae2dfe037ea25f,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/bf31f2336f0b7e437fcb482e02ae2dfe037ea25f,No
2712,aleju/imgaug,test/augmenters/test_pillike.py,53d483264c15a1633e6c9590ea4fe07b6640bbf5,TODO add test for unusual channel numbers,https://github.com/aleju/imgaug/commit/53d483264c15a1633e6c9590ea4fe07b6640bbf5,No
2713,aleju/imgaug,imgaug/augmentables/bbs.py,466fc0c1b1ed6375e3a276fa29135fa212bf2313,TODO add explicit test for zero-sized BBs (worked when tested by hand),https://github.com/aleju/imgaug/commit/466fc0c1b1ed6375e3a276fa29135fa212bf2313,Yes
2714,aleju/imgaug,test/augmenters/test_weather.py,3b30aa91cb58e079cc0730b5244875e04ebf6fcf,TODO add more tests; improve testability,https://github.com/aleju/imgaug/commit/3b30aa91cb58e079cc0730b5244875e04ebf6fcf,Yes
2715,aleju/imgaug,test/augmenters/test_collections.py,af5d2476933b9a52b990df9500dde3c5e18a893f,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/af5d2476933b9a52b990df9500dde3c5e18a893f,No
2716,aleju/imgaug,imgaug/random.py,76fefd85f85aba6e5039cb3a775ac027c45f74d4,TODO add tests,https://github.com/aleju/imgaug/commit/76fefd85f85aba6e5039cb3a775ac027c45f74d4,Yes
2717,aleju/imgaug,test/augmenters/test_imgcorruptlike.py,76fefd85f85aba6e5039cb3a775ac027c45f74d4,fix execution of tests involving matplotlib on travis,https://github.com/aleju/imgaug/commit/76fefd85f85aba6e5039cb3a775ac027c45f74d4,No
2718,aleju/imgaug,imgaug/augmenters/meta.py,9d5958294c4ff52ccecfa99f2e3f518a86b71125,TODO add more tests,https://github.com/aleju/imgaug/commit/9d5958294c4ff52ccecfa99f2e3f518a86b71125,No
2719,aleju/imgaug,test/augmenters/test_color.py,c8c8540ac9d0ea3e3c77ccef2386b47f38dd4fd0,TODO this test exists two times,https://github.com/aleju/imgaug/commit/c8c8540ac9d0ea3e3c77ccef2386b47f38dd4fd0,Yes
2720,deepfakes/faceswap,plugins/train/model/dlight.py,3f29b72a933ac4fbecd5ad0e4a40cb2ce0327d4d,[P] TODO Move upscale2x_hyb to nnblocks.py (after testing),https://github.com/deepfakes/faceswap/commit/3f29b72a933ac4fbecd5ad0e4a40cb2ce0327d4d,Yes
2721,deepfakes/faceswap,tests/lib/model/optimizers_test.py,815c843f63ea7cba29915c609550a4047a945794,TODO PlaidML fails this test,https://github.com/deepfakes/faceswap/commit/815c843f63ea7cba29915c609550a4047a945794,No
2722,deepfakes/faceswap,plugins/train/model/_base.py,d8557c1970939ee9bb90bd41edcd86c6fcf84d19,TODO remove this hacky fix to disable mixed precision compatibility testing if\/when,https://github.com/deepfakes/faceswap/commit/d8557c1970939ee9bb90bd41edcd86c6fcf84d19,No
2723,deepfakes/faceswap,lib/training_data.py,05018f6119b4f90b91d18203e6bcd39868b0b662,TODO Test _get_closest_match for speed and correctness,https://github.com/deepfakes/faceswap/commit/05018f6119b4f90b91d18203e6bcd39868b0b662,Yes
2724,deepfakes/faceswap,lib/gui/theme.py,616bd7c50021c45be22da6eeec4611a7e643d568,TODO This lags out the GUI; so need to test where this is failing prior to implementing,https://github.com/deepfakes/faceswap/commit/616bd7c50021c45be22da6eeec4611a7e643d568,Yes
2725,nimeshabuddhika/Tensorflow-Chatbot,venv/Lib/_dummy_thread.py,a543442aa1cc64ec3202a1e6897a347b11cd1489,XXX Perhaps shouldn't actually bother to test?  Could lead,https://github.com/nimeshabuddhika/Tensorflow-Chatbot/commit/a543442aa1cc64ec3202a1e6897a347b11cd1489,Yes
2726,nimeshabuddhika/Tensorflow-Chatbot,venv/Lib/site.py,a543442aa1cc64ec3202a1e6897a347b11cd1489,encoding after initialization.  The test for presence is needed when,https://github.com/nimeshabuddhika/Tensorflow-Chatbot/commit/a543442aa1cc64ec3202a1e6897a347b11cd1489,Yes
2727,prajjwal1/language-modelling,fastai/conv_learner.py,87f5906f976667ac23ef6e1377145955ca536bd6,TODO: Somehow check that directory names haven't changed (e.g. added test set),https://github.com/prajjwal1/language-modelling/commit/87f5906f976667ac23ef6e1377145955ca536bd6,Yes
2728,themightyoarfish/deepVO,model.py,f433ed8087ccad6d22ab6402b5bcba5fc62bd556,TODO: test if needed if set in parent scope,https://github.com/themightyoarfish/deepVO/commit/f433ed8087ccad6d22ab6402b5bcba5fc62bd556,Yes
2729,lopuhin/transformer-lm,tests/test_model.py,a3ad7c6efdde6cf1253295b03eca427a1d4aa66c,TODO test devices,https://github.com/lopuhin/transformer-lm/commit/a3ad7c6efdde6cf1253295b03eca427a1d4aa66c,Yes
2730,lopuhin/transformer-lm,tests/test_model.py,f33049413056fc8e058a7597f5faa2696548887a,TODO test past,https://github.com/lopuhin/transformer-lm/commit/f33049413056fc8e058a7597f5faa2696548887a,No
2731,lopuhin/transformer-lm,tests/test_model.py,e2466a3f4e8f32a7fcebd79053ea2f2f58ae34a9,TODO test past,https://github.com/lopuhin/transformer-lm/commit/e2466a3f4e8f32a7fcebd79053ea2f2f58ae34a9,No
2732,AFAgarap/gru-svm,model/svm_classifier.py,4d64d11229e91b90dd6ccea94d234a820e2f2aa0,todo Get test examples and test labels by batch,https://github.com/AFAgarap/gru-svm/commit/4d64d11229e91b90dd6ccea94d234a820e2f2aa0,No
2733,AFAgarap/gru-svm,model/softmax_classifier.py,fcd73ba1784cc8d39d155f6982e60f43714c3c42,todo Get test examples and test labels by batch,https://github.com/AFAgarap/gru-svm/commit/fcd73ba1784cc8d39d155f6982e60f43714c3c42,No
2734,AFAgarap/gru-svm,model/svm_classifier.py,158e967ea2abe4a5ab8b7e74cc2e7a7e6c91a103,todo Get test examples and test labels by batch,https://github.com/AFAgarap/gru-svm/commit/158e967ea2abe4a5ab8b7e74cc2e7a7e6c91a103,No
2735,sarnthil/unify-emotion-datasets,create_unified_dataset.py,89d65cf01f32745b0a623ffec1e1d384a2707fa8,TODO add a field _train; _test ; _dev in the unified dataset,https://github.com/sarnthil/unify-emotion-datasets/commit/89d65cf01f32745b0a623ffec1e1d384a2707fa8,Yes
2736,mdangschat/ctc-asr,python/s_input.py,b697f6f86a100d5b9ba4f801d676842c1b829307,TODO: Remove this; this is only for testing!,https://github.com/mdangschat/ctc-asr/commit/b697f6f86a100d5b9ba4f801d676842c1b829307,Yes
2737,mdangschat/ctc-asr,python/s_input.py,4918fae0b89505da612876343a8a915faf58a150,bucket_boundaries=[l for l in         TODO test above,https://github.com/mdangschat/ctc-asr/commit/4918fae0b89505da612876343a8a915faf58a150,Yes
2738,mdangschat/ctc-asr,python/loader/gen_librispeech_txt.py,07edc6276779ada351ddbcf8f0bf665a7416b38c,TODO: No test set yet.,https://github.com/mdangschat/ctc-asr/commit/07edc6276779ada351ddbcf8f0bf665a7416b38c,Yes
2739,mdangschat/ctc-asr,python/s_input.py,87707ea66dfee298e28cf003254345e48f49e089,TODO test.txt,https://github.com/mdangschat/ctc-asr/commit/87707ea66dfee298e28cf003254345e48f49e089,Yes
2740,mdangschat/ctc-asr,asr/s_input.py,bc3ba6500c40d84609549da8e1629f2414ebbd46,TODO Testing if sorted is faster,https://github.com/mdangschat/ctc-asr/commit/bc3ba6500c40d84609549da8e1629f2414ebbd46,Yes
2741,mdangschat/ctc-asr,asr/dataset/download_if_necessary.py,93457113aeb7feb766a33131f666c85363b98cbc,TODO Wrapper for all used dataset wrappers; that creates the train.txt; test.txt; dev.txt,https://github.com/mdangschat/ctc-asr/commit/93457113aeb7feb766a33131f666c85363b98cbc,No
2742,mdangschat/ctc-asr,asr/dataset/download_if_necessary.py,93457113aeb7feb766a33131f666c85363b98cbc,TODO delete afterwards; only for testing.,https://github.com/mdangschat/ctc-asr/commit/93457113aeb7feb766a33131f666c85363b98cbc,No
2743,mdangschat/ctc-asr,asr/dataset/download.py,17e93a3aaf005e4b2adcf2b819a32f1d8a883636,TODO Wrapper for all used dataset wrappers; that creates the train.txt; test.txt; dev.txt,https://github.com/mdangschat/ctc-asr/commit/17e93a3aaf005e4b2adcf2b819a32f1d8a883636,No
2744,mdangschat/ctc-asr,asr/dataset/download.py,17e93a3aaf005e4b2adcf2b819a32f1d8a883636,TODO delete afterwards; only for testing.,https://github.com/mdangschat/ctc-asr/commit/17e93a3aaf005e4b2adcf2b819a32f1d8a883636,No
2745,bzier/gym-mupen64plus,gym_mupen64plus/envs/mupen64plus_env.py,9c9a4d0757ad235fe154131e48fc06cd89b05a31,TODO: Test and cleanup:,https://github.com/bzier/gym-mupen64plus/commit/9c9a4d0757ad235fe154131e48fc06cd89b05a31,No
2746,bzier/gym-mupen64plus,gym_mupen64plus/envs/mupen64plus_env.py,7344ba3133ec66fbdb027f28c1c2706b2c281858,TODO: Test and cleanup:,https://github.com/bzier/gym-mupen64plus/commit/7344ba3133ec66fbdb027f28c1c2706b2c281858,No
2747,gabrielspmoreira/chameleon_recsys,nar_module/nar/nar_model.py,c2c3a7f7c3421d2fbb3a9d1987cf068ba8e8c10a,TODO: Test again batch normalization instead of layer norm (applying activation function after the normalization - dense(activation=none) + batch_norm(activation=X)),https://github.com/gabrielspmoreira/chameleon_recsys/commit/c2c3a7f7c3421d2fbb3a9d1987cf068ba8e8c10a,Yes
2748,oscarknagg/voicemap,tests/tests.py,04c7c22eb26f32a62bf752d52b6dcbebc452550b,TODO: Write this test,https://github.com/oscarknagg/voicemap/commit/04c7c22eb26f32a62bf752d52b6dcbebc452550b,No
2749,oscarknagg/voicemap,voicemap/utils.py,04c7c22eb26f32a62bf752d52b6dcbebc452550b,TODO: write a test for this,https://github.com/oscarknagg/voicemap/commit/04c7c22eb26f32a62bf752d52b6dcbebc452550b,No
2750,ComputationalCryoEM/ASPIRE-Python,aspire/utils.py,7eaafa9dc792c69b4816a8154c89b0157962f98a,TODO test for single projection. This would most-prob fail,https://github.com/ComputationalCryoEM/ASPIRE-Python/commit/7eaafa9dc792c69b4816a8154c89b0157962f98a,Yes
2751,ComputationalCryoEM/ASPIRE-Python,aspire/utils/array_utils.py,b465bfb7c6af09b1b2d263adc9f18f7c5c75d734,TODO test for single projection. This would most-prob fail,https://github.com/ComputationalCryoEM/ASPIRE-Python/commit/b465bfb7c6af09b1b2d263adc9f18f7c5c75d734,Yes
2752,ComputationalCryoEM/ASPIRE-Python,src/aspire/basis/polar_2d.py,06ac8782ca3481f9744f648cb480be256c38442b,TODO: need check the normalization factor and develop unit test,https://github.com/ComputationalCryoEM/ASPIRE-Python/commit/06ac8782ca3481f9744f648cb480be256c38442b,No
2753,ComputationalCryoEM/ASPIRE-Python,tests/test_nfft.py,a8e6125cc1bc6b1c72a7f61f2472aaae3996babd,For now I think maybe better to error.  Even in this unittest file we load data in singles,https://github.com/ComputationalCryoEM/ASPIRE-Python/commit/a8e6125cc1bc6b1c72a7f61f2472aaae3996babd,Yes
2754,skyportal/skyportal,skyportal/tests/conftest.py,4326a15cb25c40f4a51bd0d8bde1045bba4dfc9f,"\""\""\""TODO || pytest-factoryboy seems like more trouble than it's worth here. || For now; I'm switching back to creating my own regular old pytest fixtures; || this could be problematic in the future if we end needing any tricky circular || dependenices; but it's much simpler than trying to keep track of how all the || variable injection performed by `pytest_factoryboy.register` is working. || \""\""\""",https://github.com/skyportal/skyportal/commit/4326a15cb25c40f4a51bd0d8bde1045bba4dfc9f,Yes
2755,tasoc/starclass,starclass/SORTINGHATClassifier/pyentropy.py,f78aa287576307f2c0b92e92a373fea46bbd4339,TODO add tests,https://github.com/tasoc/starclass/commit/f78aa287576307f2c0b92e92a373fea46bbd4339,Yes
2756,valohai/valohai-cli,tests/commands/execution/test_watch.py,125c10f40c6b3da2b291d7e7074c6d5bc4604869,TODO: Further test output?,https://github.com/valohai/valohai-cli/commit/125c10f40c6b3da2b291d7e7074c6d5bc4604869,No
2757,valohai/valohai-cli,tests/commands/execution/test_info.py,f761aff167ca0bcade1cd6491aa521d6167d429d,TODO: Further test output?,https://github.com/valohai/valohai-cli/commit/f761aff167ca0bcade1cd6491aa521d6167d429d,No
2758,valohai/valohai-cli,tests/commands/execution/test_delete.py,2298408f1bc64d859e6e59e420a927a255d1c683,TODO: test better?,https://github.com/valohai/valohai-cli/commit/2298408f1bc64d859e6e59e420a927a255d1c683,Yes
2759,valohai/valohai-cli,tests/commands/execution/test_summarize.py,80a287e0a1f98c22268e7de5889ec01c7366c825,TODO: test better?,https://github.com/valohai/valohai-cli/commit/80a287e0a1f98c22268e7de5889ec01c7366c825,Yes
2760,Acellera/moleculekit,moleculekit/smallmol/tools/test_tools.py,41c1ef369d95d2a9574937af97bb5e35cd00ebc3,TODO: Add here tests for tautomers,https://github.com/Acellera/moleculekit/commit/41c1ef369d95d2a9574937af97bb5e35cd00ebc3,Yes
2761,friendsofagape/mt2414,mt2414/FeedbackAligner.py,ccbced51a800e80db1ec22852ab1c039dfcfc18d,"obj.save_alignment(123;[(\""xxx\"";\""YYY\"")];'testcase')",https://github.com/friendsofagape/mt2414/commit/ccbced51a800e80db1ec22852ab1c039dfcfc18d,Yes
2762,ParrotPrediction/pyalcs,alcs/agent/acs2/Classifier.py,bad1ec5572c5281e70b2fce16257d90f429e16ae,TODO p5: write test,https://github.com/ParrotPrediction/pyalcs/commit/bad1ec5572c5281e70b2fce16257d90f429e16ae,Yes
2763,ParrotPrediction/pyalcs,alcs/agent/acs2/ClassifiersList.py,bad1ec5572c5281e70b2fce16257d90f429e16ae,TODO p4: write test,https://github.com/ParrotPrediction/pyalcs/commit/bad1ec5572c5281e70b2fce16257d90f429e16ae,No
2764,ParrotPrediction/pyalcs,alcs/agent/acs2/ClassifiersList.py,bad1ec5572c5281e70b2fce16257d90f429e16ae,TODO: p0: write tests,https://github.com/ParrotPrediction/pyalcs/commit/bad1ec5572c5281e70b2fce16257d90f429e16ae,Yes
2765,ParrotPrediction/pyalcs,alcs/agent/acs2/Effect.py,bad1ec5572c5281e70b2fce16257d90f429e16ae,TODO p1: write some tests,https://github.com/ParrotPrediction/pyalcs/commit/bad1ec5572c5281e70b2fce16257d90f429e16ae,No
2766,ParrotPrediction/pyalcs,alcs/environment/maze/Maze.py,bad1ec5572c5281e70b2fce16257d90f429e16ae,TODO: p0 write tests (edge conditions),https://github.com/ParrotPrediction/pyalcs/commit/bad1ec5572c5281e70b2fce16257d90f429e16ae,No
2767,ParrotPrediction/pyalcs,lcs/agents/racs/Classifier.py,b24b7d1805828310e15cde4738c6279ef21274a1,TODO p5: write test,https://github.com/ParrotPrediction/pyalcs/commit/b24b7d1805828310e15cde4738c6279ef21274a1,Yes
2768,ParrotPrediction/pyalcs,lcs/agents/racs/components/alp.py,6080cb07776cb40e34ae793a9b79466e015c7b37,TODO: write tests,https://github.com/ParrotPrediction/pyalcs/commit/6080cb07776cb40e34ae793a9b79466e015c7b37,Yes
2769,ParrotPrediction/pyalcs,lcs/agents/racs/ClassifierList.py,d1464f2554ffcb6c0c7ebdcde3edf849075586d2,TODO: p0: write tests,https://github.com/ParrotPrediction/pyalcs/commit/d1464f2554ffcb6c0c7ebdcde3edf849075586d2,Yes
2770,ParrotPrediction/pyalcs,lcs/strategies/anticipatory_learning_process.py,0bc327f2aa5866ad67b7610a69dfcab9fa5fccc6,TODO: p0: write tests,https://github.com/ParrotPrediction/pyalcs/commit/0bc327f2aa5866ad67b7610a69dfcab9fa5fccc6,Yes
2771,ParrotPrediction/pyalcs,tests/lcs/agents/xcs/test_XCS.py,c3b4e5cb06718ec9feb2e6ae8f9f58e79f64cb9c,TODO: Do more tests here,https://github.com/ParrotPrediction/pyalcs/commit/c3b4e5cb06718ec9feb2e6ae8f9f58e79f64cb9c,Yes
2772,praekelt/feersum-nlu-api-wrappers,examples/dashboard_import_models.py,764449cf873e7efd7ec6609e15f9b6b369b6c0ab,"\""\""\"" || Example: Shows how to import a model using feersum_nlu_utils. The file formats is defined in the export example. || A model's json object was split into three separate files for instance_detail; training and testing data during export. || These files are combined into on json object used by feersum_nlu_util's import function. || \""\""\""",https://github.com/praekelt/feersum-nlu-api-wrappers/commit/764449cf873e7efd7ec6609e15f9b6b369b6c0ab,Yes
2773,UGentBiomath/wwdata,setup.py,6a8213c3c49d97e921fd869e16382411fa7db37e,TODO: put package test requirements here,https://github.com/UGentBiomath/wwdata/commit/6a8213c3c49d97e921fd869e16382411fa7db37e,No
2774,AllenInstitute/mouse_connectivity_models,voxel_model/test/test_masks.py,a5b9e7fe31df5669300dd25b9fc4570992016115,TODO : test load\/save,https://github.com/AllenInstitute/mouse_connectivity_models/commit/a5b9e7fe31df5669300dd25b9fc4570992016115,No
2775,AllenInstitute/mouse_connectivity_models,voxel_model/regressors/least_squares/tests/test_scipy_least_squares.py,659fe554e29c0c7d4d77e8b3b957f097788d1d3a,TODO: all testing,https://github.com/AllenInstitute/mouse_connectivity_models/commit/659fe554e29c0c7d4d77e8b3b957f097788d1d3a,Yes
2776,AllenInstitute/mouse_connectivity_models,voxel_model/models/tests/test_homogeneous.py,9ee13993fa9f8fd7862b493da61516bbc0c4d02d,test method removes correct columns,https://github.com/AllenInstitute/mouse_connectivity_models/commit/9ee13993fa9f8fd7862b493da61516bbc0c4d02d,Yes
2777,AllenInstitute/mouse_connectivity_models,voxel_model/models/tests/test_homogeneous.py,9ee13993fa9f8fd7862b493da61516bbc0c4d02d,test columns is set,https://github.com/AllenInstitute/mouse_connectivity_models/commit/9ee13993fa9f8fd7862b493da61516bbc0c4d02d,Yes
2778,AllenInstitute/mouse_connectivity_models,voxel_model/models/homogeneous/tests/test_homogeneous_model.py,1b878fec760dec4b499ff4006b7e48e00a7359b2,test columns is set,https://github.com/AllenInstitute/mouse_connectivity_models/commit/1b878fec760dec4b499ff4006b7e48e00a7359b2,Yes
2779,AllenInstitute/mouse_connectivity_models,voxel_model/models/homogeneous/tests/test_subset_selection.py,1b878fec760dec4b499ff4006b7e48e00a7359b2,test method removes correct columns,https://github.com/AllenInstitute/mouse_connectivity_models/commit/1b878fec760dec4b499ff4006b7e48e00a7359b2,Yes
2780,spacetelescope/jwql,jwql/instrument_monitors/common_monitors/bias_monitor.py,4d27cf13033e0a1b084fd6af12fd6ff9bc63c3a2,TODO need to write\/test this function,https://github.com/spacetelescope/jwql/commit/4d27cf13033e0a1b084fd6af12fd6ff9bc63c3a2,Yes
2781,spacetelescope/jwql,jwql/instrument_monitors/common_monitors/readnoise_monitor.py,63d793447bb46eb62784d9bf0b844211e1560afb,TODO set this to 10 after testing so MIRI will also be ok,https://github.com/spacetelescope/jwql/commit/63d793447bb46eb62784d9bf0b844211e1560afb,No
2782,spacetelescope/jwql,jwql/instrument_monitors/common_monitors/readnoise_monitor.py,a4240334c3c32745489189a925a30984f4fcfa4a,Skip processing if the file doesnt have enough groups to calculate the readnoise TODO change to 10 after testing so MIRI is also oK,https://github.com/spacetelescope/jwql/commit/a4240334c3c32745489189a925a30984f4fcfa4a,Yes
2783,UiO-CS/tf-wavelets,tfwavelets/dwtcoeffs.py,0f9b62707e12eaf5cebfea0771b55127266e4e27,TODO: Indexing not thoroughly tested,https://github.com/UiO-CS/tf-wavelets/commit/0f9b62707e12eaf5cebfea0771b55127266e4e27,Yes
2784,ur-whitelab/hoomd-tf,tensorflow_plugin/test-py/test_tensorflow.py,17252e683ddad0504684c60b6b71fd50a0ae9161,TODO: write test for changing particle number dynamically,https://github.com/ur-whitelab/hoomd-tf/commit/17252e683ddad0504684c60b6b71fd50a0ae9161,Yes
2785,ur-whitelab/hoomd-tf,tensorflow_plugin/test-py/test_utils.py,0e1bb432577639e0017a15b4428742d32c4486ee,TODO: Come up with a real test of this.,https://github.com/ur-whitelab/hoomd-tf/commit/0e1bb432577639e0017a15b4428742d32c4486ee,Yes
2786,ur-whitelab/hoomd-tf,htf/test-py/build_examples.py,37fd121612e0f04231bae54a59ab52904e948e62,TODO: Smoke test. Think of a better test.,https://github.com/ur-whitelab/hoomd-tf/commit/37fd121612e0f04231bae54a59ab52904e948e62,Yes
2787,nearthlab/image-segmentation,submodules/classification_models/classification_models/keras_applications/keras_applications/tests/applications_test.py,81154272814b6caa3a49d30d0018569ea40a71d0,TODO: remove the use of multiprocessing from these tests,https://github.com/nearthlab/image-segmentation/commit/81154272814b6caa3a49d30d0018569ea40a71d0,No
2788,pandoraboxchain/pyrrha-pynode,src/processor.py,277a24a7c5f900f7caa43047272c45fc830e252f,TODO: Test and raise exceptions,https://github.com/pandoraboxchain/pyrrha-pynode/commit/277a24a7c5f900f7caa43047272c45fc830e252f,Yes
2789,pandoraboxchain/pyrrha-pynode,tests/unit/eth/test_eth_module.py,aab388404217e3b2debbb28f90334e4968d4f105,TODO The test passes; but it may be necessary to refactor the connector for more control,https://github.com/pandoraboxchain/pyrrha-pynode/commit/aab388404217e3b2debbb28f90334e4968d4f105,Yes
2790,trungnt13/odin-ai,odin/backend/theano/tensor.py,b8a3726aa054d2764b13c1041b6ad080a23d833d,TODO: remove this when stop testing,https://github.com/trungnt13/odin-ai/commit/b8a3726aa054d2764b13c1041b6ad080a23d833d,Yes
2791,Neuraxio/Neuraxle,testing/test_pipeline.py,88cf1d133d11ed8fec0acbe0202d3fefa5ed8003,"TODO: test the \""patch_missing_names\"" method.",https://github.com/Neuraxio/Neuraxle/commit/88cf1d133d11ed8fec0acbe0202d3fefa5ed8003,Yes
2792,Neuraxio/Neuraxle,testing/api/flask.py,614bf536b01ce3a482ae58f881e82b9790e9810c,TODO: make the test pass as it is designed.,https://github.com/Neuraxio/Neuraxle/commit/614bf536b01ce3a482ae58f881e82b9790e9810c,No
2793,Neuraxio/Neuraxle,testing/metaopt/test_automl.py,05e908bdf1ab97cc679a7976009bd520d9682408,TODO: fix this unit test,https://github.com/Neuraxio/Neuraxle/commit/05e908bdf1ab97cc679a7976009bd520d9682408,Yes
2794,Neuraxio/Neuraxle,testing/steps/test_if_execution_phase_is_then_do.py,dc261396df8312818d1f2bc466b5a398b9644cc1,TODO : add test for ExecutionPhaseSwitch,https://github.com/Neuraxio/Neuraxle/commit/dc261396df8312818d1f2bc466b5a398b9644cc1,Yes
2795,mikevoets/jama16-retina-replication,tensorflow/retina5.py,d8462726fb17293635c680b29421008e5dfc38cb,TODO: Perform test.,https://github.com/mikevoets/jama16-retina-replication/commit/d8462726fb17293635c680b29421008e5dfc38cb,No
2796,mikevoets/jama16-retina-replication,tensorflow/retina5.py,c6ccae82f81ae6d18ee869e1ed62181f11e46eb2,TODO: Perform test.,https://github.com/mikevoets/jama16-retina-replication/commit/c6ccae82f81ae6d18ee869e1ed62181f11e46eb2,No
2797,SimGus/Chatette,chatette/refactor_units/generating_item.py,cc23fb055f67adf3109a5159858381f942935b0c,TODO TMP (for testing purposes),https://github.com/SimGus/Chatette/commit/cc23fb055f67adf3109a5159858381f942935b0c,Yes
2798,Zenohm/Friday,setup.py,89960b9b4ee43a55dc9ff00defe4f89560445564,TODO: put package test requirements here,https://github.com/Zenohm/Friday/commit/89960b9b4ee43a55dc9ff00defe4f89560445564,No
2799,snakeztc/NeuralDialog-LaRL,latent_dialog/evaluators.py,7840453b5153c5e1c1289ac6daa7ca01cd36b080,TODO test it,https://github.com/snakeztc/NeuralDialog-LaRL/commit/7840453b5153c5e1c1289ac6daa7ca01cd36b080,Yes
2800,ShreyAmbesh/Traffic-Rule-Violation-Detection-System,core/losses_test.py,c92e89118945f181e91391c12c51117e58def69e,TODO: Also test logit_scale with anchorwise=False.,https://github.com/ShreyAmbesh/Traffic-Rule-Violation-Detection-System/commit/c92e89118945f181e91391c12c51117e58def69e,Yes
2801,ShreyAmbesh/Traffic-Rule-Violation-Detection-System,dataset_tools/create_pet_tf_record.py,c92e89118945f181e91391c12c51117e58def69e,TODO: Add test for pet\/PASCAL main files.,https://github.com/ShreyAmbesh/Traffic-Rule-Violation-Detection-System/commit/c92e89118945f181e91391c12c51117e58def69e,Yes
2802,ShreyAmbesh/Traffic-Rule-Violation-Detection-System,inputs_test.py,c92e89118945f181e91391c12c51117e58def69e,TODO: Make sure these tests work fine outside google3.,https://github.com/ShreyAmbesh/Traffic-Rule-Violation-Detection-System/commit/c92e89118945f181e91391c12c51117e58def69e,No
2803,ShreyAmbesh/Traffic-Rule-Violation-Detection-System,models/feature_map_generators_test.py,c92e89118945f181e91391c12c51117e58def69e,TODO: add tests with different anchor strides.,https://github.com/ShreyAmbesh/Traffic-Rule-Violation-Detection-System/commit/c92e89118945f181e91391c12c51117e58def69e,Yes
2804,henrysky/astroNN,astroNN/shared/patch_util.py,d0aefca817f46dd66147212869e75179ddf42f0c,XXX testcase,https://github.com/henrysky/astroNN/commit/d0aefca817f46dd66147212869e75179ddf42f0c,Yes
2805,henrysky/astroNN,astroNN/models/base_master_nn.py,547c12318f1dbd32769b1ba8dc6a2b7d9217e6be,TODO: add detail error msg; add test,https://github.com/henrysky/astroNN/commit/547c12318f1dbd32769b1ba8dc6a2b7d9217e6be,No
2806,henrysky/astroNN,astroNN/shared/dict_tools.py,04a0d01e16ac0bc371893726dd3055224ec1241d,TODO: need detailed test,https://github.com/henrysky/astroNN/commit/04a0d01e16ac0bc371893726dd3055224ec1241d,Yes
2807,araffin/srl-zoo,evaluation/create_mosaic.py,701b04a5a70821b044e0e078b44c4300316ad513,"\""\""\"" || NOTE; if sklearn.neighbours import fails; remove  and install: || Either use conda (in which case all your installed packages would be in ~\/miniconda\/ or pip install --user don't mix the two and do not use -U; nor sudo. ||  Removing either || rm -rf ~\/.local\/lib\/python2.7\/site-packages\/sklearn or your ~\/miniconda folder and reinstalling it cleanly should fix this. || sudo rm -rf scikit_learn-0.18.1.egg-info\/ || pip uninstall sklearn || and || 1)  pip install --user scikit-learn || or 2) conda install -c anaconda scikit-learn=0.18.1 || If needed; also do || pip install --user numpy || pip install --user scipy ||  || NOTE: Q: if this error is obtained: _tkinter.TclError: no display name and no $DISPLAY environment variable || A: Instead of ssh account@machine; do: ssh -X ||  || Example to run this program for a given trained model: || python generateNNImages.py 5 5 Log\/modelY2017_D24_M06_H06M19S10_staticButtonSimplest_resnet_cont_MCD0_8_S0_4 || IMPORTANT: In order to run it with a non random fixed test set of images; || call it with only one argument (the number of neigbours to generate for each || image in the test set and it will assess the test set of 50 images defined in Const.lua and Utils.py) ||  || \""\""\""",https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,Yes
2808,araffin/srl-zoo,evaluation/generateNNImages.py,701b04a5a70821b044e0e078b44c4300316ad513,"\""\""\"" || NOTE; if sklearn.neighbours import fails; remove  and install: || Either use conda (in which case all your installed packages would be in ~\/miniconda\/ or pip install --user don't mix the two and do not use -U; nor sudo. ||  Removing either || rm -rf ~\/.local\/lib\/python2.7\/site-packages\/sklearn or your ~\/miniconda folder and reinstalling it cleanly should fix this. || sudo rm -rf scikit_learn-0.18.1.egg-info\/ || pip uninstall sklearn || and || 1)  pip install --user scikit-learn || or 2) conda install -c anaconda scikit-learn=0.18.1 || If needed; also do || pip install --user numpy || pip install --user scipy ||  || NOTE: Q: if this error is obtained: _tkinter.TclError: no display name and no $DISPLAY environment variable || A: Instead of ssh account@machine; do: ssh -X ||  || Example to run this program for a given trained model: || python generateNNImages.py 5 5 Log\/modelY2017_D24_M06_H06M19S10_staticButtonSimplest_resnet_cont_MCD0_8_S0_4 || IMPORTANT: In order to run it with a non random fixed test set of images; || call it with only one argument (the number of neigbours to generate for each || image in the test set and it will assess the test set of 50 images defined in Const.lua and Utils.py) ||  || \""\""\""",https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,Yes
2809,araffin/srl-zoo,evaluation/generateNNImages.py,701b04a5a70821b044e0e078b44c4300316ad513,TODO process separate test set,https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,Yes
2810,araffin/srl-zoo,evaluation/generateNNImages.py,701b04a5a70821b044e0e078b44c4300316ad513,TODO: more efficient: for img_name in test_set.keys() revising data above:,https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,Yes
2811,araffin/srl-zoo,evaluation/plotStates.py,701b04a5a70821b044e0e078b44c4300316ad513,ONLY FOR FAST TESTING !!:   model_name = MOBILE_ROBOT#STATIC_BUTTON_SIMPLEST#'pushingButton3DAugmented' #TODO REMOVE-testing  model_name = MOBILE_ROBOT,https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,Yes
2812,araffin/srl-zoo,const.py,1b48736e4499aa6960c5faed727effdfd6d7e18e,TODO create NONSTATIC_BUTTON_MOVIE_TEST_SET,https://github.com/araffin/srl-zoo/commit/1b48736e4499aa6960c5faed727effdfd6d7e18e,No
2813,araffin/srl-zoo,utils.py,e03f1711d8b00bbb778cb37149d938dbe5adb92a,TODO create NONSTATIC_BUTTON_MOVIE_TEST_SET,https://github.com/araffin/srl-zoo/commit/e03f1711d8b00bbb778cb37149d938dbe5adb92a,No
2814,davidsbatista/BREDS,build-ground-truth/evaluate.py,8bc3cb40b8662272bd819523e98e2c586546eec4,TODO: testar o difference,https://github.com/davidsbatista/BREDS/commit/8bc3cb40b8662272bd819523e98e2c586546eec4,No
2815,davidsbatista/BREDS,automatic-evaluation/evaluate-tiago.py,d228126a2642ac91fbd663b4e73d3b3724a2db33,TODO: testar o difference,https://github.com/davidsbatista/BREDS/commit/d228126a2642ac91fbd663b4e73d3b3724a2db33,No
2816,davidsbatista/BREDS,automatic-evaluation/evaluate-tiago2.py,d228126a2642ac91fbd663b4e73d3b3724a2db33,TODO: testar o difference,https://github.com/davidsbatista/BREDS/commit/d228126a2642ac91fbd663b4e73d3b3724a2db33,No
2817,davidsbatista/BREDS,automatic-evaluation/evaluate-tiago3.py,d228126a2642ac91fbd663b4e73d3b3724a2db33,TODO: testar o difference,https://github.com/davidsbatista/BREDS/commit/d228126a2642ac91fbd663b4e73d3b3724a2db33,No
2818,davidsbatista/BREDS,BREDS/Tuple.py,ef24353239b0ec3c213db6fb69d467ff918f656f,TODO: e outro teste em que usas a m\u00E9dia dos embeddings das palavras que est\u00E3o no padr\u00E3o de ReVerb.,https://github.com/davidsbatista/BREDS/commit/ef24353239b0ec3c213db6fb69d467ff918f656f,Yes
2819,davidsbatista/BREDS,patterns-word2vec.py,683862a84b3eba84bf1a3bd2de2d3b8b30c5f194,TODO: testar este caso,https://github.com/davidsbatista/BREDS/commit/683862a84b3eba84bf1a3bd2de2d3b8b30c5f194,No
2820,davidsbatista/BREDS,patterns-word2vec.py,bbb4e314a13624124ed882e1bc663faa1b2db8c2,TODO: testar este caso,https://github.com/davidsbatista/BREDS/commit/bbb4e314a13624124ed882e1bc663faa1b2db8c2,No
2821,davidsbatista/BREDS,Misc./example_parse.py,afe93c6bece33679ceda90f74e00a2879a0fe533,TODO: compute shortest dependency path,https://github.com/davidsbatista/BREDS/commit/afe93c6bece33679ceda90f74e00a2879a0fe533,No
2822,davidsbatista/BREDS,BREDS/Sentence.py,3ed54cdfb146a7399f5eacb6c1f66e435996f730,TODO: teste com simple,https://github.com/davidsbatista/BREDS/commit/3ed54cdfb146a7399f5eacb6c1f66e435996f730,Yes
2823,josedolz/HyperDenseNet,src/HyperDenseNet/HyperDenseNet.py,18c35dd81927707d8e0b17b357b38540a266b33e,TODO: Change to output_Test,https://github.com/josedolz/HyperDenseNet/commit/18c35dd81927707d8e0b17b357b38540a266b33e,Yes
2824,Mogeng/IOHMM,code/family.py,c4c3b9487528d8e65e6ab4428738256d34cdd420,TODO: add the ability to use the power links with an if test,https://github.com/Mogeng/IOHMM/commit/c4c3b9487528d8e65e6ab4428738256d34cdd420,No
2825,Mogeng/IOHMM,code/links.py,c4c3b9487528d8e65e6ab4428738256d34cdd420,TODO: the CDFLink is untested,https://github.com/Mogeng/IOHMM/commit/c4c3b9487528d8e65e6ab4428738256d34cdd420,No
2826,Mogeng/IOHMM,auxiliary/family.py,d2f5b627944481d5f27ba499736fd90636b12a27,TODO: add the ability to use the power links with an if test,https://github.com/Mogeng/IOHMM/commit/d2f5b627944481d5f27ba499736fd90636b12a27,No
2827,YannDubs/disentangling-vae,disvae/models/losses.py,6b505767722b58f17436e193d8f719ded30b19f9,TODO: test if batchTC a lot worst; if not remove `is_mutual_info` in factorKL,https://github.com/YannDubs/disentangling-vae/commit/6b505767722b58f17436e193d8f719ded30b19f9,No
2828,proycon/clam,clamservice.py,17aafc799f90ecfcf19e77e06a6a9d7f1da4cdc7,TODO: test,https://github.com/proycon/clam/commit/17aafc799f90ecfcf19e77e06a6a9d7f1da4cdc7,No
2829,nmstoker/lockebot,basebot.py,d49862174dc0de6a53740058bd407bad91c3b98e,TODO: test the behaviour on Windows,https://github.com/nmstoker/lockebot/commit/d49862174dc0de6a53740058bd407bad91c3b98e,Yes
2830,nmstoker/lockebot,roybot.py,3d8056558140661ae8a9b73fce71a41f0267e071,TODO: test the behaviour on Windows,https://github.com/nmstoker/lockebot/commit/3d8056558140661ae8a9b73fce71a41f0267e071,Yes
2831,nmstoker/lockebot,roybot.py,d0de4df0efe20e67e10adcaee015e005cb84049a,TODO: test the behaviour on Windows,https://github.com/nmstoker/lockebot/commit/d0de4df0efe20e67e10adcaee015e005cb84049a,Yes
2832,JingqingZ/BaiduTraffic,src/train.py,deae36ee1d3dfbc72be9c94c7455802857f746ec,TODO: test has not GT,https://github.com/JingqingZ/BaiduTraffic/commit/deae36ee1d3dfbc72be9c94c7455802857f746ec,Yes
2833,tlkh/prowler,wordlists/compute.py,5b9a897a6ea89b74b0ada85f04d6c538017cdaf4,"''' ||         import pyrebase ||         config = { ||             \""apiKey\"": \""\""; ||             \""authDomain\"": \""clusterscanner.firebaseio.com\""; ||             \""databaseURL\"": \""https:\/\/clusterscanner.firebaseio.com\/\""; ||             \""storageBucket\"": \""clusterscanner.appspot.com\"" ||         } ||         firebase = pyrebase.initialize_app(config) ||         auth = firebase.auth() ||         user = auth.sign_in_with_email_and_password(\""pi@cluster.pi\""; \""\"") ||         db = firebase.database()  # reference to the database service ||         hoststruct = hostname.split(\"".\"") ||         data = {\""hostname\"": hostname; ||                 \""services\"": services; ||                 \""status\"": status} ||         results = db.child(hoststruct[0]).child(hoststruct[1]).child( ||             hoststruct[2]).child(hoststruct[3]).set(data; user['idToken']) ||     else: ||         valid = \""dead\"" ||     return (hostname; valid) ||  ||  || if __name__ == '__main__': ||     import dispy ||     import dispy.httpd ||     import time ||  ||     workers = ['192.168.0.133';'192.168.0.110'] ||  ||     cluster = dispy.JobCluster( ||         compute; nodes=workers; ip_addr='192.168.0.142') ||     http_server = dispy.httpd.DispyHTTPServer(cluster) ||  ||     jobs = [] ||     test_range = [] ||     for i in range(0; 1): ||         for j in range(100; 200): ||             test_range.append(\""172.22.\"" + str(i) + \"".\"" + str(j)) ||     print(\""Testing \"" + str(len(test_range)) + \"" hostnames\"") ||  ||     time.sleep(4) ||     cluster.print_status() ||  ||     start = time.time() ||  ||     for i; address in enumerate(test_range): ||         # schedule execution of 'compute' on a node (running 'dispynode') ||         # with a parameter (random number in this case) ||         job = cluster.submit(address) ||         job.id = i  # optionally associate an ID to job (if needed later) ||         jobs.append(job) ||     # cluster.wait() # waits for all scheduled jobs to finish ||  ||     for job in jobs: ||         try: ||             hostname; valid = job()  # waits for job to finish and returns results ||             print(job.ip_addr + \"": \"" + hostname + \"" is \"" + valid + \"".\"") ||             # other fields of 'job' that may be useful: ||             # print(job.stdout; job.stderr; job.exception; job.ip_addr; job.start_time; job.end_time) ||         except Exception as e: ||             print(str(job) + \"" failed: \"" + str(e)) ||  ||     end = time.time() ||     cluster.print_status() ||     http_server.shutdown() ||     cluster.close() ||  ||     print(\""\"") ||     print(\""Total time taken = \"" + str(end - start)) || '''",https://github.com/tlkh/prowler/commit/5b9a897a6ea89b74b0ada85f04d6c538017cdaf4,No
2834,tlkh/prowler,compute.py,f0d9ff7ec7d455a41597c8829c18aed7fee7abbc,"''' || if __name__ == '__main__': ||     import dispy ||     import logging ||     import dispy.httpd ||     import time ||  ||     print(\""Initialising Cluster\"") ||  ||     workers = ['192.168.0.133';'192.168.0.110';'169.254.102.163';'169.254.116.199';'169.254.114.226';'169.254.156.34'] ||  ||     cluster = dispy.JobCluster(compute; nodes=workers; ip_addr='192.168.0.142') ||     http_server = dispy.httpd.DispyHTTPServer(cluster) ||  ||     jobs; test_range = []; [] ||  ||     for i in range(0; 1): ||         for j in range(0; 255): ||             test_range.append(\""192.168.\"" + str(i) + \"".\"" + str(j)) ||  ||     print(\""Testing \"" + str(len(test_range)) + \"" hostnames\"") ||  ||     time.sleep(4) # make sure cluster is connected ||     cluster.print_status() ||  ||     start = time.time() ||  ||     for i; address in enumerate(test_range): ||         # schedule execution of 'compute' on a node (running 'dispynode.py') with a parameter ||         job = cluster.submit(address) ||         job.id = i  # optionally associate an ID to job (if needed later) ||         jobs.append(job) ||     # cluster.wait() # waits for all scheduled jobs to finish ||  ||     for job in jobs: ||         try: ||             result = job() ||             hostname; valid; breached; credentials = result  # waits for job to finish and returns results ||             print(job.ip_addr + \"": \"" + hostname + \"" is \"" + valid + \"". Breached:\""; breached; \""with credentials\""; credentials)  ||             print('OS Description : {0}'.format(osclass['osfamily']) for osclass in nmap.Portscanner[job.ip_addr]['osclass']) ||             # other fields of 'job' that may be useful: ||             # print(job.stdout; job.stderr; job.exception; job.ip_addr; job.start_time; job.end_time) ||         except Exception as e: ||             print(str(job);\""failed with error:\"";str(e)) ||             print(\""debug:\""; job.stdout; job.stderr; job.exception) ||  ||     end = time.time() ||     cluster.print_status() ||     http_server.shutdown() ||     cluster.close() ||  ||     print(\""\ || \"";\""Total time taken =\""; str(end - start)) || '''",https://github.com/tlkh/prowler/commit/f0d9ff7ec7d455a41597c8829c18aed7fee7abbc,Yes
2835,mnicnc404/CartoonGan-tensorflow,train.py,7502b964b8786af28b8090cff73e2dbeb4d118e4,FIXME: test only,https://github.com/mnicnc404/CartoonGan-tensorflow/commit/7502b964b8786af28b8090cff73e2dbeb4d118e4,No
2836,mnicnc404/CartoonGan-tensorflow,train.py,7502b964b8786af28b8090cff73e2dbeb4d118e4,fig.add_subplot(num_rows; 1; i + 1) # FIXME: test only,https://github.com/mnicnc404/CartoonGan-tensorflow/commit/7502b964b8786af28b8090cff73e2dbeb4d118e4,Yes
2837,mnicnc404/CartoonGan-tensorflow,tf2/cartoonizer.py,32b3c6863637fba2ba1e72e62d39c26e5dfe7b0d,TODO: test the feature,https://github.com/mnicnc404/CartoonGan-tensorflow/commit/32b3c6863637fba2ba1e72e62d39c26e5dfe7b0d,No
2838,mnicnc404/CartoonGan-tensorflow,tf2/keras-contrib/examples/improved_wgan.py,ec970dc57b0f613c179dda9c61418e437ef146e9,"\""\""\""An implementation of the improved WGAN described in https:\/\/arxiv.org\/abs\/1704.00028 ||  || The improved WGAN has a term in the loss function which penalizes the network if its || gradient norm moves away from 1. This is included because the Earth Mover (EM) distance || used in WGANs is only easy to calculate for 1-Lipschitz functions (i.e. functions where || the gradient norm has a constant upper bound of 1). ||  || The original WGAN paper enforced this by clipping weights to very small values || [-0.01; 0.01]. However; this drastically reduced network capacity. Penalizing the || gradient norm is more natural; but this requires second-order gradients. These are not || supported for some tensorflow ops (particularly MaxPool and AveragePool) in the current || release (1.0.x); but they are supported in the current nightly builds || (1.1.0-rc1 and higher). ||  || To avoid this; this model uses strided convolutions instead of Average\/Maxpooling for || downsampling. If you wish to use pooling operations in your discriminator; please ensure || you update Tensorflow to 1.1.0-rc1 or higher. I haven't tested this with Theano at all. ||  || The model saves images using pillow. If you don't have pillow; either install it or || remove the calls to generate_images. || \""\""\""",https://github.com/mnicnc404/CartoonGan-tensorflow/commit/ec970dc57b0f613c179dda9c61418e437ef146e9,Yes
2839,mnicnc404/CartoonGan-tensorflow,tf2/keras-contrib/tests/keras_contrib/layers/test_crf.py,ec970dc57b0f613c179dda9c61418e437ef146e9,test marginal learn mode; fix length,https://github.com/mnicnc404/CartoonGan-tensorflow/commit/ec970dc57b0f613c179dda9c61418e437ef146e9,No
2840,joeylitalien/noise2noise-pytorch,src/noise2noise.py,c36f7df69280a7a1d2b9beeb6a10f9e308665e8f,Use batch size of 1; if needed (e.g. test set),https://github.com/joeylitalien/noise2noise-pytorch/commit/c36f7df69280a7a1d2b9beeb6a10f9e308665e8f,No
2841,joeylitalien/noise2noise-pytorch,src/dataset.py,7943264b7f9f41b89de537d8e94e1997b49eb8c4,Use batch size of 1; if needed (e.g. test set),https://github.com/joeylitalien/noise2noise-pytorch/commit/7943264b7f9f41b89de537d8e94e1997b49eb8c4,No
2842,joeylitalien/noise2noise-pytorch,src/datasets.py,7aa25c617e95629269677dcec048befbbb46c692,Fix noise parameter when testing,https://github.com/joeylitalien/noise2noise-pytorch/commit/7aa25c617e95629269677dcec048befbbb46c692,No
2843,joeylitalien/noise2noise-pytorch,src/datasets.py,0b394b787fd3ca25682f3987bb7dacdf5cdadaf1,Fix noise parameter when testing,https://github.com/joeylitalien/noise2noise-pytorch/commit/0b394b787fd3ca25682f3987bb7dacdf5cdadaf1,No
2844,budzianowski/multiwoz,evaluate.py,51e70e3a99aba9bee412b916ba5cfe85bb836727,TODO test it,https://github.com/budzianowski/multiwoz/commit/51e70e3a99aba9bee412b916ba5cfe85bb836727,Yes
2845,regel/loudml,loudml/loudml/som.py,2aee10efd09bd0d6e84cbbc1c95e646321235d74,TODO: clean-up and unit tests,https://github.com/regel/loudml/commit/2aee10efd09bd0d6e84cbbc1c95e646321235d74,Yes
2846,regel/loudml,loudml-import/loudml/phone_rates.py,7d05482ec58ec831dcc65263db9dd5a0e13fd857,TODO: Add unit test for non empty group,https://github.com/regel/loudml/commit/7d05482ec58ec831dcc65263db9dd5a0e13fd857,Yes
2847,regel/loudml,loudml/loudml/donut.py,be739a3a28157899fb339579dc2bcd134b587abd,FIXME: test x_dim and y_dim; must be in [0; latent_dim-1] range,https://github.com/regel/loudml/commit/be739a3a28157899fb339579dc2bcd134b587abd,Yes
2848,regel/loudml,tests/test_opentsdb.py,054c3ea22e9b93fbf46865e304b49300456e8c40,TODO: clear test data somehow; drop() does nothing right now,https://github.com/regel/loudml/commit/054c3ea22e9b93fbf46865e304b49300456e8c40,Yes
2849,drckf/paysage,paysage/models/gradient_util.py,26e62d863cfd2eec1126b72d82cd5676f2407a30,TODO: add a test for zero_grad,https://github.com/drckf/paysage/commit/26e62d863cfd2eec1126b72d82cd5676f2407a30,Yes
2850,drckf/paysage,test/test_backends.py,5831e065b3bc3a361bae31eab5d0bde565e9da1a,TODO: add tests,https://github.com/drckf/paysage/commit/5831e065b3bc3a361bae31eab5d0bde565e9da1a,Yes
2851,drckf/paysage,paysage/batch/hdf.py,d955c5d5fd9e42372be705976fe0f563b2f4b4ab,change parameters as needed with a test call,https://github.com/drckf/paysage/commit/d955c5d5fd9e42372be705976fe0f563b2f4b4ab,Yes
2852,drckf/paysage,paysage/batch/in_memory.py,d955c5d5fd9e42372be705976fe0f563b2f4b4ab,change parameters as needed with a test call,https://github.com/drckf/paysage/commit/d955c5d5fd9e42372be705976fe0f563b2f4b4ab,Yes
2853,maximecb/gym-miniworld,gym_miniworld/envs/tmaze.py,e7d1f8b760d84e36bb9c78f9ea0e81ca4edfc467,TODO: proper intersection test method for entities,https://github.com/maximecb/gym-miniworld/commit/e7d1f8b760d84e36bb9c78f9ea0e81ca4edfc467,No
2854,quadrismegistus/prosodic,croll/croll.py,06fb02e8d3e38bb813088049413764ecf67ceadf,"\""\""\"" ||  || According to medieval theory; the cursus was used at tile ends  || of the commata; cola; and periodus (or conclusio); the parts; large  || or small; of which a rhetorical period is constructed. (4) ||  || In practice; I will try to show; it was not used  || only in the final positions. ||  || One is to include in the number all rhetorical  || divisions of a period (all the Sunday collects consist of a single  || period; that is; a single articulated sentence); however short; all  || the commata and cola; that is; which according to Latin rule might  || have cursus-endings. ||  || The only other natural process is  || to count those divisions of a prayer which are indicated by semi-  || colons in the authorized editions. ||  || There would be advantages in each method; but I choose the latter;  || the method of counting the pauses marked by semi-colons; because  || it eliminates the necessity of doubtful decisions. ||  || The result; it must be said; is not favorable to Shelly's conclu-  || sions. Of the 113 endings occurring at the places described only  || 43; or 38%o  || of the whole number; are in the three forms; according  || to the strictest possible interpretation of the requirements of these  || forms; only 45; or 40%; according to the freest interpretation of  || them.9 ||  ||  ||  ||  || the cadences occur in English where there are  || none in Latin and vice versa. ||  || The purpose of the two following chapters  || of this paper will be to show that he has limited too narrowly the  || area in which we may properly look for the influence of the cursus  || in the Collects; in the first place in his study of the forms of English  || cadence; and in the second place in his ideas concerning the places  || where cadence may occur. ||  || There are three variations of the regular Latin forms which  || would be most likely to appear with frequency if the translators  || worked in the free way we have described.  ||  || 1. The endinog  || velox would easily become 8 -4 -2 in English;  || and woitud  || not lose its essential character in so doing. Some ex-  || amples are:-carry us through all temptations (4th Sun. aft. Ep.);  || defended by thy mighty power (5th Sun. aft. Ep.); partakers of  || thy resurrection (Sun. bef. East.); the weakness of our mortal  || nature (Tr. Sun.); declarest thy almighty power (1lth Sun. aft.  || Tr.); continually to be given (17th Sun. aft. Tr.).  ||  || 2. Velox again could be modified by the addition of a light  || syllable at the end; the form tlhus becoming 8-5-3 instead of  || 7 - 4- 2; or 9 - 5 - 3 instead of 8 - 4 - 2. This is a very common  || ending:-defend us from all adversities (Tr. Sun.) ; serve thee in  || all godly quietness (5th Sun. aft. Tr.) ; return into the way of  || righteousness (3d Sun. aft. East.) ; always prevent and follow us 14  || (17th Sun. aft. Tr.) ; visit us in great humility (lst Sun. in Adv.);  || the example of his great humility (Sun. bef. East.); our defence  || against all our enemies (3d Sun. in Lent); protection of thy good  || Providence (2d Sun. aft. Tr.); hearts of the disobedient (3d Sun.  || in Adv.).  ||  || 3. Tardus would often become 7 - 3 instead of 6 - 3. This is  || in fact the commoner form; I believe; in elevated prose; and cer-  || tainly some of the most beautiful phrases in the prayer-book owe  || their character to it. Examples are:-several necessities (All  || Cond. of Men); dangers and adversities (3d Sun. aft. Tr.);  || troubles and adversities (Collect in the Litany); free from all ad-  || versities (22d Sun. aft. Tr.); acknowledging our wretchedness  || (Ash-Wed.); ordered by thy governance (5th Sun. aft. Tr.);  || never-failing Providence (8th Sun. aft. Tr.).  ||  ||  ||  || What are the reasons then for accepting this principle; that is;  || for expecting the three variations; and perhaps still others; to  || appear in English as equivalents of the regular Latin forms?  || There are two; both derived from differences between the two lan-  || guages: the first from a difference in the character of their words;  || the second from a difference in their metrical character and cus-  || toms.  || I. English is far less polysyllabic than Latin. It had been so  || even in its classical Anglo-Saxon form; in the period when Anglo-  || Saxon was enjoying its highest courtly ana literary cultivation;  || and with the loss of inflections which attended its rapid decline ||  ||  || The only point to be made here; however; is the more general one  || that in as far as this process of Latinization of the vocabulary had  || gone on it was possible to have the cadences in English;-and no  || furtlher. Native English was not of a character to lend itself to  || them; aind it had become still more foreign to them during the  || period of its decline. ||  ||  || [SECONDARY STRESS:] || This point may first be illustrated by a rather full consideration  || of velox. This form is very inadequately represented by the for-  || mula 7-4-2; _uuvu-- u for it is of its essence that the accent  || on 4 shall be subordinate to that on 2; and the characteristic case  || of it is that in which it ends in a four- or five-syllable word; with  || the main accent on the penult; and hence (according to Latin rule)  || a subordinate accent on the second syllable preceding. Thus-  || et ad implenda quae viderint convalescant (lst Sun. aft. Ep.);  || misericorditer liberemur (Sept. Sun). ||  ||  ||  || [LATIN WORDS:] || And;  || moreover; it is to be observed that there were not so many of them  || in the middle of the sixteentlh century as thlere  || are now. Not many  || can be gathered from the prayer-book  || itself :-confirmation; media-  || tion; resurrection; supplications; satisfaction; regeneration; circum-  || cision; advantageous and a few others; nearly all words in -ion or  || else words that are not likely to occur at the ends of phrases.' ||  ||  || [VELOX] || Again: In the case of the two-word phrases ending a velox; as  || mortal nature; faithful servants; etc.; there is a departure from  || the exact Latin effect; but in the opposite direction from that just  || mentioned. That is; there is here a tendency to put too strong an  || accent on the adjective; and hence to give too mueh importance to  || the minor accent of the cadence ||  || This effect will not be produced; however; if the last accent of the  || cadence is followed by two unaccented syllables instead of by one;  || because the lengthening of this unaccented part of the period has  || the effect of strengthening its accent; and the minor accent of the  || preceding period is thus relatively reduced. Defend us from all  || adversities; our defence against all our enemies; and serve thee in  || all godly quietness are better reproductions of velox than phrases  || of the form 7 - 4 - 2 would be in their places. ||  ||  || The syllable-counting custom of medieval Latin gives a  || definite inalterable value to each unaccented syllable of a metrical  || unit; and a slight difference between the number of such syllables  || in one part of a cadence and another; between the two of the first  || period of velox; for instance; and the one of each of its other  || periods; may be depended upon to produce an effect and establish a  || desired relation between the parts. ||  ||  || It follows that English cadence can never be pro-  || erly described by a numerical system; and that it can never produce  || the same effect as the Latin cadence unless it is allowed a certain  || freedom in its use of unaccented syllables. ||  ||  ||  ||  || [THE PERIOD] ||  || 1. There is no better definition of the period than Hobbes' curt  || translation of Aristotle in his Brief of the Art of Rhetorick  || (1681) : 26 \"" A period is such a part as is perfect in itself; and has  || such length as may easily be comprehended by the understanding.\""  || Aristotle's statement in full (Rhetoric; iii; ch. 9) is as follows:  || \""I call a period a form of words which has independently in itself  || a beginning and ending; and a length easily taken in at a glance.\"" || \tnot a syntactic or logical unit; but on the one hand a psycho-  || \tlogical; and on the other a rhythmical; unit.28 ||  ||  ||  || 2. The parts of a divided period are called members (membra)  || or cola (in medieval Latin also distinctiones or versus); and the  || number of these that may constitute a period is undefined ||  ||  || The \"" harmony;\"" \"" number;\"" or \"" rhythm \"" of a period depends  || chiefly upon the relations between the members of which it consists:  || relations of length; form; and sound. ||  ||  || 3. Some theorists give a place in the doctrine of the period to a  || phenomenon which is very frequent in every oratorical style in  || which there is a certain amplitude and dignity; namely; the com-  || bination of two members; related to each other syntactically in  || certain ways; to form a larger unit within the period. This double  || unit; consisting of two members; is called a phrase ||  || Unless there are at least two phrases;  || balanced in form; we may describe the period as consisting merely  || of members. ||  ||  ||  || 4. A colon of a certain length may fall into two (sometimes even  || three) parts in utterance; the division between them being indicated  || by a pause shorter than that at the end of a colon. One of these  || parts; which; however; like the phrases; never occur singly; is called  || a comma (caesum; incisum; or sometimes in medieval Latin sub-  || distinctio). The division of the colon into commata is not con-  || nected apparently with the physiological process of breathing; or at  || least is not primarily due to this; but is chiefly the effect of a law  || of beauty of sound which seems to demand such a break ||  || It corresponds; that is; to the division of the  || line made by the cesura in formal verse.80 ||  ||  ||  ||  ||  ||  ||  ||  || The fact is that the neglect of this study has  || been due to the tendency to avoid the oratorical models on which  || all the theory of rhetoric is formed; and to consider prose chiefly  || as it is addressed to the intellect; rather than as language spoken  || and heard. The characteristic prose of the nineteenth century  || has been the essay; rather than the address; and even in the eigh-  || teenth century; the great authority of the Addisonian model of  || style; especially as it was described in Blair's widely-used rhetoric;  || tended to outweigh the influence of Johnson; Gibbon; Burke; Rob-  || ertson; and other writers of the latter part of the century; who  || wrote the more copious and sonorous language of oratory. ||  ||  ||  ||  || The English Collects themselves are the best possible corpus for  || such an experiment; first; because they fulfill ideally the conditions  || of an oral prose; and secondly; because they are made in close  || rhetorical imitation of Latin models in which the formal rules of  || the period were observed. ||  ||  ||  || We may be sure that a science of the rhythmic period will  || never be discovered. And if it is true that even in our older prose;  || composed in the regular manner of the rhetorical tradition; we  || often find it necessary to defend by an appeal to personal preference  || our choice of this or that reading; it is certain that the reader will  || find an ever-widening range for the exercise of his artistic gifts of  || interpretation as he approaches the prose of our own time. ||  ||  ||  || [PHRASES!] || The end of any phrase felt as having a unitary character may  || be cadenced; whether or not it coincides with the end of one of the  || divisions of a period.  ||  ||  || 1. A very simple type is that which consists of a noun preceded  || by its adjective. ||  ||  || 2. More interesting is the phrase in which two words; often  || synonyms; are connected by and ||  ||  || 3. The prepositional phrase; that is; a noun; adjective; or verb || with a prepositional modifier following it; is an equally common  || form: ||  ||  ||  ||  ||  || The point urged in this section of my paper certainly does not  || tend to simplify the subject of cadence. It tends rather to blur  || and disarrange some of the definite lines that have been drawn  || about it heretofore. || The same remark may be made; indeed; about  || the preceding section; for the doctrine of the period; though it  || seems to be the only trustworthy guide through the uncertainties  || of cadence-occurrence;  || is itself full of uncertainties; difficulties; and  || problems ||  ||  ||  ||  ||  ||  ||  ||  || When velox may vary in length from  || seven to ten; or even more syllables; and its later accents move about  || as freely as we have been asserting they may; the method of scansion  || becomes absurd. It is true that a table might still be made of the  || forms that produce the required effect; and those that do not; but  || it is far simpler to state general rules which will allow for all the  || varieties of forms that we have discovered. ||  ||  ||  ||  ||  ||  ||  ||  ||  || The rules; then.; are as follows:  ||  || 1. The English cadence ordinarily begins on one of the syllables  || five to ten; counting from the end. It never begins later than the  || fifth; but sometimes the long cadence may begin as far back as the  || eleventh syllable; as in 11 - 7 - 3; or even on the twelfth; as in  || 12 - 8 -4. These are; however; extreme cases.  ||  || 2. The first accent is the strongest in the cadence; as marking  || its beginning. It is the climax as to height of pitch and strength  || of accent of the member in which the cadence occurs; and indicates  || the point at which the tendency to rhythmical form always observ-  || able in oratory; but restrained earlier in the phrase by the necessities  || of logical statement; is finally allowed to appear without check. ||  || 3. At this point a trochaic movement begins which carries  || through to the end of the phrase and cadence. The trochaic move-  || ment of the English cadence is alone enough to mark the influence  || of the classical cadences upon it; for it is not the nature of English  || prose; except under this influence; to keep to the same movement  || (rising or falling) throughout a phrase. ||  || It inclines to shift from  || one to the other; and perhaps prefers; on the whole; to end in a  || rising movement rather than a falling one.  ||  ||  || 4. Each cadence has two accents; of which the first is stronger  || than the second; and is followed by a greater number of unaccented  || syllables; or by an equal number of syllables which makes the effect  || of being greater; than the second. Stated differently; this law is  || that there is an effect of decreasing length of period and strength  || of accent from the beginning of a cadence to the end ||  ||   || 5. If the number of syllables following an accent exceeds three  || a secondary subsidiary accent appears. This rule applies in practice  || only to the period of the first accent because if the second period  || contained more than four syllables it could not seem shorter than  || the first (see rule 4); that is; this rule explains the form of the long  || cadence ||  || Velox in  || Latin is a binary rhythm; the accent on 4 being only of importance || as serving to prop up or carry on the long run of syllables between  || the accent on 7 and the accent on 2. ||  ||  || If they are  || observed with the utmost freedom allowable to English rythmical  || custom; they still produce cadences which have the essential rhyth- || mical-though not the exact metrical-character of these three  || Latin cadences.  ||  ||  ||  ||  ||  ||  ||  ||  ||  || Cadence; then; is perhaps the euphonious way of accompanying  || in speech this natural fall or subsidence of energy. ||  ||  || Since we have drifted so  || far from these actual metrical schemes in following the facts of  || English practice; is it not safer to assume that the rules merely  || describe a necessary and universal tendency of oratorical style; and  || that the frequent occurrence in English of the exact metrical form  || of the Latin cursus is due; not to medieval tradition; but to the  || fact that these forms are the perfect and simplest manifestation of  || this tendency? ||  ||  ||  ||  ||  || \""\""\""",https://github.com/quadrismegistus/prosodic/commit/06fb02e8d3e38bb813088049413764ecf67ceadf,No
2855,quadrismegistus/prosodic,lib/lexconvert.py,7a6f7c226deb397b9d81b40cfa517573f2c96ee6,TODO: test,https://github.com/quadrismegistus/prosodic/commit/7a6f7c226deb397b9d81b40cfa517573f2c96ee6,No
2856,quadrismegistus/prosodic,lib/lexconvert.py,7a6f7c226deb397b9d81b40cfa517573f2c96ee6,(TODO: test leading 0s & leading decimal),https://github.com/quadrismegistus/prosodic/commit/7a6f7c226deb397b9d81b40cfa517573f2c96ee6,Yes
2857,philipperemy/tensorflow-ctc-speech-recognition,audio_reader.py,2b5eb21280473f8abca85ba0281a8f3a47ab9517,TODO: test also with import scipy.io.wavfile as wav; fs; audio = wav.read(audio_filename),https://github.com/philipperemy/tensorflow-ctc-speech-recognition/commit/2b5eb21280473f8abca85ba0281a8f3a47ab9517,No
2858,yzou2/CBST,issegm/evaluate-segresnet.py,10ff3a8f96ba43ff134952f96eabe56a07e0f4f0,TODO: multi-scale testing,https://github.com/yzou2/CBST/commit/10ff3a8f96ba43ff134952f96eabe56a07e0f4f0,Yes
2859,Ugness/PiCANet-Implementation,pytorch/image_test.py,46321e27833a48d2e8a6979d3584b61e89181843,TODO: Add Image Save application on image_test.py and Delete image_test_file_output.py,https://github.com/Ugness/PiCANet-Implementation/commit/46321e27833a48d2e8a6979d3584b61e89181843,Yes
2860,carpedm20/SPIRAL-tensorflow,utils/train.py,09d57135d681eaee5c98880ae6b69ec0ceb64fc8,XXX: actually `skip_list` is quite important during test time,https://github.com/carpedm20/SPIRAL-tensorflow/commit/09d57135d681eaee5c98880ae6b69ec0ceb64fc8,Yes
2861,amineHorseman/facial-expression-recognition-using-cnn,data_loader.py,5669907899102f453f77f8b40bae2758d1f215d5,# TODO : load landmarks in validation and test set also. I'm too sleepy to do it today -_- zzz... zzz...,https://github.com/amineHorseman/facial-expression-recognition-using-cnn/commit/5669907899102f453f77f8b40bae2758d1f215d5,Yes
2862,amineHorseman/facial-expression-recognition-using-cnn,data_loader.py,248eb5cb152edfb8d4104aa7fbbbf8bdfa7a4b9f,# TODO : load landmarks in validation and test set also. I'm too sleepy to do it today -_- zzz... zzz...,https://github.com/amineHorseman/facial-expression-recognition-using-cnn/commit/248eb5cb152edfb8d4104aa7fbbbf8bdfa7a4b9f,Yes
2863,CAMeL-Lab/camel_tools,tests/test_meta.py,f704c70c21438445e40344aae742debba9cfa7d9,"\""\""\"" || This is a dummy test module to make sure tox and pytest are properly || configured. ||  || This will eventually be removed. || \""\""\""",https://github.com/CAMeL-Lab/camel_tools/commit/f704c70c21438445e40344aae742debba9cfa7d9,Yes
2864,cbg-ethz/pybda,tix_plate_parser/python/plate_parser/plate_parser.py,2fff12c82bf37a5ecd0c6dd86e0aeba10e27f3a5,TODO: remove this after testing,https://github.com/cbg-ethz/pybda/commit/2fff12c82bf37a5ecd0c6dd86e0aeba10e27f3a5,Yes
2865,clusterking/clusterking,bclustering/maths/binning.py,9467de0d9fe60867acd809a184f834be15f6f60a,todo: unittest,https://github.com/clusterking/clusterking/commit/9467de0d9fe60867acd809a184f834be15f6f60a,Yes
2866,clusterking/clusterking,bclustering/maths/metric.py,ce491716bb09cd50a88daefedb4800d602b4c4d5,todo: unittest,https://github.com/clusterking/clusterking/commit/ce491716bb09cd50a88daefedb4800d602b4c4d5,Yes
2867,clusterking/clusterking,bclustering/bpoints.py,90e380a0698c789eea3f2baa796406749b0c7ce2,todo: test this,https://github.com/clusterking/clusterking/commit/90e380a0698c789eea3f2baa796406749b0c7ce2,No
2868,clusterking/clusterking,clusterking/data/test/test_dfmd.py,c7256e38c5c481caeaa1ffa03b86c40ceb58d0c4,todo: implement working tests for copying,https://github.com/clusterking/clusterking/commit/c7256e38c5c481caeaa1ffa03b86c40ceb58d0c4,Yes
2869,clusterking/clusterking,clusterking/data/test/test_data.py,dfd934f15499d0c3d208e676dffa33a418dde391,todo: we should have a larger dataset to test this properly,https://github.com/clusterking/clusterking/commit/dfd934f15499d0c3d208e676dffa33a418dde391,Yes
2870,clusterking/clusterking,clusterking/data/test/test_data.py,1c4af84b9570ee89cd99ece96fa2574a699e3d33,todo: we should have a larger dataset to test this properly,https://github.com/clusterking/clusterking/commit/1c4af84b9570ee89cd99ece96fa2574a699e3d33,Yes
2871,clusterking/clusterking,clusterking/data/test/test_dwe.py,9de2dae877ce1d6e412631113f7296f66822b7e7,todo: test rel_err,https://github.com/clusterking/clusterking/commit/9de2dae877ce1d6e412631113f7296f66822b7e7,No
2872,DTUComputeStatisticsAndDataAnalysis/MBPLS,pls_package/test_scripts/SpectralSimulation.py,e7b36997e8f37791f9958f6cc2b629116ded2466,"\""\""\"" || Created on Thu Mar 15 09:03:12 2018 ||  || - This test script generates data using three spectral parts as input for loadings 1; 2 and 3 || - The data matrix X is composed as X = TP.T || - columns in T are orthoghonal || - num_vars_x1 + num_vars_x2 must equal 40 (at current stage) || - BIPs are calculated from blocked loadings || - MBPLS is performed; results are plotted and compared to given BIPs ||  || @author: Andreas Baum; andba@dtu.dk || \""\""\""",https://github.com/DTUComputeStatisticsAndDataAnalysis/MBPLS/commit/e7b36997e8f37791f9958f6cc2b629116ded2466,Yes
2873,DTUComputeStatisticsAndDataAnalysis/MBPLS,examples/MBPLS_Noise_Andreas.py,af230796950dac90a28d42b93c92ba2c19865c22,"\""\""\"" || 08.10.18 ||  || Script to test dependency of MBPLS fit on added noise levels ||  || author: Andreas Baum; andba@dtu.dk || \""\""\""",https://github.com/DTUComputeStatisticsAndDataAnalysis/MBPLS/commit/af230796950dac90a28d42b93c92ba2c19865c22,No
2874,ECRL/ECNet,examples/select_from_test_set_performance.py,6328e4c653da8d28cba321e9310ef30fca25f557,"\""\""\"" || EXAMPLE SCRIPT: || Select best trial from each node using static test set performance ||  || This script operates as follows: || \tTrain models using training (learning + validation) set || \tImport test set; select best trials using test set performance || \tObtain results and errors for test set || \tObtain results and errors for training set || \tPublish project || \""\""\""",https://github.com/ECRL/ECNet/commit/6328e4c653da8d28cba321e9310ef30fca25f557,No
2875,ECRL/ECNet,examples/select_from_test_set_performance.py,6328e4c653da8d28cba321e9310ef30fca25f557,import the test set; select best trials based on test set performance,https://github.com/ECRL/ECNet/commit/6328e4c653da8d28cba321e9310ef30fca25f557,No
2876,ECRL/ECNet,ecnet/server.py,cad9a7ddbdc2d301d0abdd75a55fe49c6af0a357,Use validation sets to periodically test model's performance and determine when done training,https://github.com/ECRL/ECNet/commit/cad9a7ddbdc2d301d0abdd75a55fe49c6af0a357,Yes
2877,phievo/phievo,phievo/run_evolution.py,f4d136000a16429cd900ce9a1aa16527fc0a5257,"\""\""\""Top level routine to launch the evolutionary algorithm ||  ||    the model (the directory) may be specified through the -m option (relative to CWD) ||    it have to contain a file called init*.py and will collect all the output ||    example: python run_evolution.py -m StaticHox ||  ||    a particular file describing a network object may also be tested with -t option ||    in this case; an init* file should be provided; through a model with -m options ||    or directly with -i option. ||    a particular cell may be displayed with -n ||    a detailed list of species to display may be ask with -l ||    example: python run_evolution.py -m StaticHox -t cobaye.py ||  ||    Various subdirectories for data files created by this script if needed ||    Type -h or --help for options. ||  ||    Need explicitly input the directory with python modules and the Ccode directory; below ||    All inputs and their description in init*.py file. || \""\""\""",https://github.com/phievo/phievo/commit/f4d136000a16429cd900ce9a1aa16527fc0a5257,No
2878,PennyLaneAI/pennylane-sf,tests/defaults.py,b8b382758e5b68a1e5d5ca94463c2d113228d059,HACK: We only parse known args to enable unittest test discovery without parsing errors.,https://github.com/PennyLaneAI/pennylane-sf/commit/b8b382758e5b68a1e5d5ca94463c2d113228d059,No
2879,PennyLaneAI/pennylane-sf,tests/conftest.py,effd7fae3668bbea7487d7cd59f71c3168529dba,TODO: once tests are ported to pytest completely;,https://github.com/PennyLaneAI/pennylane-sf/commit/effd7fae3668bbea7487d7cd59f71c3168529dba,Yes
2880,PennyLaneAI/pennylane-pq,openqml_pq/projectq.py,2f9d1d86a9218ed897ac2f1d7a4f218ba05b6b98,todo: remove when done testing,https://github.com/PennyLaneAI/pennylane-pq/commit/2f9d1d86a9218ed897ac2f1d7a4f218ba05b6b98,Yes
2881,PennyLaneAI/pennylane-pq,openqml_pq/projectq.py,79e179360fd370982a1bfc174d7fdbaff752c45f,kwargs['verbose'] = True #todo: remove when done testing,https://github.com/PennyLaneAI/pennylane-pq/commit/79e179360fd370982a1bfc174d7fdbaff752c45f,Yes
2882,PennyLaneAI/pennylane-pq,openqml_pq/projectq.py,79e179360fd370982a1bfc174d7fdbaff752c45f,kwargs['log'] = True #todo: remove when done testing,https://github.com/PennyLaneAI/pennylane-pq/commit/79e179360fd370982a1bfc174d7fdbaff752c45f,No
2883,PennyLaneAI/pennylane-pq,tests/defaults.py,20a8f26bd428fcf15be8e08079e0d80c36f5e5f0,HACK: We only parse known args to enable unittest test discovery without parsing errors.,https://github.com/PennyLaneAI/pennylane-pq/commit/20a8f26bd428fcf15be8e08079e0d80c36f5e5f0,No
2884,astropy/astrowidgets,ah_bootstrap.py,54e458993e6ad8272e8fcc5a5deadb037f6f15e7,"\""\""\"" || This bootstrap module contains code for ensuring that the astropy_helpers || package will be importable by the time the setup.py script runs.  It also || includes some workarounds to ensure that a recent-enough version of setuptools || is being used for the installation. ||  || This module should be the first thing imported in the setup.py of distributions || that make use of the utilities in astropy_helpers.  If the distribution ships || with its own copy of astropy_helpers; this module will first attempt to import || from the shipped copy.  However; it will also check PyPI to see if there are || any bug-fix releases on top of the current version that may be useful to get || past platform-specific bugs that have been fixed.  When running setup.py; use || the ``--offline`` command-line option to disable the auto-upgrade checks. ||  || When this module is imported or otherwise executed it automatically calls a || main function that attempts to read the project's setup.cfg file; which it || checks for a configuration section called ``[ah_bootstrap]`` the presences of || that section; and options therein; determine the next step taken:  If it || contains an option called ``auto_use`` with a value of ``True``; it will || automatically call the main function of this module called || `use_astropy_helpers` (see that function's docstring for full details). || Otherwise no further action is taken and by default the system-installed version || of astropy-helpers will be used (however; ``ah_bootstrap.use_astropy_helpers`` || may be called manually from within the setup.py script). ||  || This behavior can also be controlled using the ``--auto-use`` and || ``--no-auto-use`` command-line flags. For clarity; an alias for || ``--no-auto-use`` is ``--use-system-astropy-helpers``; and we recommend using || the latter if needed. ||  || Additional options in the ``[ah_boostrap]`` section of setup.cfg have the same || names as the arguments to `use_astropy_helpers`; and can be used to configure || the bootstrap script when ``auto_use = True``. ||  || See https:\/\/github.com\/astropy\/astropy-helpers for more details; and for the || latest version of this module. || \""\""\""",https://github.com/astropy/astrowidgets/commit/54e458993e6ad8272e8fcc5a5deadb037f6f15e7,Yes
2885,astropy/astrowidgets,astrowidgets/tests/test_api.py,54e458993e6ad8272e8fcc5a5deadb037f6f15e7,TODO add test of actually removing markers...,https://github.com/astropy/astrowidgets/commit/54e458993e6ad8272e8fcc5a5deadb037f6f15e7,Yes
2886,astropy/astrowidgets,astrowidgets/tests/test_api.py,038ee6eeb33fd7b0664412aca59f4796599075e0,TODO add test of actually removing markers...,https://github.com/astropy/astrowidgets/commit/038ee6eeb33fd7b0664412aca59f4796599075e0,Yes
2887,astropy/astrowidgets,astrowidgets/tests/test_api.py,ef3c76d78ba4e70ee07a64d1a0e5cf696a87cfd6,TODO add test of actually removing markers...,https://github.com/astropy/astrowidgets/commit/ef3c76d78ba4e70ee07a64d1a0e5cf696a87cfd6,Yes
2888,ELS-RD/anonymisation,test/generate_names_test.py,f7bc42d374ba4026e28f4f227c8a92ac0c845e9e,TODO finish this test,https://github.com/ELS-RD/anonymisation/commit/f7bc42d374ba4026e28f4f227c8a92ac0c845e9e,Yes
2889,ELS-RD/anonymisation,generate_trainset/extract_node_values.py,74a4b9b4230e7a3e687e9bbd02688a9a435c22a1,TODO replace by unit test,https://github.com/ELS-RD/anonymisation/commit/74a4b9b4230e7a3e687e9bbd02688a9a435c22a1,No
2890,ELS-RD/anonymisation,test/match_entities_test.py,b3b73b3cc6d198983b67bcd619f9ceb09e0a30e6,TODO review tests (code is now very strict; does these tests make sense?),https://github.com/ELS-RD/anonymisation/commit/b3b73b3cc6d198983b67bcd619f9ceb09e0a30e6,Yes
2891,ELS-RD/anonymisation,test/match_header_test.py,a051c1c05c655304368b1cf7a7fba5f35f8eb4b3,TODO review tests (code is now very strict; does these tests make sense?),https://github.com/ELS-RD/anonymisation/commit/a051c1c05c655304368b1cf7a7fba5f35f8eb4b3,Yes
2892,OFAI/hub-toolbox-python3,hub_toolbox/MutualProximity.py,315a33d0945801355c4395981cc63f1c27d5c1d7,TODO implement train_test split,https://github.com/OFAI/hub-toolbox-python3/commit/315a33d0945801355c4395981cc63f1c27d5c1d7,No
2893,OFAI/hub-toolbox-python3,hub_toolbox/MutualProximity_parallel.py,8b3627e29ab9ccd99fd4f028c1eced855d05989e,TODO implement train_test split,https://github.com/OFAI/hub-toolbox-python3/commit/8b3627e29ab9ccd99fd4f028c1eced855d05989e,No
2894,OFAI/hub-toolbox-python3,hub_toolbox/MutualProximity_parallel.py,e9ae86af153db3ad89d78b7e297b9fa5871eb9c9,TODO implement train_test split,https://github.com/OFAI/hub-toolbox-python3/commit/e9ae86af153db3ad89d78b7e297b9fa5871eb9c9,No
2895,OFAI/hub-toolbox-python3,hub_toolbox/MutualProximity.py,ffdaf604a6a6f129f2da68e7c6e240fbba97cb70,TODO implement train_test split,https://github.com/OFAI/hub-toolbox-python3/commit/ffdaf604a6a6f129f2da68e7c6e240fbba97cb70,No
2896,OFAI/hub-toolbox-python3,tests/knn_test.py,4f435421641b168855c0544baa42f503a7ec67bd,TODO create a stricter test,https://github.com/OFAI/hub-toolbox-python3/commit/4f435421641b168855c0544baa42f503a7ec67bd,No
2897,undertheseanlp/languageflow,setup.py,ee564cede717ab8c2c3c098cadfe8b536740c150,TODO: put package test requirements here,https://github.com/undertheseanlp/languageflow/commit/ee564cede717ab8c2c3c098cadfe8b536740c150,No
2898,intel/dffml,service/http/dffml_service_http/routes.py,c4d5d2f3921ca00989a9b7f031f748e93ca8d002,TODO Add test for this,https://github.com/intel/dffml/commit/c4d5d2f3921ca00989a9b7f031f748e93ca8d002,Yes
2899,intel/dffml,service/http/dffml_service_http/routes.py,c4d5d2f3921ca00989a9b7f031f748e93ca8d002,TODO Add test that iterkey is removed on last iteration,https://github.com/intel/dffml/commit/c4d5d2f3921ca00989a9b7f031f748e93ca8d002,Yes
2900,intel/dffml,service/http/tests/test_routes.py,c4d5d2f3921ca00989a9b7f031f748e93ca8d002,TODO Test path traversal; aiohttp client keeps changing \/path\/..\/ into,https://github.com/intel/dffml/commit/c4d5d2f3921ca00989a9b7f031f748e93ca8d002,No
2901,intel/dffml,model/scikit/dffml_model_scikit/__init__.py,264cf1a7b77b9797e9e254b2261c364d93b338b5,"\""\""\"" || Machine Learning models implemented with `scikit-learn <https:\/\/scikit-learn.org\/stable\/>`_. || Models are saved under the directory in subdirectories named after the hash of || their feature names. ||  || **General Usage:** ||  || Training: ||  || .. code-block:: console ||  ||     $ dffml train \\\\ ||         -model SCIKIT_MODEL_ENTRYPOINT \\\\ ||         -features FEATURE_DEFINITION \\\\ ||         -model-predict TO_PREDICT \\\\ ||         -model-SCIKIT_PARAMETER_NAME SCIKIT_PARAMETER_VALUE \\\\ ||         -sources f=TRAINING_DATA_SOURCE_TYPE \\\\ ||         -source-filename TRAINING_DATA_FILE_NAME \\\\ ||         -source-readonly \\\\ ||         -log debug ||  || Testing and Accuracy: ||  || .. code-block:: console ||  ||     $ dffml accuracy \\\\ ||         -model SCIKIT_MODEL_ENTRYPOINT \\\\ ||         -features FEATURE_DEFINITION \\\\ ||         -model-predict TO_PREDICT \\\\ ||         -sources f=TESTING_DATA_SOURCE_TYPE \\\\ ||         -source-filename TESTING_DATA_FILE_NAME \\\\ ||         -source-readonly \\\\ ||         -log debug ||  || Predicting with trained model: ||  || .. code-block:: console ||  ||     $ dffml predict all \\\\ ||         -model SCIKIT_MODEL_ENTRYPOINT \\\\ ||         -features FEATURE_DEFINITION \\\\ ||         -model-predict TO_PREDICT \\\\ ||         -sources f=PREDICT_DATA_SOURCE_TYPE \\\\ ||         -source-filename PREDICT_DATA_FILE_NAME \\\\ ||         -source-readonly \\\\ ||         -log debug ||  ||  || **Models Available:** ||  || +----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || | Type           | Model                         | Entrypoint     | Parameters                                                                                                                                                                                    | || +================+===============================+================+===============================================================================================================================================================================================+ || | Regression     | LinearRegression              | scikitlr       | `scikitlr <https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\/>`_                                             | || +----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || | Classification | KNeighborsClassifier          | scikitknn      | `scikitknn <https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\/>`_                                          | || |                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || |                | AdaBoostClassifier            | scikitadaboost | `scikitadaboost <https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier\/>`_                                           | || |                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || |                | GaussianProcessClassifier     | scikitgpc      | `scikitgpc <https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier\/>`_                  | || |                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || |                | DecisionTreeClassifier        | scikitdtc      | `scikitdtc <https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\/>`_                                                | || |                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || |                | RandomForestClassifier        | scikitrfc      | `scikitrfc <https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\/>`_                                        | || |                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || |                | QuadraticDiscriminantAnalysis | scikitqda      | `scikitqda <https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis\/>`_| || |                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || |                | MLPClassifier                 | scikitmlp      | `scikitmlp <https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\/>`_                                              | || |                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || |                | GaussianNB                    | scikitgnb      | `scikitgnb <https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\/>`_                                                          | || +----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ||  ||  || **Usage Example:** ||  || Example below uses LinearRegression Model on a small dataset. ||  || Let us take a simple example: ||  || +----------------------+------------+--------------+--------+ || | Years of Experience  |  Expertise | Trust Factor | Salary | || +======================+============+==============+========+ || |          0           |     01     |      0.2     |   10   | || +----------------------+------------+--------------+--------+ || |          1           |     03     |      0.4     |   20   | || +----------------------+------------+--------------+--------+ || |          2           |     05     |      0.6     |   30   | || +----------------------+------------+--------------+--------+ || |          3           |     07     |      0.8     |   40   | || +----------------------+------------+--------------+--------+ || |          4           |     09     |      1.0     |   50   | || +----------------------+------------+--------------+--------+ || |          5           |     11     |      1.2     |   60   | || +----------------------+------------+--------------+--------+ ||  || .. code-block:: console ||  ||     $ cat > train.csv << EOF ||     Years;Expertise;Trust;Salary ||     0;1;0.2;10 ||     1;3;0.4;20 ||     2;5;0.6;30 ||     3;7;0.8;40 ||     EOF ||     $ cat > test.csv << EOF ||     Years;Expertise;Trust;Salary ||     4;9;1.0;50 ||     5;11;1.2;60 ||     EOF ||     $ dffml train \\\\ ||         -model scikitlr \\\\ ||         -features def:Years:int:1 def:Expertise:int:1 def:Trust:float:1 \\\\ ||         -model-predict Salary \\\\ ||         -sources f=csv \\\\ ||         -source-filename train.csv \\\\ ||         -source-readonly \\\\ ||         -log debug ||     $ dffml accuracy \\\\ ||         -model scikitlr \\\\ ||         -features def:Years:int:1 def:Expertise:int:1 def:Trust:float:1 \\\\ ||         -model-predict Salary \\\\ ||         -sources f=csv \\\\ ||         -source-filename test.csv \\\\ ||         -source-readonly \\\\ ||         -log debug ||     1.0 ||     $ echo -e 'Years;Expertise;Trust\\\ || 6;13;1.4\\\ || ' | \\\\ ||       dffml predict all \\\\ ||         -model scikitlr \\\\ ||         -features def:Years:int:1 def:Expertise:int:1 def:Trust:float:1 \\\\ ||         -model-predict Salary \\\\ ||         -sources f=csv \\\\ ||         -source-filename \/dev\/stdin \\\\ ||         -source-readonly \\\\ ||         -log debug ||     [ ||         { ||             \""extra\"": {}; ||             \""features\"": { ||                 \""Expertise\"": 13; ||                 \""Trust\"": 1.4; ||                 \""Years\"": 6 ||             }; ||             \""last_updated\"": \""2019-09-18T19:04:18Z\""; ||             \""prediction\"": { ||                 \""confidence\"": 1.0; ||                 \""value\"": 70.00000000000001 ||             }; ||             \""src_url\"": 0 ||         } ||     ] || \""\""\""",https://github.com/intel/dffml/commit/264cf1a7b77b9797e9e254b2261c364d93b338b5,No
2902,intel/dffml,tests/test_df.py,95229ff8049b9b34b5cd319a919d88b33e12ea0a,TODO(p0) Implement and test asyncgenerator,https://github.com/intel/dffml/commit/95229ff8049b9b34b5cd319a919d88b33e12ea0a,No
2903,TheMTank/ai-distillery,aidistillery/graph_embeddings.py,2017a617ed5e3c9dc0c09328e1c251881255b544,"\""\""\"" Create graph embedding from dbp file ||  ||  || Example data :  ||  || >>> db['1808.03258'] || {'id': 'http:\/\/arxiv.org\/abs\/1808.03258v1'; 'guidislink': True; 'link': ||  'http:\/\/arxiv.org\/abs\/1808.03258v1'; 'updated': '2018-08-04T13:37:51Z'; ||  'updated_parsed': time.struct_time(tm_year=2018; tm_mon=8; tm_mday=4; ||  tm_hour=13; tm_min=37; tm_sec=51; tm_wday=5; tm_yday=216; tm_isdst=0); ||  'published': '2018-08-04T13:37:51Z'; 'published_parsed': ||  time.struct_time(tm_year=2018; tm_mon=8; tm_mday=4; tm_hour=13; tm_min=37; ||  tm_sec=51; tm_wday=5; tm_yday=216; tm_isdst=0); 'title': 'Application of ||  Bounded Total Variation Denoising in Urban Traffic\ ||   Analysis'; ||  'title_detail': {'type': 'text\/plain'; 'language': None; 'base': ''; 'value': ||  'Application of Bounded Total Variation Denoising in Urban Traffic\ ||  ||  Analysis'}; 'summary': 'While it is believed that denoising is not always ||  necessary in many big data\ || applications; we show in this paper that denoising ||  is helpful in urban traffic\ || analysis by applying the method of bounded total ||  variation denoising to the\ || urban road traffic prediction and clustering ||  problem. We propose two\ || easy-to-implement methods to estimate the noise ||  strength parameter in the\ || denoising algorithm; and apply the denoising ||  algorithm to GPS-based traffic\ || data from Beijing taxi system. For the traffic ||  prediction problem; we combine\ || neural network and history matching method for ||  roads randomly chosen from an\ || urban area of Beijing. Numerical experiments ||  show that the predicting accuracy\ || is improved significantly by applying the ||  proposed bounded total variation\ || denoising algorithm. We also test the ||  algorithm on clustering problem; where a\ || recently developed clustering ||  analysis method is applied to more than one\ || hundred urban road segments in ||  Beijing based on their velocity profiles. Better\ || clustering result is ||  obtained after denoising.'; 'summary_detail': {'type': 'text\/plain'; ||  'language': None; 'base': ''; 'value': 'While it is believed that denoising is ||  not always necessary in many big data\ || applications; we show in this paper ||  that denoising is helpful in urban traffic\ || analysis by applying the method of ||  bounded total variation denoising to the\ || urban road traffic prediction and ||  clustering problem. We propose two\ || easy-to-implement methods to estimate the ||  noise strength parameter in the\ || denoising algorithm; and apply the denoising ||  algorithm to GPS-based traffic\ || data from Beijing taxi system. For the traffic ||  prediction problem; we combine\ || neural network and history matching method for ||  roads randomly chosen from an\ || urban area of Beijing. Numerical experiments ||  show that the predicting accuracy\ || is improved significantly by applying the ||  proposed bounded total variation\ || denoising algorithm. We also test the ||  algorithm on clustering problem; where a\ || recently developed clustering ||  analysis method is applied to more than one\ || hundred urban road segments in ||  Beijing based on their velocity profiles. Better\ || clustering result is ||  obtained after denoising.'}; 'authors': [{'name': 'Shanshan Tang'}; {'name': ||  'Haijun Yu'}]; 'author_detail': {'name': 'Haijun Yu'}; 'author': 'Haijun Yu'; ||  'links': [{'href': 'http:\/\/arxiv.org\/abs\/1808.03258v1'; 'rel': 'alternate'; ||  'type': 'text\/html'}; {'title': 'pdf'; 'href': ||  'http:\/\/arxiv.org\/pdf\/1808.03258v1'; 'rel': 'related'; 'type': ||  'application\/pdf'}]; 'arxiv_primary_category': {'term': 'cs.LG'; 'scheme': ||  'http:\/\/arxiv.org\/schemas\/atom'}; 'tags': [{'term': 'cs.LG'; 'scheme': ||  'http:\/\/arxiv.org\/schemas\/atom'; 'label': None}; {'term': 'math.NA'; 'scheme': ||  'http:\/\/arxiv.org\/schemas\/atom'; 'label': None}; {'term': 'stat.ML'; 'scheme': ||  'http:\/\/arxiv.org\/schemas\/atom'; 'label': None}]; '_rawid': '1808.03258'; ||  '_version': 1} || \""\""\""",https://github.com/TheMTank/ai-distillery/commit/2017a617ed5e3c9dc0c09328e1c251881255b544,Yes
2904,drivendataorg/zamba,src/tests/tests.py,c8cb8befca500dd30c0de6a5d50fad8aa665fc1a,TODO develop click.tests,https://github.com/drivendataorg/zamba/commit/c8cb8befca500dd30c0de6a5d50fad8aa665fc1a,Yes
2905,sonidosmutantes/apicultor,tests/Test_MIR_Analysis.py,0cddf9f2dfce70bba7c00b40b0706e3baef56607,TODO: add test cases asserting expected results (values of well known sounds),https://github.com/sonidosmutantes/apicultor/commit/0cddf9f2dfce70bba7c00b40b0706e3baef56607,Yes
2906,sonidosmutantes/apicultor,apicultor/tests/Test_MIR_Analysis.py,2722a75c6507ce376b1f5983271866cabdb2a924,TODO: add test cases asserting expected results (values of well known sounds),https://github.com/sonidosmutantes/apicultor/commit/2722a75c6507ce376b1f5983271866cabdb2a924,Yes
2907,sonidosmutantes/apicultor,run_MIR_analysis.py,54eb6ffb97a516bb663b3163b50a99e3ee060199,logattacktime #TODO Latest version of Essentia currently returns three values of LogAttackTime; we should decide to use exponentials or use latest Essentia's output in s,https://github.com/sonidosmutantes/apicultor/commit/54eb6ffb97a516bb663b3163b50a99e3ee060199,Yes
2908,sonidosmutantes/apicultor,MIR_State_Machine/SMComposition.py,a9306a330832c00aa22386e89084dfe0ebe56af1,FIXME: test purposes,https://github.com/sonidosmutantes/apicultor/commit/a9306a330832c00aa22386e89084dfe0ebe56af1,Yes
2909,sonidosmutantes/apicultor,MIR_State_Machine/SMComposition.py,290625525a1192e4e80a38d1a99f2564b1b4d420,#FIXME: test purposes,https://github.com/sonidosmutantes/apicultor/commit/290625525a1192e4e80a38d1a99f2564b1b4d420,No
2910,sonidosmutantes/apicultor,mir/db/FreesoundDB.py,d79eb58ead1913d454bc13921d855dc0488ef86b,TODO: add unit tests,https://github.com/sonidosmutantes/apicultor/commit/d79eb58ead1913d454bc13921d855dc0488ef86b,Yes
2911,sonidosmutantes/apicultor,mir/db/FreesoundDB.py,d79eb58ead1913d454bc13921d855dc0488ef86b,Extended implementation tests. TODO,https://github.com/sonidosmutantes/apicultor/commit/d79eb58ead1913d454bc13921d855dc0488ef86b,Yes
2912,sonidosmutantes/apicultor,state_machine/PlayCtrlState.py,800500fdd445dd72fdf9ebdbf9b5ce08953d3d99,#FIXME: test purposes,https://github.com/sonidosmutantes/apicultor/commit/800500fdd445dd72fdf9ebdbf9b5ce08953d3d99,No
2913,sonidosmutantes/apicultor,apicultor/state_machine/PlayCtrlState.py,2fd11ce3c668a0e6f5d8d0d1dff2cb0ec6ab978e,#FIXME: test purposes,https://github.com/sonidosmutantes/apicultor/commit/2fd11ce3c668a0e6f5d8d0d1dff2cb0ec6ab978e,No
2914,sonidosmutantes/apicultor,apicultor/state_machine/SMComposition.py,c4bd0b23ffe4e1e7213b5dd0d7964e22c3c7873a,#FIXME: test purposes,https://github.com/sonidosmutantes/apicultor/commit/c4bd0b23ffe4e1e7213b5dd0d7964e22c3c7873a,No
2915,sonidosmutantes/apicultor,cloud_instrument/tests/Test_MIR_Analysis.py,a36dd80c10566accbd27d9eeab3787ceb5293066,TODO: add test cases asserting expected results (values of well known sounds),https://github.com/sonidosmutantes/apicultor/commit/a36dd80c10566accbd27d9eeab3787ceb5293066,Yes
2916,sonidosmutantes/apicultor,mir/MirAnalysis.py,a36dd80c10566accbd27d9eeab3787ceb5293066,logattacktime #TODO Latest version of Essentia currently returns three values of LogAttackTime; we should decide to use exponentials or use latest Essentia's output in s,https://github.com/sonidosmutantes/apicultor/commit/a36dd80c10566accbd27d9eeab3787ceb5293066,Yes
2917,sonidosmutantes/apicultor,prototypes/PlayCtrlState.py,a36dd80c10566accbd27d9eeab3787ceb5293066,#FIXME: test purposes,https://github.com/sonidosmutantes/apicultor/commit/a36dd80c10566accbd27d9eeab3787ceb5293066,No
2918,sonidosmutantes/apicultor,prototypes/SMComposition.py,a36dd80c10566accbd27d9eeab3787ceb5293066,#FIXME: test purposes,https://github.com/sonidosmutantes/apicultor/commit/a36dd80c10566accbd27d9eeab3787ceb5293066,No
2919,sonidosmutantes/apicultor,module/run_mir_analysis.py,db416b9c5094920f5f8f25dbf280344724b5aa59,logattacktime #TODO Latest version of Essentia currently returns three values of LogAttackTime; we should decide to use exponentials or use latest Essentia's output in s,https://github.com/sonidosmutantes/apicultor/commit/db416b9c5094920f5f8f25dbf280344724b5aa59,Yes
2920,sonidosmutantes/apicultor,state_machine/PlayCtrlState.py,326210c23bd05f7d7b434087381ccdc9787b90a4,#FIXME: test purposes,https://github.com/sonidosmutantes/apicultor/commit/326210c23bd05f7d7b434087381ccdc9787b90a4,No
2921,WorldWideTelescope/pywwt,pywwt/conftest.py,8f5cddb96217353a97feb3962ea7c5d47974879e,This is (no surprise) a hack to enable the Windows testing,https://github.com/WorldWideTelescope/pywwt/commit/8f5cddb96217353a97feb3962ea7c5d47974879e,No
2922,holm-aune-bachelor2018/ctc,venv/lib/python3.6/site.py,a50d8363cfd585965f54477b0fd643189100ff9a,encoding after initialization.  The test for presence is needed when,https://github.com/holm-aune-bachelor2018/ctc/commit/a50d8363cfd585965f54477b0fd643189100ff9a,Yes
2923,pailabteam/pailab,pailab/externals/pytorch_interface.py,4ab67e65813054d7d57a8aa803aff0a92d9da584,"\""\""\""Module with interfaces for using PyTorch in pailab. ||  || This module provides some useful methods and code snippets to integrate PyTorch projects into pailab. || The main entry point is the method `add_model` that setups all methods and objects needed to train the individual  || PyTorch model.  ||  || Note: ||     Currently; pailab supports only numpy data which is connected via a DataSet object to the training and test functions  ||     in the repository. Pailab's DataSet object has nothing to do with PyTorch's DataSet object and to use all functionality out of the box  ||     one has to convert the given PyTorch DataSet object into a numpy array (this module provides the method `convert_to_numpy` to do this for simple DataSets). ||     It may not be difficult to integrate PyTorch DataSets (e.g. using a specialization of class NumpyStore) in a more clever and adapted way which will be subject  ||     of future work. || \""\""\""",https://github.com/pailabteam/pailab/commit/4ab67e65813054d7d57a8aa803aff0a92d9da584,No
2924,WMD-group/SMACT,tests/test.py,989ecb2de00da872aca014a582f37b18f10a1e88,TODO(AJJ): Implement warnings in a testable way. Have a Pu testcase.,https://github.com/WMD-group/SMACT/commit/989ecb2de00da872aca014a582f37b18f10a1e88,Yes
2925,yoshida-lab/XenonPy,tests/models/test_trainer.py,5c243e1271dcaa94de8be5bc46d4b935f01d2528,todo: need a real testing,https://github.com/yoshida-lab/XenonPy/commit/5c243e1271dcaa94de8be5bc46d4b935f01d2528,Yes
2926,datarobot/pic2vec,setup.py,498626fe896bb52f14d437ea7f90f8b5b47f4369,TODO: put package test requirements here,https://github.com/datarobot/pic2vec/commit/498626fe896bb52f14d437ea7f90f8b5b47f4369,No
2927,markovmodel/molPX,projX/tests/test_bmutils.py,6d805d9a9798fbeada478ab3d4b7bddc4292d49a,TODO: figure out a good way of testing this; at the moment it just chekcs that it runs,https://github.com/markovmodel/molPX/commit/6d805d9a9798fbeada478ab3d4b7bddc4292d49a,Yes
2928,markovmodel/molPX,molpx/tests/test_bmutils.py,49c5aac667bbcb1fa49623e18ae435fd25af216c,TODO: figure out a good way of testing this; at the moment it just chekcs that it runs,https://github.com/markovmodel/molPX/commit/49c5aac667bbcb1fa49623e18ae435fd25af216c,Yes
2929,markovmodel/molPX,molpx/tests/test_bmutils.py,1748f0b2d71a8595457ce73da1c5de34217b4483,TODO FINISH THIS TESTS,https://github.com/markovmodel/molPX/commit/1748f0b2d71a8595457ce73da1c5de34217b4483,Yes
2930,markovmodel/molPX,molpx/bmutils.py,a871a1af5c065329f14cce6d5861837ca00624e9,TODO: TEST,https://github.com/markovmodel/molPX/commit/a871a1af5c065329f14cce6d5861837ca00624e9,No
2931,markovmodel/molPX,molpx/visualize.py,b108af69babff0da3e212b0704e6b0818aefca82,todo test,https://github.com/markovmodel/molPX/commit/b108af69babff0da3e212b0704e6b0818aefca82,No
2932,markovmodel/molPX,molpx/bmutils.py,1bf9986b212923209e5e0efee2b54352ff6a8169,TODO TEST,https://github.com/markovmodel/molPX/commit/1bf9986b212923209e5e0efee2b54352ff6a8169,No
2933,markovmodel/molPX,molpx/visualize.py,2c77b77be16cf33f8665a38f3c261f78aaf2af3a,TODO TEST ALL THIS,https://github.com/markovmodel/molPX/commit/2c77b77be16cf33f8665a38f3c261f78aaf2af3a,Yes
2934,markovmodel/molPX,molpx/visualize.py,c7e11ca56b172f35dbf6c72009ede58b051f3c5d,TODO TEST,https://github.com/markovmodel/molPX/commit/c7e11ca56b172f35dbf6c72009ede58b051f3c5d,No
2935,markovmodel/molPX,molpx/visualize.py,c55d69a50ab48d66d5a8ad168aefe55351068fc1,TODO TEST,https://github.com/markovmodel/molPX/commit/c55d69a50ab48d66d5a8ad168aefe55351068fc1,No
2936,markovmodel/molPX,molpx/_bmutils.py,5c9c446a556205597a5622580653aa210937f23c,TODO TEST,https://github.com/markovmodel/molPX/commit/5c9c446a556205597a5622580653aa210937f23c,No
2937,markovmodel/molPX,molpx/tests/test_linkutils.py,37ef90cd4df5b14bb93fc18dbeab2a6b81e0ba9d,TODO abstact this to a test class,https://github.com/markovmodel/molPX/commit/37ef90cd4df5b14bb93fc18dbeab2a6b81e0ba9d,No
2938,bio-ontology-research-group/walking-rdf-and-owl,deepwalk/setup.py,d0ab53846b70606872f95b4a558d6e64f166e17d,TODO: put package test requirements here,https://github.com/bio-ontology-research-group/walking-rdf-and-owl/commit/d0ab53846b70606872f95b4a558d6e64f166e17d,No
2939,betterlife/betterlifepsi,tests/utils/date_util_test.py,c4a1a0e057850600ce6a1302468cbb801d4b89c7,TODO: Fix the issue that the result of testing depends on current date,https://github.com/betterlife/betterlifepsi/commit/c4a1a0e057850600ce6a1302468cbb801d4b89c7,No
2940,betterlife/betterlifepsi,tests/__init__.py,b15122777ac9ea2c2ec17bdf60bcc7246a2a40b7,__init__.py under each sub-directory is needed for nose to pick up the test,https://github.com/betterlife/betterlifepsi/commit/b15122777ac9ea2c2ec17bdf60bcc7246a2a40b7,Yes
2941,diffgram/diffgram,sdk/diffgram/file/file_constructor.py,30bce901877824893faab6501a184a816bc39d99,TODO due to changes; this no longer tests anything ; choose new way to sample,https://github.com/diffgram/diffgram/commit/30bce901877824893faab6501a184a816bc39d99,Yes
2942,hackingmaterials/automatminer,matbench/tests/test_featurize.py,3e1c3544891c74660d793b100225a12cce1f083e,TODO: add this test after it returns numbers and not dict!,https://github.com/hackingmaterials/automatminer/commit/3e1c3544891c74660d793b100225a12cce1f083e,No
2943,hackingmaterials/automatminer,matbench/tests/test_featurize.py,3e1c3544891c74660d793b100225a12cce1f083e,TODO: add tests for the following once they return features not matrixes:,https://github.com/hackingmaterials/automatminer/commit/3e1c3544891c74660d793b100225a12cce1f083e,No
2944,hackingmaterials/automatminer,matbench/metalearning/metafeatures.py,2fc4d5d6ffc0544e189aecab8863a6c1b7d46524,"\""\""\"" || Derive a list of meta-features of a given dataset to get recommendation of  || featurizers. ||  || The meta-features serve as indicators of the dataset characteristics that may || affect the choice of featurizers. ||  || Based on the meta-features and then benchmarking (i) featurizer availability;  || (ii) featurizer importance and (iii) featurizer computational budget of  || existing featurizers on a variety of datasets; we can get a sense of how these || featurizers perform for datasets with different meta-features; and then make  || some strategies of featurizer selection. ||  || When given a new dataset; we can compute its meta-features; and then get the  || recommended featurizers based on the pre-defined strategies (e.g. one way is  || to get the L1 distances of meta-features of all pre-defined strategies and  || meta-features of the new dataset; then find some \""nearest\"" strategies and make || an estimation of computational budget; and finally taking all these factors  || together to make a final recommendation of featurizers) ||  || Current meta-features to be considered (many can be further added): || (i) Composition-related: ||     - Number of formulas: ||     - Percent of all-metallic alloys: ||     - Percent of metallic-nonmetallic compounds: ||     - Percent of nonmetallic compounds: ||     - Number of elements present in the entire dataset:  ||         e.g. can help to decided whether to use ChemicalSRO or Bob featurizers ||         that can return O(N^2) features (increase rapidly with the number of  ||         elements present) ||     - Avg. number of elements in compositions: ||     - Max. number of elements in compositions: ||     - Min. number of elements in compositions: ||     To do: ||     - Percent of transitional-metal-containing alloys (dependency: percent of  ||         all-metallic alloys):  ||         e.g. can be used to determisne whether to use featurizers such as Miedema  ||         that is more applicable to transitional alloys. ||     - Percent of transitional-nonmetallic compounds (dependency: percent of  ||         metallic-nonmetallic compounds):  ||     - Prototypes of phases in the dataset: ||         e.g. AB; AB2O4; MAX phase; etc maybe useful. ||     - Percent of organic\/molecules:  ||         may need to call other packages e.g. deepchem or just fail this task as  ||         we cannot directly support it in matminer. ||          || (ii) Structure-related: ||     - Percent of ordered structures: ||         e.g. can help to decide whether to use some featurizers that only apply ||         to ordered structure such as GlobalSymmetryFeatures ||     - Avg. number of atoms in structures: ||         e.g. can be important for deciding on some extremely computational  ||         expensive featurizers such as Voronoi-based ones or site statistics  ||         ones such as SiteStatsFingerprint. They are assumed to be quite slow  ||         if there are too many atoms in the structures. ||     - Max. number of sites in structures:  ||     To do: ||     - Percent of 3D structure: ||     - Percent of 2D structure: ||     - Percent of 1D structure: ||  || (iii) Missing_values-related: ||     - Number of instances with missing_values ||     - Percent of instances with missing_values ||     - Number of missing_values ||     - Percent of missing_values ||      || To do: || (iv) Task-related: ||     - Regression or Classification:  ||         maybe some featurizers work better for classification or better for  ||         regression ||  || \""\""\""",https://github.com/hackingmaterials/automatminer/commit/2fc4d5d6ffc0544e189aecab8863a6c1b7d46524,Yes
2945,hackingmaterials/automatminer,matbench/preprocess.py,2d87a5708304e0df1cf5e329d118b7f19218027a,Todo: Need basic tests for this!,https://github.com/hackingmaterials/automatminer/commit/2d87a5708304e0df1cf5e329d118b7f19218027a,No
2946,hackingmaterials/automatminer,matbench/tests/test_preprocess.py,4eb8acccb5894ac644240af2fa8053811b138084,Todo: Add more tests,https://github.com/hackingmaterials/automatminer/commit/4eb8acccb5894ac644240af2fa8053811b138084,Yes
2947,hackingmaterials/automatminer,matbench/featurization/tests/test_featurize.py,e27e8d76438b95bbd4212487cb7ea415f8a840ad,TODO: add this test after it returns numbers and not dict!,https://github.com/hackingmaterials/automatminer/commit/e27e8d76438b95bbd4212487cb7ea415f8a840ad,No
2948,hackingmaterials/automatminer,matbench/featurization/tests/test_featurize.py,e27e8d76438b95bbd4212487cb7ea415f8a840ad,TODO: add tests for the following once they return features not matrixes:,https://github.com/hackingmaterials/automatminer/commit/e27e8d76438b95bbd4212487cb7ea415f8a840ad,No
2949,hackingmaterials/automatminer,matbench/data/tests/test_data.py,85297fe4e122d903190fdef74127e132e1210259,todo: needs separate tests for each load function...,https://github.com/hackingmaterials/automatminer/commit/85297fe4e122d903190fdef74127e132e1210259,No
2950,hackingmaterials/automatminer,matbench/featurization/metalearning/metafeatures.py,753e68dd5105576e615e137202374237bae7fd48,"\""\""\"" || Derive a list of meta-features of a given dataset to get recommendation of  || featurizers. ||  || The meta-features serve as indicators of the dataset characteristics that may || affect the choice of featurizers. ||  || Based on the meta-features and then benchmarking (i) featurizer availability;  || (ii) featurizer importance and (iii) featurizer computational budget of  || existing featurizers on a variety of datasets; we can get a sense of how these || featurizers perform for datasets with different meta-features; and then make  || some strategies of featurizer selection. ||  || When given a new dataset; we can compute its meta-features; and then get the  || recommended featurizers based on the pre-defined strategies (e.g. one way is  || to get the L1 distances of meta-features of all pre-defined strategies and  || meta-features of the new dataset; then find some \""nearest\"" strategies and make || an estimation of computational budget; and finally taking all these factors  || together to make a final recommendation of featurizers) ||  || Current meta-feat ures to be considered (many can be further added): || (i) formula-related: ||     - Number of formulas: ||     - Percent of all-metallic alloys: ||     - Percent of metallic-nonmetallic compounds: ||     - Percent of nonmetallic compounds: ||     - Number of elements present in the entire dataset:  ||         e.g. can help to decided whether to use ChemicalSRO or Bob featurizers ||         that can return O(N^2) features (increase rapidly with the number of  ||         elements present) ||     - Avg. number of elements in compositions: ||     - Max. number of elements in compositions: ||     - Min. number of elements in compositions: ||     To do: ||     - Percent of transitional-metal-containing alloys (dependency: percent of  ||         all-metallic alloys):  ||         e.g. can be used to determisne whether to use featurizers such as Miedema  ||         that is more applicable to transitional alloys. ||     - Percent of transitional-nonmetallic compounds (dependency: percent of  ||         metallic-nonmetallic compounds):  ||     - Prototypes of phases in the dataset: ||         e.g. AB; AB2O4; MAX phase; etc maybe useful. ||     - Percent of organic\/molecules:  ||         may need to call other packages e.g. deepchem or just fail this task as  ||         we cannot directly support it in matminer. ||          || (ii) Structure-related: ||     - Percent of  ordered structures: ||         e.g. can help to decide whether to use some featurizers that only apply ||         to ordered structure such as GlobalSymmetryFeatures ||     - Avg. number of atoms in structures: ||         e.g. can be important for deciding on some extremely computational  ||         expensive featurizers such as Voronoi-based ones or site statistics  ||         ones such as SiteStatsFingerprint. They are assumed to be quite slow  ||         if there are too many atoms in the structures. ||     - Max. number of sites in structures:  ||     To do: ||     - Percent of 3D structure: ||     - Percent of 2D structure: ||     - Percent of 1D structure: ||  || (iii) Missing_values-related: ||     - Number of instances with missing_values ||     - Percent of instances with missing_values ||     - Number of missing_values ||     - Percent of missing_values ||      || To do: || (iv) Task-related: ||     - Regression or Classification:  ||         maybe some featurizers work better for classification or better for  ||         regression ||  || \""\""\""",https://github.com/hackingmaterials/automatminer/commit/753e68dd5105576e615e137202374237bae7fd48,Yes
2951,hackingmaterials/automatminer,matbench/pipeline.py,5176afafa26057304f0f159699bd5c3e98888dac,todo: needs tests - alex,https://github.com/hackingmaterials/automatminer/commit/5176afafa26057304f0f159699bd5c3e98888dac,Yes
2952,hackingmaterials/automatminer,matbench/automl/tests/test_tpot.py,46da0b2d28140a7d74664cee299e07d585eae5af,todo: condense these tests,https://github.com/hackingmaterials/automatminer/commit/46da0b2d28140a7d74664cee299e07d585eae5af,Yes
2953,hackingmaterials/automatminer,matbench/pipeline.py,98a0135bdac67dcf0eaa6370a0e3c498b64c56a3,todo: tests should include using custom (user speficied) features as well,https://github.com/hackingmaterials/automatminer/commit/98a0135bdac67dcf0eaa6370a0e3c498b64c56a3,Yes
2954,hackingmaterials/automatminer,mslearn/featurization/metaselection/metafeatures.py,99a1fc05dffe986df963ae8d92e02d50993520b5,"\""\""\"" || Derive a list of meta-features of a given dataset to get recommendation of  || featurizers. ||  || The meta-features serve as indicators of the dataset characteristics that may || affect the choice of featurizers. ||  || Based on the meta-features and then benchmarking (i) featurizer availability;  || (ii) featurizer importance and (iii) featurizer computational budget of  || existing featurizers on a variety of datasets; we can get a sense of how these || featurizers perform for datasets with different meta-features; and then make  || some strategies of featurizer selection. ||  || When given a new dataset; we can compute its meta-features; and then get the  || recommended featurizers based on the pre-defined strategies (e.g. one way is  || to get the L1 distances of meta-features of all pre-defined strategies and  || meta-features of the new dataset; then find some \""nearest\"" strategies and make || an estimation of computational budget; and finally taking all these factors  || together to make a final recommendation of featurizers) ||  || Current meta-feat ures to be considered (many can be further added): || (i) formula-related: ||     - Number of formulas: ||     - Percent of all-metallic alloys: ||     - Percent of metallic-nonmetallic compounds: ||     - Percent of nonmetallic compounds: ||     - Number of elements present in the entire dataset:  ||         e.g. can help to decided whether to use ChemicalSRO or Bob featurizers ||         that can return O(N^2) features (increase rapidly with the number of  ||         elements present) ||     - Avg. number of elements in compositions: ||     - Max. number of elements in compositions: ||     - Min. number of elements in compositions: ||     To do: ||     - Percent of transitional-metal-containing alloys (dependency: percent of  ||         all-metallic alloys):  ||         e.g. can be used to determisne whether to use featurizers such as Miedema  ||         that is more applicable to transitional alloys. ||     - Percent of transitional-nonmetallic compounds (dependency: percent of  ||         metallic-nonmetallic compounds):  ||     - Prototypes of phases in the dataset: ||         e.g. AB; AB2O4; MAX phase; etc maybe useful. ||     - Percent of organic\/molecules:  ||         may need to call other packages e.g. deepchem or just fail this task as  ||         we cannot directly support it in matminer. ||          || (ii) Structure-related: ||     - Percent of  ordered structures: ||         e.g. can help to decide whether to use some featurizers that only apply ||         to ordered structure such as GlobalSymmetryFeatures ||     - Avg. number of atoms in structures: ||         e.g. can be important for deciding on some extremely computational  ||         expensive featurizers such as Voronoi-based ones or site statistics  ||         ones such as SiteStatsFingerprint. They are assumed to be quite slow  ||         if there are too many atoms in the structures. ||     - Max. number of sites in structures:  ||     To do: ||     - Percent of 3D structure: ||     - Percent of 2D structure: ||     - Percent of 1D structure: ||  || (iii) Missing_values-related: ||     - Number of instances with missing_values ||     - Percent of instances with missing_values ||     - Number of missing_values ||     - Percent of missing_values ||      || To do: || (iv) Task-related: ||     - Regression or Classification:  ||         maybe some featurizers work better for classification or better for  ||         regression ||  || \""\""\""",https://github.com/hackingmaterials/automatminer/commit/99a1fc05dffe986df963ae8d92e02d50993520b5,Yes
2955,hackingmaterials/automatminer,mslearn/pipeline.py,99a1fc05dffe986df963ae8d92e02d50993520b5,todo: needs tests - alex,https://github.com/hackingmaterials/automatminer/commit/99a1fc05dffe986df963ae8d92e02d50993520b5,Yes
2956,hackingmaterials/automatminer,mslearn/pipeline.py,99a1fc05dffe986df963ae8d92e02d50993520b5,todo: tests should include using custom (user speficied) features as well,https://github.com/hackingmaterials/automatminer/commit/99a1fc05dffe986df963ae8d92e02d50993520b5,Yes
2957,hackingmaterials/automatminer,mslearn/pipeline.py,5d9d4e54690f7ee5db18ce2fa2bbad6811288873,todo: needs tests - alex,https://github.com/hackingmaterials/automatminer/commit/5d9d4e54690f7ee5db18ce2fa2bbad6811288873,Yes
2958,hackingmaterials/automatminer,mslearn/pipeline.py,5d9d4e54690f7ee5db18ce2fa2bbad6811288873,todo: tests should include using custom (user speficied) features as well,https://github.com/hackingmaterials/automatminer/commit/5d9d4e54690f7ee5db18ce2fa2bbad6811288873,Yes
2959,hackingmaterials/automatminer,dev_scripts/evaluation/benchmark.py,0540940c7a745c98490bbe1e025acaf769a806b6,todo: eventually this should use a test_idx so ensure that for every dataset for every repetition the same test set is used!,https://github.com/hackingmaterials/automatminer/commit/0540940c7a745c98490bbe1e025acaf769a806b6,Yes
2960,hackingmaterials/automatminer,dev_scripts/evaluation/benchmark.py,0540940c7a745c98490bbe1e025acaf769a806b6,todo: link fws together and test,https://github.com/hackingmaterials/automatminer/commit/0540940c7a745c98490bbe1e025acaf769a806b6,Yes
2961,hackingmaterials/automatminer,automatminer/featurization/metaselection/metafeatures.py,e60f0893cdeee11429ded37f1d21bc5d9e5e6e81,"\""\""\""Derive a list of meta-features of a given dataset to get recommendation of || featurizers. ||  || The meta-features serve as indicators of the dataset characteristics that may || affect the choice of featurizers. ||  || Based on the meta-features and then benchmarking (i) featurizer availability;  || (ii) featurizer importance and (iii) featurizer computational budget of  || existing featurizers on a variety of datasets; we can get a sense of how these || featurizers perform for datasets with different meta-features; and then make  || some strategies of featurizer selection. ||  || When given a new dataset; we can compute its meta-features; and then get the  || recommended featurizers based on the pre-defined strategies (e.g. one way is  || to get the L1 distances of meta-features of all pre-defined strategies and  || meta-features of the new dataset; then find some \""nearest\"" strategies and make || an estimation of computational budget; and finally taking all these factors  || together to make a final recommendation of featurizers) ||  || Current meta-feat ures to be considered (many can be further added): || (i) Composition-related: ||     - Number of compositions: ||     - Percent of all-metallic alloys: ||     - Percent of metallic-nonmetallic compounds: ||     - Percent of nonmetallic compounds: ||     - Number of elements present in the entire dataset:  ||         e.g. can help to decided whether to use ChemicalSRO or Bob featurizers ||         that can return O(N^2) features (increase rapidly with the number of  ||         elements present) ||     - Avg. number of elements in compositions: ||     - Max. number of elements in compositions: ||     - Min. number of elements in compositions: ||     To do: ||     - Percent of transitional-metal-containing alloys (dependency: percent of  ||         all-metallic alloys):  ||         e.g. can be used to determisne whether to use featurizers such as Miedema  ||         that is more applicable to transitional alloys. ||     - Percent of transitional-nonmetallic compounds (dependency: percent of  ||         metallic-nonmetallic compounds):  ||     - Prototypes of phases in the dataset: ||         e.g. AB; AB2O4; MAX phase; etc maybe useful. ||     - Percent of organic\/molecules:  ||         may need to call other packages e.g. deepchem or just fail this task as  ||         we cannot directly support it in matminer. ||          || (ii) Structure-related: ||     - Percent of  ordered structures: ||         e.g. can help to decide whether to use some featurizers that only apply ||         to ordered structure such as GlobalSymmetryFeatures ||     - Avg. number of atoms in structures: ||         e.g. can be important for deciding on some extremely computational  ||         expensive featurizers such as Voronoi-based ones or site statistics  ||         ones such as SiteStatsFingerprint. They are assumed to be quite slow  ||         if there are too many atoms in the structures. ||     - Max. number of sites in structures:  ||     To do: ||     - Percent of 3D structure: ||     - Percent of 2D structure: ||     - Percent of 1D structure: ||  || (iii) Missing_values-related: ||     - Number of instances with missing_values ||     - Percent of instances with missing_values ||     - Number of missing_values ||     - Percent of missing_values ||      || To do: || (iv) Task-related: ||     - Regression or Classification:  ||         maybe some featurizers work better for classification or better for  ||         regression || \""\""\""",https://github.com/hackingmaterials/automatminer/commit/e60f0893cdeee11429ded37f1d21bc5d9e5e6e81,No
2962,hackingmaterials/automatminer,automatminer/tests/test_pipeline.py,872a94baeb54e98d589aa07d060a5cb00c3c4378,todo: figure out a way to run these tests with TPOTAdaptor!,https://github.com/hackingmaterials/automatminer/commit/872a94baeb54e98d589aa07d060a5cb00c3c4378,Yes
2963,hackingmaterials/automatminer,automatminer/featurization/tests/test_core.py,95a94ebd69745b5783638501164a3a908b8da791,todo: Fix test once metaselector is converted over to precheck,https://github.com/hackingmaterials/automatminer/commit/95a94ebd69745b5783638501164a3a908b8da791,Yes
2964,hackingmaterials/automatminer,automatminer/featurization/tests/test_core.py,9f364f8758d04ae56b8a37821d6c5734aae4c7a7,todo: re-enable this test_functionalization,https://github.com/hackingmaterials/automatminer/commit/9f364f8758d04ae56b8a37821d6c5734aae4c7a7,Yes
2965,mirapy-org/mirapy,ah_bootstrap.py,6f2e0c96ccd0118faf50c7ff327be7e97e06bb30,"\""\""\"" || This bootstrap module contains code for ensuring that the astropy_helpers || package will be importable by the time the setup.py script runs.  It also || includes some workarounds to ensure that a recent-enough version of setuptools || is being used for the installation. ||  || This module should be the first thing imported in the setup.py of distributions || that make use of the utilities in astropy_helpers.  If the distribution ships || with its own copy of astropy_helpers; this module will first attempt to import || from the shipped copy.  However; it will also check PyPI to see if there are || any bug-fix releases on top of the current version that may be useful to get || past platform-specific bugs that have been fixed.  When running setup.py; use || the ``--offline`` command-line option to disable the auto-upgrade checks. ||  || When this module is imported or otherwise executed it automatically calls a || main function that attempts to read the project's setup.cfg file; which it || checks for a configuration section called ``[ah_bootstrap]`` the presences of || that section; and options therein; determine the next step taken:  If it || contains an option called ``auto_use`` with a value of ``True``; it will || automatically call the main function of this module called || `use_astropy_helpers` (see that function's docstring for full details). || Otherwise no further action is taken and by default the system-installed version || of astropy-helpers will be used (however; ``ah_bootstrap.use_astropy_helpers`` || may be called manually from within the setup.py script). ||  || This behavior can also be controlled using the ``--auto-use`` and || ``--no-auto-use`` command-line flags. For clarity; an alias for || ``--no-auto-use`` is ``--use-system-astropy-helpers``; and we recommend using || the latter if needed. ||  || Additional options in the ``[ah_boostrap]`` section of setup.cfg have the same || names as the arguments to `use_astropy_helpers`; and can be used to configure || the bootstrap script when ``auto_use = True``. ||  || See https:\/\/github.com\/astropy\/astropy-helpers for more details; and for the || latest version of this module. || \""\""\""",https://github.com/mirapy-org/mirapy/commit/6f2e0c96ccd0118faf50c7ff327be7e97e06bb30,Yes
2966,theislab/diffxpy,diffxpy/base.py,9348d34382ccc2bf118bd0bdfb7356e12e652c01,TODO adjust group allocation that group g1 is tested versus union of all other groups.,https://github.com/theislab/diffxpy/commit/9348d34382ccc2bf118bd0bdfb7356e12e652c01,No
2967,theislab/diffxpy,diffxpy/testing/base.py,9467a895fe91f54e01283e407784b4b7a1af5ce9,TODO test nestedness,https://github.com/theislab/diffxpy/commit/9467a895fe91f54e01283e407784b4b7a1af5ce9,No
2968,theislab/diffxpy,diffxpy/testing/tests.py,4c085d330b6a8cad89ed93ccb57e872a69138b9a,TODO test nestedness,https://github.com/theislab/diffxpy/commit/4c085d330b6a8cad89ed93ccb57e872a69138b9a,No
2969,bioidiap/bob,src/python/test/test_array.py,d5c4abf13625a7b6c9cfdbe1ed4e6c5bb95cfa0d,TODO: Tests for &; |; ^; <<; >>,https://github.com/bioidiap/bob/commit/d5c4abf13625a7b6c9cfdbe1ed4e6c5bb95cfa0d,No
2970,bioidiap/bob,src/python/database/test/test_dataset.py,423b4ae3274cec78eab9c91d634bc4ee83ccd800,"\""\""\""Tests and examplify the dataset functionality. Loading and accessing the || data. || \""\""\""",https://github.com/bioidiap/bob/commit/423b4ae3274cec78eab9c91d634bc4ee83ccd800,No
2971,bioidiap/bob,src/python/database/test/test_writing.py,76dcf21ffdd1318e08829d1ae58b666fc565e5cb,"\""\""\""Tests and examplify the dataset functionality. This suite exercises the || dataset writing.  || \""\""\""",https://github.com/bioidiap/bob/commit/76dcf21ffdd1318e08829d1ae58b666fc565e5cb,Yes
2972,bioidiap/bob,python/measure/lib/script/recurse_results.py,baf28f649bc8b5e7a15abdf064328bfafa9a5a5f,"\""\""\"" || This script parses through the given directory; collects all results of || verification experiments that are stored in file with the given file name. || It supports the split into development and test set of the data; as well as || ZT-normalized scores. ||  || All result files are parsed and evaluated. For each directory; the following || information are given in columns: ||  ||   * The Equal Error Rate of the development set ||   * The Equal Error Rate of the development set after ZT-Normalization ||   * The Half Total Error Rate of the evaluation set ||   * The Half Total Error Rate of the evaluation set after ZT-Normalization ||   * The sub-directory where the scores can be found ||  || The measure type of the development set can be changed to compute \""HTER\"" or || \""FAR\"" thresholds instead; using the --criterion option. || \""\""\""",https://github.com/bioidiap/bob/commit/baf28f649bc8b5e7a15abdf064328bfafa9a5a5f,Yes
2973,bioidiap/bob,python/bob/core/test/insulate.py,fcb6bdd42e9f5e2e5372f0fcbd67f9334833ef32,TODO: prints in tests (captured stuff),https://github.com/bioidiap/bob/commit/fcb6bdd42e9f5e2e5372f0fcbd67f9334833ef32,No
2974,bioidiap/bob,python/bob/ip/test/test_glcm.py,cebab61667f1b83fd93981dfbd5d2ff40d2da71a,The testing of the properties tests whether the results are compatible with the code given in http:\/\/www.mathworks.com\/matlabcentral\/fileexchange\/22354-glcmfeatures4-m-vectorized-version-of-glcmfeatures1-m-with-code-changes. However; the indexing of the arrays there starts from 1 and in Bob it starts from 0. To avoid the descrepencies; some changes in that code is needed; in particluar in the i_matrix and j_matrix variables; as well as xplusy_index,https://github.com/bioidiap/bob/commit/cebab61667f1b83fd93981dfbd5d2ff40d2da71a,No
2975,nengo/nengo-dl,nengo_deeplearning/graph_optimizer.py,5927076ae38eca69d46c82c66a2efaebf2d047a4,"TODO: add a \""noop\"" planner for testing\/debugging",https://github.com/nengo/nengo-dl/commit/5927076ae38eca69d46c82c66a2efaebf2d047a4,Yes
2976,nengo/nengo-dl,nengo_deeplearning/simulator.py,5927076ae38eca69d46c82c66a2efaebf2d047a4,TODO: this is just for testing,https://github.com/nengo/nengo-dl/commit/5927076ae38eca69d46c82c66a2efaebf2d047a4,No
2977,nengo/nengo-dl,nengo_deeplearning/tests/test_graph_optimizer.py,5927076ae38eca69d46c82c66a2efaebf2d047a4,TODO: test this,https://github.com/nengo/nengo-dl/commit/5927076ae38eca69d46c82c66a2efaebf2d047a4,No
2978,nengo/nengo-dl,nengo_deeplearning/signals.py,d86d0addba1cdd82b36bcdd947ae70495d743ee4,TODO: documentation\/tests,https://github.com/nengo/nengo-dl/commit/d86d0addba1cdd82b36bcdd947ae70495d743ee4,Yes
2979,nengo/nengo-dl,nengo_deeplearning/tests/test_simulator.py,c48998e0ad9a7a8e037558a91a28b9dfdd421878,TODO: tests for minibatching (operators; processes; neurons; learning rules),https://github.com/nengo/nengo-dl/commit/c48998e0ad9a7a8e037558a91a28b9dfdd421878,Yes
2980,nengo/nengo-dl,nengo_dl/tests/test_simulator.py,02f5084313ce5bfdf0a912ce2cc714cb43e2eb87,TODO: add test with non-gradientdescent optimizer,https://github.com/nengo/nengo-dl/commit/02f5084313ce5bfdf0a912ce2cc714cb43e2eb87,No
2981,nengo/nengo-dl,nengo_dl/tests/test_tensor_graph.py,aa1a83622a4973643ad6969d7beb8dff5d04a559,TODO: add test for optimizer caching,https://github.com/nengo/nengo-dl/commit/aa1a83622a4973643ad6969d7beb8dff5d04a559,No
2982,nengo/nengo-dl,nengo_dl/simulator.py,fdda1de6680007ee35effbaaeaf946ab395ce6c0,TODO: put this test back in when we bump nengo version,https://github.com/nengo/nengo-dl/commit/fdda1de6680007ee35effbaaeaf946ab395ce6c0,No
2983,nengo/nengo-dl,nengo_dl/tests/test_simulator.py,f56616396da6224b63c6df5474714e938a9561a0,TODO: separate this into two different inputs to test that,https://github.com/nengo/nengo-dl/commit/f56616396da6224b63c6df5474714e938a9561a0,Yes
2984,nengo/nengo-dl,nengo_dl/tests/test_simulator.py,f56616396da6224b63c6df5474714e938a9561a0,TODO: make this have multiple outputs to test that functionality,https://github.com/nengo/nengo-dl/commit/f56616396da6224b63c6df5474714e938a9561a0,No
2985,nengo/nengo-dl,nengo_dl/benchmarks.py,3523a6b879ba3d64f0e6ef4eb4dcb49fa02de2e1,TODO: test these functions,https://github.com/nengo/nengo-dl/commit/3523a6b879ba3d64f0e6ef4eb4dcb49fa02de2e1,Yes
2986,nengo/nengo-dl,nengo_dl/benchmarks.py,e2d6a906b1a8762e22a9431c2a26abb08587b9c1,TODO: test these functions,https://github.com/nengo/nengo-dl/commit/e2d6a906b1a8762e22a9431c2a26abb08587b9c1,Yes
2987,nengo/nengo-dl,setup.py,4d71f20c916b9ac310f238c0cd310c7948a179f5,>=4.1.0 doesn't work with nengo tests TODO: remove this once https:\/\/github.com\/nengo\/nengo\/pull\/1497 is merged,https://github.com/nengo/nengo-dl/commit/4d71f20c916b9ac310f238c0cd310c7948a179f5,No
2988,nengo/nengo-dl,nengo_dl/tensor_graph.py,5da45781a5cea40efb89b79b2cddc61842f03478,TODO: test that rebuilding multiple times works properly,https://github.com/nengo/nengo-dl/commit/5da45781a5cea40efb89b79b2cddc61842f03478,No
2989,scvae/scvae,models/importance_weighted_variational_auto_encoder.py,16a3b2b6414eaa75aa272c30d8eda4db72f5ba32,"Dictionary holding number of samples needed for the \""monte carlo\"" estimator and \""importance weighting\"" during both \""train\"" and \""test\"" time.",https://github.com/scvae/scvae/commit/16a3b2b6414eaa75aa272c30d8eda4db72f5ba32,No
2990,scvae/scvae,main.py,319ff8ca440524194012b7fdfb73991727f03d13,"Dictionary holding number of samples needed for the \""monte carlo\"" estimator and \""importance weighting\"" during both \""train\"" and \""test\"" time.",https://github.com/scvae/scvae/commit/319ff8ca440524194012b7fdfb73991727f03d13,No
2991,scvae/scvae,models/iw_vae.py,858453fee4b8bead14bac2074d44798799dd3387,"Dictionary holding number of samples needed for the \""monte carlo\"" estimator and \""importance weighting\"" during both \""train\"" and \""test\"" time.",https://github.com/scvae/scvae/commit/858453fee4b8bead14bac2074d44798799dd3387,No
2992,shineware/PyKOMORAN,python/PyKomoran/tests/core_test.py,8d15c0da4a8017ac5fe7ec7533e6c806720a6839,TODO: implement test_to_set_user_dic() test code,https://github.com/shineware/PyKOMORAN/commit/8d15c0da4a8017ac5fe7ec7533e6c806720a6839,Yes
2993,shineware/PyKOMORAN,python/PyKomoran/tests/core_test.py,8d15c0da4a8017ac5fe7ec7533e6c806720a6839,TODO: implement test_to_set_fw_dic() test code,https://github.com/shineware/PyKOMORAN/commit/8d15c0da4a8017ac5fe7ec7533e6c806720a6839,Yes
2994,shineware/PyKOMORAN,python/PyKomoran/tests/core_test.py,4ed53366550f5a5d48b111f712d954cef4206cc5,TODO: implement test_to_set_fw_dic() test code,https://github.com/shineware/PyKOMORAN/commit/4ed53366550f5a5d48b111f712d954cef4206cc5,Yes
2995,mikgroup/sigpy,sigpy/mri/rf/linop.py,1a38fcb52ad751cfb6528ba19f0dcbd23d49fcfb,TODO: UNTESTED,https://github.com/mikgroup/sigpy/commit/1a38fcb52ad751cfb6528ba19f0dcbd23d49fcfb,Yes
2996,vecto-ai/vecto,tests/benchmarks/test_analogy.py,3f459a992baa30c3e26b22042776821ebb656842,TODO: suppress concatenating timestamp and uncomment viz test,https://github.com/vecto-ai/vecto/commit/3f459a992baa30c3e26b22042776821ebb656842,Yes
2997,vecto-ai/vecto,vecto/benchmarks/text_classification/text_classification.py,bd4d4605a2502e1cd3bf8b092ce5b4dae504619d,TODO: old version was returning last test value; make a footnote,https://github.com/vecto-ai/vecto/commit/bd4d4605a2502e1cd3bf8b092ce5b4dae504619d,No
2998,vecto-ai/vecto,corpus_test.py,038abc56e0cf35bfbb640470a9d44c6f91814cb0,TODO: move all this to unittests,https://github.com/vecto-ai/vecto/commit/038abc56e0cf35bfbb640470a9d44c6f91814cb0,No
2999,hachmannlab/chemml,cheml/chem/MagpieFeatures.py,753a6e2b16ba8e4616ca96225fca2035288ab61b,TODO: Write unit tests.,https://github.com/hachmannlab/chemml/commit/753a6e2b16ba8e4616ca96225fca2035288ab61b,Yes
3000,AllenCellModeling/pytorch_integrated_cell,integrated_cell/tests/test_dataprovider.py,2d4355168b3dfc6fb0c89ae23ec5f4540e04fe34,TODO: Add croping; rescaling; normalization tests,https://github.com/AllenCellModeling/pytorch_integrated_cell/commit/2d4355168b3dfc6fb0c89ae23ec5f4540e04fe34,No
3001,IBM/mi-prometheus,problems/seq_to_seq/text2text/translation.py,28a866da7a713df7603261aba7afd51d131ca554,TODO: is it useful? How to delimitate train & test dataset?,https://github.com/IBM/mi-prometheus/commit/28a866da7a713df7603261aba7afd51d131ca554,Yes
3002,IBM/mi-prometheus,miprometheus/models/cog/network.py,33827f09a2cc1d58fbff035ace331feb5f7bcb45,Hack for easy testing,https://github.com/IBM/mi-prometheus/commit/33827f09a2cc1d58fbff035ace331feb5f7bcb45,No
3003,NRCan/geo-deep-learning,utils/visualization.py,d728da5afb04d213a3da93e334ca9093d54bd5a1,TODO: test when ignore_index is smaller than 1.,https://github.com/NRCan/geo-deep-learning/commit/d728da5afb04d213a3da93e334ca9093d54bd5a1,Yes
3004,NRCan/geo-deep-learning,utils/visualization.py,d728da5afb04d213a3da93e334ca9093d54bd5a1,TODO: test this with grayscale input.,https://github.com/NRCan/geo-deep-learning/commit/d728da5afb04d213a3da93e334ca9093d54bd5a1,No
3005,NRCan/geo-deep-learning,utils/visualization.py,d728da5afb04d213a3da93e334ca9093d54bd5a1,TODO: test this.,https://github.com/NRCan/geo-deep-learning/commit/d728da5afb04d213a3da93e334ca9093d54bd5a1,Yes
3006,NRCan/geo-deep-learning,train_segmentation.py,1b35ba165c3455645778b29d1356076879d1cdd1,TODO: Remove after testing phase,https://github.com/NRCan/geo-deep-learning/commit/1b35ba165c3455645778b29d1356076879d1cdd1,Yes
3007,NRCan/geo-deep-learning,data_analysis.py,84abd76bc84054664290c2fe8debe0010d40e547,TODO: test this.,https://github.com/NRCan/geo-deep-learning/commit/84abd76bc84054664290c2fe8debe0010d40e547,Yes
3008,NRCan/geo-deep-learning,inference.py,84abd76bc84054664290c2fe8debe0010d40e547,TODO: test this,https://github.com/NRCan/geo-deep-learning/commit/84abd76bc84054664290c2fe8debe0010d40e547,No
3009,NRCan/geo-deep-learning,utils/CreateDataset.py,84abd76bc84054664290c2fe8debe0010d40e547,if self.num_bands < sat_img.shape[-1]:  # FIXME: remove after NIR integration tests,https://github.com/NRCan/geo-deep-learning/commit/84abd76bc84054664290c2fe8debe0010d40e547,Yes
3010,NRCan/geo-deep-learning,utils/augmentation.py,84abd76bc84054664290c2fe8debe0010d40e547,FIXME: test this. Assure compatibility with CRIM devs (don't trim metadata),https://github.com/NRCan/geo-deep-learning/commit/84abd76bc84054664290c2fe8debe0010d40e547,No
3011,NRCan/geo-deep-learning,utils/augmentation.py,84abd76bc84054664290c2fe8debe0010d40e547,TODO: test this.,https://github.com/NRCan/geo-deep-learning/commit/84abd76bc84054664290c2fe8debe0010d40e547,Yes
3012,NRCan/geo-deep-learning,utils/augmentation.py,84abd76bc84054664290c2fe8debe0010d40e547,TODO: can this for loop be optimized? Also; this hasn't been tested with non 8-bit data. Should be fine though.,https://github.com/NRCan/geo-deep-learning/commit/84abd76bc84054664290c2fe8debe0010d40e547,Yes
3013,NRCan/geo-deep-learning,utils/verifications.py,84abd76bc84054664290c2fe8debe0010d40e547,TODO: test this with invalid features.,https://github.com/NRCan/geo-deep-learning/commit/84abd76bc84054664290c2fe8debe0010d40e547,Yes
3014,NRCan/geo-deep-learning,inference.py,1fb33027e4dbe1465cf4d8cc56ad30307ac96dde,TODO: test this,https://github.com/NRCan/geo-deep-learning/commit/1fb33027e4dbe1465cf4d8cc56ad30307ac96dde,No
3015,NRCan/geo-deep-learning,inference.py,fea6a3daaf1fcc0118a7d6edd3e2153f9ae2024a,TODO: test this,https://github.com/NRCan/geo-deep-learning/commit/fea6a3daaf1fcc0118a7d6edd3e2153f9ae2024a,No
3016,NRCan/geo-deep-learning,inference.py,5865c25bd83b5f14ec843b43e6f878d4b4517c1e,TODO: test this,https://github.com/NRCan/geo-deep-learning/commit/5865c25bd83b5f14ec843b43e6f878d4b4517c1e,No
3017,NRCan/geo-deep-learning,inference.py,2697eec24a88faa865f7671957f5530c6cb9abcb,TODO: test this,https://github.com/NRCan/geo-deep-learning/commit/2697eec24a88faa865f7671957f5530c6cb9abcb,No
3018,NRCan/geo-deep-learning,utils/augmentation.py,2697eec24a88faa865f7671957f5530c6cb9abcb,FIXME: test this. Assure compatibility with CRIM devs (don't trim metadata),https://github.com/NRCan/geo-deep-learning/commit/2697eec24a88faa865f7671957f5530c6cb9abcb,No
3019,NRCan/geo-deep-learning,utils/augmentation.py,2697eec24a88faa865f7671957f5530c6cb9abcb,TODO: can this for loop be optimized? Also; this hasn't been tested with non 8-bit data. Should be fine though.,https://github.com/NRCan/geo-deep-learning/commit/2697eec24a88faa865f7671957f5530c6cb9abcb,Yes
3020,NRCan/geo-deep-learning,utils/visualization.py,2697eec24a88faa865f7671957f5530c6cb9abcb,TODO: test this with grayscale input.,https://github.com/NRCan/geo-deep-learning/commit/2697eec24a88faa865f7671957f5530c6cb9abcb,No
3021,prodo-dev/plz,services/controller/main.py,a05bb2b50e4e80715bdbc76680a617821b745ffa,TODO: use private ip? (It's harder for testing),https://github.com/prodo-dev/plz/commit/a05bb2b50e4e80715bdbc76680a617821b745ffa,No
3022,eellak/gsoc2018-3gm,src/tests.py,7e48f2d99f2f5f9589d75270f3c5efbc0d486c6f,TODO New tests,https://github.com/eellak/gsoc2018-3gm/commit/7e48f2d99f2f5f9589d75270f3c5efbc0d486c6f,No
3023,asreview/asreview,systematic_review_passive.py,31794d75713f858686daf018b715e0d126a01141,Split train\/test for depression dataset. It is an iterative systematic review.,https://github.com/asreview/asreview/commit/31794d75713f858686daf018b715e0d126a01141,Yes
3024,ncbi-nlp/NegBio,tests/negbio/ngrex/test_parser.py,78f08f839a129cc2d1a8acc89f08b384a10948eb,"_test_yacc(\""{lemma:\/xxx\/} <{dependency:\/nmod:without|x\/} {lemma:\/yyy\/}\"")",https://github.com/ncbi-nlp/NegBio/commit/78f08f839a129cc2d1a8acc89f08b384a10948eb,Yes
3025,ncbi-nlp/NegBio,tests/negbio/ngrex/test_parser.py,78f08f839a129cc2d1a8acc89f08b384a10948eb,"_test_yacc(\""{lemma:\/xxx\/} >{dependency:\/nmod:without\/} {lemma:\/yyy\/}\"")",https://github.com/ncbi-nlp/NegBio/commit/78f08f839a129cc2d1a8acc89f08b384a10948eb,Yes
3026,ncbi-nlp/NegBio,tests/negbio/ngrex/test_parser.py,78f08f839a129cc2d1a8acc89f08b384a10948eb,"_test_yacc(\""{lemma:\/xxx\/} >{dependency:\/nmod:without\/} ({lemma:\/yyy\/} >{} {lemma:\/zzz\/})\"")",https://github.com/ncbi-nlp/NegBio/commit/78f08f839a129cc2d1a8acc89f08b384a10948eb,Yes
3027,okpy/ok-client,ok-client/tests/__init__.py,db16a07c694549b95515727559efcb260206df26,"\""\""\"" || Initializes any variables needed for testing. || \""\""\""",https://github.com/okpy/ok-client/commit/db16a07c694549b95515727559efcb260206df26,Yes
3028,okpy/ok-client,ok-client/utils/loading.py,db16a07c694549b95515727559efcb260206df26,TODO(albert): have a better way to sort tests.,https://github.com/okpy/ok-client/commit/db16a07c694549b95515727559efcb260206df26,No
3029,okpy/ok-client,tests/__init__.py,db16a07c694549b95515727559efcb260206df26,"\""\""\"" || Initializes any variables needed for testing. || \""\""\""",https://github.com/okpy/ok-client/commit/db16a07c694549b95515727559efcb260206df26,Yes
3030,okpy/ok-client,client/protocols/grading.py,c4113151d71615dcee10c3be79e9fd31c15e0373,TODO(albert): implement a fuzzy matching for tests,https://github.com/okpy/ok-client/commit/c4113151d71615dcee10c3be79e9fd31c15e0373,Yes
3031,okpy/ok-client,client/protocols/scoring.py,c4113151d71615dcee10c3be79e9fd31c15e0373,TODO(albert): implement a fuzzy matching for tests,https://github.com/okpy/ok-client/commit/c4113151d71615dcee10c3be79e9fd31c15e0373,Yes
3032,okpy/ok-client,client/sources/ok_test/sqlite.py,473682ed6d31d16a7933df0bb79d1428f28b4f56,TODO: show test with prompt,https://github.com/okpy/ok-client/commit/473682ed6d31d16a7933df0bb79d1428f28b4f56,Yes
3033,okpy/ok-client,client/sources/ok_test/doctest.py,26ac7eac21357305db7eb88c7ceedf087f6ebe2b,A hack that allows OkTest to identify DoctestSuites without circular,https://github.com/okpy/ok-client/commit/26ac7eac21357305db7eb88c7ceedf087f6ebe2b,Yes
3034,ucam-smt/sgnmt,cam/sgnmt/predictors/chainer_lstm.py,d07e2b22f2412b86458f31d7d4c28ec6c8f209d1,TODO Marcin: src_sentence is a list of integers in a line of --src_test,https://github.com/ucam-smt/sgnmt/commit/d07e2b22f2412b86458f31d7d4c28ec6c8f209d1,No
3035,delph-in/pydelphin,tests/simplemrs_test.py,abcdc8e5a63804734d476b1c2711f07c3d84633d,TODO: test missing >; etc.,https://github.com/delph-in/pydelphin/commit/abcdc8e5a63804734d476b1c2711f07c3d84633d,Yes
3036,delph-in/pydelphin,delphin/tsql.py,acd62d5d3f247a89bd14dfdafcd208d89104eff4,"\""\""\"" || TSQL -- Test Suite Query Language ||  || PyDelphin differences to standard TSQL: ||  || * `select *` requires a `from` statement || * `select * from item result` does not also include `parse` columns || * `select i-input from result` returns a matching `i-input` for every ||   row in `result`; rather than only the unique rows ||  || PyDelphin additions to standard TSQL: ||  || * optional table specifications on columns (e.g.; `item:i-id`) || * multiple `where` clauses (e.g.; `where X where Y` is the same as ||   `where (X) and (Y)`); this helps when appending to queries || \""\""\""",https://github.com/delph-in/pydelphin/commit/acd62d5d3f247a89bd14dfdafcd208d89104eff4,Yes
3037,delph-in/pydelphin,delphin/tsdb.py,c0bd3fd74a1ac39ce4af4313ec8219266f9635d6,"\""\""\"" || TSDB: Test Suite Databases ||  || .. note:: ||  ||   This module implements the basic; low-level functionality for ||   working with TSDB databases. For higher-level views and uses of ||   these databases; see :mod:`delphin.itsdb`. For complex queries of ||   the databases; see :mod:`delphin.tsql`. ||  || TSDB databases are plain-text file-based relational databases || minimally consisting of a directory with a file; called `relations`; || containing the database's schema (see `Schemas`_). Every table in the || database has its own file; which may be `gzipped || <https:\/\/en.wikipedia.org\/wiki\/Gzip>`_ to save space. The tables have || a simple format with columns delimited by ``@`` and rows delimited by || newlines. This makes them easy to inspect at the command line with || standard Unix tools such as ``cut`` and ``awk`` (but gzipped tables need || to be decompressed or piped from a tool such as ``zcat``). ||  || This module handles the technical details of reading and writing TSDB || databases; including: ||  || - parsing database schemas ||  || - transparently opening either the plain-text or gzipped tables on ||   disk; as appropriate ||  || - escaping and unescaping reserved characters in table data ||  || - pairing columns with their schema descriptions ||  || - casting types (such as ``:integer``; ``:date``; etc.) ||  || Additionally; this module provides very basic abstractions of || databases and tables as the :class:`Database` and :class:`Table` || classes; respectively. These serve as base classes for the more || featureful :class:`delphin.itsdb.TestSuite` class; but may be useful || for simple needs. || \""\""\""",https://github.com/delph-in/pydelphin/commit/c0bd3fd74a1ac39ce4af4313ec8219266f9635d6,Yes
3038,delph-in/pydelphin,delphin/tsql.py,09f4efd0b8fd240bbc0f6e539ab45eb85bd0c881,"\""\""\"" || TSQL -- Test Suite Query Language ||  || .. note:: ||  ||   This module deals with queries of TSDB databases. For basic; ||   low-level access to the databases; see :mod:`delphin.tsdb`. For ||   high-level operations and structures on top of the databases; see ||   :mod:`delphin.itsdb`. ||  || This module implements a subset of TSQL; namely the 'select' (or || 'retrieve') queries for extracting data from test suites. The general || form of a select query is:: ||  ||     [select] <projection> [from <relations>] [where <condition>]* ||  || For example; the following selects item identifiers that took more || than half a second to parse:: ||  ||     select i-id from item where total > 500 ||  || The `select` string is necessary when querying with the generic || :func:`query` function; but is implied and thus disallowed when using || the :func:`select` function. ||  || The `<projection>` is a list of space-separated field names (e.g.; || `i-id i-input mrs`); or the special string `*` which selects all || columns from the joined relations. ||  || The optional `from` clause provides a list of relation names (e.g.; || `item parse result`) that are joined on shared keys. The `from` clause || is required when `*` is used for the projection; but it can also be || used to select columns from non-standard relations (e.g.; `i-id from || output`). Alternatively; `delphin.itsdb`-style data specifiers (see || :func:`delphin.itsdb.get_data_specifier`) may be used to specify the || relation on the column name (e.g.; `item.i-id`). ||  || The `where` clause provide conditions for filtering the list of || results. Conditions are binary operations that take a column or data || specifier on the left side and an integer (e.g.; `10`); a date (e.g.; || `2018-10-07`); or a string (e.g.; `\""sleep\""`) on the right side of the || operator. The allowed conditions are: ||  ||     ================  ====================================== ||     Condition         Form ||     ================  ====================================== ||     Regex match       ``<field> ~ \""regex\""`` ||     Regex fail        ``<field> !~ \""regex\""`` ||     Equality          ``<field> = (integer|date|\""string\"")`` ||     Inequality        ``<field> != (integer|date|\""string\"")`` ||     Less-than         ``<field> < (integer|date)`` ||     Less-or-equal     ``<field> <= (integer|date)`` ||     Greater-than      ``<field> > (integer|date)`` ||     Greater-or-equal  ``<field> >= (integer|date)`` ||     ================  ====================================== ||  || Boolean operators can be used to join multiple conditions or for || negation: ||  ||     ===========  ===================================== ||     Operation    Form ||     ===========  ===================================== ||     Disjunction  ``X | Y``; ``X || Y``; or ``X or Y`` ||     Conjunction  ``X & Y``; ``X && Y``; or ``X and Y`` ||     Negation     ``!X`` or ``not X`` ||     ===========  ===================================== ||  || Normally; disjunction scopes over conjunction; but parentheses may be || used to group clauses; so the following are equivalent:: ||  ||     ... where i-id = 10 or i-id = 20 and i-input ~ \""[Dd]og\"" ||     ... where i-id = 10 or (i-id = 20 and i-input ~ \""[Dd]og\"") ||  || Multiple `where` clauses may also be used as a conjunction that scopes || over disjunction; so the following are equivalent:: ||  ||     ... where (i-id = 10 or i-id = 20) and i-input ~ \""[Dd]og\"" ||     ... where i-id = 10 or i-id = 20 where i-input ~ \""[Dd]og\"" ||  || This facilitates query construction; where a user may want to apply || additional global constraints by appending new conditions to the query || string. ||  || PyDelphin has several differences to standard TSQL: ||  || * `select *` requires a `from` clause || * `select * from item result` does not also include columns from the ||   intervening `parse` relation || * `select i-input from result` returns a matching `i-input` for every ||   row in `result`; rather than only the unique rows ||  || PyDelphin also adds some features to standard TSQL: ||  || * qualified column names (e.g.; `item.i-id`) || * multiple `where` clauses (as described above) || \""\""\""",https://github.com/delph-in/pydelphin/commit/09f4efd0b8fd240bbc0f6e539ab45eb85bd0c881,No
3039,delph-in/pydelphin,delphin/cli/process.py,4b9f3d798f8287b4f0443e42da0cc48f00d91130,"\""\""\"" || Use a processor (namely ACE) to process each item in the [incr tsdb()] || testsuite given by --source (TESTSUITE if --source is not given). For || standard [incr tsdb()] schemata; input items given by the following || selectors for each task (configurable via the --select option): ||  ||     * parse:    i-input ||     * transfer: mrs ||     * generate: mrs ||  || In addition; the following TSQL condition is applied if --source is a || standard [incr tsdb()] profile and --all-items is not used: ||  ||     where i-wf != 2 || \""\""\""",https://github.com/delph-in/pydelphin/commit/4b9f3d798f8287b4f0443e42da0cc48f00d91130,Yes
3040,MaestroGraph/sparse-hyper,tests.py,f0559e721f4444bfad8aeda5a10ed9a6512fcdd7,TODO figure out how unit tests are supposed to work,https://github.com/MaestroGraph/sparse-hyper/commit/f0559e721f4444bfad8aeda5a10ed9a6512fcdd7,Yes
3041,MaestroGraph/sparse-hyper,sampling.py,2736e68fe87798ebad74f9827452b512285de350,TODO Make this a unit test,https://github.com/MaestroGraph/sparse-hyper/commit/2736e68fe87798ebad74f9827452b512285de350,Yes
3042,BaderLab/saber,kari/test/test_dataset.py,fc5829bdfac9e6267563d77b5f505174ddd2cec1,TODO (johngiorgi): Need tests for compound datasets,https://github.com/BaderLab/saber/commit/fc5829bdfac9e6267563d77b5f505174ddd2cec1,No
3043,BaderLab/saber,kari/test/test_sequence_processing_model.py,6a13a47174c5f632dd0dcbb9c12cfc8909020693,TODO (johngiorgi): more tests need to be written; especially pertaining,https://github.com/BaderLab/saber/commit/6a13a47174c5f632dd0dcbb9c12cfc8909020693,Yes
3044,BaderLab/saber,kari/models/multi_task_lstm_crf.py,ae1f5b49942c664ba1d7fe4d5c4cfa6b936fcfaf,TODO (johngiorgi): read up on train_test_split; do I want to shuffle?,https://github.com/BaderLab/saber/commit/ae1f5b49942c664ba1d7fe4d5c4cfa6b936fcfaf,No
3045,BaderLab/saber,kari/models/multi_task_lstm_crf.py,59fb29cdbc7d3f197b65e08562d9be547f03e990,TODO (johngiorgi): the way I get train\/test partitions is likely copying,https://github.com/BaderLab/saber/commit/59fb29cdbc7d3f197b65e08562d9be547f03e990,No
3046,BaderLab/saber,saber/test/test_multi_task_lstm_crf.py,2f8b223cb9b531c8e71bb940a56573005aac64d9,TODO (johngiorgi): fix some of the test_model_attributes_after_creation_of_model tests,https://github.com/BaderLab/saber/commit/2f8b223cb9b531c8e71bb940a56573005aac64d9,No
3047,BaderLab/saber,saber/test/test_sequence_processor.py,b51319ca6297ad012ab09a7a81712a119d71d0f3,TODO (johngiorgi): add some kind of test that accounts for the error thrown,https://github.com/BaderLab/saber/commit/b51319ca6297ad012ab09a7a81712a119d71d0f3,Yes
3048,BaderLab/saber,saber/test/test_dataset.py,aeadc30c97e958795860d7efd7c2e0167eb68a87,TODO: Figure out how to perform this test,https://github.com/BaderLab/saber/commit/aeadc30c97e958795860d7efd7c2e0167eb68a87,Yes
3049,BaderLab/saber,saber/trainer.py,46878d31599b5681abfda9b53310f1ffcfb03e4f,valid\/test set was provided; train via standard train\/valid\/test split,https://github.com/BaderLab/saber/commit/46878d31599b5681abfda9b53310f1ffcfb03e4f,No
3050,BaderLab/saber,saber/tests/test_dataset.py,97623f68836314834a6869dc912eeb5842367722,TODO: Figure out how to perform this test,https://github.com/BaderLab/saber/commit/97623f68836314834a6869dc912eeb5842367722,Yes
3051,BaderLab/saber,saber/tests/test_multi_task_lstm_crf.py,97623f68836314834a6869dc912eeb5842367722,TODO (johngiorgi): fix some of the test_model_attributes_after_creation_of_model tests,https://github.com/BaderLab/saber/commit/97623f68836314834a6869dc912eeb5842367722,No
3052,BaderLab/saber,saber/tests/test_sequence_processor.py,97623f68836314834a6869dc912eeb5842367722,TODO (johngiorgi): add some kind of test that accounts for the error thrown,https://github.com/BaderLab/saber/commit/97623f68836314834a6869dc912eeb5842367722,Yes
3053,BaderLab/saber,saber/tests/test_trainer.py,97623f68836314834a6869dc912eeb5842367722,TODO (johngiorgi): begin writing tests; start with _split_train_valid,https://github.com/BaderLab/saber/commit/97623f68836314834a6869dc912eeb5842367722,No
3054,BaderLab/saber,saber/tests/test_dataset.py,f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,TODO (johngiorgi): Need to include tests for valid\/test partitions,https://github.com/BaderLab/saber/commit/f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,No
3055,BaderLab/saber,saber/tests/test_dataset.py,f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,as a workaround to testing this directly; just check that shapes are as expected,https://github.com/BaderLab/saber/commit/f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,Yes
3056,BaderLab/saber,saber/tests/test_embeddings.py,f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,TODO (johngiorgi): write tests using a binary format file,https://github.com/BaderLab/saber/commit/f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,Yes
3057,BaderLab/saber,saber/tests/test_embeddings.py,f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,TODO (johngiorgi): write tests to test for debug functionality,https://github.com/BaderLab/saber/commit/f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,Yes
3058,BaderLab/saber,saber/tests/test_saber.py,f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,TODO (johngiorgi): Write tests for compound dataset,https://github.com/BaderLab/saber/commit/f31b8d9d79220c74aeeab7810e9ca7c5979a1f08,Yes
3059,BaderLab/saber,saber/tests/test_grounding.py,a96ed67975735aa622a7326b5a3425652e74bb2e,TODO: implement _query_hgnc; make test fail; then fix,https://github.com/BaderLab/saber/commit/a96ed67975735aa622a7326b5a3425652e74bb2e,No
3060,EducationalTestingService/rsmtool,tests/update_skll_models.py,c297893ad434b8f9d3106ae82bde3f8e8ed12dcf,"\""\""\"" || This script is designed to update the sci-kit learn model files to ensure they || are compatible with the current version. || The script goes through all tests in the data\/experiments directory; finds the model files and || saves them again. || Note that this will overwrite the original test data. || Before running this script; make sure that the outputs work as expected ||  || :author: Anastassia Loukina || :author: Nitin Madnani || :date: February 2017 || \""\""\""",https://github.com/EducationalTestingService/rsmtool/commit/c297893ad434b8f9d3106ae82bde3f8e8ed12dcf,No
3061,EducationalTestingService/rsmtool,tests/test_experiment_rsmcompare.py,c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,Yes
3062,EducationalTestingService/rsmtool,tests/test_experiment_rsmeval.py,c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,Yes
3063,EducationalTestingService/rsmtool,tests/test_experiment_rsmpredict.py,c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,Yes
3064,EducationalTestingService/rsmtool,tests/test_experiment_rsmsummarize.py,c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,Yes
3065,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_1.py,c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,Yes
3066,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_2.py,c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,Yes
3067,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_3.py,c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,Yes
3068,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_4.py,c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/c90aa4d4a3eeeb5870d8b1712a9d2a944449f1de,Yes
3069,EducationalTestingService/rsmtool,tests/test_experiment_rsmcompare.py,e13837307c8a2b33ac835ec008da6f71b35120ef,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/e13837307c8a2b33ac835ec008da6f71b35120ef,Yes
3070,EducationalTestingService/rsmtool,tests/test_experiment_rsmeval.py,e13837307c8a2b33ac835ec008da6f71b35120ef,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/e13837307c8a2b33ac835ec008da6f71b35120ef,Yes
3071,EducationalTestingService/rsmtool,tests/test_experiment_rsmpredict.py,e13837307c8a2b33ac835ec008da6f71b35120ef,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/e13837307c8a2b33ac835ec008da6f71b35120ef,Yes
3072,EducationalTestingService/rsmtool,tests/test_experiment_rsmsummarize.py,e13837307c8a2b33ac835ec008da6f71b35120ef,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/e13837307c8a2b33ac835ec008da6f71b35120ef,Yes
3073,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_1.py,e13837307c8a2b33ac835ec008da6f71b35120ef,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/e13837307c8a2b33ac835ec008da6f71b35120ef,Yes
3074,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_2.py,e13837307c8a2b33ac835ec008da6f71b35120ef,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/e13837307c8a2b33ac835ec008da6f71b35120ef,Yes
3075,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_3.py,e13837307c8a2b33ac835ec008da6f71b35120ef,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/e13837307c8a2b33ac835ec008da6f71b35120ef,Yes
3076,EducationalTestingService/rsmtool,tests/test_experiment_rsmtool_4.py,e13837307c8a2b33ac835ec008da6f71b35120ef,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/e13837307c8a2b33ac835ec008da6f71b35120ef,Yes
3077,EducationalTestingService/rsmtool,tests/test_test_utils.py,0975b07b36c01c2fe515bef5157ccf73e2e11db8,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/0975b07b36c01c2fe515bef5157ccf73e2e11db8,Yes
3078,EducationalTestingService/rsmtool,tests/test_utils.py,478ead2fb146390569a88aeaeec4e0f37e94f748,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/478ead2fb146390569a88aeaeec4e0f37e94f748,Yes
3079,EducationalTestingService/rsmtool,tests/test_cli.py,f37fddd7dbc6a9ac89c2098ecb8b2f7b3bc840ff,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/f37fddd7dbc6a9ac89c2098ecb8b2f7b3bc840ff,Yes
3080,EducationalTestingService/rsmtool,tests/test_utils_prmse.py,760df9585a898093b51939cd948829f80e36b400,which is needed for package testing,https://github.com/EducationalTestingService/rsmtool/commit/760df9585a898093b51939cd948829f80e36b400,Yes
3081,google-research/google-research,cnn_quantization/tf_cnn_benchmarks/benchmark_cnn_distributed_test.py,7772843ee5eadeb625a3932882f1fb0e852e2e84,on every test. See the TODO in benchmark_cnn_test.py.,https://github.com/google-research/google-research/commit/7772843ee5eadeb625a3932882f1fb0e852e2e84,Yes
3082,google-research/google-research,cnn_quantization/tf_cnn_benchmarks/benchmark_cnn_distributed_test_runner.py,7772843ee5eadeb625a3932882f1fb0e852e2e84,"\""\""\""Used to run benchmark_cnn for distributed tests. ||  || In distributed tests; we spawn processes to run tf_cnn_benchmark tasks. We could || directly spawn tf_cnn_benchmark processes; but we want some added functionality; || such as being able to inject custom images during training. So instead; this || file is spawned as a Python process; which supports the added functionality. || \""\""\""",https://github.com/google-research/google-research/commit/7772843ee5eadeb625a3932882f1fb0e852e2e84,Yes
3083,google-research/google-research,cnn_quantization/tf_cnn_benchmarks/preprocessing.py,7772843ee5eadeb625a3932882f1fb0e852e2e84,image = tf.cast(image; tf.uint8) # HACK TESTING,https://github.com/google-research/google-research/commit/7772843ee5eadeb625a3932882f1fb0e852e2e84,No
3084,google-research/google-research,generalized_rates/datasets/load_adult.py,f973bd35cbb0d7091e77fd5658ae203ef6d19aae,Make sure the train and test dataframes have the same binarized columns.,https://github.com/google-research/google-research/commit/f973bd35cbb0d7091e77fd5658ae203ef6d19aae,No
3085,google-research/google-research,generalized_rates/datasets/load_adult.py,f973bd35cbb0d7091e77fd5658ae203ef6d19aae,Identify columns in train set not in test set and fill them in test set.,https://github.com/google-research/google-research/commit/f973bd35cbb0d7091e77fd5658ae203ef6d19aae,Yes
3086,google-research/google-research,generalized_rates/datasets/load_adult.py,f973bd35cbb0d7091e77fd5658ae203ef6d19aae,Identify columns in test set not in train set and fill them in train set.,https://github.com/google-research/google-research/commit/f973bd35cbb0d7091e77fd5658ae203ef6d19aae,Yes
3087,google-research/google-research,grouptesting/group_selectors/informative_dorfman.py,ac572b7d577cfb1f0846c241e1ac242642bab6f3,sample randomly extra_tests_needed groups,https://github.com/google-research/google-research/commit/ac572b7d577cfb1f0846c241e1ac242642bab6f3,Yes
3088,google-research/google-research,constrained_language_typology/sigtyp_reader_main.py,07c42b122d364de5f29b18b195e0d5bc779d9af2,"r\""\""\""Reader for the format provided by SIGTYP 2020 Shared Task. ||  || More information on the format is available here: ||   https:\/\/sigtyp.github.io\/st2020.html ||  || Example: || -------- ||  Clone the GitHub data to ST2020_DIR. Then run: ||  ||  > ST2020_DIR=... ||  > python3 sigtyp_reader_main.py --sigtyp_dir ${ST2020_DIR}\/data \\ ||     --output_dir ${OUTPUT_DIR} ||  ||  The above will create \""train.csv\""; \""dev.csv\"" and \""test_blinded.csv\"" files ||  converted from the format provided by SIGTYP. Our models should be able to ||  injest these csv files. Along each of the above files; an accompanying ||  \""data_train_*.json.gz\"" file is generated that contains metainformation on ||  various features and their values. ||  || TODO: || ----- || Following needs to be done: ||   - Latitude and longitude need to be on a point on a unit sphere? Keep as is ||     and add three further columns for (x;y;z)? ||   - Country codes are *several*. ||   - Other types of SOMs. ||   - Use BaseMap for visualizations? || \""\""\""",https://github.com/google-research/google-research/commit/07c42b122d364de5f29b18b195e0d5bc779d9af2,No
3089,google-research/google-research,many_constraints/robust_fairness.py,df56d264428bfc11d5acf883944bb04c31a66cfb,Make sure the train and test dataframes have the same binarized columns.,https://github.com/google-research/google-research/commit/df56d264428bfc11d5acf883944bb04c31a66cfb,No
3090,google-research/google-research,social_rl/adversarial_env/run_transfer_experiments.py,b4396b2f8e72afe733fb7d7c75ce25db80f9f4b7,"\""\""\""Tests trained models on transfer environments to generate videos and scores. ||  || Note that this code assumes it will be provided with a .csv file indicating || which checkpoints it should load based on finding the best hyperparameters || for a given metric; such as 'SolvedPathLength_last20%'. It assumes this csv will || have columns labeled 'metric'; 'exp_id'; 'best_seeds'; and 'settings'. Such a || csv can be created using the function utils.save_best_work_units_csv() || \""\""\""",https://github.com/google-research/google-research/commit/b4396b2f8e72afe733fb7d7c75ce25db80f9f4b7,Yes
3091,google-research/google-research,flax_models/mlperf/transformer/mlperf_encoder.py,dd7945869dc62ea397acb721a620ed6d3268f058,"\""\""\""SubWordTextEncoder taken from MLPerf's version of T2T encoder. ||  || This is solely needed to decode MLPerf's WMT test dataset for BLEU score || calculation; as TFDS's tokenizer encoding behaves slightly but critically || different around non-alphanumeric characters. || \""\""\""",https://github.com/google-research/google-research/commit/dd7945869dc62ea397acb721a620ed6d3268f058,Yes
3092,google-research/google-research,aptamers_mlpd/preprocess/fastq_to_sstable.py,328de318e5ad9baf62bf541cb0f24e6b31f2f264,mark required flags here so unit test doesn't need the sys.argv.extend hack,https://github.com/google-research/google-research/commit/328de318e5ad9baf62bf541cb0f24e6b31f2f264,No
3093,pandas-profiling/pandas-profiling,tests/test_jupyter_notebook.py,4a3f49ee15218c0b97e3550d363e6d986b461edf,TODO: test general working,https://github.com/pandas-profiling/pandas-profiling/commit/4a3f49ee15218c0b97e3550d363e6d986b461edf,Yes
3094,pandas-profiling/pandas-profiling,tests/test_jupyter_notebook.py,4a3f49ee15218c0b97e3550d363e6d986b461edf,TODO: test javascript,https://github.com/pandas-profiling/pandas-profiling/commit/4a3f49ee15218c0b97e3550d363e6d986b461edf,No
3095,pandas-profiling/pandas-profiling,tests/test_jupyter_notebook.py,4a3f49ee15218c0b97e3550d363e6d986b461edf,TODO: test jupyter lab,https://github.com/pandas-profiling/pandas-profiling/commit/4a3f49ee15218c0b97e3550d363e6d986b461edf,No
3096,pandas-profiling/pandas-profiling,tests/issues/test_issueXXX.py,4af395625ebb415ea1ae2b036a4cf2ade7a4a7b9,"\""\""\"" || Test for issue XXX: || https:\/\/github.com\/pandas-profiling\/pandas-profiling\/issues\/XXX || \""\""\""",https://github.com/pandas-profiling/pandas-profiling/commit/4af395625ebb415ea1ae2b036a4cf2ade7a4a7b9,Yes
3097,pandas-profiling/pandas-profiling,tests/issues/test_issue537.py,577b230cd45a2050d5e43e06ddb33da181fbb1a9,"\""\""\"" || Test for issue 537: || https:\/\/github.com\/pandas-profiling\/pandas-profiling\/issues\/537 ||  || ValueError: shape mismatch: value array of shape (136;) could not be broadcast to indexing result of shape (135;) ||  || Problem : || ValueError is raised when running ProfileReport on large datasets and with multiprocessing on (pool_size >1). || This is likely due to the series.fillna(np.nan; inplace=True) in summary.py seems to be performing multiple in-place || mutations to the underlying DataFrame object through the passed series reference; resulting in some kind of race || condition where two of the processes try to write to the DataFrame at the same time and the ValueError then occurs. ||  This is also why changing the pool_size to 1 fixes the issue; and why the error doesn't always occur - ||  you probably need enough data and threads to hit the race condition. ||  || Solution : || Replace series.fillna(np.nan; inplace=True) with series = series.fillna(np.nan) ; negating any side effects from mutation. ||  ||  || \""\""\""",https://github.com/pandas-profiling/pandas-profiling/commit/577b230cd45a2050d5e43e06ddb33da181fbb1a9,No
3098,pandas-profiling/pandas-profiling,tests/issues/test_issue523.py,00bdeafee8f896ba5365f933563433794de6b8fd,"\""\""\""\r || Test for issue 523:\r || https:\/\/github.com\/pandas-profiling\/pandas-profiling\/issues\/XXX\r || \""\""\""",https://github.com/pandas-profiling/pandas-profiling/commit/00bdeafee8f896ba5365f933563433794de6b8fd,Yes
3099,apacha/Mensural-Detector,object_detection/inputs_test.py,b11b29132897160a24c47f801dee8bf7b47fa46d,TODO: Make sure these tests work fine outside google3.,https://github.com/apacha/Mensural-Detector/commit/b11b29132897160a24c47f801dee8bf7b47fa46d,No
3100,apayne19/DoubleAuctionMarket,spot_market.py,d1680379a345a4d44b19b0ee18228d32277cc3fe,TODO change file to TEST,https://github.com/apayne19/DoubleAuctionMarket/commit/d1680379a345a4d44b19b0ee18228d32277cc3fe,Yes
3101,apayne19/DoubleAuctionMarket,trader.py,f38b6094bd4d62a89a437c883e92d38bf23d4c02,self.margin = 0  # test  # TODO fix drop offs in code,https://github.com/apayne19/DoubleAuctionMarket/commit/f38b6094bd4d62a89a437c883e92d38bf23d4c02,Yes
3102,apayne19/DoubleAuctionMarket,Simulator/spot_market.py,ec345d4df613293e06e433e2debaa82addb6c99e,TODO change file to TEST,https://github.com/apayne19/DoubleAuctionMarket/commit/ec345d4df613293e06e433e2debaa82addb6c99e,Yes
3103,CT83/Facial-Recognition-Attendance-System,code/django-server/fras/attendance/utils.py,f42dbaae102ede336317c00c0e0b7d0ac03d9ce0,TODO Complete this and test it later,https://github.com/CT83/Facial-Recognition-Attendance-System/commit/f42dbaae102ede336317c00c0e0b7d0ac03d9ce0,No
3104,engapa/modeldb-basic,.tox/py27/lib/python2.7/site.py,3ffa7547ad05f7f0657bd44d3ed641b42e3c852c,encoding after initialization.  The test for presence is needed when,https://github.com/engapa/modeldb-basic/commit/3ffa7547ad05f7f0657bd44d3ed641b42e3c852c,Yes
3105,engapa/modeldb-basic,.tox/py36/lib/python3.6/site.py,3ffa7547ad05f7f0657bd44d3ed641b42e3c852c,encoding after initialization.  The test for presence is needed when,https://github.com/engapa/modeldb-basic/commit/3ffa7547ad05f7f0657bd44d3ed641b42e3c852c,Yes
3106,engapa/modeldb-basic,.venv/lib/python3.6/site.py,3ffa7547ad05f7f0657bd44d3ed641b42e3c852c,encoding after initialization.  The test for presence is needed when,https://github.com/engapa/modeldb-basic/commit/3ffa7547ad05f7f0657bd44d3ed641b42e3c852c,Yes
3107,GalDude33/Fetal-MRI-Segmentation,DataGenerator.py,07b293da3611beea1479eb3e810c5f7ed7dc5ff3,TODO: Edge case? Currently this is handled by flooring the number of training\/testing samples,https://github.com/GalDude33/Fetal-MRI-Segmentation/commit/07b293da3611beea1479eb3e810c5f7ed7dc5ff3,Yes
3108,JeevanSandhu/Intrusion-Detection,ENV/lib/python3.5/site.py,db39bfe9ad3780ee0fff21aed0637eb47a17098b,encoding after initialization.  The test for presence is needed when,https://github.com/JeevanSandhu/Intrusion-Detection/commit/db39bfe9ad3780ee0fff21aed0637eb47a17098b,Yes
3109,JustinGOSSES/predictatops,StratPickSupML/split.py,7fef51c293e7e92b8fa457f16afaa7bc29bccf98,### Split data into train & test on a well by well basis; not row basis\u00B6,https://github.com/JustinGOSSES/predictatops/commit/7fef51c293e7e92b8fa457f16afaa7bc29bccf98,Yes
3110,JustinGOSSES/predictatops,predictatops/balance.py,a7022297fb2b439d2ebf0a2d5c22811b93c972d1,####################   Now let's take out those same columns in the test only dataframe,https://github.com/JustinGOSSES/predictatops/commit/a7022297fb2b439d2ebf0a2d5c22811b93c972d1,No
3111,JustinGOSSES/predictatops,predictatops/balance_runner.py,883a249fe33ade080e3a2296abef19b8e23cb0d1,########## Now let's take out those same columns in the test only dataframe ###########,https://github.com/JustinGOSSES/predictatops/commit/883a249fe33ade080e3a2296abef19b8e23cb0d1,Yes
3112,mlflow/mlflow,examples/hyperparam/train.py,fc5d4dd5ddda3b044f598b4ba27f070f4329418d,"\""\""\"" || Train a simple Keras DL model on the dataset used in MLflow tutorial (wine-quality.csv). ||  || Dataset is split into train (~ 0.56); validation(~ 0.19) and test (0.25). || Validation data is used to select the best hyperparameters; test set performance is evaluated only || at epochs which improved performance on the validation dataset. The model with best validation set || performance is logged with MLflow. || \""\""\""",https://github.com/mlflow/mlflow/commit/fc5d4dd5ddda3b044f598b4ba27f070f4329418d,No
3113,mlflow/mlflow,examples/rest_api/mlflow_tracking_rest_api.py,4f8850473c20222d49336facf070655a62c2fcea,"\""\""\"" || This simple example shows how you could use MLFlow REST API to create new  || runs inside an experiment to log parameters\/metrics.  Using MLFlow REST API  || instead of MLFlow library might be useful to embed in an application where  || you don't want to depend on the whole mlflow library; or to make  || your own HTTP requests in another programming language (not Python). || For more details on MLFLow REST API endpoints check the following page:  ||  || https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html  || \""\""\""",https://github.com/mlflow/mlflow/commit/4f8850473c20222d49336facf070655a62c2fcea,Yes
3114,mlflow/mlflow,tests/models/test_cli.py,8e820dab991ff790957c3a2a5c679ac35bf9b7a4,TODO: Enable this test after 1.0 is out to ensure we do not break the serve \/ predict,https://github.com/mlflow/mlflow/commit/8e820dab991ff790957c3a2a5c679ac35bf9b7a4,Yes
3115,mlflow/mlflow,tests/onnx/test_onnx_model_export.py,1d0d3e6db37f150873dfb979fb0d4ecfe0e1d502,TODO: Remove test; along with explicit casting; when https:\/\/github.com\/mlflow\/mlflow\/issues\/1286,https://github.com/mlflow/mlflow/commit/1d0d3e6db37f150873dfb979fb0d4ecfe0e1d502,Yes
3116,mlflow/mlflow,tests/server/test_handlers.py,ac68e6be4eb850b765e3a3ec1315655c8bb0543b,TODO: efficient mechanism to test endpoint path,https://github.com/mlflow/mlflow/commit/ac68e6be4eb850b765e3a3ec1315655c8bb0543b,Yes
3117,mlflow/mlflow,tests/test_no_f_strings.py,7779b87540ea57ec56d1b7ac42e420a6a6f9df23,TODO: Remove this test script once we drop Python 3.5 support.,https://github.com/mlflow/mlflow/commit/7779b87540ea57ec56d1b7ac42e420a6a6f9df23,Yes
3118,mlflow/mlflow,tests/sklearn/test_sklearn_autolog.py,7fb35d636d6e0ad764a7b0ed5fe7f94ef2b4d5c0,TODO : @wendyhu\/@arjundc uncomment this test in enforcement PR,https://github.com/mlflow/mlflow/commit/7fb35d636d6e0ad764a7b0ed5fe7f94ef2b4d5c0,No
3119,mlflow/mlflow,tests/types/test_schema.py,d029e2cfa350867c956bada4e36eeae1881a7ed1,Todo: arjundc : Remove _enforce_tensor_spec and move to its own test file.,https://github.com/mlflow/mlflow/commit/d029e2cfa350867c956bada4e36eeae1881a7ed1,No
3120,jupyterhub/jupyterhub,jupyterhub/tests/test_proxy.py,5133cf02759d0c4b9ecebf1127c0fb2ca91c405b,TODO simplify this test,https://github.com/jupyterhub/jupyterhub/commit/5133cf02759d0c4b9ecebf1127c0fb2ca91c405b,Yes
3121,jupyterhub/jupyterhub,jupyterhub/tests/conftest.py,07bbb4ea027c7c46af88a43da45371d321b82013,"\""\""\""py.test fixtures ||  || Fixtures for jupyterhub components || ---------------------------------- || - `app` || - `auth_state_enabled` || - `db` || - `io_loop` || - single user servers ||     - `cleanup_after`: allows cleanup of single user servers between tests || - mocked service ||     - `MockServiceSpawner` ||     - `mockservice`: mocked service with no external service url ||     - `mockservice_url`: mocked service with a url to test external services ||  || Fixtures to add functionality or spawning behavior || -------------------------------------------------- || - `admin_access` || - `no_patience` || - `slow_spawn` || - `never_spawn` || - `bad_spawn` || - `slow_bad_spawn` ||  || \""\""\""",https://github.com/jupyterhub/jupyterhub/commit/07bbb4ea027c7c46af88a43da45371d321b82013,Yes
3122,jupyterhub/jupyterhub,jupyterhub/tests/test_proxy.py,40176a667f61fc59e211332af31499b4e022865b,Todo: Update test to use proxy instance,https://github.com/jupyterhub/jupyterhub/commit/40176a667f61fc59e211332af31499b4e022865b,Yes
3123,ludwig-ai/ludwig,ludwig/models/modules/recurrent_modules.py,7e4261cb06e471de170ed623aef587f7ae8b7ebb,# todo tf2: test code,https://github.com/ludwig-ai/ludwig/commit/7e4261cb06e471de170ed623aef587f7ae8b7ebb,Yes
3124,ludwig-ai/ludwig,ludwig/models/model.py,0cab062a4a25a8e7211b65d04c95459b0d5eb1f6,todo tf2 clean up testing code,https://github.com/ludwig-ai/ludwig/commit/0cab062a4a25a8e7211b65d04c95459b0d5eb1f6,Yes
3125,ludwig-ai/ludwig,ludwig/models/model.py,96ff2d26c457062818582d390f2fbb72b6e7e055,todo tf2 test is needed only during development,https://github.com/ludwig-ai/ludwig/commit/96ff2d26c457062818582d390f2fbb72b6e7e055,No
3126,ludwig-ai/ludwig,ludwig/models/model.py,d8e58f0f8318554a7a128017ebb36bb4bb4c15e4,todo tf2 test only needed during development,https://github.com/ludwig-ai/ludwig/commit/d8e58f0f8318554a7a128017ebb36bb4bb4c15e4,Yes
3127,ludwig-ai/ludwig,ludwig/models/model.py,360f6e8aee7989b7e649c21883026612964b9cf7,if measure_fn is not None:  # todo tf2 test is needed only during development,https://github.com/ludwig-ai/ludwig/commit/360f6e8aee7989b7e649c21883026612964b9cf7,Yes
3128,ludwig-ai/ludwig,ludwig/models/model.py,360f6e8aee7989b7e649c21883026612964b9cf7,todo tf2 test is needed only during development,https://github.com/ludwig-ai/ludwig/commit/360f6e8aee7989b7e649c21883026612964b9cf7,No
3129,ludwig-ai/ludwig,ludwig/models/ecd.py,ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,todo tf2: tied encoder mechanism to be tested,https://github.com/ludwig-ai/ludwig/commit/ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,Yes
3130,ludwig-ai/ludwig,ludwig/models/model.py,ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,if measure_fn is not None:  # todo tf2 test only needed during development,https://github.com/ludwig-ai/ludwig/commit/ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,Yes
3131,ludwig-ai/ludwig,ludwig/models/model.py,ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,todo tf2 test only needed during development,https://github.com/ludwig-ai/ludwig/commit/ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,Yes
3132,ludwig-ai/ludwig,ludwig/models/model.py,ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,if measure_fn is not None:  # todo tf2 test is needed only during development,https://github.com/ludwig-ai/ludwig/commit/ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,Yes
3133,ludwig-ai/ludwig,ludwig/models/model.py,ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,todo tf2 test is needed only during development,https://github.com/ludwig-ai/ludwig/commit/ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,No
3134,ludwig-ai/ludwig,ludwig/features/base_feature.py,7d74e0b497e71bc96a89904c77207f7d3259b6ff,todo tf2: test all 4 branches; for now only vector x vector is tested,https://github.com/ludwig-ai/ludwig/commit/7d74e0b497e71bc96a89904c77207f7d3259b6ff,No
3135,ludwig-ai/ludwig,ludwig/models/model.py,ef667de0580f28c3c55352523e5fb0f4659d61c2,if metric_fn is not None:  # todo tf2 test is needed only during development,https://github.com/ludwig-ai/ludwig/commit/ef667de0580f28c3c55352523e5fb0f4659d61c2,Yes
3136,ludwig-ai/ludwig,ludwig/models/model.py,d35f8f67473a98df298ac96a17b6d8ccf170c136,if metric_fn is not None:  # todo tf2 test is needed only during development,https://github.com/ludwig-ai/ludwig/commit/d35f8f67473a98df298ac96a17b6d8ccf170c136,Yes
3137,ludwig-ai/ludwig,ludwig/models/model.py,91bc289ceee24916533e81e56b5497413b4a95fe,if metric_fn is not None:  # todo tf2 test is needed only during development,https://github.com/ludwig-ai/ludwig/commit/91bc289ceee24916533e81e56b5497413b4a95fe,Yes
3138,ludwig-ai/ludwig,ludwig/models/model.py,acb8bed03b3603a65a157b17315629e4b0a932eb,if metric_fn is not None:  # todo tf2 test is needed only during development,https://github.com/ludwig-ai/ludwig/commit/acb8bed03b3603a65a157b17315629e4b0a932eb,Yes
3139,ludwig-ai/ludwig,ludwig/models/model.py,ddc715790a25e2b971f2bcba30d243e3e5a7c0e2,todo tf2 testing code,https://github.com/ludwig-ai/ludwig/commit/ddc715790a25e2b971f2bcba30d243e3e5a7c0e2,No
3140,ludwig-ai/ludwig,ludwig/models/model.py,48a9f41d4262b404ba6216fa2aed578f033d28f3,todo tf2 testing code,https://github.com/ludwig-ai/ludwig/commit/48a9f41d4262b404ba6216fa2aed578f033d28f3,No
3141,ludwig-ai/ludwig,ludwig/models/modules/recurrent_modules.py,904049eabc5ad507078dd8e791bd3469549d8412,# # todo tf2: test code,https://github.com/ludwig-ai/ludwig/commit/904049eabc5ad507078dd8e791bd3469549d8412,Yes
3142,ludwig-ai/ludwig,tests/ludwig/models/modules/test_encoder.py,4f8084665fd0e52ccf2663e8fbd725bc17b65479,todo tf2: fix these tests to work with the TF2 way,https://github.com/ludwig-ai/ludwig/commit/4f8084665fd0e52ccf2663e8fbd725bc17b65479,Yes
3143,ludwig-ai/ludwig,tests/integration_tests/test_hyperopt.py,9a4dd7d08d4d855ff6e8d33e21c9ca9c6927d6d7,todo future: add test for hyperopt.run.hyperopt function,https://github.com/ludwig-ai/ludwig/commit/9a4dd7d08d4d855ff6e8d33e21c9ca9c6927d6d7,Yes
3144,ludwig-ai/ludwig,tests/integration_tests/test_visualization_api.py,fe56c4f97540dc36d6058bf99b71267b8fbb703e,todo determine feasibility of putting Experiment() into a pytest.fixture,https://github.com/ludwig-ai/ludwig/commit/fe56c4f97540dc36d6058bf99b71267b8fbb703e,Yes
3145,ludwig-ai/ludwig,ludwig/data/preprocessing.py,496009ec369f8d78311afde2b62123596122f076,TODO dask: https:\/\/docs.dask.org\/en\/latest\/dataframe-api.html#dask.dataframe.DataFrame.random_split,https://github.com/ludwig-ai/ludwig/commit/496009ec369f8d78311afde2b62123596122f076,Yes
3146,pyro-ppl/pyro,pyro/poutine/tracegraph_poutine.py,68b80a8d8543cf55405369fa7a1fe7c0cb920c0f,XXX FIX ME! this only looks at the shortest path; which,https://github.com/pyro-ppl/pyro/commit/68b80a8d8543cf55405369fa7a1fe7c0cb920c0f,Yes
3147,pyro-ppl/pyro,tests/test_conjugate_gaussian_models.py,d5248f3c26a7c4c01257351a92befe0b0e09a88e,def test_elbo_nonreparameterized(self): XXX to add,https://github.com/pyro-ppl/pyro/commit/d5248f3c26a7c4c01257351a92befe0b0e09a88e,No
3148,pyro-ppl/pyro,tests/infer/test_syntax.py,582829d6571a8869f33021cc6eaf2201fde87e56,TODO Simplify this test when test_iarange_enum_discrete_batch_ok passes:,https://github.com/pyro-ppl/pyro/commit/582829d6571a8869f33021cc6eaf2201fde87e56,Yes
3149,pyro-ppl/pyro,tests/test_examples.py,8db897244589e95e2cc6917d4613267a077cdb6c,TODO: May be worth whitelisting the set of arguments to test,https://github.com/pyro-ppl/pyro/commit/8db897244589e95e2cc6917d4613267a077cdb6c,No
3150,pyro-ppl/pyro,examples/contrib/oed/ab_test.py,157194b92c9abc74738a49cacfb0a3f9b2211aef,"\""\""\"" || Example builds on the Bayesian regression tutorial [1]. It demonstrates how || to estimate the average posterior entropy (APE) under a model and use it to || make an optimal decision about experiment design. ||  || The context is a Gaussian linear model in which the design matrix `X` is a || one-hot-encoded matrix with 2 columns. This corresponds to the simplest form || of an A\/B test. Assume no data has yet be collected. The aim is to find the optimal || allocation of participants to the two groups to maximise the expected gain in || information from actually performing the experiment. ||  || For details of the implementation of average posterior entropy estimation; see || the docs for :func:`pyro.contrib.oed.eig.vi_ape`. ||  || We recommend the technical report from Long Ouyang et al [3] as an introduction || to optimal experiment design within probabilistic programs. ||  || [1] [\""Bayesian Regression\""](http:\/\/pyro.ai\/examples\/bayesian_regression.html) || [2] Long Ouyang; Michael Henry Tessler; Daniel Ly; Noah Goodman (2016); ||     \""Practical optimal experiment design with probabilistic programs\""; ||     (https:\/\/arxiv.org\/abs\/1608.05046) || \""\""\""",https://github.com/pyro-ppl/pyro/commit/157194b92c9abc74738a49cacfb0a3f9b2211aef,Yes
3151,pyro-ppl/pyro,examples/contrib/oed/ab_test.py,10fc47631fc281f0f0a0434e93853ccb90526ac3,"\""\""\"" || Example builds on the Bayesian regression tutorial [1]. It demonstrates how || to estimate the average posterior entropy (APE) under a model and use it to || make an optimal decision about experiment design. ||  || The context is a Gaussian linear model in which the design matrix `X` is a || one-hot-encoded matrix with 2 columns. This corresponds to the simplest form || of an A\/B test. Assume no data has yet be collected. The aim is to find the optimal || allocation of participants to the two groups to maximise the expected gain in || information from actually performing the experiment. ||  || For details of the implementation of average posterior entropy estimation; see || the docs for :func:`pyro.contrib.oed.eig.vi_ape`. ||  || We recommend the technical report from Long Ouyang et al [2] as an introduction || to optimal experiment design within probabilistic programs. ||  || To optimize the APE (which is required to be minimized) we used Gaussian Process || based Bayesian Optimization. See the BO tutorial [3] for details of optimizing noisy || and expensive-to-compute functions in pyro. ||  || [1] [\""Bayesian Regression\""](http:\/\/pyro.ai\/examples\/bayesian_regression.html) || [2] Long Ouyang; Michael Henry Tessler; Daniel Ly; Noah Goodman (2016); ||     \""Practical optimal experiment design with probabilistic programs\""; ||     (https:\/\/arxiv.org\/abs\/1608.05046) || [3] [\""Bayesian Optimization\""](http:\/\/pyro.ai\/examples\/bo.html) || \""\""\""",https://github.com/pyro-ppl/pyro/commit/10fc47631fc281f0f0a0434e93853ccb90526ac3,Yes
3152,pyro-ppl/pyro,pyro/infer/util.py,58eb73641525ee00894d42d57b3bcdf9afaf6dfb,I don't know why the following broadcast is needed; but it makes tests pass:,https://github.com/pyro-ppl/pyro/commit/58eb73641525ee00894d42d57b3bcdf9afaf6dfb,No
3153,pyro-ppl/pyro,pyro/generic/testing.py,8a2b87f41f2e3939f0ba9ca2a00b1e9ffa1f51b5,"\""\""\"" || Models for testing the generic interface. ||  || For specifying the arguments to model functions; the convention followed is || that positional arguments are inputs to the model and keyword arguments denote || observed data. || \""\""\""",https://github.com/pyro-ppl/pyro/commit/8a2b87f41f2e3939f0ba9ca2a00b1e9ffa1f51b5,Yes
3154,aldro61/mmit,mmit/tree.py,e0309d692b815e835015ba5b87d465216a268bc4,XXX: Runtime test case to ensure that the solver is working correctly. The solution for the cases,https://github.com/aldro61/mmit/commit/e0309d692b815e835015ba5b87d465216a268bc4,No
3155,aldro61/mmit,mmit/tree.py,e01052c3f4cba1b7e93f60d4cd6a12a4d51ca528,XXX: Runtime test case to ensure that the solver is working correctly. The solution for the cases,https://github.com/aldro61/mmit/commit/e01052c3f4cba1b7e93f60d4cd6a12a4d51ca528,No
3156,aldro61/mmit,mmit/learning.py,5289fd431a4655b9c56a0b7accfdfa80e764b49e,XXX: Runtime test case to ensure that the solver is working correctly. The solution for the cases,https://github.com/aldro61/mmit/commit/5289fd431a4655b9c56a0b7accfdfa80e764b49e,No
3157,astier/model-free-episodic-control,main.py,881c0a78386e771aeace2df2b8ae4fafc35363ee,TODO test this,https://github.com/astier/model-free-episodic-control/commit/881c0a78386e771aeace2df2b8ae4fafc35363ee,No
3158,astier/model-free-episodic-control,main.py,aa7f755bdd26f1ff09157cb1da00dfc09704fc05,TODO file issue & test,https://github.com/astier/model-free-episodic-control/commit/aa7f755bdd26f1ff09157cb1da00dfc09704fc05,No
3159,astier/model-free-episodic-control,main.py,5d912bf51d637710e0bb4f61aa83b70388d0302a,TODO test gyms setting (2; 5),https://github.com/astier/model-free-episodic-control/commit/5d912bf51d637710e0bb4f61aa83b70388d0302a,No
3160,brendanhasz/probflow,tests/unit/test_layers.py,96820e4dd4805fe28e7160821c01ff8212e1d5d6,TODO: don't need this w\/ py.test,https://github.com/brendanhasz/probflow/commit/96820e4dd4805fe28e7160821c01ff8212e1d5d6,No
3161,brendanhasz/probflow,tests/unit/test_layers.py,164bf7720fbc882b5de662a3fa24e0ab7b1b45dd,TODO: although maybe that should go in the integration tests?,https://github.com/brendanhasz/probflow/commit/164bf7720fbc882b5de662a3fa24e0ab7b1b45dd,No
3162,brendanhasz/probflow,tests/unit/test_parameters.py,e80c95f1c6dca0447d57d87b54c37f124aee0263,TODO: test it works when data is a dataset iterator obj,https://github.com/brendanhasz/probflow/commit/e80c95f1c6dca0447d57d87b54c37f124aee0263,Yes
3163,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,a2188004018a6c6e14784932675bfd521c8081e8,TODO: test 2D X and params,https://github.com/brendanhasz/probflow/commit/a2188004018a6c6e14784932675bfd521c8081e8,Yes
3164,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,a2188004018a6c6e14784932675bfd521c8081e8,TODO: test vector Y,https://github.com/brendanhasz/probflow/commit/a2188004018a6c6e14784932675bfd521c8081e8,Yes
3165,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,a2188004018a6c6e14784932675bfd521c8081e8,TODO: test 2D Y,https://github.com/brendanhasz/probflow/commit/a2188004018a6c6e14784932675bfd521c8081e8,Yes
3166,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,a2188004018a6c6e14784932675bfd521c8081e8,TODO: test the shuffles work _initialize_shuffles,https://github.com/brendanhasz/probflow/commit/a2188004018a6c6e14784932675bfd521c8081e8,Yes
3167,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,a2188004018a6c6e14784932675bfd521c8081e8,TODO: test the batches are being generated correctly _generate_batch,https://github.com/brendanhasz/probflow/commit/a2188004018a6c6e14784932675bfd521c8081e8,No
3168,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,a2188004018a6c6e14784932675bfd521c8081e8,TODO: test plot_posterior; predictive_distribution; predict; metrics; etc,https://github.com/brendanhasz/probflow/commit/a2188004018a6c6e14784932675bfd521c8081e8,No
3169,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,b4adeb8071dece1e297a0a27b6ac5ccffab0e82c,TODO: test that the flipout estimator works?,https://github.com/brendanhasz/probflow/commit/b4adeb8071dece1e297a0a27b6ac5ccffab0e82c,Yes
3170,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,dbf4e419f3b1c66130b5cc52d10a07beb9e238e7,TODO: test the validation splitting \/ shuffling happens correctly,https://github.com/brendanhasz/probflow/commit/dbf4e419f3b1c66130b5cc52d10a07beb9e238e7,No
3171,brendanhasz/probflow,tests/stats/test_LinearRegression.py,d0a33f05bfc583842d67107b3b465b0e06d442fd,TODO: test multivariate LR,https://github.com/brendanhasz/probflow/commit/d0a33f05bfc583842d67107b3b465b0e06d442fd,No
3172,brendanhasz/probflow,tests/stats/test_LinearRegression.py,d0a33f05bfc583842d67107b3b465b0e06d442fd,"TODO: test multivariate w\/ \""flipout\""",https://github.com/brendanhasz/probflow/commit/d0a33f05bfc583842d67107b3b465b0e06d442fd,Yes
3173,brendanhasz/probflow,tests/stats/test_LinearRegression.py,d0a33f05bfc583842d67107b3b465b0e06d442fd,TODO: test w\/ Dense,https://github.com/brendanhasz/probflow/commit/d0a33f05bfc583842d67107b3b465b0e06d442fd,Yes
3174,brendanhasz/probflow,tests/stats/test_LinearRegression.py,d0a33f05bfc583842d67107b3b465b0e06d442fd,TODO: test w\/ LinearRegression,https://github.com/brendanhasz/probflow/commit/d0a33f05bfc583842d67107b3b465b0e06d442fd,No
3175,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,00a01c40cfbaffc4d99cee3f49ced47e54bf1aaf,TODO: test BaseLayer.fit works w\/ pandas array,https://github.com/brendanhasz/probflow/commit/00a01c40cfbaffc4d99cee3f49ced47e54bf1aaf,No
3176,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,6651bc37e129d195fb99f028162b00a8072fff9b,TODO: test 2D X and params,https://github.com/brendanhasz/probflow/commit/6651bc37e129d195fb99f028162b00a8072fff9b,Yes
3177,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,6651bc37e129d195fb99f028162b00a8072fff9b,TODO: test vector Y,https://github.com/brendanhasz/probflow/commit/6651bc37e129d195fb99f028162b00a8072fff9b,Yes
3178,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,6651bc37e129d195fb99f028162b00a8072fff9b,TODO: test 2D Y,https://github.com/brendanhasz/probflow/commit/6651bc37e129d195fb99f028162b00a8072fff9b,Yes
3179,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,6651bc37e129d195fb99f028162b00a8072fff9b,TODO: test the shuffles work _initialize_shuffles,https://github.com/brendanhasz/probflow/commit/6651bc37e129d195fb99f028162b00a8072fff9b,Yes
3180,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,6651bc37e129d195fb99f028162b00a8072fff9b,TODO: test that the flipout estimator works?,https://github.com/brendanhasz/probflow/commit/6651bc37e129d195fb99f028162b00a8072fff9b,Yes
3181,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,6651bc37e129d195fb99f028162b00a8072fff9b,TODO: test the batches are being generated correctly _generate_batch,https://github.com/brendanhasz/probflow/commit/6651bc37e129d195fb99f028162b00a8072fff9b,No
3182,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,6651bc37e129d195fb99f028162b00a8072fff9b,TODO: test when prob=True),https://github.com/brendanhasz/probflow/commit/6651bc37e129d195fb99f028162b00a8072fff9b,Yes
3183,brendanhasz/probflow,tests/unit/test_core_BaseDistribution.py,0968af59ff1ad21d99d4b496f9c6e971dd8cde8b,TODO: test when prob=True),https://github.com/brendanhasz/probflow/commit/0968af59ff1ad21d99d4b496f9c6e971dd8cde8b,Yes
3184,brendanhasz/probflow,src/probflow/layers.py,3ae3fc9195dd7c9297c8cc61ad5471598093b334,TODO: uh; test that this is correct...,https://github.com/brendanhasz/probflow/commit/3ae3fc9195dd7c9297c8cc61ad5471598093b334,Yes
3185,brendanhasz/probflow,tests/unit/test_layers.py,177582007b2d8d3fa67e94377d5b75c1e684fa1a,TODO move test to tests\/integration,https://github.com/brendanhasz/probflow/commit/177582007b2d8d3fa67e94377d5b75c1e684fa1a,No
3186,brendanhasz/probflow,tests/unit/test_models_DenseClassifier.py,177582007b2d8d3fa67e94377d5b75c1e684fa1a,TODO: test batch_norm arg works correctly,https://github.com/brendanhasz/probflow/commit/177582007b2d8d3fa67e94377d5b75c1e684fa1a,No
3187,brendanhasz/probflow,tests/unit/test_models_DenseRegression.py,177582007b2d8d3fa67e94377d5b75c1e684fa1a,TODO: test batch_norm arg works correctly,https://github.com/brendanhasz/probflow/commit/177582007b2d8d3fa67e94377d5b75c1e684fa1a,No
3188,brendanhasz/probflow,tests/unit/test_layers_Dense.py,226a0f5a799870b4964d28336002f8240ff68c65,TODO move test to tests\/integration,https://github.com/brendanhasz/probflow/commit/226a0f5a799870b4964d28336002f8240ff68c65,No
3189,brendanhasz/probflow,tests/unit/test_layers_Sequential.py,faa51adc31d0395ea541b978b1e23d16018a7191,TODO move test to tests\/integration,https://github.com/brendanhasz/probflow/commit/faa51adc31d0395ea541b978b1e23d16018a7191,No
3190,brendanhasz/probflow,tests/unit/tensorflow/test_distributions_tensorflow.py,c76d60926dcda91991d4418419c396a5d37bff3c,TODO: test HMM,https://github.com/brendanhasz/probflow/commit/c76d60926dcda91991d4418419c396a5d37bff3c,No
3191,brendanhasz/probflow,tests/unit/tensorflow/test_distributions_tensorflow.py,c76d60926dcda91991d4418419c396a5d37bff3c,TODO: test GP,https://github.com/brendanhasz/probflow/commit/c76d60926dcda91991d4418419c396a5d37bff3c,Yes
3192,brendanhasz/probflow,tests/unit/pytorch/test_distributions_pytorch.py,85f54b97889815b089c4c1b9f82537348ad2caed,TODO: test HMM,https://github.com/brendanhasz/probflow/commit/85f54b97889815b089c4c1b9f82537348ad2caed,No
3193,brendanhasz/probflow,tests/unit/pytorch/test_distributions_pytorch.py,85f54b97889815b089c4c1b9f82537348ad2caed,TODO: test GP,https://github.com/brendanhasz/probflow/commit/85f54b97889815b089c4c1b9f82537348ad2caed,Yes
3194,Cartus/AMR-Parser,data/align/scripts/feat2tree_v9.py,13085d32760add0a9e3fdd2c3db42f9bfa88914b,TODO: test if the global_rule_node_id works for restruct rules,https://github.com/Cartus/AMR-Parser/commit/13085d32760add0a9e3fdd2c3db42f9bfa88914b,Yes
3195,cyschneck/Hydra,NN_gender_class.py,141390e8a63aedf9bdfe4b05a791cb81e72cb51d,TODO: update with better model for testing (currently ~85% on testing; ~99% on training),https://github.com/cyschneck/Hydra/commit/141390e8a63aedf9bdfe4b05a791cb81e72cb51d,No
3196,cyschneck/Hydra,raw_text_processing.py,7fb6dff72bf33bb692e445eb2552f32cb2e73e93,TODO: test accuracy on sample page,https://github.com/cyschneck/Hydra/commit/7fb6dff72bf33bb692e445eb2552f32cb2e73e93,Yes
3197,emlynjdavies/PySilCam,pysilcam/silcamgui/interactive_summary.py,f66685dde4f18d1f1cf9a137e0d48397c12933c3,@todo nrows is for testing only!,https://github.com/emlynjdavies/PySilCam/commit/f66685dde4f18d1f1cf9a137e0d48397c12933c3,No
3198,emlynjdavies/PySilCam,pysilcam/tests/synthesizer.py,275b1e91046aa0353aaabeec5fd25da6c5ed2be6,@todo this should be handles properly as part of testing,https://github.com/emlynjdavies/PySilCam/commit/275b1e91046aa0353aaabeec5fd25da6c5ed2be6,Yes
3199,emlynjdavies/PySilCam,pysilcam/tests/synthesizer.py,253ce8e809c27d37f7fd5a3158633c4a1c888dee,@todo this should be handles properly as part of testing,https://github.com/emlynjdavies/PySilCam/commit/253ce8e809c27d37f7fd5a3158633c4a1c888dee,Yes
3200,hoya012/shake-shake-tensorflow,test.py,45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,FIXME: Test hyperparameters,https://github.com/hoya012/shake-shake-tensorflow/commit/45ae9a2e204a0d9b02bf5c2dfb75c6c59ea714b8,No
3201,akihiro-inui/MusicGenreClassifiaction,traintest.py,a96073644edc0ec8d642942e3b8e15891f2e7795,Mode1(Data_Type=0); move images into Train\/Test folder,https://github.com/akihiro-inui/MusicGenreClassifiaction/commit/a96073644edc0ec8d642942e3b8e15891f2e7795,Yes
3202,Shenggan/DeepCell-Keras,deepcell/tifffile/tifffile.py,e43b302cf0952a147a44dfd5af9cbbc053c94071,TODO: test this. Need example file.,https://github.com/Shenggan/DeepCell-Keras/commit/e43b302cf0952a147a44dfd5af9cbbc053c94071,No
3203,Shenggan/DeepCell-Keras,deepcell/tifffile/tifffile.py,e43b302cf0952a147a44dfd5af9cbbc053c94071,TODO: test this,https://github.com/Shenggan/DeepCell-Keras/commit/e43b302cf0952a147a44dfd5af9cbbc053c94071,No
3204,HazyResearch/fonduer,lf_helpers.py,4416afd52a8fe4366dc05514ba9fdec35c66f4c2,TODO: fix this function and retest,https://github.com/HazyResearch/fonduer/commit/4416afd52a8fe4366dc05514ba9fdec35c66f4c2,Yes
3205,HazyResearch/fonduer,fonduer/snorkel/learning/utils.py,611d23a9e43d2539475a328138323b00e5fdc86b,Test for categorical vs. binary in hack-ey way for now...,https://github.com/HazyResearch/fonduer/commit/611d23a9e43d2539475a328138323b00e5fdc86b,Yes
3206,HazyResearch/fonduer,fonduer/parser/spacy_parser.py,33e55ce2ea328489062384ca09697d8dd2a2265d,TODO: We could do this in parallel. Test speedup in the future,https://github.com/HazyResearch/fonduer/commit/33e55ce2ea328489062384ca09697d8dd2a2265d,Yes
3207,HazyResearch/fonduer,tests/parser/test_parser.py,6ef8ac3389535b678ffaeb570661d4e33c4485fa,TODO: Add unit tests for tonkenized foreign alpha language sentences,https://github.com/HazyResearch/fonduer/commit/6ef8ac3389535b678ffaeb570661d4e33c4485fa,Yes
3208,HazyResearch/fonduer,tests/parser/test_parser.py,871f28721eca238a5e4b2fad213d8cc5912f18a8,TODO: Add unit tests for tonkenized foreign alpha language sentences,https://github.com/HazyResearch/fonduer/commit/871f28721eca238a5e4b2fad213d8cc5912f18a8,Yes
3209,HazyResearch/fonduer,tests/parser/test_parser.py,871f28721eca238a5e4b2fad213d8cc5912f18a8,# TODO: Add unit tests for tonkenized foreign alpha language sentences,https://github.com/HazyResearch/fonduer/commit/871f28721eca238a5e4b2fad213d8cc5912f18a8,No
3210,HazyResearch/fonduer,tests/parser/test_parser.py,871f28721eca238a5e4b2fad213d8cc5912f18a8,"docs_path = \""tests\/data\/parser_htmls\/brot.html\""  # TODO: replace with japanese doc",https://github.com/HazyResearch/fonduer/commit/871f28721eca238a5e4b2fad213d8cc5912f18a8,No
3211,HazyResearch/fonduer,tests/utils/test_visualizer.py,e95c7c35db8a9da0003f494d09cb03b1769de4c0,TODO test on MacOSX too,https://github.com/HazyResearch/fonduer/commit/e95c7c35db8a9da0003f494d09cb03b1769de4c0,Yes
3212,HazyResearch/fonduer,tests/candidates/test_matchers.py,1a5ebebd18ec25dff4540aec8426cc091bc53576,TODO: Add a test for ignore_sep=False,https://github.com/HazyResearch/fonduer/commit/1a5ebebd18ec25dff4540aec8426cc091bc53576,No
3213,HazyResearch/fonduer,tests/candidates/test_matchers.py,1a5ebebd18ec25dff4540aec8426cc091bc53576,TODO: test with plural words,https://github.com/HazyResearch/fonduer/commit/1a5ebebd18ec25dff4540aec8426cc091bc53576,Yes
3214,HealthCatalyst/healthcareai-py,healthcareai/tests/test_dataframe_filters.py,e2bd2055547bd78eb9325efbdc3c0959215d37b5,TODO fill out with more tests,https://github.com/HealthCatalyst/healthcareai-py/commit/e2bd2055547bd78eb9325efbdc3c0959215d37b5,Yes
3215,HealthCatalyst/healthcareai-py,healthcareai/tests/test_dataframe_filters.py,dd52ab89e72122a295ecbe676dd314f01ec7742d,TODO test exclusions!,https://github.com/HealthCatalyst/healthcareai-py/commit/dd52ab89e72122a295ecbe676dd314f01ec7742d,Yes
3216,HealthCatalyst/healthcareai-py,example_simple_regression.py,079b38ed277b1b5471a51a98006204364a60e897,TODO what about the test window flag - can we deprecate it?,https://github.com/HealthCatalyst/healthcareai-py/commit/079b38ed277b1b5471a51a98006204364a60e897,Yes
3217,HealthCatalyst/healthcareai-py,example_simple_regression.py,4a25a4c0d506a4c0ea64cd35abdfe88c76f2a7aa,TODO swap this out for testing,https://github.com/HealthCatalyst/healthcareai-py/commit/4a25a4c0d506a4c0ea64cd35abdfe88c76f2a7aa,Yes
3218,HealthCatalyst/healthcareai-py,example_simple_classification.py,fa122381d514c7f2c050c7331bc811bcb0d5a4b4,TODO swap this out for testing,https://github.com/HealthCatalyst/healthcareai-py/commit/fa122381d514c7f2c050c7331bc811bcb0d5a4b4,Yes
3219,HealthCatalyst/healthcareai-py,healthcareai/tests/test_simple_develop_supervised.py,ef6a39766b547a21e3452d30a236248a0d832b6a,TODO is this even a valid test at a 0.5 auc?,https://github.com/HealthCatalyst/healthcareai-py/commit/ef6a39766b547a21e3452d30a236248a0d832b6a,Yes
3220,HealthCatalyst/healthcareai-py,healthcareai/common/model_eval.py,47d671cccaa72079ee28690324326b3b7a0d4a8a,TODO test this,https://github.com/HealthCatalyst/healthcareai-py/commit/47d671cccaa72079ee28690324326b3b7a0d4a8a,No
3221,HealthCatalyst/healthcareai-py,healthcareai/common/model_eval.py,47d671cccaa72079ee28690324326b3b7a0d4a8a,TODO low priority; but test this,https://github.com/HealthCatalyst/healthcareai-py/commit/47d671cccaa72079ee28690324326b3b7a0d4a8a,No
3222,HealthCatalyst/healthcareai-py,healthcareai/common/model_eval.py,eaf1b8fcfe0d3f85fe8849bc01c11123cfe660ec,TODO this portion could probably be extracted and tested; since the plot is difficult to test,https://github.com/HealthCatalyst/healthcareai-py/commit/eaf1b8fcfe0d3f85fe8849bc01c11123cfe660ec,Yes
3223,HealthCatalyst/healthcareai-py,healthcareai/common/transformers.py,6cf4c8650416cac84ecb6091466cd9cced3ccf4c,TODO how do we validate this happens before train\/test split? Or do we need to? Can we implement it in the,https://github.com/HealthCatalyst/healthcareai-py/commit/6cf4c8650416cac84ecb6091466cd9cced3ccf4c,No
3224,HealthCatalyst/healthcareai-py,healthcareai/common/transformers.py,d28f470b0c7267e4a9cdcb77ae55c384d17a24de,TODO how do we validate this happens before train\/test split? Or do we need to? Can we implement it in the,https://github.com/HealthCatalyst/healthcareai-py/commit/d28f470b0c7267e4a9cdcb77ae55c384d17a24de,No
3225,HealthCatalyst/healthcareai-py,healthcareai/common/write_predictions_to_database.py,d1db144349d5200d3d03fad208b24c74e5d1b1c2,TODO need to find a good list of errors to catch here and how to test them,https://github.com/HealthCatalyst/healthcareai-py/commit/d1db144349d5200d3d03fad208b24c74e5d1b1c2,Yes
3226,HealthCatalyst/healthcareai-py,healthcareai/common/model_eval.py,ef3fc40e55c5462e7cb4ba273617219572f8b17c,TODO test this,https://github.com/HealthCatalyst/healthcareai-py/commit/ef3fc40e55c5462e7cb4ba273617219572f8b17c,No
3227,HealthCatalyst/healthcareai-py,healthcareai/common/model_eval.py,7d874e3ef077a71b4b93636818f8e068d6adb251,TODO test this,https://github.com/HealthCatalyst/healthcareai-py/commit/7d874e3ef077a71b4b93636818f8e068d6adb251,No
3228,HealthCatalyst/healthcareai-py,healthcareai/trained_models/trained_supervised_model.py,929d40088a31bd60f9f6e0e500f95f73983bbc1f,TODO test this,https://github.com/HealthCatalyst/healthcareai-py/commit/929d40088a31bd60f9f6e0e500f95f73983bbc1f,No
3229,HealthCatalyst/healthcareai-py,healthcareai/tests/test_trainer.py,444943b18473a3d3ddb778823a8121b236f461b2,TODO is this even a valid test at a 0.5 auc?,https://github.com/HealthCatalyst/healthcareai-py/commit/444943b18473a3d3ddb778823a8121b236f461b2,Yes
3230,HealthCatalyst/healthcareai-py,healthcareai/trained_models/trained_supervised_model.py,1e844efe5cf09f07a433655f1d8c4b8e5843706b,TODO low priority; but test this,https://github.com/HealthCatalyst/healthcareai-py/commit/1e844efe5cf09f07a433655f1d8c4b8e5843706b,No
3231,HealthCatalyst/healthcareai-py,healthcareai/advanced_trainer.py,5f469a34c7679e26396bb402bfb69a600fd692d5,TODO test this,https://github.com/HealthCatalyst/healthcareai-py/commit/5f469a34c7679e26396bb402bfb69a600fd692d5,No
3232,HealthCatalyst/healthcareai-py,healthcareai/tests/test_trainer.py,c91c5cb193042195549f71dee1c58a79c3926c05,TODO why is this test skipped?,https://github.com/HealthCatalyst/healthcareai-py/commit/c91c5cb193042195549f71dee1c58a79c3926c05,Yes
3233,HealthCatalyst/healthcareai-py,healthcareai/tests/test_trainer.py,a2fa1f1c7dfc3ce628b9e60c11fbba56fe4f5ead,TODO see if there is a way to make this test work - it fails on travisCI because of this:,https://github.com/HealthCatalyst/healthcareai-py/commit/a2fa1f1c7dfc3ce628b9e60c11fbba56fe4f5ead,Yes
3234,HealthCatalyst/healthcareai-py,healthcareai/advanced_supvervised_model_trainer.py,ff47d0e8e37c71c4da79e08d8d1711981fa1be68,TODO test,https://github.com/HealthCatalyst/healthcareai-py/commit/ff47d0e8e37c71c4da79e08d8d1711981fa1be68,Yes
3235,HealthCatalyst/healthcareai-py,healthcareai/common/top_factors.py,ff47d0e8e37c71c4da79e08d8d1711981fa1be68,TODO Low priority; consider testing,https://github.com/HealthCatalyst/healthcareai-py/commit/ff47d0e8e37c71c4da79e08d8d1711981fa1be68,Yes
3236,HealthCatalyst/healthcareai-py,healthcareai/tests/test_advanced_trainer.py,ff47d0e8e37c71c4da79e08d8d1711981fa1be68,TODO this is pretty spartan testing only looking for happy path on binary classification,https://github.com/HealthCatalyst/healthcareai-py/commit/ff47d0e8e37c71c4da79e08d8d1711981fa1be68,Yes
3237,HealthCatalyst/healthcareai-py,healthcareai/tests/helpers.py,9fde758bbe4c923d0e40052fdaf736b9b1101803,TODO deprecate after a better test for impact coding is devised.,https://github.com/HealthCatalyst/healthcareai-py/commit/9fde758bbe4c923d0e40052fdaf736b9b1101803,Yes
3238,HealthCatalyst/healthcareai-py,healthcareai/tests/test_impact_coding.py,9fde758bbe4c923d0e40052fdaf736b9b1101803,TODO deprecate `fixture` after a better test for impact coding is devised.,https://github.com/HealthCatalyst/healthcareai-py/commit/9fde758bbe4c923d0e40052fdaf736b9b1101803,Yes
3239,HealthCatalyst/healthcareai-py,healthcareai/common/database_validators.py,fa696b1f1dc40029de71caf90c3c0b408a146127,TODO figure out some way to test this.,https://github.com/HealthCatalyst/healthcareai-py/commit/fa696b1f1dc40029de71caf90c3c0b408a146127,No
3240,HealthCatalyst/healthcareai-py,healthcareai/tests/test_database_validation.py,fa696b1f1dc40029de71caf90c3c0b408a146127,TODO clarify EXACTLY what this is testing,https://github.com/HealthCatalyst/healthcareai-py/commit/fa696b1f1dc40029de71caf90c3c0b408a146127,Yes
3241,HealthCatalyst/healthcareai-py,healthcareai/common/top_factors.py,46fed75382f305fc158f48b5a21786d445d36040,TODO Low priority; consider testing,https://github.com/HealthCatalyst/healthcareai-py/commit/46fed75382f305fc158f48b5a21786d445d36040,Yes
3242,hellohaptik/chatbot_ner,model/crf/read_model.py,21b9b0f78e5e73145387f39520f406a68e686301,"\""\""\"" || step1--> selects messages of type in RES and also columns in KEEP_COLS and returns a dataframe ||  || step2--> removing json strings and extracting text from messages containing json while keeping other messages as they are and returns a new dataframe ||  || step3--> changing dataframe returned by step2 to conversations. like ->>coll_id; outbound_msg; inbound_msg ||         This step returns a dataframe of conversations ||  || step4--> runs NER on inbound messages with bot_message as outbound_msg and retrieves arrival_city; departure_city; original text returned by city and city advance and cities detected by city along with inbound_msg; outbound_msg and coll_id ||  || step5--> used to remove unwanted text from messages like product_id; image_id etc ||  || step6--> used to convert entire dataframe to lowercase ||  || step7--> divides dataframe from step4 to three parts--> csv containing messages which has only arrival_city; csv containing ||         messages which has only departure city and csv which contains messages which has both arrival as well as departure city ||  || step8--> generates crf files from each of the csv created by step7 (calls crf_generate functions to convert each type of crf(to; from; from-to) into crf files) ||  || step9--> divides data to 75-25 for test train from three crf files in step8 and joins them to create one test and one train file(calls split_test_train()) ||  || write_to_file--> takes a file name; which mode to write in nad text to write as input and writes to that file ||  || makeDir--> take a string as input to check if that directory exists or not and if not creates that directory ||  || remove_noise--> removes noise from messages. where noise is elements of list NOISE_LIST ||  || remove_non_ascii--> reomves non ascii character from text(emojis) ||  || capitalize_first--> changes the case of fist letter of a string ||  || capitalize_first_only--> changes the case of first letters of token in inbound and outbound messages and returns two lists ||  || split_test_train--> function which takes a file name which we wants to divide into test and train; train file name and test file name. ||  || --> The input to the main function variable is path to the file which contains data in csv format. (path with respect to current directory) ||  || --> At the end; conversations will be saved in relevant_data directory. And a directory of crd_data will be created which will contain your  ||     test; train and crf-from; to; from-to files. || \""\""\""",https://github.com/hellohaptik/chatbot_ner/commit/21b9b0f78e5e73145387f39520f406a68e686301,Yes
3243,hellohaptik/chatbot_ner,datastore/datastore.py,1ccbae3f9e59b47d763f14f7aaa13a881b907574,cleanup deprecated and buggy code and write tests for CRUD operations. Maybe even remove multi engine support,https://github.com/hellohaptik/chatbot_ner/commit/1ccbae3f9e59b47d763f14f7aaa13a881b907574,Yes
3244,hellohaptik/chatbot_ner,datastore/datastore.py,79bf165ea8acaccf5834d28b0199fe264e78f560,cleanup deprecated and buggy code and write tests for CRUD operations. Maybe even remove multi engine support,https://github.com/hellohaptik/chatbot_ner/commit/79bf165ea8acaccf5834d28b0199fe264e78f560,Yes
3245,kymatio/kymatio,kymatio/scattering3d/tests/test_tensorflow_scattering3d.py,a75200b19728f4777ddcfefceefbc9e0c0058ec0,TODO: order_0 test,https://github.com/kymatio/kymatio/commit/a75200b19728f4777ddcfefceefbc9e0c0058ec0,Yes
3246,geomstats/geomstats,tests/test_hypersphere_tensorflow.py,3ffc450398f64c676759707bd010843c2c8b31e0,TODO(nina): Fix that this test fails; in numpy,https://github.com/geomstats/geomstats/commit/3ffc450398f64c676759707bd010843c2c8b31e0,Yes
3247,geomstats/geomstats,tests/test_invariant_metric_tensorflow.py,952813a0ab85962ec1ba07b95dccac1c596a9717,TODO(nina): Fix this test.,https://github.com/geomstats/geomstats/commit/952813a0ab85962ec1ba07b95dccac1c596a9717,Yes
3248,geomstats/geomstats,tests/test_invariant_metric_tensorflow.py,a87e4416265ccb7e1f57cfa263e2ca5fcea29cdd,# TODO(nina): Fix this test.,https://github.com/geomstats/geomstats/commit/a87e4416265ccb7e1f57cfa263e2ca5fcea29cdd,Yes
3249,geomstats/geomstats,tests/test_special_orthogonal_group.py,7bdbee4b75b7329e0e21876dcdf715003d24e4be,TODO(nina): Fix this test,https://github.com/geomstats/geomstats/commit/7bdbee4b75b7329e0e21876dcdf715003d24e4be,No
3250,geomstats/geomstats,tests/test_special_euclidean_group.py,ce31656f363382ee70ef17723c44bbaa51caba5c,TODO(nina): Fix this test.,https://github.com/geomstats/geomstats/commit/ce31656f363382ee70ef17723c44bbaa51caba5c,Yes
3251,geomstats/geomstats,tests/test_hypersphere.py,6bd02a666455b9a9e37d81088de6dee2425457ec,TODO(nina): Fix that this test fails; in numpy,https://github.com/geomstats/geomstats/commit/6bd02a666455b9a9e37d81088de6dee2425457ec,Yes
3252,geomstats/geomstats,tests/test_hypersphere.py,5bf3499a936d67c1a5288382a9830c94bf4175b4,TODO(nina): Fix that this test fails; in numpy,https://github.com/geomstats/geomstats/commit/5bf3499a936d67c1a5288382a9830c94bf4175b4,Yes
3253,geomstats/geomstats,tests/test_estimators.py,f722d46a4950a1eb829608e0da6e91b49776401d,XXX: Should these tests run on all backends?,https://github.com/geomstats/geomstats/commit/f722d46a4950a1eb829608e0da6e91b49776401d,No
3254,geomstats/geomstats,tests/test_pca.py,f722d46a4950a1eb829608e0da6e91b49776401d,XXX: Should these tests run on all backends?,https://github.com/geomstats/geomstats/commit/f722d46a4950a1eb829608e0da6e91b49776401d,No
3255,geomstats/geomstats,tests/test_pca.py,636c1f67071c8d6ddc701e7bed7f54bfa17212a0,XXX: Should these tests run on all backends?,https://github.com/geomstats/geomstats/commit/636c1f67071c8d6ddc701e7bed7f54bfa17212a0,No
3256,geomstats/geomstats,tests/test_estimators.py,f96edb06514a726975be7713bf2851e50183d60f,XXX: Should these tests run on all backends?,https://github.com/geomstats/geomstats/commit/f96edb06514a726975be7713bf2851e50183d60f,No
3257,geomstats/geomstats,tests/test_pca.py,f96edb06514a726975be7713bf2851e50183d60f,XXX: Should these tests run on all backends?,https://github.com/geomstats/geomstats/commit/f96edb06514a726975be7713bf2851e50183d60f,No
3258,geomstats/geomstats,tests/test_pca.py,7a6c9f24c577ca7a0a1a788769341b4720e5679e,XXX: Should these tests run on all backends?,https://github.com/geomstats/geomstats/commit/7a6c9f24c577ca7a0a1a788769341b4720e5679e,No
3259,geomstats/geomstats,tests/test_special_euclidean_group.py,8ec080c4f9915a87aff6f9d8a08e189c84350e0b,TODO(nina): Fix this test.,https://github.com/geomstats/geomstats/commit/8ec080c4f9915a87aff6f9d8a08e189c84350e0b,Yes
3260,geomstats/geomstats,tests/test_hyperbolic_coords.py,0f5e017896b604e910ae6ecf0322774a23f4d632,TODO(Hazaatiti): Fix this test,https://github.com/geomstats/geomstats/commit/0f5e017896b604e910ae6ecf0322774a23f4d632,Yes
3261,geomstats/geomstats,tests/test_invariant_metric.py,5e06b935344befe944f28a1b4b464ca295f6ba34,TODO(ninamiolane): Fix this test when invariant metric,https://github.com/geomstats/geomstats/commit/5e06b935344befe944f28a1b4b464ca295f6ba34,Yes
3262,geomstats/geomstats,tests/test_backends.py,512b986a5ca76bce62a49a8c058e53fc325ccdb9,TODO(pchauchat): Fix this test,https://github.com/geomstats/geomstats/commit/512b986a5ca76bce62a49a8c058e53fc325ccdb9,No
3263,allenai/scibert,scripts/KYLE_analyze_beaker_experiments.py,5453bfe740589d1e51a28701193f1d524cb40df7,lookup (dataset; model) test performance under best hyperparam,https://github.com/allenai/scibert/commit/5453bfe740589d1e51a28701193f1d524cb40df7,Yes
3264,Drakkar-Software/OctoBot,bot.py,2b2b2a3dd527e9f642049f30d25dd43ebb97cbd5,TODO : remove ? only for test purpose,https://github.com/Drakkar-Software/OctoBot/commit/2b2b2a3dd527e9f642049f30d25dd43ebb97cbd5,Yes
3265,Drakkar-Software/OctoBot,bot.py,65eb4680433dc2bfeab8b9244d7d6b053ae16c71,TODO : remove ? only for test purpose,https://github.com/Drakkar-Software/OctoBot/commit/65eb4680433dc2bfeab8b9244d7d6b053ae16c71,Yes
3266,Drakkar-Software/OctoBot,setup.py,504fdd09ea66388832c63c425533ae946c319f7e,TODO  tests_require=[];,https://github.com/Drakkar-Software/OctoBot/commit/504fdd09ea66388832c63c425533ae946c319f7e,No
3267,Drakkar-Software/OctoBot,tests/endurance_tests/test_backtesting.py,61b23beec5f8684adfae01d410d7098fa75ed78b,fix backtesting exit,https://github.com/Drakkar-Software/OctoBot/commit/61b23beec5f8684adfae01d410d7098fa75ed78b,No
3268,Drakkar-Software/OctoBot,tests/test_utils/backtesting_util.py,04916b544f75f47be6832bfdc317cd1241a17ed7,fix backtesting exit,https://github.com/Drakkar-Software/OctoBot/commit/04916b544f75f47be6832bfdc317cd1241a17ed7,No
3269,Drakkar-Software/OctoBot,core/task_manager.py,1b72496d86dd22f0c42bde6c49c51d95007a5ff8,if backtesting_enabled(self.octobot.get_config()) and self.async_loop.is_closed(): TODO,https://github.com/Drakkar-Software/OctoBot/commit/1b72496d86dd22f0c42bde6c49c51d95007a5ff8,No
3270,X-DataInitiative/tick,setup.py,86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,TODO: change name of extension to array_test,https://github.com/X-DataInitiative/tick/commit/86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,Yes
3271,X-DataInitiative/tick,tick/base/array_test/tests/array_performance_test.py,86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,"\""\""\"" || ref_size = 10000 || ref_n_loops = 10000 || start = time.process_time() || ref_result = test_sum_double_pointer(ref_size; ref_n_loops) || end = time.process_time() || ref_needed_time = end - start || \""\""\""",https://github.com/X-DataInitiative/tick/commit/86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,Yes
3272,X-DataInitiative/tick,tick/simulation/poisreg.py,86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,TODO: unittest for SimuPoisReg,https://github.com/X-DataInitiative/tick/commit/86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,Yes
3273,X-DataInitiative/tick,doc/sphinxext/gen_rst.py,f62367e0a1423f7bda264ece43fd72d663ff4cdd,HACK: Stop nosetests running setup() above,https://github.com/X-DataInitiative/tick/commit/f62367e0a1423f7bda264ece43fd72d663ff4cdd,Yes
3274,X-DataInitiative/tick,tick/array_test/tests/array_performance_test.py,3a586bf2e5b1942b959e1751c305656d3043386e,"\""\""\"" || ref_size = 10000 || ref_n_loops = 10000 || start = time.process_time() || ref_result = test_sum_double_pointer(ref_size; ref_n_loops) || end = time.process_time() || ref_needed_time = end - start || \""\""\""",https://github.com/X-DataInitiative/tick/commit/3a586bf2e5b1942b959e1751c305656d3043386e,Yes
3275,X-DataInitiative/tick,setup.py,3011aaaa9dc6ae4b4e3f1e9268a04ee9e075757d,TODO: change name of extension to array_test,https://github.com/X-DataInitiative/tick/commit/3011aaaa9dc6ae4b4e3f1e9268a04ee9e075757d,Yes
3276,X-DataInitiative/tick,setup.py,aad38ed09d20ba8c0db98d36338e5ddd9a087b89,TODO: change name of extension to array_test,https://github.com/X-DataInitiative/tick/commit/aad38ed09d20ba8c0db98d36338e5ddd9a087b89,Yes
3277,X-DataInitiative/tick,tick/linear_model/simu_poisreg.py,aad38ed09d20ba8c0db98d36338e5ddd9a087b89,TODO: unittest for SimuPoisReg,https://github.com/X-DataInitiative/tick/commit/aad38ed09d20ba8c0db98d36338e5ddd9a087b89,Yes
3278,X-DataInitiative/tick,setup.py,db1751cbe169482081a6437dcfbdc2b8b96051fe,TODO: change name of extension to array_test,https://github.com/X-DataInitiative/tick/commit/db1751cbe169482081a6437dcfbdc2b8b96051fe,Yes
3279,X-DataInitiative/tick,setup.py,d65e2fa3bc39d346d1aea253e43a9b86d4c2ea5d,TODO: change name of extension to array_test,https://github.com/X-DataInitiative/tick/commit/d65e2fa3bc39d346d1aea253e43a9b86d4c2ea5d,Yes
3280,X-DataInitiative/tick,tick/survival/convolutional_sccs.py,44925fde378750b918604bb92a599cc9cf1db317,TODO: test this method,https://github.com/X-DataInitiative/tick/commit/44925fde378750b918604bb92a599cc9cf1db317,No
3281,X-DataInitiative/tick,setup.py,8ac553534acbc0f8b307b4cf2058f715033bb00d,TODO: change name of extension to array_test,https://github.com/X-DataInitiative/tick/commit/8ac553534acbc0f8b307b4cf2058f715033bb00d,Yes
3282,X-DataInitiative/tick,setup.py,c762fa43cdb35fcca781a0ec95938c977d9a1936,TODO: change name of extension to array_test,https://github.com/X-DataInitiative/tick/commit/c762fa43cdb35fcca781a0ec95938c977d9a1936,Yes
3283,X-DataInitiative/tick,setup.py,da666236f97dcd2f5e4bab1fde70fc1ba7ea15c5,TODO: change name of extension to array_test,https://github.com/X-DataInitiative/tick/commit/da666236f97dcd2f5e4bab1fde70fc1ba7ea15c5,Yes
3284,X-DataInitiative/tick,setup.py,a7c9d086add382c9a32655782f9e2cd0fe59db64,TODO: change name of extension to array_test,https://github.com/X-DataInitiative/tick/commit/a7c9d086add382c9a32655782f9e2cd0fe59db64,Yes
3285,anki/vector-python-sdk,anki_vector/robot.py,66aa0966ea388fe1739c468856ec5d13a4775827,TODO When audio is ready; convert `.. code-block:: python` to `.. testcode::`,https://github.com/anki/vector-python-sdk/commit/66aa0966ea388fe1739c468856ec5d13a4775827,Yes
3286,anki/vector-python-sdk,examples/apps/proximity_mapper/proximity_mapper.py,66aa0966ea388fe1739c468856ec5d13a4775827,@TODO enable when testing shows it is ready to go,https://github.com/anki/vector-python-sdk/commit/66aa0966ea388fe1739c468856ec5d13a4775827,No
3287,anki/vector-python-sdk,anki_vector/faces.py,ac427dddda54fd7b24a43d4327afbb971312991e,TODO Review this file and add pytests as is reasonable; like name_face (requires a face object); request_enrolled_names; maybe update_enrolled_face_by_id; etc.,https://github.com/anki/vector-python-sdk/commit/ac427dddda54fd7b24a43d4327afbb971312991e,Yes
3288,anki/vector-python-sdk,examples/experimental/sign_language_recognition_system/data_gen.py,065c48785b2a89cda3b077ee07c48304e7574f28,"\""\""\""Data generation script to build a training and test dataset. ||  || A sample dataset is included in the project (\""dataset.zip\""). Unzip the folder and use the || --dataset_root_folder option to specify the file path and expand this dataset. ||  || Use this script to build\/expand the data needed to train the sign language recognition system. || \""\""\""",https://github.com/anki/vector-python-sdk/commit/065c48785b2a89cda3b077ee07c48304e7574f28,Yes
3289,dipy/dipy,dipy/reconst/tests/test_dti.py,91cae9daf06003078623e8d174c47040c0cfc6dc,Test fitting with different methods: #XXX Add NNLS methods!,https://github.com/dipy/dipy/commit/91cae9daf06003078623e8d174c47040c0cfc6dc,Yes
3290,dipy/dipy,doc/examples/kfold_xval.py,be5897cec1da00f7d50cfa362e1f69b821682b98,"\""\""\"" ||  || ============================================ || K-fold cross-validation for model comparison || ============================================ ||  || Different models of diffusion MRI can be compared based on their accuracy in || fitting the diffusion signal. Here; we demonstrate this by comparing two || models: the diffusion tensor model (DTI) and constrained spherical || deconvolution (CSD). These models differ from each other substantially. DTI || approximates the diffusion pattern as a 3D Gaussian distribution; and has only || 6 free parameters. CSD; on the other hand; fits many more parameters. The || models aare also not nested; so they cannot be compared using the || log-likelihood ratio. ||  || A general way to perform model comparison is cross-validation [1]_. In this || method; a model is fit to some of the data (a *learning set*) and the model is || then used to predict a held-out set (a *testing set*). The model predictions || can then be compared to estimate prediction error on the held out set. This || method has been used for comparison of models such as DTI and CSD [2]_; and has || the advantage that it is imprevious to the effects of over-fitting to noise in || the data; because the model predictions are always compared to data that was || not used in fitting the model. Cross-validation can be used to compare models || with different numbers of parameters; that are not nested. ||  || In `dipy`; we include an implementation of k-fold cross-validation. In this || method; the data is divided into $k$ different segments. In each iteration || $\\frac{1}{k}th$ of the data is held out and the model is fit to the other || $\\frac{k-1}{k}$ parts of the data. A prediction of the held out data is done || and recorded. At the end of $k$ iterations a prediction of all of the data will || have been conducted; and this can be compared directly to all of the data. ||  || First; we import that modules needed for this example. In particular; the || `xvalidation` module implements k-fold cross-validation ||  || \""\""\""",https://github.com/dipy/dipy/commit/be5897cec1da00f7d50cfa362e1f69b821682b98,Yes
3291,dipy/dipy,dipy/segment/tests/test_clustering.py,e4334e40f4878ebcbb7b3287305de9c0db58f427,TODO: move this test into test_metric.,https://github.com/dipy/dipy/commit/e4334e40f4878ebcbb7b3287305de9c0db58f427,Yes
3292,dipy/dipy,dipy/segment/tests/test_quickbundles.py,cf8ef6177b8e0c411f9cb89030c2110085562939,TODO: move this test into test_metric.,https://github.com/dipy/dipy/commit/cf8ef6177b8e0c411f9cb89030c2110085562939,Yes
3293,dipy/dipy,dipy/viz/polydata.py,55af76982ab128121de1baea71b4f3bd9488846f,todo test,https://github.com/dipy/dipy/commit/55af76982ab128121de1baea71b4f3bd9488846f,No
3294,dipy/dipy,dipy/viz/polydata.py,55af76982ab128121de1baea71b4f3bd9488846f,todo test all,https://github.com/dipy/dipy/commit/55af76982ab128121de1baea71b4f3bd9488846f,Yes
3295,dipy/dipy,doc/examples/reconst_qtdmri.py,f534e0d7e82200ff1051722c3188a7405d640385,"\""\""\"" || .. figure:: qt_indices_rtap.png ||    : align: center || .. figure:: qt_indices_rtpp.png ||    : align: center ||  || As those of RTOP; the trends in RTAP and RTPP also decrease over time. It can || be seen that RTAP$^{1\/2}$ is always bigger than RTPP; which makes sense as || particles in coherent white matter experience more restriction perpendicular to || the white matter orientation than parallel to it. Again; in both subjects the || test-retest RTAP and RTPP is nearly perfectly consistent. ||  || Aside from the estimation of q$\\tau$-space indices; q$\\tau$-dMRI also allows || for the estimation of time-dependent ODFs. Once the Qtdmri model is fitted || it can be simply called by qtdmri_fit.odf(sphere; s=sharpening_factor). This || is identical to how the mapmri module functions; and allows to study the || time-dependence of ODF directionallity. ||  || This concludes the example on qt-dMRI. As we showed; approaches such as qt-dMRI || can help in studying the (finite-$\\tau$) temporal properties of diffusion in || biological tissues. Differences in q$\\tau$-index trends could be indicative || of underlying structural differences that affect the time-dependence of the || diffusion process. ||  || .. [Fick2017]_ Fick; Rutger HJ; et al. \""Non-Parametric GraphNet-Regularized ||             Representation of dMRI in Space and Time\""; Medical Image Analysis; ||             2017. || .. [Wassermann2017]_ Wassermann; Demian; et al. \""Test-Retest qt-dMRI datasets ||             for 'Non-Parametric GraphNet-Regularized Representation of dMRI in ||             Space and Time' [Data set]\"". Zenodo. ||             http:\/\/doi.org\/10.5281\/zenodo.996889; 2017. || \""\""\""",https://github.com/dipy/dipy/commit/f534e0d7e82200ff1051722c3188a7405d640385,No
3296,YerevaNN/mimic3-benchmarks,models/decompensation/main.py,5039a785bc106a2db8282e1ea36a1228387dec77,TODO: remove this; to test on full data,https://github.com/YerevaNN/mimic3-benchmarks/commit/5039a785bc106a2db8282e1ea36a1228387dec77,Yes
3297,YerevaNN/mimic3-benchmarks,models/length_of_stay/main.py,5039a785bc106a2db8282e1ea36a1228387dec77,TODO: remove this; to test on full data,https://github.com/YerevaNN/mimic3-benchmarks/commit/5039a785bc106a2db8282e1ea36a1228387dec77,Yes
3298,YerevaNN/mimic3-benchmarks,mimic3models/keras_utils.py,39e6232af3c56843c48d89466ecf8478eec5b9af,TODO: test on tensorflow,https://github.com/YerevaNN/mimic3-benchmarks/commit/39e6232af3c56843c48d89466ecf8478eec5b9af,No
3299,datmo/datmo,datmo/cli/driver/test/test_cli_helper.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,"TODO: figure out how to replace \""print\"" with a testable function",https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,No
3300,datmo/datmo,datmo/controller/code/driver/test/test_git.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: test all options,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,Yes
3301,datmo/datmo,datmo/controller/environment/driver/test/test_dockerenv.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: Add more cases for each test,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,Yes
3302,datmo/datmo,datmo/controller/environment/driver/test/test_dockerenv.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: test with all variables provided,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,No
3303,datmo/datmo,datmo/controller/environment/driver/test/test_dockerenv.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: Do a more comprehensive test; test out optional variables,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,Yes
3304,datmo/datmo,datmo/controller/environment/driver/test/test_dockerenv.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: Test out more commands at the system level,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,Yes
3305,datmo/datmo,datmo/controller/environment/driver/test/test_dockerenv.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: add more robust tests,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,No
3306,datmo/datmo,datmo/controller/file/driver/test/test_local.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: Add more cases for each test,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,Yes
3307,datmo/datmo,datmo/controller/test/test_base.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: Test all Datmo default settings,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,Yes
3308,datmo/datmo,datmo/storage/local/test/test_datmo_local.py,5654a3e0c1fa08378dd63a2713f3d268fb94341d,TODO: Add tests for other variables once figured out.,https://github.com/datmo/datmo/commit/5654a3e0c1fa08378dd63a2713f3d268fb94341d,No
3309,datmo/datmo,datmo/controller/file/driver/test/test_local.py,fb697e97d7c92ae6ceb69a30769177a42d7d3452,TODO : Add tests for code that handles various project templates,https://github.com/datmo/datmo/commit/fb697e97d7c92ae6ceb69a30769177a42d7d3452,Yes
3310,datmo/datmo,datmo/util/test/test_misc_functions.py,fb697e97d7c92ae6ceb69a30769177a42d7d3452,TODO: Add more cases for each test,https://github.com/datmo/datmo/commit/fb697e97d7c92ae6ceb69a30769177a42d7d3452,Yes
3311,datmo/datmo,datmo/controller/code/driver/test/test_git.py,3c31662d495cbac0b08a2726e07b71b69d74278d,TODO: Test remote checkout,https://github.com/datmo/datmo/commit/3c31662d495cbac0b08a2726e07b71b69d74278d,Yes
3312,datmo/datmo,datmo/controller/environment/driver/dockerenv.py,972fc34cd18161c2454dcd3119cfaaf72f70b2c9,TODO: Test this out for the API (need to verify ports work),https://github.com/datmo/datmo/commit/972fc34cd18161c2454dcd3119cfaaf72f70b2c9,Yes
3313,datmo/datmo,datmo/core/storage/local/test/test_dal_environment.py,4777f8f8b7eaafdc5156b5a8335515a41d316b45,TODO: Add tests for other variables once figured out.,https://github.com/datmo/datmo/commit/4777f8f8b7eaafdc5156b5a8335515a41d316b45,No
3314,datmo/datmo,datmo/test/test_snapshot.py,709d342758be1e80ba846e4e8dd7b5fe908c8ce7,TODO: test for failure case where tasks is not complete,https://github.com/datmo/datmo/commit/709d342758be1e80ba846e4e8dd7b5fe908c8ce7,Yes
3315,datmo/datmo,datmo/core/controller/environment/tests/test_environment.py,bdfab8c8e8d001c3ca5f11b4c9af7d2ae9d2e499,TODO: test more run options,https://github.com/datmo/datmo/commit/bdfab8c8e8d001c3ca5f11b4c9af7d2ae9d2e499,No
3316,datmo/datmo,datmo/core/controller/environment/driver/tests/test_dockerenv.py,0c2f9b4494163a4daac5e3b489b27e8ccca9f5b8,TODO: add more robust tests,https://github.com/datmo/datmo/commit/0c2f9b4494163a4daac5e3b489b27e8ccca9f5b8,No
3317,datmo/datmo,datmo/core/controller/environment/tests/test_environment.py,0c2f9b4494163a4daac5e3b489b27e8ccca9f5b8,TODO: test more run options,https://github.com/datmo/datmo/commit/0c2f9b4494163a4daac5e3b489b27e8ccca9f5b8,No
3318,datmo/datmo,datmo/core/controller/environment/driver/tests/test_dockerenv.py,9f177af4715a4ed5423a6cedf12128e6c7a14e3e,TODO: add more robust tests,https://github.com/datmo/datmo/commit/9f177af4715a4ed5423a6cedf12128e6c7a14e3e,No
3319,datmo/datmo,datmo/core/controller/environment/tests/test_environment.py,9f177af4715a4ed5423a6cedf12128e6c7a14e3e,TODO: test more run options,https://github.com/datmo/datmo/commit/9f177af4715a4ed5423a6cedf12128e6c7a14e3e,No
3320,datmo/datmo,datmo/core/controller/file/driver/tests/test_local.py,d8df8c23c968676bc036d8640b05b90e701f1c38,TODO: Create test for Windows platform,https://github.com/datmo/datmo/commit/d8df8c23c968676bc036d8640b05b90e701f1c38,No
3321,datmo/datmo,datmo/core/controller/code/driver/git.py,69234d60a3c3a3bfeb0d9c1bfc944125630ce52a,# TODO: Test this function,https://github.com/datmo/datmo/commit/69234d60a3c3a3bfeb0d9c1bfc944125630ce52a,No
3322,datmo/datmo,datmo/core/controller/code/driver/tests/test_git.py,69234d60a3c3a3bfeb0d9c1bfc944125630ce52a,TODO: Test remote checkout,https://github.com/datmo/datmo/commit/69234d60a3c3a3bfeb0d9c1bfc944125630ce52a,Yes
3323,datmo/datmo,datmo/core/controller/environment/tests/test_environment.py,12f218dfd03a77e693a35ffc6cdda87f22eaacd9,TODO: Fix this test,https://github.com/datmo/datmo/commit/12f218dfd03a77e693a35ffc6cdda87f22eaacd9,Yes
3324,datmo/datmo,datmo/core/controller/environment/tests/test_environment.py,a0696b6ef6e92a4783e23b18a72d3021c93859fb,TODO: Fix this test after merging PR from environment,https://github.com/datmo/datmo/commit/a0696b6ef6e92a4783e23b18a72d3021c93859fb,Yes
3325,datmo/datmo,datmo/cli/driver/tests/test_helper.py,2d4610b61bf673dacc2b07e08640e9b61f487215,Test success invalid (2 tries) - TODO: fix,https://github.com/datmo/datmo/commit/2d4610b61bf673dacc2b07e08640e9b61f487215,No
3326,datmo/datmo,datmo/cli/command/tests/test_flow.py,5cb971c2a55b74c3fbad46dc39156e3e8422a584,TODO: TEST more fundamental functions first,https://github.com/datmo/datmo/commit/5cb971c2a55b74c3fbad46dc39156e3e8422a584,No
3327,datmo/datmo,datmo/core/controller/environment/tests/test_environment.py,c098f83eff09248c201e7da66bd30117afc45caf,TODO: Run all environment options and test if success,https://github.com/datmo/datmo/commit/c098f83eff09248c201e7da66bd30117afc45caf,Yes
3328,datmo/datmo,datmo/core/controller/environment/driver/tests/test_dockerenv.py,8d99b46a1c383be56699bd5ab11506c77af4e590,TODO: test with all variables provided,https://github.com/datmo/datmo/commit/8d99b46a1c383be56699bd5ab11506c77af4e590,No
3329,datmo/datmo,datmo/core/util/tests/test_remote_api.py,cc7c84114561406516b3b6b40057534164017fc6,TODO: Add more cases for each test,https://github.com/datmo/datmo/commit/cc7c84114561406516b3b6b40057534164017fc6,Yes
3330,datmo/datmo,datmo/tests/test_monitoring.py,cc7c84114561406516b3b6b40057534164017fc6,TODO: move this only into test_set_track,https://github.com/datmo/datmo/commit/cc7c84114561406516b3b6b40057534164017fc6,No
3331,h2oai/h2o4gpu,tests/utils.py,27a4d2708fb2fea33bd67185d137e2fc813f964a,"TODO(navdeep): You can mimic my \""showresults.sh\"" process for make testperf and have timers that measure fit and predict performance for h2ogpuml and h2o-3.  I think just showing the ratio of the two is good; showing time for h2ogpuml over h2o-3 (so smaller is better).",https://github.com/h2oai/h2o4gpu/commit/27a4d2708fb2fea33bd67185d137e2fc813f964a,Yes
3332,h2oai/h2o4gpu,src/interface_py/h2o4gpu/solvers/elastic_net.py,1d55b91cd1fd08df814f95fdd9c8991c94a97650,XXX: should we rather test if instance of estimator?,https://github.com/h2oai/h2o4gpu/commit/1d55b91cd1fd08df814f95fdd9c8991c94a97650,No
3333,h2oai/h2o4gpu,src/interface_py/h2o4gpu/solvers/kmeans.py,72f6a6d6a6756047825a700201fe4fc0d84eccf2,XXX : should we rather test if instance of estimator ?,https://github.com/h2oai/h2o4gpu/commit/72f6a6d6a6756047825a700201fe4fc0d84eccf2,No
3334,h2oai/h2o4gpu,tests_open/test_glm_np_input.py,e9cfe789e382256fbd1804461d491dd63a8e66c6,"TODO: Succeeds as individual test; but during \""make test\""",https://github.com/h2oai/h2o4gpu/commit/e9cfe789e382256fbd1804461d491dd63a8e66c6,Yes
3335,h2oai/h2o4gpu,src/interface_py/h2o4gpu/solvers/truncated_svd.py,449f4158a98982ebdbe1a1ca0ef640120dce707a,XXX : should we rather test if instance of estimator ?,https://github.com/h2oai/h2o4gpu/commit/449f4158a98982ebdbe1a1ca0ef640120dce707a,No
3336,python-adaptive/adaptive,adaptive/tests/test_learner.py,b1af8b698ec5ae36fde2e1b5fa72907736d23f86,Some non-determinism is needed to make this test fail so we keep,https://github.com/python-adaptive/adaptive/commit/b1af8b698ec5ae36fde2e1b5fa72907736d23f86,Yes
3337,python-adaptive/adaptive,adaptive/tests/test_learner1d.py,0d8731fbf9a45ec43cfa58ce77cbc0bee2aa5ef7,Some non-determinism is needed to make this test fail so we keep,https://github.com/python-adaptive/adaptive/commit/0d8731fbf9a45ec43cfa58ce77cbc0bee2aa5ef7,Yes
3338,mme/vergeml,tests/test_cache.py,b5cffa8a8e532d64bc38e58baa51ec38bf30285e,TODO test composite values,https://github.com/mme/vergeml/commit/b5cffa8a8e532d64bc38e58baa51ec38bf30285e,Yes
3339,mme/vergeml,tests/test_model.py,b5cffa8a8e532d64bc38e58baa51ec38bf30285e,TODO test setting defaults,https://github.com/mme/vergeml/commit/b5cffa8a8e532d64bc38e58baa51ec38bf30285e,Yes
3340,mila-iqia/babyai,levels/envs.py,4af11e5037562444bb0fd8708eb367063c4ec11d,TODO: test relative and absolute locations,https://github.com/mila-iqia/babyai/commit/4af11e5037562444bb0fd8708eb367063c4ec11d,No
3341,mila-iqia/babyai,levels/envs.py,4af11e5037562444bb0fd8708eb367063c4ec11d,TODO: test doors,https://github.com/mila-iqia/babyai/commit/4af11e5037562444bb0fd8708eb367063c4ec11d,Yes
3342,mila-iqia/babyai,run_tests.py,238223f8931b1991baf246e80f5dab364b0567f2,TODO: instruction generation tests,https://github.com/mila-iqia/babyai/commit/238223f8931b1991baf246e80f5dab364b0567f2,Yes
3343,mila-iqia/babyai,run_tests.py,238223f8931b1991baf246e80f5dab364b0567f2,TODO: verifier tests,https://github.com/mila-iqia/babyai/commit/238223f8931b1991baf246e80f5dab364b0567f2,No
3344,mila-iqia/babyai,scripts/train_intelligent_curriculum.py,94c5ff8c61413d202a87033afe4c6a6a2f1e903d,TODO: adjust values. These are only small for testing purposes.,https://github.com/mila-iqia/babyai/commit/94c5ff8c61413d202a87033afe4c6a6a2f1e903d,No
3345,mila-iqia/babyai,scripts/train_intelligent_expert.py,94c5ff8c61413d202a87033afe4c6a6a2f1e903d,TODO: adjust values. These are only small for testing purposes.,https://github.com/mila-iqia/babyai/commit/94c5ff8c61413d202a87033afe4c6a6a2f1e903d,No
3346,mila-iqia/babyai,scripts/train_intelligent_expert.py,838f60e4ea855758afaae1c862b3883724c724e2,TODO: make this higher after tests pass,https://github.com/mila-iqia/babyai/commit/838f60e4ea855758afaae1c862b3883724c724e2,Yes
3347,mila-iqia/babyai,babyai/agents/bot.py,c7c1c4be2fe4253be36922b0a0dca1e8d1337141,TODO: Ideally; we should compare the length of the paths in terms of actions necessary to go there. That is closely related to the TODO of the shortest_path function,https://github.com/mila-iqia/babyai/commit/c7c1c4be2fe4253be36922b0a0dca1e8d1337141,No
3348,mozilla/TTS,TTS/tts/utils/synthesis.py,639fa292616db7aa740088f5e06094cd35d5ba1a,TODO: test this for tacotron models,https://github.com/mozilla/TTS/commit/639fa292616db7aa740088f5e06094cd35d5ba1a,No
3349,mozilla/TTS,TTS/tts/utils/synthesis.py,13c6665c92668e54d6ad10cda3e87e61fb8def39,TODO: test this for tacotron models,https://github.com/mozilla/TTS/commit/13c6665c92668e54d6ad10cda3e87e61fb8def39,No
3350,jupyterhub/binderhub,doc/script/flow_diagram.py,9f1f76e082628f2e1433938d318175182a0a950b,HACK: Stop nosetests running setup() above,https://github.com/jupyterhub/binderhub/commit/9f1f76e082628f2e1433938d318175182a0a950b,Yes
3351,scikit-learn-contrib/lightning,doc/sphinxext/gen_rst.py,b7469f4c4c9a80d88ecedcd586956b1f9e9d1eb9,HACK: Stop nosetests running setup() above,https://github.com/scikit-learn-contrib/lightning/commit/b7469f4c4c9a80d88ecedcd586956b1f9e9d1eb9,Yes
3352,jupyter/nbdime,nbmerge/tests/test_patch.py,d253d18473da18e09d73a6d6421e53d83e2d94b6,TODO: Check and improve test coverage,https://github.com/jupyter/nbdime/commit/d253d18473da18e09d73a6d6421e53d83e2d94b6,Yes
3353,jupyter/nbdime,nbmerge/tests/test_patch.py,d253d18473da18e09d73a6d6421e53d83e2d94b6,TODO: Add tests for invalid input and error handling,https://github.com/jupyter/nbdime/commit/d253d18473da18e09d73a6d6421e53d83e2d94b6,No
3354,jupyter/nbdime,nbdime/dformat.py,44cbba7dc3f02646b58f82956b829e63e7c18a64,FIXME: Reverse this or not? Read RFC carefully and\/or test with some conforming tool.,https://github.com/jupyter/nbdime/commit/44cbba7dc3f02646b58f82956b829e63e7c18a64,Yes
3355,jupyter/nbdime,nbdime/tests/test_merge_notebooks.py,6ef5d4ae5bb177b55cd41f197d8e18f916f88905,FIXME: Extend tests to more merge situations!,https://github.com/jupyter/nbdime/commit/6ef5d4ae5bb177b55cd41f197d8e18f916f88905,Yes
3356,jupyter/nbdime,nbdime/diffing/generic.py,360fa2e8de90d673b235d0f132dda66a52a3d01e,FIXME: Not covered in tests; create test situation,https://github.com/jupyter/nbdime/commit/360fa2e8de90d673b235d0f132dda66a52a3d01e,Yes
3357,jupyter/nbdime,nbdime/merging/autoresolve.py,2ce0ddbe7b5dbac0f03c4c0989dff0abca6c5f1c,FIXME: Test this!,https://github.com/jupyter/nbdime/commit/2ce0ddbe7b5dbac0f03c4c0989dff0abca6c5f1c,Yes
3358,jupyter/nbdime,nbdime/diff_format.py,a3ef7ccb465c19fdb220d85c8c49f69bed149f2e,FIXME: Test with some conforming tool.,https://github.com/jupyter/nbdime/commit/a3ef7ccb465c19fdb220d85c8c49f69bed149f2e,No
3359,jupyter/nbdime,nbdime/tests/test_autoresolve.py,025a4ff28e30a879226f08cc6303624b9d50347c,FIXME: Extend tests to more merge situations!,https://github.com/jupyter/nbdime/commit/025a4ff28e30a879226f08cc6303624b9d50347c,Yes
3360,jupyter/nbdime,nbdime/tests/test_merge_notebooks.py,50230e3cbc469dd2a70c5edc7a8c698f837e53ce,FIXME: These tests are postphoned until the new merge spec format is in place,https://github.com/jupyter/nbdime/commit/50230e3cbc469dd2a70c5edc7a8c698f837e53ce,No
3361,jupyter/nbdime,nbdime/tests/test_autoresolve_decisions.py,b3945490c45df6cddfab36a0aa414002eb495e40,FIXME: Extend tests to more merge situations!,https://github.com/jupyter/nbdime/commit/b3945490c45df6cddfab36a0aa414002eb495e40,Yes
3362,jupyter/nbdime,nbdime/tests/test_autoresolve.py,582e2d148d5c8745e675d33b4bf028dd94552df6,FIXME: Extend tests to more merge situations!,https://github.com/jupyter/nbdime/commit/582e2d148d5c8745e675d33b4bf028dd94552df6,Yes
3363,jupyter/nbdime,nbdime/prettyprint.py,fb504bc193b078389a28710b73412ed43ffcfbbb,FIXME: This fails in cli test; reproduce with py.test -k cli,https://github.com/jupyter/nbdime/commit/fb504bc193b078389a28710b73412ed43ffcfbbb,Yes
3364,jupyter/nbdime,nbdime/tests/test_merge_notebooks.py,dd2f23952a545c87ecdce7783c0ee35a44e615f8,TODO: Make test for output_strategy == 'inline',https://github.com/jupyter/nbdime/commit/dd2f23952a545c87ecdce7783c0ee35a44e615f8,Yes
3365,jupyter/nbdime,nbdime/diffing/notebooks.py,02c73aa1801314d56561f75f24c168e05de0aab7,TODO: Test this. Match repr-style oneliners with random pointer,https://github.com/jupyter/nbdime/commit/02c73aa1801314d56561f75f24c168e05de0aab7,Yes
3366,jupyter/nbdime,nbdime/merging/decisions.py,e694d815c6874bbdfce8cd34efa905d93a552886,FIXME XXX Test that it works to apply this here,https://github.com/jupyter/nbdime/commit/e694d815c6874bbdfce8cd34efa905d93a552886,Yes
3367,jupyter/nbdime,nbdime/merging/generic.py,f83222b0ec62ef33a75ced0f2f3c24619abbb82f,- _merge_strings is probably broken; test and fix,https://github.com/jupyter/nbdime/commit/f83222b0ec62ef33a75ced0f2f3c24619abbb82f,No
3368,jupyter/nbdime,nbdime/merging/generic.py,5754cb19506ceb0022508866c72f9f27c183cc41,FIXME XXX: Test regular string merge well also for no specific strategy!,https://github.com/jupyter/nbdime/commit/5754cb19506ceb0022508866c72f9f27c183cc41,Yes
3369,jupyter/nbdime,nbdime/merging/generic.py,dd30b940a56d7538072e9345aaa9187f54d27db1,FIXME XXX: Test regular string merge well also for no specific strategy!,https://github.com/jupyter/nbdime/commit/dd30b940a56d7538072e9345aaa9187f54d27db1,Yes
3370,doccano/doccano,app/api/tests/test_api.py,01cfbd56d288193ee5a41b6c5add185e51adf34f,Todo: refactoring testing.,https://github.com/doccano/doccano/commit/01cfbd56d288193ee5a41b6c5add185e51adf34f,No
3371,scikit-optimize/scikit-optimize,skopt/__init__.py,b30d5ea01bcf68e77c6177fb914e9d263e2fa8a5,"\""\""\"" || Scikit-Optimize; or `skopt`; is meant to be a simple and efficient library || for black box optimization; accessible to everybody and reusable in various || contexts. ||  || The library is built on top of NumPy; SciPy and Scikit-Learn. ||  || [![Build Status](https:\/\/travis-ci.org\/scikit-optimize\/scikit-optimize.svg?branch=master)](https:\/\/travis-ci.org\/scikit-optimize\/scikit-optimize) ||  || ## Development ||  || The library is still experimental and under heavy development. ||  || The development version can be installed through: ||  ||     git clone https:\/\/github.com\/scikit-optimize\/scikit-optimize.git ||     cd scikit-optimize ||     pip install -r requirements.txt ||     python setup.py develop ||  || Run the tests by executing `nosetests` in the top level directory. || \""\""\""",https://github.com/scikit-optimize/scikit-optimize/commit/b30d5ea01bcf68e77c6177fb914e9d263e2fa8a5,No
3372,deepmind/dm_control,dm_control/render/base.py,9f5da6c411fa24a2e9541577c780a4216e8c99c1,"\""\""\""Base class for OpenGL context handlers. ||  || The module lays foundation for defining various rendering contexts in a uniform || manner. ||  || ContextBase defines a common interface rendering contexts should fulfill. In || addition; it provides a context activation method that can be used in 'with' || statements to ensure symmetrical context activation and deactivation. ||  || The problem of optimizing context swaps falls to ContextPolicyManager and the || accompanying policy classes. OptimizedContextPolicy will attempt to reduce || the number of context swaps; increasing application's performance. || DebugContextPolicy; on the other; hand will rigorously keep activating and || deactivating contexts for each request; providing a reliable framework for || functional tests of the new context implementations. || \""\""\""",https://github.com/deepmind/dm_control/commit/9f5da6c411fa24a2e9541577c780a4216e8c99c1,No
3373,deepchem/deepchem,deep_chem/scripts/modeler.py,8e94c5972ed2264b6becdb14a728a998bafc6919,better method is here sicne often the test-set isn't actually stratified,https://github.com/deepchem/deepchem/commit/8e94c5972ed2264b6becdb14a728a998bafc6919,Yes
3374,deepchem/deepchem,deepchem/splits/tests/test_splitter.py,a1a2b8475e0a1bde9eec14125b15829228aa55e9,data. Make a test for properly splitting of sharded data. Perhaps using,https://github.com/deepchem/deepchem/commit/a1a2b8475e0a1bde9eec14125b15829228aa55e9,Yes
3375,deepchem/deepchem,examples/benchmark2.py,297eff9d7e1813607cd9c3119bc58e8d4f695823,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of:  ||     Random forest(rf); MultitaskDNN(tf);  ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv)                  || on datasets: muv; pcba; tox21; sider; toxcast; clintox; hiv ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg) || on datasets: delaney(ESOL); nci; kaggle; pdbbind;  ||              qm7; qm7b; qm9; chembl; sampl(FreeSolv) ||  || time estimation listed in README file ||  || Total time of running a benchmark test(for one splitting function): 20h || \""\""\""",https://github.com/deepchem/deepchem/commit/297eff9d7e1813607cd9c3119bc58e8d4f695823,Yes
3376,deepchem/deepchem,examples/benchmark.py,8545a6323f051d8d0cefb16f0fbae4895ce36bb1,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of:  ||     Random forest(rf); MultitaskDNN(tf);  ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv)                  || on datasets: bace_c; bbbp; clintox; hiv; muv; pcba; sider; tox21; toxcast   ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Fit Transformer MultitaskDNN(tf_regression_ft); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg) || on datasets: bace_r; chembl; clearance; delaney(ESOL); hopv; kaggle; lipo; ||              nci; pdbbind; ppb; qm7; qm7b; qm8; qm9; sampl(FreeSolv) ||                  ||  || time estimation listed in README file ||  || \""\""\""",https://github.com/deepchem/deepchem/commit/8545a6323f051d8d0cefb16f0fbae4895ce36bb1,Yes
3377,deepchem/deepchem,examples/benchmark.py,12d9a22db09ba2d41b2482be8c556800131cbfda,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of: ||     Random forest(rf); MultitaskDNN(tf); ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv) || on datasets: muv; pcba; tox21; sider; toxcast; clintox; hiv ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg) || on datasets: delaney(ESOL); nci; kaggle; pdbbind; ||              qm7; qm7b; qm9; chembl; sampl(FreeSolv) ||  || time estimation listed in README file ||  || Total time of running a benchmark test(for one splitting function): 20h || \""\""\""",https://github.com/deepchem/deepchem/commit/12d9a22db09ba2d41b2482be8c556800131cbfda,Yes
3378,deepchem/deepchem,examples/benchmark.py,6d91f03c7d2040bcd1624bc4485701570d853b83,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of:  ||     Random forest(rf); MultitaskDNN(tf);  ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv)                  || on datasets: bace_c; bbbp; clintox; hiv; muv; pcba; sider; tox21; toxcast   ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Fit Transformer MultitaskDNN(tf_regression_ft); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg) || on datasets: bace_r; chembl; clearance; delaney(ESOL); hopv; kaggle; lipo; ||              nci; pdbbind; ppb; qm7; qm7b; qm8; qm9; sampl(FreeSolv) ||                  ||  || time estimation listed in README file ||  || \""\""\""",https://github.com/deepchem/deepchem/commit/6d91f03c7d2040bcd1624bc4485701570d853b83,Yes
3379,deepchem/deepchem,examples/benchmark.py,6c4008c8da1743e38909569cf030e4b3c43c6d40,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of: ||     Random forest(rf); MultitaskDNN(tf); ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv) || on datasets: muv; pcba; tox21; sider; toxcast; clintox; hiv ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg) || on datasets: delaney(ESOL); nci; kaggle; pdbbind; ||              qm7; qm7b; qm9; chembl; sampl(FreeSolv) ||  || time estimation listed in README file ||  || Total time of running a benchmark test(for one splitting function): 20h || \""\""\""",https://github.com/deepchem/deepchem/commit/6c4008c8da1743e38909569cf030e4b3c43c6d40,Yes
3380,deepchem/deepchem,examples/benchmark2.py,367f18b199876fea6e1b94d50da5f207454fee52,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of:  ||     Random forest(rf); MultitaskDNN(tf);  ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv)                  || on datasets: bace_c; bbbp; clintox; hiv; muv; pcba; sider; tox21; toxcast   ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Fit Transformer MultitaskDNN(tf_regression_ft); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg) || on datasets: bace_r; chembl; clearance; delaney(ESOL); hopv; kaggle; lipo; ||              nci; pdbbind; ppb; qm7; qm7b; qm8; qm9; sampl(FreeSolv) ||                  ||  || time estimation listed in README file ||  || \""\""\""",https://github.com/deepchem/deepchem/commit/367f18b199876fea6e1b94d50da5f207454fee52,Yes
3381,deepchem/deepchem,examples/benchmark2.py,367f18b199876fea6e1b94d50da5f207454fee52,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of:  ||     Random forest(rf); MultitaskDNN(tf);  ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv)                  || on datasets: muv; pcba; tox21; sider; toxcast; clintox; hiv; bace_c ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg) || on datasets: delaney(ESOL); nci; kaggle; pdbbind;  ||              qm7; qm7b; qm8; qm9; chembl; sampl(FreeSolv); ||              bace_r; ppb; clearance; lipo; hopv ||  || time estimation listed in README file ||  || Total time of running a benchmark test(for one splitting function): 20h || \""\""\""",https://github.com/deepchem/deepchem/commit/367f18b199876fea6e1b94d50da5f207454fee52,Yes
3382,deepchem/deepchem,examples/benchmark_xgboost.py,3dce3dbc787fce07f620f6db144d41b992e66c9f,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of: ||     Random forest(rf); MultitaskDNN(tf); ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv); ||     Xgboost classifier(xgb_classifier) || on datasets: muv; pcba; tox21; sider; toxcast; clintox; hiv ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg); ||     Xgboost regression(xgb_regression) || on datasets: delaney(ESOL); nci; kaggle; pdbbind; ||              qm7; qm7b; qm9; chembl; sampl(FreeSolv) ||  || time estimation listed in README file ||  || Total time of running a benchmark test(for one splitting function): 20h || \""\""\""",https://github.com/deepchem/deepchem/commit/3dce3dbc787fce07f620f6db144d41b992e66c9f,No
3383,deepchem/deepchem,deepchem/models/tensorgraph/tests/test_layers.py,da09707b1a5cd94e523a02ffc6dd4602d72c6498,TODO(rbharath): This test should pass. Fix it!,https://github.com/deepchem/deepchem/commit/da09707b1a5cd94e523a02ffc6dd4602d72c6498,Yes
3384,deepchem/deepchem,examples/benchmark.py,caa85a5bf44a3fedf9fb991afb8609bdd394b7f0,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of:  ||     Random forest(rf); MultitaskDNN(tf);  ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv); xgboost(xgb); ||     Directed acyclic graph(dag); Weave(weave)  || on datasets: bace_c; bbbp; clintox; hiv; muv; pcba; sider; tox21; toxcast   ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Fit Transformer MultitaskDNN(tf_regression_ft); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg); ||     xgboost(xgb_regression); Deep tensor neural net(dtnn); ||     Directed acyclic graph(dag_regression); ||     Weave(weave_regression) || on datasets: bace_r; chembl; clearance; delaney(ESOL); hopv; kaggle; lipo; ||              nci; pdbbind; ppb; qm7; qm7b; qm8; qm9; sampl(FreeSolv) ||                  ||  || time estimation listed in README file ||  || \""\""\""",https://github.com/deepchem/deepchem/commit/caa85a5bf44a3fedf9fb991afb8609bdd394b7f0,Yes
3385,deepchem/deepchem,examples/benchmark2.py,7cb3989c8ea29721cf8d7616ff519b02077bcd56,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of: ||     Random forest(rf); MultitaskDNN(tf); ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv); xgboost(xgb); ||     Directed acyclic graph(dag); Weave(weave) || on datasets: bace_c; bbbp; clintox; hiv; muv; pcba; sider; tox21; toxcast ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Fit Transformer MultitaskDNN(tf_regression_ft); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg); ||     xgboost(xgb_regression); Deep tensor neural net(dtnn); ||     Directed acyclic graph(dag_regression); ||     Weave(weave_regression) || on datasets: bace_r; chembl; clearance; delaney(ESOL); hopv; kaggle; lipo; ||              nci; pdbbind; ppb; qm7; qm7b; qm8; qm9; sampl(FreeSolv) ||  ||  || time estimation listed in README file ||  || \""\""\""",https://github.com/deepchem/deepchem/commit/7cb3989c8ea29721cf8d7616ff519b02077bcd56,No
3386,deepchem/deepchem,examples/benchmark2.py,7cb3989c8ea29721cf8d7616ff519b02077bcd56,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of:  ||     Random forest(rf); MultitaskDNN(tf);  ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv); xgboost(xgb); ||     Directed acyclic graph(dag); Weave(weave)  || on datasets: bace_c; bbbp; clintox; hiv; muv; pcba; sider; tox21; toxcast   ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Fit Transformer MultitaskDNN(tf_regression_ft); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg); ||     xgboost(xgb_regression); Deep tensor neural net(dtnn); ||     Directed acyclic graph(dag_regression); ||     Weave(weave_regression) || on datasets: bace_r; chembl; clearance; delaney(ESOL); hopv; kaggle; lipo; ||              nci; pdbbind; ppb; qm7; qm7b; qm8; qm9; sampl(FreeSolv) ||                  ||  || time estimation listed in README file ||  || \""\""\""",https://github.com/deepchem/deepchem/commit/7cb3989c8ea29721cf8d7616ff519b02077bcd56,Yes
3387,deepchem/deepchem,examples/benchmark_10.py,6d903deee9b37324129cab8ec9f6843259b97176,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of: ||     Random forest(rf); MultitaskDNN(tf); ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv); xgboost(xgb); ||     Directed acyclic graph(dag); Weave(weave) || on datasets: bace_c; bbbp; clintox; hiv; muv; pcba; sider; tox21; toxcast ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Fit Transformer MultitaskDNN(tf_regression_ft); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg); ||     xgboost(xgb_regression); Deep tensor neural net(dtnn); ||     Directed acyclic graph(dag_regression); ||     Weave(weave_regression) || on datasets: bace_r; chembl; clearance; delaney(ESOL); hopv; kaggle; lipo; ||              nci; pdbbind; ppb; qm7; qm7b; qm8; qm9; sampl(FreeSolv) ||  ||  || time estimation listed in README file ||  || \""\""\""",https://github.com/deepchem/deepchem/commit/6d903deee9b37324129cab8ec9f6843259b97176,No
3388,deepchem/deepchem,examples/benchmark_20.py,6d903deee9b37324129cab8ec9f6843259b97176,"\""\""\"" || Created on Tue Oct 18 15:53:27 2016 ||  || @author: Michael Wu ||  || Benchmark test: ||  || Giving classification performances of: ||     Random forest(rf); MultitaskDNN(tf); ||     RobustMultitaskDNN(tf_robust); ||     Logistic regression(logreg); IRV(irv) ||     Graph convolution(graphconv); xgboost(xgb); ||     Directed acyclic graph(dag); Weave(weave) || on datasets: bace_c; bbbp; clintox; hiv; muv; pcba; sider; tox21; toxcast ||  || Giving regression performances of: ||     MultitaskDNN(tf_regression); ||     Fit Transformer MultitaskDNN(tf_regression_ft); ||     Random forest(rf_regression); ||     Graph convolution regression(graphconvreg); ||     xgboost(xgb_regression); Deep tensor neural net(dtnn); ||     Directed acyclic graph(dag_regression); ||     Weave(weave_regression) || on datasets: bace_r; chembl; clearance; delaney(ESOL); hopv; kaggle; lipo; ||              nci; pdbbind; ppb; qm7; qm7b; qm8; qm9; sampl(FreeSolv) ||  ||  || time estimation listed in README file ||  || \""\""\""",https://github.com/deepchem/deepchem/commit/6d903deee9b37324129cab8ec9f6843259b97176,No
3389,deepchem/deepchem,deepchem/feat/tests/test_rdkit_grid_features.py,7a53caec37602bff2870d30a5c5dd6162fec693b,TODO test more formats for ligand,https://github.com/deepchem/deepchem/commit/7a53caec37602bff2870d30a5c5dd6162fec693b,Yes
3390,deepchem/deepchem,deepchem/feat/tests/test_rdkit_grid_features.py,7a53caec37602bff2870d30a5c5dd6162fec693b,TODO test if dict contains smiles,https://github.com/deepchem/deepchem/commit/7a53caec37602bff2870d30a5c5dd6162fec693b,Yes
3391,deepchem/deepchem,deepchem/feat/tests/test_rdkit_grid_features.py,fd06a6b294bfc524bc2313eeb022e122bdf36839,TODO test if dict contains smiles,https://github.com/deepchem/deepchem/commit/fd06a6b294bfc524bc2313eeb022e122bdf36839,Yes
3392,deepchem/deepchem,deepchem/feat/tests/test_rdkit_grid_features.py,7b1f65004dcee005418feac661cb465b6dc27fbf,TODO test if dict contains smiles,https://github.com/deepchem/deepchem/commit/7b1f65004dcee005418feac661cb465b6dc27fbf,Yes
3393,deepchem/deepchem,deepchem/molnet/run_benchmark_low_data.py,9ff134bc2cbee85999a69bdff980b4be2557427a,"''' || from __future__ import print_function || from __future__ import division || from __future__ import unicode_literals ||  || import os || import time || import csv || import numpy as np || import tensorflow as tf || import deepchem || from deepchem.molnet.run_benchmark_models import low_data_benchmark_classification || from deepchem.molnet.check_availability import CheckFeaturizer ||  ||  || def run_benchmark_low_data(datasets; ||                            model; ||                            split='task'; ||                            metric=None; ||                            featurizer=None; ||                            n_features=0; ||                            out_path='.'; ||                            K=4; ||                            hyper_parameters=None; ||                            cross_valid=False; ||                            seed=123): ||   \""\""\"" ||   Run low data benchmark test on designated datasets  ||   with deepchem(or user-defined) model ||    ||   Parameters ||   ---------- ||   datasets: list of string ||       choice of which datasets to use; should be: muv; tox21; sider  ||   model: string or user-defined model stucture ||       choice of which model to use; should be: siamese; attn; res ||   split: string;  optional (default='task') ||       choice of splitter function; only task splitter supported ||   metric: string;  optional (default=None) ||       choice of evaluation metrics; None = using the default metrics(AUC) ||   featurizer: string or dc.feat.Featurizer;  optional (default=None) ||       choice of featurization; None = using the default corresponding to model ||       (string only applicable to deepchem models) ||   n_features: int; optional(default=0) ||       depending on featurizers; redefined when using deepchem featurizers; ||       need to be specified for user-defined featurizers(if using deepchem models) ||   out_path: string; optional(default='.') ||       path of result file ||   K: int; optional(default=4) ||       K-fold splitting of datasets ||   hyper_parameters: dict; optional (default=None) ||       hyper parameters for designated model; None = use preset values ||   cross_valid: boolean; optional(default=False) ||       whether to cross validate ||   \""\""\"" ||   for dataset in datasets: ||     if dataset in ['muv'; 'sider'; 'tox21']: ||       mode = 'classification' ||       if metric == None: ||         metric = str('auc') ||     else: ||       raise ValueError('Dataset not supported') ||  ||     metric_all = { ||         'auc': deepchem.metrics.Metric(deepchem.metrics.roc_auc_score; np.mean) ||     } ||  ||     if isinstance(metric; str): ||       metric = metric_all[metric] ||  ||     if featurizer == None and isinstance(model; str): ||       # Assigning featurizer if not user defined ||       pair = (dataset; model) ||       if pair in CheckFeaturizer: ||         featurizer = CheckFeaturizer[pair][0] ||         n_features = CheckFeaturizer[pair][1] ||       else: ||         continue ||  ||     loading_functions = { ||         'muv': deepchem.molnet.load_muv; ||         'sider': deepchem.molnet.load_sider; ||         'tox21': deepchem.molnet.load_tox21 ||     } ||     assert split == 'task' ||  ||     print('-------------------------------------') ||     print('Benchmark on dataset: %s' % dataset) ||     print('-------------------------------------') ||     # loading datasets ||     print('Splitting function: %s' % split) ||     tasks; all_dataset; transformers = loading_functions[dataset]( ||         featurizer=featurizer; split=split; K=K) ||  ||     if cross_valid: ||       num_iter = K  # K iterations for cross validation ||     else: ||       num_iter = 1 ||     for count_iter in range(num_iter): ||       # Assembling train and valid datasets ||       train_folds = all_dataset[:K - count_iter - 1] + all_dataset[K - ||                                                                    count_iter:] ||       train_dataset = deepchem.splits.merge_fold_datasets(train_folds) ||       valid_dataset = all_dataset[K - count_iter - 1] ||  ||       time_start_fitting = time.time() ||       train_score = {} ||       valid_score = {} ||  ||       if isinstance(model; str): ||         if mode == 'classification': ||           valid_score = low_data_benchmark_classification( ||               train_dataset; ||               valid_dataset; ||               n_features; ||               metric; ||               model=model; ||               hyper_parameters=hyper_parameters; ||               seed=seed) ||       else: ||         model.fit(train_dataset) ||         valid_score['user_defined'] = model.evaluate(valid_dataset; metric; ||                                                      transformers) ||  ||       time_finish_fitting = time.time() ||  ||       with open(os.path.join(out_path; 'results.csv'); 'a') as f: ||         writer = csv.writer(f) ||         for i in valid_score: ||           output_line = [dataset; str(split); mode; 'valid'; i] ||           for task in valid_score[i][0]: ||             output_line.extend( ||                 [task; valid_score[i][0][task]; valid_score[i][1][task]]) ||           output_line.extend( ||               ['time_for_running'; time_finish_fitting - time_start_fitting]) ||           writer.writerow(output_line) || '''",https://github.com/deepchem/deepchem/commit/9ff134bc2cbee85999a69bdff980b4be2557427a,Yes
3394,deepchem/deepchem,deepchem/utils/test/test_vina_utils.py,e392516f0073ae2ff5cab0701a4103a83928270f,TODO test more formats for ligand,https://github.com/deepchem/deepchem/commit/e392516f0073ae2ff5cab0701a4103a83928270f,Yes
3395,deepchem/deepchem,deepchem/utils/test/test_fragment_util.py,d184be88966867f3a3135985deb32c7202aaf598,TODO test more formats for ligand,https://github.com/deepchem/deepchem/commit/d184be88966867f3a3135985deb32c7202aaf598,Yes
3396,deepchem/deepchem,deepchem/utils/test/test_rdkit_util.py,d54147be9745b2130a2e8f97a978a2cecd750a1d,TODO test more formats for ligand,https://github.com/deepchem/deepchem/commit/d54147be9745b2130a2e8f97a978a2cecd750a1d,Yes
3397,deepchem/deepchem,deepchem/utils/test/test_evaluate.py,b3463f05941b7a08f1e9564ceb1402f6f17f0ae2,TODO: Add metrics for images here as a test,https://github.com/deepchem/deepchem/commit/b3463f05941b7a08f1e9564ceb1402f6f17f0ae2,No
3398,deepchem/deepchem,deepchem/utils/test/test_evaluate.py,f7a42ba0582c6504834ca40e49296d807158809f,TODO: Finish this test,https://github.com/deepchem/deepchem/commit/f7a42ba0582c6504834ca40e49296d807158809f,Yes
3399,deepchem/deepchem,deepchem/utils/test/test_evaluate.py,385b763b76558f5506e1bfd0e70aeba6884dddd7,TODO: Finish this test,https://github.com/deepchem/deepchem/commit/385b763b76558f5506e1bfd0e70aeba6884dddd7,Yes
3400,deepchem/deepchem,deepchem/molnet/load_function/tests/test_molnet_loaders.py,b414182237fecef9fd2874c6f9bac900348e28af,TODO: add unit tests for other dataset loaders that comply with,https://github.com/deepchem/deepchem/commit/b414182237fecef9fd2874c6f9bac900348e28af,Yes
3401,deepchem/deepchem,deepchem/feat/tests/test_mol_graph_conv_featurizer.py,3892a54a59898947ce6c589ab7088d5207ecad72,TODO: Add more test cases,https://github.com/deepchem/deepchem/commit/3892a54a59898947ce6c589ab7088d5207ecad72,Yes
3402,deepchem/deepchem,deepchem/utils/test/test_graph_conv_utils.py,3892a54a59898947ce6c589ab7088d5207ecad72,TODO: add more test cases,https://github.com/deepchem/deepchem/commit/3892a54a59898947ce6c589ab7088d5207ecad72,No
3403,deepchem/deepchem,deepchem/models/tests/test_cgcnn.py,90ae77793254d5d1a4a76f6c66fbd730dfff9aff,TODO: Multi task classification test,https://github.com/deepchem/deepchem/commit/90ae77793254d5d1a4a76f6c66fbd730dfff9aff,No
3404,deepchem/deepchem,deepchem/splits/splitters.py,c1d13d23aa6fb2e19e584eb376ceb4f99bde39dc,"FIXME: Signature of \""train_valid_test_split\"" incompatible with supertype \""Splitter\""",https://github.com/deepchem/deepchem/commit/c1d13d23aa6fb2e19e584eb376ceb4f99bde39dc,No
3405,deepchem/deepchem,deepchem/splits/splitters.py,a3f1fd671baab33c4284cb51d7df5ec38a3f5051,"FIXME: Signature of \""train_valid_test_split\"" incompatible with supertype \""Splitter\""",https://github.com/deepchem/deepchem/commit/a3f1fd671baab33c4284cb51d7df5ec38a3f5051,No
3406,deepchem/deepchem,deepchem/models/tests/test_reload.py,8a015062717a7eee6d6325e06ed9fc4d369fedc6,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.,https://github.com/deepchem/deepchem/commit/8a015062717a7eee6d6325e06ed9fc4d369fedc6,No
3407,deepchem/deepchem,deepchem/feat/tests/test_grid_featurizers.py,0988767f102608cf39d41ca795d878016c8fcada,# TODO: Add shape test,https://github.com/deepchem/deepchem/commit/0988767f102608cf39d41ca795d878016c8fcada,Yes
3408,deepchem/deepchem,deepchem/feat/tests/test_grid_featurizers.py,0988767f102608cf39d41ca795d878016c8fcada,TODO: Add shape test,https://github.com/deepchem/deepchem/commit/0988767f102608cf39d41ca795d878016c8fcada,No
3409,dmlc/gluon-nlp,src/gluonnlp/data/word_embedding_evaluation.py,02056eacb93ce168718c6a041d2f409d22d1cf2b,TODO: reenable doctest once semeval17task2 is available again,https://github.com/dmlc/gluon-nlp/commit/02056eacb93ce168718c6a041d2f409d22d1cf2b,Yes
3410,dmlc/gluon-nlp,tests/test_layers.py,70a188776f7470c838dd22b1636462b75573a734,TODO This test even passes without sharing the parameters. It needs to be improved.,https://github.com/dmlc/gluon-nlp/commit/70a188776f7470c838dd22b1636462b75573a734,Yes
3411,dmlc/gluon-nlp,tests/test_op.py,32e87d4d4aa20a6eb658ee90d765ccffbd160571,TODO(?) Improve the test case here,https://github.com/dmlc/gluon-nlp/commit/32e87d4d4aa20a6eb658ee90d765ccffbd160571,Yes
3412,dmlc/gluon-nlp,scripts/conversion_toolkits/convert_t5.py,36092e2ae66342ee1d7ad5678cd99c46090373eb,test model if needed,https://github.com/dmlc/gluon-nlp/commit/36092e2ae66342ee1d7ad5678cd99c46090373eb,No
3413,dmlc/gluon-nlp,scripts/conversion_toolkits/convert_mt5.py,2b4ac5ff617d8b5f5b755e3d28a6712a4ff37a1f,test model if needed,https://github.com/dmlc/gluon-nlp/commit/2b4ac5ff617d8b5f5b755e3d28a6712a4ff37a1f,No
3414,google/tangent,tangent/fence.py,3b8a7adf13da0bfa22c5256a7e07138b305c784d,TODO: Add tests that cover all nodes.,https://github.com/google/tangent/commit/3b8a7adf13da0bfa22c5256a7e07138b305c784d,Yes
3415,google/tangent,tests/functions.py,3b8a7adf13da0bfa22c5256a7e07138b305c784d,TODO: split this into a bunch of tinier tests,https://github.com/google/tangent/commit/3b8a7adf13da0bfa22c5256a7e07138b305c784d,Yes
3416,google/tangent,tests/functions.py,3b8a7adf13da0bfa22c5256a7e07138b305c784d,TODO: Why doesn't test_forward see that shape is non-differentiable?,https://github.com/google/tangent/commit/3b8a7adf13da0bfa22c5256a7e07138b305c784d,Yes
3417,google/tangent,tests/test_forward_mode.py,3b8a7adf13da0bfa22c5256a7e07138b305c784d,TODO: remove trace test exemption when tests are consolidated.,https://github.com/google/tangent/commit/3b8a7adf13da0bfa22c5256a7e07138b305c784d,No
3418,google/tangent,tests/test_reverse_mode.py,3b8a7adf13da0bfa22c5256a7e07138b305c784d,TODO: Upgrade utils.py to allow simultaneous testing of uneven args.,https://github.com/google/tangent/commit/3b8a7adf13da0bfa22c5256a7e07138b305c784d,Yes
3419,IDSIA/sacred,sacred/config/config_summary.py,507e408e38c2d9e43e8a7dcd0747a5e4fd0bc589,TODO: test,https://github.com/IDSIA/sacred/commit/507e408e38c2d9e43e8a7dcd0747a5e4fd0bc589,No
3420,IDSIA/sacred,tests/test_utils.py,d88deb2555bb311eb779f81f22fe16dd3b703527,dirty hack to work around py.test also hijacking the stdout\/stderr,https://github.com/IDSIA/sacred/commit/d88deb2555bb311eb779f81f22fe16dd3b703527,Yes
3421,IDSIA/sacred,tests/test_utils.py,ccd7e02b47ad5d80880d6910a138be666a275107,dirty hack to work around py.test also hijacking the stdout\/stderr,https://github.com/IDSIA/sacred/commit/ccd7e02b47ad5d80880d6910a138be666a275107,Yes
3422,DistrictDataLabs/yellowbrick,yellowbrick/regressor.py,742f93774c7ffa12b73dadb4389ba5edeb0c0e86,TODO: make test size a parameter and do better data storage on viz.,https://github.com/DistrictDataLabs/yellowbrick/commit/742f93774c7ffa12b73dadb4389ba5edeb0c0e86,No
3423,DistrictDataLabs/yellowbrick,yellowbrick/regressor.py,ae2601fdbe8e08d54ab013b45c637aef83c23d4c,TODO Is there a better way to differentiate between train and test points?,https://github.com/DistrictDataLabs/yellowbrick/commit/ae2601fdbe8e08d54ab013b45c637aef83c23d4c,Yes
3424,DistrictDataLabs/yellowbrick,tests/test_regressor/test_alphas.py,6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,TODO: parametrize with models when unittest dependency removed,https://github.com/DistrictDataLabs/yellowbrick/commit/6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,No
3425,DistrictDataLabs/yellowbrick,tests/test_regressor/test_alphas.py,6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,TODO: parametrize with models when unittest dependency removed (new test case),https://github.com/DistrictDataLabs/yellowbrick/commit/6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,No
3426,DistrictDataLabs/yellowbrick,tests/test_features/test_rfecv.py,6e1099daee47e664f4b128c0f25af0177d6fca77,TODO: parametrize when unittest is removed,https://github.com/DistrictDataLabs/yellowbrick/commit/6e1099daee47e664f4b128c0f25af0177d6fca77,No
3427,DistrictDataLabs/yellowbrick,yellowbrick/classifier/confusion_matrix.py,870022ab7abce46f9bff47c8a9c787808a630e59,TODO: determine how to use quick methods that require train and test data.,https://github.com/DistrictDataLabs/yellowbrick/commit/870022ab7abce46f9bff47c8a9c787808a630e59,Yes
3428,DistrictDataLabs/yellowbrick,tests/conftest.py,913aea3ed811614e5ecb59c5aeaa667209ab0535,TODO: this is broken with pytest 4.2 (no attribute _genid),https://github.com/DistrictDataLabs/yellowbrick/commit/913aea3ed811614e5ecb59c5aeaa667209ab0535,Yes
3429,DistrictDataLabs/yellowbrick,tests/conftest.py,07e759791b8c771210c9ec1903f298024d482748,TODO: this is currently being reset before each test; needs fixing.,https://github.com/DistrictDataLabs/yellowbrick/commit/07e759791b8c771210c9ec1903f298024d482748,No
3430,DistrictDataLabs/yellowbrick,tests/test_target/test_class_balance.py,6c16c5e6dc9180f8c3e026b73cccdf94808e754e,TODO: convert to Pytest fixture,https://github.com/DistrictDataLabs/yellowbrick/commit/6c16c5e6dc9180f8c3e026b73cccdf94808e754e,No
3431,pyannote/pyannote-audio,pyannote/audio/embedding/models.py,0bebd031af729548b397de8cc61f31e1558fdca1,TODO - test with use_bias = False,https://github.com/pyannote/pyannote-audio/commit/0bebd031af729548b397de8cc61f31e1558fdca1,Yes
3432,pyannote/pyannote-audio,pyannote/audio/applications/speaker_embedding_keras.py,e638a4760c0519cac825d9661a3b5f0a928a507c,"\""\""\"" || Speaker embedding ||  || Usage: ||   pyannote-speaker-embedding data [--database=<db.yml> --duration=<duration> --step=<step> --heterogeneous] <root_dir> <database.task.protocol> ||   pyannote-speaker-embedding train [--subset=<subset> --start=<epoch> --end=<epoch>] <experiment_dir> <database.task.protocol> ||   pyannote-speaker-embedding validate [--subset=<subset> --from=<epoch> --to=<epoch> --every=<epoch>] <train_dir> <database.task.protocol> ||   pyannote-speaker-embedding apply [--database=<db.yml> --step=<step> --internal] <validate.txt> <database.task.protocol> <output_dir> ||   pyannote-speaker-embedding compare (<validate.txt> <legend>)... <output.png> ||   pyannote-speaker-embedding -h | --help ||   pyannote-speaker-embedding --version ||  || Options: ||   <root_dir>                 Set root directory. This script expects a ||                              configuration file called \""config.yml\"" to live in ||                              this directory. See '\""data\"" mode' section below ||                              for more details. ||   <database.task.protocol>   Set evaluation protocol (e.g. \""Etape.SpeakerDiarization.TV\"") ||   --database=<db.yml>        Path to database configuration file. ||                              [default: ~\/.pyannote\/db.yml] ||   --duration=<duration>      Set duration of embedded sequences [default: 3.2] ||   --step=<step>              Set step between sequences; in seconds. ||                              Defaults to 0.5 x <duration>. ||   --heterogeneous            Allow heterogeneous sequences. In this case; the ||                              label given to heterogeneous sequences is the most ||                              overlapping one. ||   --start=<epoch>            Restart training after that many epochs. ||   --end=<epoch>              Stop training after than many epochs [default: 1000] ||   <experiment_dir>           Set experiment directory. This script expects a ||                              configuration file called \""config.yml\"" to live ||                              in this directory. See '\""train\"" mode' section ||                              for more details. ||   --subset=<subset>          Set subset (train|developement|test). ||                              In \""train\"" mode; defaults subset is \""train\"". ||                              In \""validate\"" mode; defaults to \""development\"". ||   --every=<epoch>            Defaults to every epoch [default: 1]. ||   --from=<epoch>             Start at this epoch [default: 0]. ||   <train_dir>                Path to directory created by \""train\"" mode. ||   --internal                 Extract internal representation. ||   -h --help                  Show this screen. ||   --version                  Show version. ||  ||  || Database configuration file: ||     The database configuration provides details as to where actual files are ||     stored. See `pyannote.audio.util.FileFinder` docstring for more information ||     on the expected format. ||  || \""data\"" mode: ||  ||     A file called <root_dir>\/config.yml should exist; that describes the ||     feature extraction process (e.g. MFCCs): ||  ||     ................... <root_dir>\/config.yml ......................... ||     feature_extraction: ||        name: YaafeMFCC ||        params: ||           e: False                   # this experiments relies ||           De: True                   # on 11 MFCC coefficients ||           DDe: True                  # with 1st and 2nd derivatives ||           D: True                    # without energy; but with ||           DD: True                   # energy derivatives ||     ................................................................... ||  ||     Using \""data\"" mode will create the following directory that contains ||     the pre-computed sequences for train; development; and test subsets: ||  ||         <root_dir>\/<duration>+<step>\/sequences\/<database.task.protocol>.{train|development|test}.h5 ||  ||     This means that <duration>-long sequences were generated with a step of ||     <step> seconds; from the <database.task.protocol> protocol. This directory ||     is called <data_dir> in the subsequent modes. ||  || \""train\"" mode: ||  ||     The configuration of each experiment is described in a file called ||     <data_dir>\/<xp_id>\/config.yml; that describes the architecture of the ||     neural network; and the approach (e.g. triplet loss) used for training the ||     network: ||  ||     ................... <train_dir>\/config.yml ................... ||     architecture: ||        name: TristouNet ||        params: ||          lstm: [16] ||          mlp: [16; 16] ||          bidirectional: concat ||  ||     approach: ||        name: TripletLoss ||        params: ||          per_label: 2 ||          per_fold: 10 ||     ................................................................... ||  ||     Using \""train\"" mode will create the following directory that contains a ||     bunch of files including the pre-trained neural network weights after each ||     epoch: ||  ||         <data_dir>\/<xp_id>\/train\/<database.task.protocol>.<subset> ||  ||     This means that the network was trained using the <subset> subset of the ||     <database.task.protocol> protocol; using the configuration described in ||     <data_dir>\/<xp_id>\/config.yml. This directory  is called <train_dir> in the ||     subsequent modes. ||  || \""validate\"" mode: ||     Use the \""validate\"" mode to run validation in parallel to training. ||     \""validate\"" mode will watch the <train_dir> directory; and run validation ||     experiments every time a new epoch has ended. This will create the ||     following directory that contains validation results: ||  ||         <train_dir>\/validate\/<database.task.protocol> ||  ||     You can run multiple \""validate\"" in parallel (e.g. for every subset; ||     protocol; task; or database). || \""\""\""",https://github.com/pyannote/pyannote-audio/commit/e638a4760c0519cac825d9661a3b5f0a928a507c,Yes
3433,pyannote/pyannote-audio,pyannote/audio/embedding/extraction.py,a494415343ae6cfe3781a51d978b0876b98adc47,this test is needed because .apply() may be called,https://github.com/pyannote/pyannote-audio/commit/a494415343ae6cfe3781a51d978b0876b98adc47,Yes
3434,HDI-Project/ATM,atm/model.py,ceb3293b6ecbca72d84bb61bfc538cc11efc3611,load train and (maybe) test data,https://github.com/HDI-Project/ATM/commit/ceb3293b6ecbca72d84bb61bfc538cc11efc3611,No
3435,theislab/scanpy,scanpy/tools/difftest.py,c22e48abe45a6ccca5918bbf689637caa4b31250,"\""\""\"" || Differential Gene Expression Analysis || ===================================== ||  || From package Scanpy (https:\/\/github.com\/theislab\/scanpy). || Written in Python 3 (compatible with 2). || Copyright 2016-2017 F. Alexander Wolf (http:\/\/falexwolf.de). ||  || This is a Beta Version of a tool for differential gene expression testing || between sets detected in previous tools. Tools such as dpt; cluster;... || \""\""\""",https://github.com/theislab/scanpy/commit/c22e48abe45a6ccca5918bbf689637caa4b31250,No
3436,theislab/scanpy,scanpy/tests/ann_data.py,60f962031e4c65bbe3a683104b3a7941247c2ed1,we don\u2019t need this in requirements.txt; as it\u2019s only needed for testing,https://github.com/theislab/scanpy/commit/60f962031e4c65bbe3a683104b3a7941247c2ed1,No
3437,theislab/scanpy,scanpy/tests/ann_data.py,60f962031e4c65bbe3a683104b3a7941247c2ed1,TODO: remove logging and actually test values,https://github.com/theislab/scanpy/commit/60f962031e4c65bbe3a683104b3a7941247c2ed1,Yes
3438,theislab/scanpy,scanpy/tools/rank_genes_groups.py,0d2bcff2c75cb9f08908a63041ace338315837b7,TODO: Adapt logging such that test-type is included (here or at a later point ?,https://github.com/theislab/scanpy/commit/0d2bcff2c75cb9f08908a63041ace338315837b7,No
3439,theislab/scanpy,scanpy/tests/rank_genes_group.py,04991c45eca46f718378a433c5bbb384b3e531c4,we don\u2019t need this in requirements.txt; as it\u2019s only needed for testing,https://github.com/theislab/scanpy/commit/04991c45eca46f718378a433c5bbb384b3e531c4,No
3440,theislab/scanpy,scanpy/tests/rank_genes_group.py,04991c45eca46f718378a433c5bbb384b3e531c4,TODO: Useful tests:,https://github.com/theislab/scanpy/commit/04991c45eca46f718378a433c5bbb384b3e531c4,No
3441,theislab/scanpy,scanpy/tests/preprocessing.py,f6a41f140a646c350ab12d8bd6aeff7499df069e,we don\u2019t need this in requirements.txt; as it\u2019s only needed for testing,https://github.com/theislab/scanpy/commit/f6a41f140a646c350ab12d8bd6aeff7499df069e,No
3442,theislab/scanpy,scanpy/tests/score_genes.py,cccda28fcb32ff7205322bb43f0c340b267a6d9a,TODO: write a test that costs less resources and is more meaningful,https://github.com/theislab/scanpy/commit/cccda28fcb32ff7205322bb43f0c340b267a6d9a,Yes
3443,theislab/scanpy,scanpy/utils.py,e0ba43f85537ba7e516de294c27947a8ad6f0abb,NOTE: pytest does not correctly retrieve anndata's version? why?,https://github.com/theislab/scanpy/commit/e0ba43f85537ba7e516de294c27947a8ad6f0abb,No
3444,theislab/scanpy,scanpy/tests/test_read_10x.py,dfc47d7c39485a0594b1a8e5871310b3f43aa0a3,TODO: What is the purpose of this test?,https://github.com/theislab/scanpy/commit/dfc47d7c39485a0594b1a8e5871310b3f43aa0a3,No
3445,theislab/scanpy,scanpy/tests/test_combat.py,a668296c77fa9ad64b82d5d2fb94a199ca959e7d,Test for fix to #1170,https://github.com/theislab/scanpy/commit/a668296c77fa9ad64b82d5d2fb94a199ca959e7d,Yes
3446,theislab/scanpy,scanpy/tests/test_plotting.py,6ab348d954c8900cb4a5e49158f19431a2cd8bd7,TODO: Generalize test to more plotting types,https://github.com/theislab/scanpy/commit/6ab348d954c8900cb4a5e49158f19431a2cd8bd7,No
3447,theislab/scanpy,scanpy/preprocessing/_simple.py,d69aa1862f77db07d8aee64303fa6576701850af,TODO: figure out how to test that this doesn't oversubscribe resources,https://github.com/theislab/scanpy/commit/d69aa1862f77db07d8aee64303fa6576701850af,No
3448,BYU-PCCL/holodeck,holodeck/tests/testenvironments.py,7c6c6dde5601dd7645d8497865c4cf3e6e3c141f,"\""\""\"" ||     Future Tests to Implement: ||      ||         Test to be run when releasing a new version: ||          ||         Pixel camera images can be retrieved properly and the sizes can be adjusted. ||         Do the package manager functions work properly ||         Test that multi agents work properly ||         Test that the tasks work properly for the packaged worlds. ||         Hyper parameters can be set correctly ||         Sensors sense correctly ||         Each agent in packaged worlds have all the sensors they should ||         -Take screenshots at the beginning of each world as well sensor data and  ||         ensure that all the numbers and images line up. We don't want these changing between releases. ||  ||  || \""\""\""",https://github.com/BYU-PCCL/holodeck/commit/7c6c6dde5601dd7645d8497865c4cf3e6e3c141f,Yes
3449,BYU-PCCL/holodeck,tests/sensors/test_velocity_sensor.py,39df411b4ffa005e2ad1bf64b6c328de31719b28,TODO: Test other axises,https://github.com/BYU-PCCL/holodeck/commit/39df411b4ffa005e2ad1bf64b6c328de31719b28,Yes
3450,BYU-PCCL/holodeck,tests/sensors/test_cup_game_sensors.py,c140d8e69c9a1de572b374a10fafb3243bac689a,TODO: Test other axises,https://github.com/BYU-PCCL/holodeck/commit/c140d8e69c9a1de572b374a10fafb3243bac689a,Yes
3451,jupyterhub/zero-to-jupyterhub-k8s,tests/test_hub.py,030f8ad10162c13160676f6194567b071c29a9b1,FIXME: It would be good to test use against custom databases; then we,https://github.com/jupyterhub/zero-to-jupyterhub-k8s/commit/030f8ad10162c13160676f6194567b071c29a9b1,Yes
3452,catalyst-team/catalyst,data/sampler.py,a2c5700a9f2b30b951e5a5de2e4a61d7616e5c0a,@TODO: test,https://github.com/catalyst-team/catalyst/commit/a2c5700a9f2b30b951e5a5de2e4a61d7616e5c0a,No
3453,catalyst-team/catalyst,catalyst/contrib/nn/tests/test_optimizer.py,0eb02233ed05cfdf65cd100edae9dbf8ea058852,@TODO: add test for SparseAdam,https://github.com/catalyst-team/catalyst/commit/0eb02233ed05cfdf65cd100edae9dbf8ea058852,Yes
3454,tensorly/tensorly,doc/sphinx_ext/sphinx_gallery/gen_gallery.py,0967e0cdd01304c234121ad89babb2ffccd2f4d2,HACK: Stop nosetests running setup() above,https://github.com/tensorly/tensorly/commit/0967e0cdd01304c234121ad89babb2ffccd2f4d2,Yes
3455,tensorly/tensorly,doc/sphinx_ext/sphinx_gallery/gen_rst.py,0967e0cdd01304c234121ad89babb2ffccd2f4d2,XXX This check can break during testing e.g. if you uncomment the,https://github.com/tensorly/tensorly/commit/0967e0cdd01304c234121ad89babb2ffccd2f4d2,Yes
3456,tensorly/tensorly,doc/sphinx_ext/sphinx_gallery/tests/test_gen_rst.py,959c98c541ba340efccac60ab06e0f1193eda17a,TODO: test that broken thumbnail does appear when needed,https://github.com/tensorly/tensorly/commit/959c98c541ba340efccac60ab06e0f1193eda17a,No
3457,tensorly/tensorly,doc/sphinx_ext/sphinx_gallery/tests/test_gen_rst.py,959c98c541ba340efccac60ab06e0f1193eda17a,TODO: test that examples are not executed twice,https://github.com/tensorly/tensorly/commit/959c98c541ba340efccac60ab06e0f1193eda17a,No
3458,tensorly/tensorly,doc/sphinx_ext/sphinx_gallery/tests/test_gen_rst.py,959c98c541ba340efccac60ab06e0f1193eda17a,TODO: test that examples are executed after a no-plot and produce,https://github.com/tensorly/tensorly/commit/959c98c541ba340efccac60ab06e0f1193eda17a,No
3459,tensorly/tensorly,tensorly/decomposition/candecomp_parafac.py,2bb16910085d0dff035742c199234c3eabb260e5,TODO: Test this,https://github.com/tensorly/tensorly/commit/2bb16910085d0dff035742c199234c3eabb260e5,No
3460,tensorly/tensorly,tensorly/decomposition/parafac2.py,3ad0c9f8879928e6e11a3758d3020af34c03132d,TODO: Test this,https://github.com/tensorly/tensorly/commit/3ad0c9f8879928e6e11a3758d3020af34c03132d,No
3461,tensorly/tensorly,tensorly/decomposition/candecomp_parafac.py,a2a436649c9dea5dd9147a1b8de6e7030d4cabbf,TODO: Test this,https://github.com/tensorly/tensorly/commit/a2a436649c9dea5dd9147a1b8de6e7030d4cabbf,No
3462,tensorly/tensorly,tensorly/decomposition/candecomp_parafac.py,9c35ffc9aeab8b539826b471fa273e70e80c6854,TODO: Test this,https://github.com/tensorly/tensorly/commit/9c35ffc9aeab8b539826b471fa273e70e80c6854,No
3463,tensorly/tensorly,tensorly/decomposition/candecomp_parafac.py,e96f53707bc60315042b2b9824c8247827e1f737,TODO: Test this,https://github.com/tensorly/tensorly/commit/e96f53707bc60315042b2b9824c8247827e1f737,No
3464,tensorly/tensorly,tensorly/decomposition/candecomp_parafac.py,c224dd0461142555845c096db3e66f4008a61fc2,TODO: Test this,https://github.com/tensorly/tensorly/commit/c224dd0461142555845c096db3e66f4008a61fc2,No
3465,tensorly/tensorly,tensorly/decomposition/candecomp_parafac.py,b479f18d9b1658f9e849538dce1fd87931d2017a,TODO: Test this,https://github.com/tensorly/tensorly/commit/b479f18d9b1658f9e849538dce1fd87931d2017a,No
3466,tensorly/tensorly,tensorly/decomposition/_cp.py,580b11d830479aec075ba0ca873509818432bc2d,TODO: Test this,https://github.com/tensorly/tensorly/commit/580b11d830479aec075ba0ca873509818432bc2d,No
3467,tensorly/tensorly,tensorly/decomposition/_nn_cp.py,cfdfb3356a0b513a60b7c5dfa04561f78efc5016,TODO: Test this,https://github.com/tensorly/tensorly/commit/cfdfb3356a0b513a60b7c5dfa04561f78efc5016,No
3468,jupyter/nbgrader,nbgrader/tests/test_nbgrader_formgrade_hubauth.py,f9f47a1ea6dbb25191d62f2511c0d493cda1d013,this test name is a hack to make sure it is always run first; because,https://github.com/jupyter/nbgrader/commit/f9f47a1ea6dbb25191d62f2511c0d493cda1d013,No
3469,jupyter/nbgrader,nbgrader/tests/apps/test_nbgrader_removehidden.py,522f5b2dc4b1c3720a521d1ee59cdcad53a87cc8,TODO: understand how to access the TransferApp settings; like in the collect test?,https://github.com/jupyter/nbgrader/commit/522f5b2dc4b1c3720a521d1ee59cdcad53a87cc8,Yes
3470,jupyter/nbgrader,nbgrader/tests/nbextensions/test_course_list.py,f248290fa539b09d7ff805d5407f15231da50353,TODO: add a test case where jupyterhub is running; and a test case where a,https://github.com/jupyter/nbgrader/commit/f248290fa539b09d7ff805d5407f15231da50353,Yes
3471,probcomp/bayeslite,tests/test_smoke.py,5eb4c0cfde43ce5c42f0ca77da276093b314708a,XXX Also too few columns for this test.,https://github.com/probcomp/bayeslite/commit/5eb4c0cfde43ce5c42f0ca77da276093b314708a,No
3472,probcomp/bayeslite,tests/test_smoke.py,a25284daeb1873c20445b8953024333590ac00bb,XXX Automatically test the correct exception.,https://github.com/probcomp/bayeslite/commit/a25284daeb1873c20445b8953024333590ac00bb,Yes
3473,probcomp/bayeslite,tests/test_smoke.py,5960c7e95bd6cbca62afa52d8f503f727f4cf2df,XXX Also too few columns for this test.,https://github.com/probcomp/bayeslite/commit/5960c7e95bd6cbca62afa52d8f503f727f4cf2df,No
3474,probcomp/bayeslite,tests/test_parse.py,60ad797320c99fff9ecb59b1a9eb8f116847a8d3,XXX Should really be this test; but getting the grammar to,https://github.com/probcomp/bayeslite/commit/60ad797320c99fff9ecb59b1a9eb8f116847a8d3,Yes
3475,probcomp/bayeslite,tests/test_bql.py,409fd96e312e82133ebbd7ded9fb298883e49ee8,XXX Test what query this actually executes...,https://github.com/probcomp/bayeslite/commit/409fd96e312e82133ebbd7ded9fb298883e49ee8,No
3476,probcomp/bayeslite,tests/test_core.py,c207b2f6c966d43a21ad35b84835bb6bf51e3cae,FIXME (2\/17\/2015): All tests pass w\/ multiprocessing engine; but they take,https://github.com/probcomp/bayeslite/commit/c207b2f6c966d43a21ad35b84835bb6bf51e3cae,Yes
3477,probcomp/bayeslite,tests/test_legacy.py,bae3f8793ffe2877ee9900937e4decfa94e381b5,XXX This is turning into more than just a test of legacy models...,https://github.com/probcomp/bayeslite/commit/bae3f8793ffe2877ee9900937e4decfa94e381b5,No
3478,probcomp/bayeslite,tests/test_csv.py,31c232a1db143769ec0e3d172f98841f63a63c66,XXX Test the automatic column type guessing too.,https://github.com/probcomp/bayeslite/commit/31c232a1db143769ec0e3d172f98841f63a63c66,No
3479,probcomp/bayeslite,tests/test_bql.py,37bf9cb4bb3b1449f7bac1aa2b2e3583c04a7b7b,XXX Test to make sure TEMP is passed through; and the table,https://github.com/probcomp/bayeslite/commit/37bf9cb4bb3b1449f7bac1aa2b2e3583c04a7b7b,No
3480,probcomp/bayeslite,tests/test_csv.py,24369f4920d0649060f8f514f312e1958e2a0715,XXX Currently this test fails because we compile the query,https://github.com/probcomp/bayeslite/commit/24369f4920d0649060f8f514f312e1958e2a0715,Yes
3481,probcomp/bayeslite,tests/test_core.py,5ba3581c4ff4584d56ef45ffbd9b94560d23b658,XXX Rest of test originally exercised default metamodel; but,https://github.com/probcomp/bayeslite/commit/5ba3581c4ff4584d56ef45ffbd9b94560d23b658,Yes
3482,probcomp/bayeslite,tests/test_stats.py,943da0020c5633f03f10aeaca134167647a43b0c,XXX Why are we testing chi2_sf here?,https://github.com/probcomp/bayeslite/commit/943da0020c5633f03f10aeaca134167647a43b0c,No
3483,probcomp/bayeslite,setup.py,e0c9dda403c758ba7a1737dbeae8b1f9d048fc6e,XXX Several horrible kludges here to make `python setup.py test' work:,https://github.com/probcomp/bayeslite/commit/e0c9dda403c758ba7a1737dbeae8b1f9d048fc6e,No
3484,probcomp/bayeslite,tests/test_simulate.py,e196027bd480a57245d5421ab7bcf029db1478e3,XXX This should be a metamodel-independent test.,https://github.com/probcomp/bayeslite/commit/e196027bd480a57245d5421ab7bcf029db1478e3,Yes
3485,probcomp/bayeslite,tests/test_bql.py,0124d878afd8d277a35a805c86435a0720f08988,XXX Test other metamodels too; because they have a role in ensuring that,https://github.com/probcomp/bayeslite/commit/0124d878afd8d277a35a805c86435a0720f08988,Yes
3486,probcomp/bayeslite,tests/test_cgpm.py,859b4b9e5741ab58286ea0621cd3cd40b8455fe7,XXX KLUDGE TAKEN FROM cgpm\/tests\/test_gpmcc_simple_composite.py,https://github.com/probcomp/bayeslite/commit/859b4b9e5741ab58286ea0621cd3cd40b8455fe7,No
3487,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpLit,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,No
3488,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpNumpar,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,Yes
3489,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpNampar,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,No
3490,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpCol,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,Yes
3491,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpSub,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,No
3492,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpCollate,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,Yes
3493,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpIn,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,Yes
3494,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpCast,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,Yes
3495,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpExists,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,Yes
3496,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpApp,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,Yes
3497,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpAppStar,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,No
3498,probcomp/bayeslite,tests/test_macro.py,29902742a57cb633dee9b258ba84cf915756cf5f,XXX test descent into ExpCase,https://github.com/probcomp/bayeslite/commit/29902742a57cb633dee9b258ba84cf915756cf5f,No
3499,probcomp/bayeslite,tests/test_bql.py,d96e793af4d819bf9043c6c634657caf37cb85ed,XXX This test is a little unsatisfactory -- we do not get to see,https://github.com/probcomp/bayeslite/commit/d96e793af4d819bf9043c6c634657caf37cb85ed,Yes
3500,probcomp/bayeslite,tests/test_loom_metamodel.py,636014fbab895248d46f0903fc418c7c16d826b1,TODO fix fail when two tests are run with the same prefix,https://github.com/probcomp/bayeslite/commit/636014fbab895248d46f0903fc418c7c16d826b1,Yes
3501,probcomp/bayeslite,tests/test_cmi.py,9a7a27ad5855f89987a98deac770909820759941,XXX: for normal unit tests; I'd have one function per assert here. But since,https://github.com/probcomp/bayeslite/commit/9a7a27ad5855f89987a98deac770909820759941,Yes
3502,konlpy/konlpy,test/test_stream_google_trend.py,82368b6f59daaede482368b15d1331e3c1cd9ea5,FIXME: init_date\uAC00 \uACE0\uC815\uB418\uC5B4 \uC2DC\uAC04\uC774 \uC9C0\uB0AC\uC744 \uB54C test\uC5D0 \uC2E4\uD328\uD560 \uAC00\uB2A5\uC131\uC774 \uC788\uC74C.,https://github.com/konlpy/konlpy/commit/82368b6f59daaede482368b15d1331e3c1cd9ea5,No
3503,DragonComputer/Dragonfire,dragonfire/conversational/__init__.py,1b000c167a7d08dafa0673308ded2f494c5a12fa,TODO: Log the questions asked for latter re-use (merge with test\/samples.txt),https://github.com/DragonComputer/Dragonfire/commit/1b000c167a7d08dafa0673308ded2f494c5a12fa,Yes
3504,mne-tools/mne-python,mne/tests/test_forward.py,5c7c9935e4fefe775b4a4e4adc24fa8b7589a97b,XXX : test something,https://github.com/mne-tools/mne-python/commit/5c7c9935e4fefe775b4a4e4adc24fa8b7589a97b,Yes
3505,mne-tools/mne-python,mne/tests/test_cov.py,12b029ae872ad3a06118ec3299147093a5a18ef2,XXX : test something,https://github.com/mne-tools/mne-python/commit/12b029ae872ad3a06118ec3299147093a5a18ef2,Yes
3506,mne-tools/mne-python,mne/tests/test_source_space.py,12b029ae872ad3a06118ec3299147093a5a18ef2,XXX : test something,https://github.com/mne-tools/mne-python/commit/12b029ae872ad3a06118ec3299147093a5a18ef2,Yes
3507,mne-tools/mne-python,mne/tests/test_cov.py,9f006784ab3e6bec45f31a0efa42a466a96ae08a,XXX : test something,https://github.com/mne-tools/mne-python/commit/9f006784ab3e6bec45f31a0efa42a466a96ae08a,Yes
3508,mne-tools/mne-python,mne/tests/test_inverse.py,eb67b7adc3f21436a86b904c18167ad7585e1400,XXX : test something,https://github.com/mne-tools/mne-python/commit/eb67b7adc3f21436a86b904c18167ad7585e1400,Yes
3509,mne-tools/mne-python,mne/minimum_norm/tests/test_inverse.py,35c7ba14cb0fba1b0933217b9bd07940ad75896d,XXX : test something,https://github.com/mne-tools/mne-python/commit/35c7ba14cb0fba1b0933217b9bd07940ad75896d,Yes
3510,mne-tools/mne-python,mne/fiff/tests/test_raw.py,decb721537c486b62aa1f6077cd3ac95086ba73d,XXX what to test?,https://github.com/mne-tools/mne-python/commit/decb721537c486b62aa1f6077cd3ac95086ba73d,Yes
3511,mne-tools/mne-python,mne/tests/test_proj.py,93074a19ada9dfe7f5155110f2ed34ab91931f5f,XXX : test something,https://github.com/mne-tools/mne-python/commit/93074a19ada9dfe7f5155110f2ed34ab91931f5f,Yes
3512,mne-tools/mne-python,mne/beamformer/tests/test_lcmv.py,db09465a5d83070d4aa1ef8c0dc8788fbeb8f3d9,TODO: test more things,https://github.com/mne-tools/mne-python/commit/db09465a5d83070d4aa1ef8c0dc8788fbeb8f3d9,No
3513,mne-tools/mne-python,mne/stats/cluster_level.py,a8e8738b698874c01a2004a20f645b655683323d,XXX need to add code to make it a full perm test when possible,https://github.com/mne-tools/mne-python/commit/a8e8738b698874c01a2004a20f645b655683323d,Yes
3514,mne-tools/mne-python,mne/tests/test_source_estimate.py,ee07eb147e5f7fb562b98aacc50447ff868c0d5b,XXX Should design a fool-proof test case; but here were the results:,https://github.com/mne-tools/mne-python/commit/ee07eb147e5f7fb562b98aacc50447ff868c0d5b,No
3515,mne-tools/mne-python,mne/preprocessing/tests/test_ssp.py,63f76822772722d38dfd131126ab51fa35fb5be0,XXX: better tests,https://github.com/mne-tools/mne-python/commit/63f76822772722d38dfd131126ab51fa35fb5be0,Yes
3516,mne-tools/mne-python,mne/tests/test_forward.py,ed3d9bee466e632c08d0b7fbeb133a41a258f8c7,the transform I\/O reduces our accuracy -- so we'll just hack a test here,https://github.com/mne-tools/mne-python/commit/ed3d9bee466e632c08d0b7fbeb133a41a258f8c7,Yes
3517,mne-tools/mne-python,mne/tests/test_epochs.py,6431c81671840a95fc45371f00edf20fe204b573,should probably add test + functionality for non-replacement XXX,https://github.com/mne-tools/mne-python/commit/6431c81671840a95fc45371f00edf20fe204b573,No
3518,mne-tools/mne-python,mne/tests/test_epochs.py,74a75c9ffcb7df812e8764b4ff00956f67ce8d80,should probably add test + functionality for non-replacement XXX,https://github.com/mne-tools/mne-python/commit/74a75c9ffcb7df812e8764b4ff00956f67ce8d80,No
3519,mne-tools/mne-python,mne/tests/test_epochs.py,99a6358243761a9f242fed7fef3740b66d48da52,should probably add test + functionality for non-replacement XXX,https://github.com/mne-tools/mne-python/commit/99a6358243761a9f242fed7fef3740b66d48da52,No
3520,mne-tools/mne-python,mne/beamformer/tests/test_dics.py,f44e4fe8fe7f66258241c77680b2bbf58a6f7d0a,TODO: dics_epochs would best be tested by comparing to dics done on,https://github.com/mne-tools/mne-python/commit/f44e4fe8fe7f66258241c77680b2bbf58a6f7d0a,Yes
3521,mne-tools/mne-python,mne/beamformer/tests/test_dics.py,1e72ef318547a9e5789af78efd646ea350260144,TODO: The results still have to be tested for whether they make sense;,https://github.com/mne-tools/mne-python/commit/1e72ef318547a9e5789af78efd646ea350260144,Yes
3522,mne-tools/mne-python,mne/beamformer/tests/test_dics.py,ed38454f057e3801007dc5d5d272d7ca73ca5427,TODO: This test doesn't work because the relevant code has been commented,https://github.com/mne-tools/mne-python/commit/ed38454f057e3801007dc5d5d272d7ca73ca5427,No
3523,mne-tools/mne-python,mne/forward/tests/test_do_forward.py,dfd7708e1bb4eed4884198789d586407dfbfe81d,the transform I\/O reduces our accuracy -- so we'll just hack a test here,https://github.com/mne-tools/mne-python/commit/dfd7708e1bb4eed4884198789d586407dfbfe81d,Yes
3524,mne-tools/mne-python,mne/tests/test_viz.py,d278fe0d22d805a4a8e59cf5cf565614101d4993,test mouse clicks (XXX not complete yet),https://github.com/mne-tools/mne-python/commit/d278fe0d22d805a4a8e59cf5cf565614101d4993,Yes
3525,mne-tools/mne-python,mne/tests/test_viz.py,c68ec9574ccd78418e11d7a9b577b40d1933d8fd,test mouse clicks (XXX not complete yet),https://github.com/mne-tools/mne-python/commit/c68ec9574ccd78418e11d7a9b577b40d1933d8fd,Yes
3526,mne-tools/mne-python,mne/fiff/edf/tests/test_edf.py,9e5cec5f5b5bb978beb4277ab14b95a2164b50d7,TODO: meaningful tests,https://github.com/mne-tools/mne-python/commit/9e5cec5f5b5bb978beb4277ab14b95a2164b50d7,No
3527,mne-tools/mne-python,mne/fiff/edf/tests/test_edf.py,8429cc242aea5259ccb016bd5a4c65a757f2bd1b,TODO: meaningful tests,https://github.com/mne-tools/mne-python/commit/8429cc242aea5259ccb016bd5a4c65a757f2bd1b,No
3528,mne-tools/mne-python,mne/evoked.py,cb44cd65ba139ad3f91f848d3befc5be339f42f2,XXX: this should use round and be tested,https://github.com/mne-tools/mne-python/commit/cb44cd65ba139ad3f91f848d3befc5be339f42f2,No
3529,mne-tools/mne-python,mne/tests/test_label.py,1998ccc49fb5d4ce8b1fa466f24c9eefc812a082,XXX : this was added for backward compat and keep the old test_label_in_src,https://github.com/mne-tools/mne-python/commit/1998ccc49fb5d4ce8b1fa466f24c9eefc812a082,Yes
3530,mne-tools/mne-python,mne/simulation/tests/test_source.py,cce5d7837dcb3304001079190d28e387cc61915d,"XXX Not using \""testing\"" dataset here b\/c labels don't contain source verts",https://github.com/mne-tools/mne-python/commit/cce5d7837dcb3304001079190d28e387cc61915d,No
3531,mne-tools/mne-python,mne/viz/tests/test_evoked.py,516957766051f5948c490d4d83652dc98a774813,Hack to test plotting of maxfiltered data,https://github.com/mne-tools/mne-python/commit/516957766051f5948c490d4d83652dc98a774813,Yes
3532,mne-tools/mne-python,doc/sphinxext/gen_rst.py,095cf8755962ff8ce4735630477300afe3274b6c,HACK: Stop nosetests running setup() above,https://github.com/mne-tools/mne-python/commit/095cf8755962ff8ce4735630477300afe3274b6c,Yes
3533,mne-tools/mne-python,doc/sphinxext/flow_diagram.py,cd2a16d693513e85b3e52232358fa101c327b3d2,HACK: Stop nosetests running setup() above,https://github.com/mne-tools/mne-python/commit/cd2a16d693513e85b3e52232358fa101c327b3d2,Yes
3534,mne-tools/mne-python,mne/decoding/tests/test_time_gen.py,0fd71fa03ae2d03922ed7780ad55e3f8d9cbbe5b,XXX Test parallelization ?,https://github.com/mne-tools/mne-python/commit/0fd71fa03ae2d03922ed7780ad55e3f8d9cbbe5b,Yes
3535,mne-tools/mne-python,mne/decoding/time_gen.py,b07fc366dd6a560fb2bbaec5ceaed8a5e79e7785,XXX actually the test seemed wrong and obsolete (D.E.),https://github.com/mne-tools/mne-python/commit/b07fc366dd6a560fb2bbaec5ceaed8a5e79e7785,No
3536,mne-tools/mne-python,mne/decoding/time_gen.py,d66c9f65e447a7e62fd1eb0b2f4b1c16bf578c73,XXX branching fixes test for binary cases.,https://github.com/mne-tools/mne-python/commit/d66c9f65e447a7e62fd1eb0b2f4b1c16bf578c73,Yes
3537,mne-tools/mne-python,mne/decoding/tests/test_time_gen.py,ba01204925ec4a898f7cbdb67af9281d8c6f8ac8,XXX use case and test seem unclear to me (D.E.),https://github.com/mne-tools/mne-python/commit/ba01204925ec4a898f7cbdb67af9281d8c6f8ac8,No
3538,mne-tools/mne-python,mne/viz/decoding.py,ab8819c2826aa3bc655bb067c9d2afe27c782fac,XXX actually the test seemed wrong and obsolete (D.E.),https://github.com/mne-tools/mne-python/commit/ab8819c2826aa3bc655bb067c9d2afe27c782fac,No
3539,mne-tools/mne-python,mne/decoding/time_gen.py,e1d09e7463f4fdb54074bc1008222354f3a377ab,XXX JRK: Should this test whether test_times if a DecodingTime,https://github.com/mne-tools/mne-python/commit/e1d09e7463f4fdb54074bc1008222354f3a377ab,Yes
3540,mne-tools/mne-python,mne/preprocessing/tests/test_ssp.py,df7877b540ef54f61f863bf35efe666be3915412,XXX: better tests,https://github.com/mne-tools/mne-python/commit/df7877b540ef54f61f863bf35efe666be3915412,Yes
3541,mne-tools/mne-python,mne/beamformer/tests/test_dics.py,682aff3754dd50cc93550a097e71143f53a131fd,bit of a hack to deal with old scipy\/numpy throwing warnings in tests,https://github.com/mne-tools/mne-python/commit/682aff3754dd50cc93550a097e71143f53a131fd,Yes
3542,mne-tools/mne-python,mne/utils.py,682aff3754dd50cc93550a097e71143f53a131fd,hack to deal with old scipy\/numpy in tests,https://github.com/mne-tools/mne-python/commit/682aff3754dd50cc93550a097e71143f53a131fd,Yes
3543,mne-tools/mne-python,mne/minimum_norm/tests/test_inverse.py,373218d3dfc2173b176cfeee54ccd10bf6de33fd,trans and bem needed for channel reordering tests incl. forward computation,https://github.com/mne-tools/mne-python/commit/373218d3dfc2173b176cfeee54ccd10bf6de33fd,No
3544,mne-tools/mne-python,mne/preprocessing/tests/test_maxwell.py,1d3b8e50c1e577427cb05e4fa2fe085ba4eaa3df,TODO: Future tests integrate with mne\/io\/tests\/test_proc_history,https://github.com/mne-tools/mne-python/commit/1d3b8e50c1e577427cb05e4fa2fe085ba4eaa3df,Yes
3545,mne-tools/mne-python,mne/preprocessing/tests/test_maxwell.py,ac15af8201e7e0c0f0b6e7b6c1f4f81917c1bbcc,TODO: Future tests integrate with mne\/io\/tests\/test_proc_history,https://github.com/mne-tools/mne-python/commit/ac15af8201e7e0c0f0b6e7b6c1f4f81917c1bbcc,Yes
3546,mne-tools/mne-python,mne/preprocessing/tests/test_maxwell.py,15a4f1799ffbb4bad4c24b38768f3fe09d1ca1bd,TODO: Future tests integrate with mne\/io\/tests\/test_proc_history,https://github.com/mne-tools/mne-python/commit/15a4f1799ffbb4bad4c24b38768f3fe09d1ca1bd,Yes
3547,mne-tools/mne-python,mne/preprocessing/tests/test_maxwell.py,15a4f1799ffbb4bad4c24b38768f3fe09d1ca1bd,Test covariance calculation XXX add this,https://github.com/mne-tools/mne-python/commit/15a4f1799ffbb4bad4c24b38768f3fe09d1ca1bd,Yes
3548,mne-tools/mne-python,mne/decoding/time_gen.py,925be394cfae4efb8ab6d7a282bddf59aafe1cc0,GAT compatibility unsqueeze testing times # XXX JRK: need cleanup,https://github.com/mne-tools/mne-python/commit/925be394cfae4efb8ab6d7a282bddf59aafe1cc0,Yes
3549,mne-tools/mne-python,mne/simulation/tests/test_raw.py,ff08f5644bb5ba2a1b3e2b8b8a8dc33db340c38e,TODO: Eventually; add ECG channels. Testing data raw file doesn't contain,https://github.com/mne-tools/mne-python/commit/ff08f5644bb5ba2a1b3e2b8b8a8dc33db340c38e,No
3550,mne-tools/mne-python,mne/simulation/tests/test_raw.py,dd287a42e6750e216444b612a9cc7adb0e2773d8,TODO: Eventually; add ECG channels. Testing data raw file doesn't contain,https://github.com/mne-tools/mne-python/commit/dd287a42e6750e216444b612a9cc7adb0e2773d8,No
3551,mne-tools/mne-python,mne/decoding/base.py,ef2ca5bdebd29944243e423466633d3fb8833160,XXX: should we rather test if instance of estimator?,https://github.com/mne-tools/mne-python/commit/ef2ca5bdebd29944243e423466633d3fb8833160,No
3552,mne-tools/mne-python,doc/sphinxext/commands.py,8ddd669a04fc9e91b357d8435511332061300f35,HACK: Stop nosetests running setup() above,https://github.com/mne-tools/mne-python/commit/8ddd669a04fc9e91b357d8435511332061300f35,Yes
3553,mne-tools/mne-python,mne/simulation/raw.py,5035d99e70f260bd99c739be6d35c18c9b98074a,XXX remove below conditional when done with testing,https://github.com/mne-tools/mne-python/commit/5035d99e70f260bd99c739be6d35c18c9b98074a,Yes
3554,mne-tools/mne-python,mne/simulation/tests/test_raw.py,851e852eaf553a52416b5439328bd12cd742c590,XXX test that EEG-only; MEG-only both work; along with ecg\/eog options,https://github.com/mne-tools/mne-python/commit/851e852eaf553a52416b5439328bd12cd742c590,No
3555,mne-tools/mne-python,mne/simulation/tests/test_raw.py,025a5a89f763a407bc844061fa0fccfcb585d631,XXX test that EEG-only; MEG-only both work; along with ecg\/eog options,https://github.com/mne-tools/mne-python/commit/025a5a89f763a407bc844061fa0fccfcb585d631,No
3556,mne-tools/mne-python,mne/simulation/tests/test_raw.py,26e4173ad68e740d7f3bde35b6c1a244b27c313d,XXX we need to test that the cHPI signals are actually in the correct,https://github.com/mne-tools/mne-python/commit/26e4173ad68e740d7f3bde35b6c1a244b27c313d,No
3557,mne-tools/mne-python,mne/preprocessing/tests/test_maxwell.py,580d6a49701288e4d40073548c9fb89025eb32a7,TODO: Eventually add simulation tests mirroring Taulu's original papers,https://github.com/mne-tools/mne-python/commit/580d6a49701288e4d40073548c9fb89025eb32a7,Yes
3558,mne-tools/mne-python,mne/preprocessing/tests/test_maxwell.py,e4406bfb86ec971819edbbfca5ac9f17a9132eb2,very low SNR as proc differs; eventually we should add a better test,https://github.com/mne-tools/mne-python/commit/e4406bfb86ec971819edbbfca5ac9f17a9132eb2,Yes
3559,mne-tools/mne-python,mne/io/ctf/ctf.py,9d880674e4b271c352075e4a2c0a756324d6ca27,XXX WE NEED TO TEST THIS!,https://github.com/mne-tools/mne-python/commit/9d880674e4b271c352075e4a2c0a756324d6ca27,Yes
3560,mne-tools/mne-python,mne/preprocessing/tests/test_maxwell.py,7285a2f9ed3ba977347799af253b7bcd39921697,very low SNR as proc differs; eventually we should add a better test,https://github.com/mne-tools/mne-python/commit/7285a2f9ed3ba977347799af253b7bcd39921697,Yes
3561,mne-tools/mne-python,mne/preprocessing/tests/test_maxwell.py,7285a2f9ed3ba977347799af253b7bcd39921697,TODO: Eventually add simulation tests mirroring Taulu's original paper,https://github.com/mne-tools/mne-python/commit/7285a2f9ed3ba977347799af253b7bcd39921697,Yes
3562,mne-tools/mne-python,mne/decoding/tests/test_time_gen.py,dabf0984963905bb1dcd09a2df766b39655a0510,TODO JRK: test GAT with non-exhaustive CV (eg. train on 80%; test on 10%),https://github.com/mne-tools/mne-python/commit/dabf0984963905bb1dcd09a2df766b39655a0510,No
3563,mne-tools/mne-python,mne/decoding/tests/test_time_gen.py,f450fd4872e103beb99b58388a712d94893b85cd,FIXME assert_true('Some folds do not have any test epochs.' in w),https://github.com/mne-tools/mne-python/commit/f450fd4872e103beb99b58388a712d94893b85cd,No
3564,mne-tools/mne-python,mne/preprocessing/tests/test_xdawn.py,8823240c015a993c7a4585c0b268f42306faddf3,XXX This is a buggy test; the epochs here don't overlap,https://github.com/mne-tools/mne-python/commit/8823240c015a993c7a4585c0b268f42306faddf3,No
3565,mne-tools/mne-python,mne/preprocessing/tests/test_ica.py,c674fe7e305c0eebc4ccb8f4bcbd0ee170f33d17,XXX This breaks the tests :(,https://github.com/mne-tools/mne-python/commit/c674fe7e305c0eebc4ccb8f4bcbd0ee170f33d17,Yes
3566,mne-tools/mne-python,mne/fixes.py,862a5c93e132557a305b067f10da4b8c3bbfbfc8,XXX: should we rather test if instance of estimator?,https://github.com/mne-tools/mne-python/commit/862a5c93e132557a305b067f10da4b8c3bbfbfc8,No
3567,mne-tools/mne-python,mne/io/ctf/tests/test_ctf.py,95ed085204a31c4dfa6ce49d04acc469a1c134b7,XXX: Next test would fail because c-tools assign the fiducials from,https://github.com/mne-tools/mne-python/commit/95ed085204a31c4dfa6ce49d04acc469a1c134b7,Yes
3568,mne-tools/mne-python,mne/tests/test_source_estimate.py,6175ac800a991d84748ae4e9e5b325d9a8fb9fab,XXX Should design a fool-proof test case; but here were the,https://github.com/mne-tools/mne-python/commit/6175ac800a991d84748ae4e9e5b325d9a8fb9fab,Yes
3569,mne-tools/mne-python,examples/time_frequency/plot_time_frequency_erds.py,f5b8267f3750c450d411711c1c1a02ccc8b791ed,"\""\""\"" || =============================== || Compute and visualize ERDS maps || =============================== ||  || This example calculates and displays ERDS maps of event-related EEG data. ERDS || (sometimes also written as ERD\/ERS) is short for event-related || desynchronization (ERD) and event-related synchronization (ERS) [1]_. || Conceptually; ERD corresponds to a decrease in power in a specific frequency || band relative to a baseline. Similarly; ERS corresponds to an increase in || power. An ERDS map is a time\/frequency representation of ERD\/ERS over a range || of frequencies [2]_. ERDS maps are also known as ERSP (event-related spectral || perturbation) [3]_. ||  || We use a public EEG BCI data set containing two different motor imagery tasks || available at PhysioNet. The two tasks are imagined hand and feet movement. Our || goal is to generate ERDS maps for each of the two tasks. ||  || First; we load the data and create epochs of 5s length. The data sets contain || multiple channels; but we will only consider the three channels C3; Cz; and C4. || We compute maps containing frequencies ranging from 2 to 35Hz. We map ERD to || red color and ERS to blue color; which is the convention in many ERDS || publications. Note that we do not perform any significance tests on the map || values; but instead we display the whole time\/frequency maps. ||  || References || ---------- ||  || .. [1] G. Pfurtscheller; F. H. Lopes da Silva. Event-related EEG\/MEG ||        synchronization and desynchronization: basic principles. Clinical ||        Neurophysiology 110(11); 1842-1857; 1999. || .. [2] B. Graimann; J. E. Huggins; S. P. Levine; G. Pfurtscheller. ||        Visualization of significant ERD\/ERS patterns in multichannel EEG and ||        ECoG data. Clinical Neurophysiology 113(1); 43-47; 2002. || .. [3] S. Makeig. Auditory event-related dynamics of the EEG spectrum and ||        effects of exposure to tones. Electroencephalography and Clinical ||        Neurophysiology 86(4); 283-293; 1993. || \""\""\""",https://github.com/mne-tools/mne-python/commit/f5b8267f3750c450d411711c1c1a02ccc8b791ed,No
3570,mne-tools/mne-python,mne/viz/tests/test_evoked.py,daa29715cf3e4447eb3267db5a6be28aebfeb890,Hack to test plotting of maxfiltered data,https://github.com/mne-tools/mne-python/commit/daa29715cf3e4447eb3267db5a6be28aebfeb890,Yes
3571,mne-tools/mne-python,mne/rank.py,544989c7ff9922e04bddad72c1b13156ce344ace,Passing 'float32' is a hack workaround for test_maxfilter_get_rank :(,https://github.com/mne-tools/mne-python/commit/544989c7ff9922e04bddad72c1b13156ce344ace,No
3572,mne-tools/mne-python,mne/io/brainvision/tests/test_brainvision.py,f1f8e5413e04913011614e17f5daaa1ee84f973e,XXX: BUG we cannot parse test.hpts FastSCAN file to create a DigMontage,https://github.com/mne-tools/mne-python/commit/f1f8e5413e04913011614e17f5daaa1ee84f973e,Yes
3573,mne-tools/mne-python,mne/channels/tests/test_montage.py,52f064415e7c9fa8fe243d22108dcdf3d86505b9,XXX: This function tests read_montage and Montage. Should be removed in 0.20,https://github.com/mne-tools/mne-python/commit/52f064415e7c9fa8fe243d22108dcdf3d86505b9,Yes
3574,mne-tools/mne-python,mne/channels/tests/test_montage.py,52f064415e7c9fa8fe243d22108dcdf3d86505b9,XXX: deprecated to remove in 0.20; we are testing DigMontages somewhere else,https://github.com/mne-tools/mne-python/commit/52f064415e7c9fa8fe243d22108dcdf3d86505b9,No
3575,mne-tools/mne-python,mne/channels/tests/test_montage.py,52f064415e7c9fa8fe243d22108dcdf3d86505b9,XXX : to remove in 0.20 (tested separately in test_montage_readers and,https://github.com/mne-tools/mne-python/commit/52f064415e7c9fa8fe243d22108dcdf3d86505b9,Yes
3576,mne-tools/mne-python,tutorials/io/plot_10_reading_meg_data.py,b0ea04b4217d4945c13525e81607a9061d7768dc,"r\""\""\"" || .. _tut-imorting-meg-data: ||  || =============================== || Importing data from MEG devices || =============================== ||  || This section describes how to read data for various MEG manufacturers. ||  || .. contents:: Page contents ||    :local: ||    :depth: 2 ||  ||  || .. _import-neuromag: ||  || Elekta NeuroMag (.fif) || ====================== ||  || Neuromag Raw FIF files can be loaded using :func:`mne.io.read_raw_fif`. ||  || If the data were recorded with MaxShield on and have not been processed || with MaxFilter; they may need to be loaded with || ``mne.io.read_raw_fif(...; allow_maxshield=True)``. ||  ||  || .. _import-artemis: ||  || Artemis123 (.bin) || ================= || MEG data from the Artemis123 system can be read with\\ || :func:`mne.io.read_raw_artemis123`. ||  ||  || .. _import-bti: ||  || 4-D Neuroimaging \/ BTI data (dir) || ================================= ||  || MNE-Python provides :func:`mne.io.read_raw_bti` to read and convert 4D \/ BTI || data. This reader function will by default replace the original channel names; || typically composed of the letter `A` and the channel number with Neuromag. || To import the data; the following input files are mandatory: ||  || - A data file (typically c;rfDC) ||   containing the recorded MEG time series. ||  || - A hs_file ||   containing the digitizer data. ||  || - A config file ||   containing acquisition information and metadata. ||  || By default :func:`mne.io.read_raw_bti` assumes that these three files are located || in the same folder. ||  || .. note:: While reading the reference or compensation channels; ||           the compensation weights are currently not processed. ||           As a result; the :class:`mne.io.Raw` object and the corresponding fif ||           file does not include information about the compensation channels ||           and the weights to be applied to realize software gradient ||           compensation. If the data are saved in the Magnes system are already ||           compensated; there will be a small error in the forward calculations; ||           whose significance has not been evaluated carefully at this time. ||  ||  || .. _import-ctf: ||  || CTF data (dir) || ============== ||  || The function :func:`mne.io.read_raw_ctf` can be used to read CTF data. ||  || CTF Polhemus data || ----------------- ||  || The function :func:`mne.channels.read_dig_polhemus_isotrak` can be used to read || Polhemus data. ||  || Applying software gradient compensation || --------------------------------------- ||  || Since the software gradient compensation employed in CTF || systems is a reversible operation; it is possible to change the || compensation status of CTF data in the data files as desired. This || section contains information about the technical details of the || compensation procedure and a description of || :func:`mne.io.Raw.apply_gradient_compensation`. ||  || The raw instances returned by :func:`mne.io.read_raw_ctf` contain several || compensation matrices which are employed to suppress external disturbances || with help of the reference channel data. The reference sensors are || located further away from the brain than the helmet sensors and || are thus measuring mainly the external disturbances rather than magnetic || fields originating in the brain. Most often; a compensation matrix || corresponding to a scheme nicknamed *Third-order gradient || compensation* is employed. ||  || Let us assume that the data contain :math:`n_1` MEG || sensor channels; :math:`n_2` reference sensor || channels; and :math:`n_3` other channels. || The data from all channels can be concatenated into a single vector ||  || .. math::    x = [x_1^T x_2^T x_3^T]^T\\ ; ||  || where :math:`x_1`; :math:`x_2`; || and :math:`x_3` are the data vectors corresponding || to the MEG sensor channels; reference sensor channels; and other || channels; respectively. The data before and after compensation; || denoted here by :math:`x_{(0)}` and :math:`x_{(k)}`; respectively; || are related by ||  || .. math::    x_{(k)} = M_{(k)} x_{(0)}\\ ; ||  || where the composite compensation matrix is ||  || .. math::    M_{(k)} = \\begin{bmatrix} ||                 I_{n_1} & C_{(k)} & 0 \\\\ ||                 0 & I_{n_2} & 0 \\\\ ||                 0 & 0 & I_{n_3} ||                 \\end{bmatrix}\\ . ||  || In the above; :math:`C_{(k)}` is a :math:`n_1` by :math:`n_2` compensation || data matrix corresponding to compensation \""grade\"" :math:`k`. || It is easy to see that ||  || .. math::    M_{(k)}^{-1} = \\begin{bmatrix} ||                 I_{n_1} & -C_{(k)} & 0 \\\\ ||                 0 & I_{n_2} & 0 \\\\ ||                 0 & 0 & I_{n_3} ||                 \\end{bmatrix}\\ . ||  || To convert from compensation grade :math:`k` to :math:`p` one || can simply multiply the inverse of one compensate compensation matrix || by another and apply the product to the data: ||  || .. math::    x_{(k)} = M_{(k)} M_{(p)}^{-1} x_{(p)}\\ . ||  || This operation is performed by :meth:`mne.io.Raw.apply_gradient_compensation`. ||  ||  || .. _import-kit: ||  || KIT MEG system data (.sqd) || ========================== ||  || MNE-Python includes the :func:`mne.io.read_raw_kit` and || :func:`mne.read_epochs_kit` to read and convert KIT MEG data. || This reader function will by default replace the original channel names; || which typically with index starting with zero; with ones with an index starting || with one. ||  || To import continuous data; only the input .sqd or .con file is needed. For || epochs; an Nx3 matrix containing the event number\/corresponding trigger value || in the third column is needed. ||  || The following input files are optional: ||  || - A KIT marker file (mrk file) or an array-like containing the locations of ||   the HPI coils in the MEG device coordinate system. ||   These data are used together with the elp file to establish the coordinate ||   transformation between the head and device coordinate systems. ||  || - A Polhemus points file (elp file) or an array-like ||   containing the locations of the fiducials and the head-position ||   indicator (HPI) coils. These data are usually given in the Polhemus ||   head coordinate system. ||  || - A Polhemus head shape data file (hsp file) or an array-like ||   containing locations of additional points from the head surface. ||   These points must be given in the same coordinate system as that ||   used for the elp file. ||  ||  || .. note:: ||    The output fif file will use the Neuromag head coordinate system convention; ||    see :ref:`coordinate_systems`. A coordinate transformation between the ||    Polhemus head coordinates and the Neuromag head coordinates is included. ||  || By default; KIT-157 systems assume the first 157 channels are the MEG channels; || the next 3 channels are the reference compensation channels; and channels 160 || onwards are designated as miscellaneous input channels (MISC 001; MISC 002; || etc.). || By default; KIT-208 systems assume the first 208 channels are the MEG channels; || the next 16 channels are the reference compensation channels; and channels 224 || onwards are designated as miscellaneous input channels (MISC 001; MISC 002; || etc.). ||  || In addition; it is possible to synthesize the digital trigger channel (STI 014) || from available analog trigger channel data by specifying the following || parameters: ||  || - A list of trigger channels (stim) or default triggers with order: '<' | '>' ||   Channel-value correspondence when converting KIT trigger channels to a ||   Neuromag-style stim channel. By default; we assume the first eight ||   miscellaneous channels are trigger channels. For '<'; the largest values are ||   assigned to the first channel (little endian; default). For '>'; the largest ||   values are assigned to the last channel (big endian). Can also be specified ||   as a list of trigger channel indexes. || - The trigger channel slope (slope) : '+' | '-' ||   How to interpret values on KIT trigger channels when synthesizing a ||   Neuromag-style stim channel. With '+'; a positive slope (low-to-high) ||   is interpreted as an event. With '-'; a negative slope (high-to-low) ||   is interpreted as an event. || - A stimulus threshold (stimthresh) : float ||   The threshold level for accepting voltage changes in KIT trigger ||   channels as a trigger event. ||  || The synthesized trigger channel data value at sample :math:`k` will || be: ||  || .. math::    s(k) = \\sum_{p = 1}^n {t_p(k) 2^{p - 1}}\\ ; ||  || where :math:`t_p(k)` are the thresholded || from the input channel data d_p(k): ||  || .. math::    t_p(k) = \\Bigg\\{ \\begin{array}{l} ||                  0 \\text{  if  } d_p(k) \\leq t\\\\ ||                  1 \\text{  if  } d_p(k) > t ||              \\end{array}\\ . ||  || The threshold value :math:`t` can || be adjusted with the ``stimthresh`` parameter. ||  ||  || .. _import-fieldtrip: ||  || FieldTrip MEG\/EEG data (.mat) || ============================= ||  || MNE-Python includes :func:`mne.io.read_raw_fieldtrip`; :func:`mne.read_epochs_fieldtrip` and :func:`mne.read_evoked_fieldtrip` to read data coming from FieldTrip. ||  || The data is imported directly from a ``.mat`` file. ||  || The ``info`` parameter can be explicitly set to ``None``. The import functions will still work but: ||  || #. All channel locations will be in head coordinates. || #. Channel orientations cannot be guaranteed to be accurate. || #. All channel types will be set to generic types. ||  || This is probably fine for anything that does not need that information; but if you intent to do things like interpolation of missing channels; source analysis or look at the RMS pairs of planar gradiometers; you most likely run into problems. ||  || It is **highly recommended** to provide the ``info`` parameter as well. The ``info`` dictionary can be extracted by loading the original raw data file with the corresponding MNE-Python functions:: ||  ||     original_data = mne.io.read_raw_fiff('original_data.fif'; preload=False) ||     original_info = original_data.info ||     data_from_ft = mne.read_evoked_fieldtrip('evoked_data.mat'; original_info) ||  || The imported data can have less channels than the original data. Only the information for the present ones is extracted from the ``info`` dictionary. ||  || As of version 0.17; importing FieldTrip data has been tested on a variety of systems with the following results: ||  || +----------+-------------------+-------------------+-------------------+ || | System   | Read Raw Data     | Read Epoched Data | Read Evoked Data  | || +==========+===================+===================+===================+ || | BTI      | Works             | Untested          | Untested          | || +----------+-------------------+-------------------+-------------------+ || | CNT      | Data imported as  | Data imported as  | Data imported as  | || |          | microvolts.       | microvolts.       | microvolts.       | || |          | Otherwise fine.   | Otherwise fine.   | Otherwise fine.   | || +----------+-------------------+-------------------+-------------------+ || | CTF      | Works             | Works             | Works             | || +----------+-------------------+-------------------+-------------------+ || | EGI      | Mostly Ok. Data   | Mostly Ok. Data   | Mostly Ok. Data   | || |          | imported as       | imported as       | imported as       | || |          | microvolts.       | microvolts.       | microvolts.       | || |          | FieldTrip does    | FieldTrip does    | FieldTrip does    | || |          | not apply         | not apply         | not apply         | || |          | calibration.      | calibration.      | calibration.      | || +----------+-------------------+-------------------+-------------------+ || | KIT      | Does not work.    | Does not work.    | Does not work.    | || |          | Channel names are | Channel names are | Channel names are | || |          | different in      | different in      | different in      | || |          | MNE-Python and    | MNE-Python and    | MNE-Python and    | || |          | FieldTrip.        | FieldTrip.        | FieldTrip.        | || +----------+-------------------+-------------------+-------------------+ || | Neuromag | Works             | Works             | Works             | || +----------+-------------------+-------------------+-------------------+ || | eximia   | Works             | Untested          | Untested          | || +----------+-------------------+-------------------+-------------------+ ||  || Creating MNE data structures from arbitrary data (from memory) || ============================================================== ||  || Arbitrary (e.g.; simulated or manually read in) raw data can be constructed || from memory by making use of :class:`mne.io.RawArray`; :class:`mne.EpochsArray` || or :class:`mne.EvokedArray` in combination with :func:`mne.create_info`. ||  || This functionality is illustrated in :ref:`ex-array-classes`. Using 3rd party || libraries such as `NEO <https:\/\/github.com\/NeuralEnsemble\/python-neo>`__ in || combination with these functions abundant electrophysiological file formats can || be easily loaded into MNE. || \""\""\""",https://github.com/mne-tools/mne-python/commit/b0ea04b4217d4945c13525e81607a9061d7768dc,No
3577,mne-tools/mne-python,mne/channels/tests/test_montage.py,8a4cef803db165e2f8a00ad0f304ab60a79bc6f3,XXX: This is testing deprecated behavior and should be removed in 0.21.,https://github.com/mne-tools/mne-python/commit/8a4cef803db165e2f8a00ad0f304ab60a79bc6f3,No
3578,mne-tools/mne-python,mne/io/meas_info.py,bb8cd6953757f051a9781f38198e5986511d7329,The following copy is needed for a test CTF dataset,https://github.com/mne-tools/mne-python/commit/bb8cd6953757f051a9781f38198e5986511d7329,No
3579,mne-tools/mne-python,mne/channels/tests/test_montage.py,f8358f7bf9920088313c08bd71d7e241f29cf7e9,XXX: This is testing deprecated behavior and should be removed in 0.21.,https://github.com/mne-tools/mne-python/commit/f8358f7bf9920088313c08bd71d7e241f29cf7e9,No
3580,mne-tools/mne-python,mne/channels/tests/test_montage.py,f8358f7bf9920088313c08bd71d7e241f29cf7e9,XXX: This function tests read_montage and Montage. Should be removed in 0.20,https://github.com/mne-tools/mne-python/commit/f8358f7bf9920088313c08bd71d7e241f29cf7e9,Yes
3581,mne-tools/mne-python,mne/channels/tests/test_montage.py,f8358f7bf9920088313c08bd71d7e241f29cf7e9,XXX: deprecated to remove in 0.20; we are testing DigMontages somewhere else,https://github.com/mne-tools/mne-python/commit/f8358f7bf9920088313c08bd71d7e241f29cf7e9,No
3582,mne-tools/mne-python,mne/io/meas_info.py,b3f22f9e044443e3b448b9e38b1234d9f2dc9380,The following copy is needed for a test CTF dataset,https://github.com/mne-tools/mne-python/commit/b3f22f9e044443e3b448b9e38b1234d9f2dc9380,No
3583,mne-tools/mne-python,mne/channels/channels.py,5f95ebb543dac03ca3b6cde31342da4d80bf4451,The following copy is needed for a test CTF dataset,https://github.com/mne-tools/mne-python/commit/5f95ebb543dac03ca3b6cde31342da4d80bf4451,No
3584,mne-tools/mne-python,mne/channels/tests/test_montage.py,676e627f3f9b9e292e131480c46aa7ab12429cdb,XXX: This is testing deprecated behavior and should be removed in 0.21.,https://github.com/mne-tools/mne-python/commit/676e627f3f9b9e292e131480c46aa7ab12429cdb,No
3585,mne-tools/mne-python,mne/viz/_figure.py,49ac85b4b0c95dc1033e217983c495f180479e58,XXX implementation works interactively; but fails tests with older,https://github.com/mne-tools/mne-python/commit/49ac85b4b0c95dc1033e217983c495f180479e58,No
3586,databricks/spark-sklearn,python/pdspark/grid_search.py,7c9c68b71f68e0b8890d865b2c0eef70a35b40f2,TODO: shall we also store the test_fold_sizes?,https://github.com/databricks/spark-sklearn/commit/7c9c68b71f68e0b8890d865b2c0eef70a35b40f2,Yes
3587,databricks/spark-sklearn,python/spark_sklearn/grid_search.py,1386bae5002046e5be4cf73b685555b9efb6d79c,# TODO: shall we also store the test_fold_sizes?,https://github.com/databricks/spark-sklearn/commit/1386bae5002046e5be4cf73b685555b9efb6d79c,No
3588,plasticityai/magnitude,pymagnitude/third_party/allennlp/tests/common/registrable_test.py,96f0fc6a8839cf1c824febcb170ab75ce8a6f854,TODO(mattg): maybe move all of these into tests for the base class?,https://github.com/plasticityai/magnitude/commit/96f0fc6a8839cf1c824febcb170ab75ce8a6f854,No
3589,plasticityai/magnitude,pymagnitude/third_party/allennlp/tests/modules/seq2seq_encoders/pytorch_seq2seq_wrapper_test.py,96f0fc6a8839cf1c824febcb170ab75ce8a6f854,To test the stateful functionality we need to call the encoder multiple times.,https://github.com/plasticityai/magnitude/commit/96f0fc6a8839cf1c824febcb170ab75ce8a6f854,Yes
3590,plasticityai/magnitude,pymagnitude/third_party/_apsw/tests.py,4f1b637d8eab27ca93f8aee2d1c24b5e38f38301,collation needed testing,https://github.com/plasticityai/magnitude/commit/4f1b637d8eab27ca93f8aee2d1c24b5e38f38301,Yes
3591,georgesterpu/avsr-tf1,avsr/visualise/beam_search.py,4daed8a10fe61db0bb22ee72f8c94e3c2bf6a732,r''' || TODO || ==== || *. properly normalise the scores || *. toggle scores on \/ off || *. URL to dataset dir - play example || *. colour legend || *. display likelihood of the ground truth sequence || *. highlight best translation - stronger line width and different colour || *. update to latest d3.js version || ''',https://github.com/georgesterpu/avsr-tf1/commit/4daed8a10fe61db0bb22ee72f8c94e3c2bf6a732,No
3592,gunthercox/ChatterBot,tests/io_adapter_tests/test_terminal_adapter.py,fc4d312adb42a422a1394cf0a2f65699cdf64a7f,TODO: How can you test this?,https://github.com/gunthercox/ChatterBot/commit/fc4d312adb42a422a1394cf0a2f65699cdf64a7f,No
3593,gunthercox/ChatterBot,chatterbot/chatterbot.py,373629ba611df317d4662cddd9fe5a5d041b3b71,TODO: test adapter validation,https://github.com/gunthercox/ChatterBot/commit/373629ba611df317d4662cddd9fe5a5d041b3b71,Yes
3594,ageitgey/face_recognition,setup.py,e15120e6ba08cde1216607d2eb27e0eb0f0ea37c,TODO: put package test requirements here,https://github.com/ageitgey/face_recognition/commit/e15120e6ba08cde1216607d2eb27e0eb0f0ea37c,No
3595,microsoft/graspologic,tests/test_models.py,aa7ad671075a1025b558970d39fcd0841ac4f8eb,hack just for testing likelihood,https://github.com/microsoft/graspologic/commit/aa7ad671075a1025b558970d39fcd0841ac4f8eb,No
3596,microsoft/graspologic,tests/test_match.py,660055a438b6eda21ea807b8f12ed0a63a662005,TODO: def test_sim(self):,https://github.com/microsoft/graspologic/commit/660055a438b6eda21ea807b8f12ed0a63a662005,Yes
3597,microsoft/graspologic,graspologic/layouts/render.py,fd8a27721e7c50b1ca2f023583541faeef9cb124,TODO; test at different dpi,https://github.com/microsoft/graspologic/commit/fd8a27721e7c50b1ca2f023583541faeef9cb124,No
3598,chakki-works/seqeval,seqeval/scheme.py,448a1e4edc4e874aca41afa01f454f04f0b92971,Todo: IOE1 hasn't yet been able to handle some cases. See unit testing.,https://github.com/chakki-works/seqeval/commit/448a1e4edc4e874aca41afa01f454f04f0b92971,No
3599,openml/openml-python,tests/entities/test_dataset.py,0cdb7c1a05920940f71d316521b5f21256d3252c,TODO test multiple ignore attributes!,https://github.com/openml/openml-python/commit/0cdb7c1a05920940f71d316521b5f21256d3252c,Yes
3600,openml/openml-python,tests/entities/test_dataset.py,8bf9625ecdf74d1491b6a598102dba2aba247b6c,TODO test multiple ignore attributes!,https://github.com/openml/openml-python/commit/8bf9625ecdf74d1491b6a598102dba2aba247b6c,Yes
3601,openml/openml-python,tests/test_datasets.py,7e051db80726145e0f6f2c2377c2f7d12f7e67a1,FIXME REFACTOR with test apiconnector,https://github.com/openml/openml-python/commit/7e051db80726145e0f6f2c2377c2f7d12f7e67a1,No
3602,openml/openml-python,openml/flows/flow.py,33c3a8101efe1cb66599831c03a6b960248a8718,TODO check with latest version of code if this raises an exception,https://github.com/openml/openml-python/commit/33c3a8101efe1cb66599831c03a6b960248a8718,Yes
3603,openml/openml-python,openml/flows/functions.py,c40edf1fcdb767b84ad6c86d4b398ec84d151e80,TODO check with latest version of code if this raises an exception,https://github.com/openml/openml-python/commit/c40edf1fcdb767b84ad6c86d4b398ec84d151e80,Yes
3604,openml/openml-python,openml/flows/sklearn.py,4882cda9f29535eda784d4437076ff4e244a3a73,TODO remove potential test sentinel during testing!,https://github.com/openml/openml-python/commit/4882cda9f29535eda784d4437076ff4e244a3a73,No
3605,openml/openml-python,openml/flows/flow.py,5f78c7365830c14714f5ef597504536c2d724a70,TODO check with latest version of code if this raises an exception,https://github.com/openml/openml-python/commit/5f78c7365830c14714f5ef597504536c2d724a70,Yes
3606,openml/openml-python,openml/flows/functions.py,8855e6efa04dfa5db6c9aa2d83a7cfd5a4fd983b,TODO check with latest version of code if this raises an exception,https://github.com/openml/openml-python/commit/8855e6efa04dfa5db6c9aa2d83a7cfd5a4fd983b,Yes
3607,openml/openml-python,tests/test_flows/test_flow.py,71ec3fc53277fd117f714baf251914f084b9ff7f,TODO: Test if parameters are set correctly!,https://github.com/openml/openml-python/commit/71ec3fc53277fd117f714baf251914f084b9ff7f,Yes
3608,openml/openml-python,tests/test_flows/test_flow.py,f9bf4f2a123e37f74fb2d0dbd084eed369e3c29b,TODO: Test if parameters are set correctly!,https://github.com/openml/openml-python/commit/f9bf4f2a123e37f74fb2d0dbd084eed369e3c29b,Yes
3609,openml/openml-python,tests/test_runs/test_run_functions.py,cb55127ea807b9b9fe4e0f323d35d5f0b7408d2d,TODO: implement testcase,https://github.com/openml/openml-python/commit/cb55127ea807b9b9fe4e0f323d35d5f0b7408d2d,No
3610,openml/openml-python,tests/test_setups/test_setup_functions.py,c0c6643f26bc12fd55abeab901edb17fd8552c1b,TODO: please remove for better test,https://github.com/openml/openml-python/commit/c0c6643f26bc12fd55abeab901edb17fd8552c1b,Yes
3611,openml/openml-python,tests/test_setups/test_setup_functions.py,6dce2740866e2e984554e2548120ec02728621aa,TODO: remove after pull on live for better testing,https://github.com/openml/openml-python/commit/6dce2740866e2e984554e2548120ec02728621aa,No
3612,openml/openml-python,tests/test_utils/test_utils.py,fdd6c2579704becbcecfc83d882f17f24090fdeb,TODO implement these tests,https://github.com/openml/openml-python/commit/fdd6c2579704becbcecfc83d882f17f24090fdeb,Yes
3613,openml/openml-python,tests/test_runs/test_run_functions.py,cd7d74bd15d642bbd1ee6a0e0dedac49c24e5cf7,TODO add test about initializing a model from a run given a parameter distribution - also,https://github.com/openml/openml-python/commit/cd7d74bd15d642bbd1ee6a0e0dedac49c24e5cf7,Yes
3614,openml/openml-python,tests/test_extensions/test_sklearn_extension/test_sklearn_extension.py,0f8b7f0966a1ebb4e7c848268e904402818891ef,TODO add some mocking here to actually test the innards of this function; too!,https://github.com/openml/openml-python/commit/0f8b7f0966a1ebb4e7c848268e904402818891ef,Yes
3615,openml/openml-python,examples/30_extended/task_manual_iteration_tutorial.py,4020c1ee836ec31cead92e29fb1188aa1173a588,"\""\""\"" || Tasks: retrieving splits || ======================== ||  || Tasks define a target and a train\/test split. Normally; they are the input to the function || ``openml.runs.run_model_on_task`` which automatically runs the model on all splits of the task. || However; sometimes it is necessary to manually split a dataset to perform experiments outside of || the functions provided by OpenML. One such example is in the benchmark library || `HPOlib2 <https:\/\/github.com\/automl\/hpolib2>`_ which extensively uses data from OpenML; || but not OpenML's functionality to conduct runs. || \""\""\""",https://github.com/openml/openml-python/commit/4020c1ee836ec31cead92e29fb1188aa1173a588,Yes
3616,openml/openml-python,tests/test_datasets/test_dataset_functions.py,1c025dbb3447cecc25b7e2561650960f0cc49a15,Test column names are automatically converted to str if needed (#819),https://github.com/openml/openml-python/commit/1c025dbb3447cecc25b7e2561650960f0cc49a15,Yes
3617,openml/openml-python,tests/test_extensions/test_sklearn_extension/test_sklearn_extension.py,bf3cd2ebaac10bd05809a1ce90e346248c4c61b1,TODO add some mocking here to actually test the innards of this function; too!,https://github.com/openml/openml-python/commit/bf3cd2ebaac10bd05809a1ce90e346248c4c61b1,Yes
3618,scikit-multiflow/scikit-multiflow,skmultiflow/classification/trees/hoeffding_adaptive_tree.py,1b29c74f81bd018cdbe0e7332b8130e1d7c0ba35,Example TODO Create a demo\/test from this,https://github.com/scikit-multiflow/scikit-multiflow/commit/1b29c74f81bd018cdbe0e7332b8130e1d7c0ba35,Yes
3619,onnx/onnx-caffe2,onnx_caffe2/backend.py,3cc1e2323b6c12c63793dc10b82801741d38f08d,TODO: replace this with a version test,https://github.com/onnx/onnx-caffe2/commit/3cc1e2323b6c12c63793dc10b82801741d38f08d,Yes
3620,onnx/onnx-caffe2,tests/caffe2_ref_test.py,6a9327f1fa4d72a663a73e06ef93a810c7ba634c,TODO: Some of these tests will be done most conveniently by running a Caffe2,https://github.com/onnx/onnx-caffe2/commit/6a9327f1fa4d72a663a73e06ef93a810c7ba634c,Yes
3621,onnx/onnx-caffe2,onnx_caffe2/backend.py,5059375a81a8ca8c6c656741c7d46fb29abdd0aa,TODO: replace this with a version test,https://github.com/onnx/onnx-caffe2/commit/5059375a81a8ca8c6c656741c7d46fb29abdd0aa,Yes
3622,onnx/onnx-caffe2,onnx_caffe2/backend.py,56e3a1127dcc615074e067358718ecc6d469332f,TODO: replace this with a version test,https://github.com/onnx/onnx-caffe2/commit/56e3a1127dcc615074e067358718ecc6d469332f,Yes
3623,scikit-learn-contrib/polylearn,doc/sphinxext/gen_rst.py,faff0fdd0d23d26c9df9d32dccffcb1b9853f902,HACK: Stop nosetests running setup() above,https://github.com/scikit-learn-contrib/polylearn/commit/faff0fdd0d23d26c9df9d32dccffcb1b9853f902,Yes
3624,scikit-learn-contrib/polylearn,polylearn/tests/test_common.py,faff0fdd0d23d26c9df9d32dccffcb1b9853f902,TODO: can't actually pass the test.,https://github.com/scikit-learn-contrib/polylearn/commit/faff0fdd0d23d26c9df9d32dccffcb1b9853f902,Yes
3625,inferno-pytorch/inferno,setup.py,294ae8801abbc7bf8ab5106f66d2c4ff721bb710,TODO: put package test requirements here,https://github.com/inferno-pytorch/inferno/commit/294ae8801abbc7bf8ab5106f66d2c4ff721bb710,No
3626,inferno-pytorch/inferno,tests/extensions/criteria/set_similarity_measures.py,aa41fb57571629213a81c5a5b954103888a19625,TODO better test,https://github.com/inferno-pytorch/inferno/commit/aa41fb57571629213a81c5a5b954103888a19625,No
3627,inferno-pytorch/inferno,tests/extensions/criteria/set_similarity_measures.py,6289c244428a855f257daeb2001bc4e72ec92875,TODO better test,https://github.com/inferno-pytorch/inferno/commit/6289c244428a855f257daeb2001bc4e72ec92875,No
3628,inferno-pytorch/inferno,inferno/extensions/model/unet.py,3c939d3da482791ad46b726f54dce6981e8004c4,TODO test,https://github.com/inferno-pytorch/inferno/commit/3c939d3da482791ad46b726f54dce6981e8004c4,Yes
3629,microsoft/NimbusML,src/python/nimbusml/tests/model_selection/test_cv.py,793739b8ab6b002bfccdbda2bedf6f915ce6125b,The following tests are only needed once. We bundle them with regressor,https://github.com/microsoft/NimbusML/commit/793739b8ab6b002bfccdbda2bedf6f915ce6125b,Yes
3630,microsoft/NimbusML,src/python/tests/test_docs_example.py,793739b8ab6b002bfccdbda2bedf6f915ce6125b,REVIEW: fix ssl issue on test centos7 & ubuntu14,https://github.com/microsoft/NimbusML/commit/793739b8ab6b002bfccdbda2bedf6f915ce6125b,Yes
3631,googleapis/python-dialogflow,synth.py,7bf592684b4d5df0cd1f66dd414efe2350d0461e,fix unit test,https://github.com/googleapis/python-dialogflow/commit/7bf592684b4d5df0cd1f66dd414efe2350d0461e,Yes
3632,tensorflow/addons,tensorflow_addons/layers/python/layers/poincare_normalize_test.py,2b004243f5a3f57e24f075a8139594916a994fb4,TODO: Is this the prefered way to run tests in TF2?,https://github.com/tensorflow/addons/commit/2b004243f5a3f57e24f075a8139594916a994fb4,Yes
3633,tensorflow/addons,tensorflow_addons/text/python/ops/skip_gram_ops_test.py,2b004243f5a3f57e24f075a8139594916a994fb4,This is needed since tests set a graph-level seed by default. We want to,https://github.com/tensorflow/addons/commit/2b004243f5a3f57e24f075a8139594916a994fb4,Yes
3634,tensorflow/addons,tensorflow_addons/optimizers/python/lazy_adam_optimizer_test.py,76843b298efee5a5c617abd76d1dc781850757e6,TODO: remove v1 tests (keep pace with adam_test.py in keras).,https://github.com/tensorflow/addons/commit/76843b298efee5a5c617abd76d1dc781850757e6,Yes
3635,tensorflow/addons,tensorflow_addons/losses/python/metric_learning.py,bfa919ed514aef264d0259b96c741d42b2530d49,TODO: add unit test later.,https://github.com/tensorflow/addons/commit/bfa919ed514aef264d0259b96c741d42b2530d49,No
3636,tensorflow/addons,tensorflow_addons/image/sparse_image_warp_test.py,6ec28e5f051c10e37ba1b6d83045f0d0d93b5cd5,TODO: port TF1 test files?,https://github.com/tensorflow/addons/commit/6ec28e5f051c10e37ba1b6d83045f0d0d93b5cd5,Yes
3637,tensorflow/addons,tensorflow_addons/layers/wrappers_test.py,28acaf444b4e405efc7f228fbb0036860dfad528,TODO: Fix the bug thats causing layer test to run a,https://github.com/tensorflow/addons/commit/28acaf444b4e405efc7f228fbb0036860dfad528,Yes
3638,tensorflow/addons,tensorflow_addons/optimizers/stochastic_weight_averaging.py,8868537159be8d1a326f32fbc70d8242647b6cc4,"\""\""\""An implementation of the Stochastic Weight Averaging optimizer. ||  || The Stochastic Weight Averaging mechanism was proposed by Pavel Izmailov || et. al in the paper [Averaging Weights Leads to Wider Optima and Better || Generalization](https:\/\/arxiv.org\/abs\/1803.05407). The optimizer || implements averaging of multiple points along the trajectory of SGD. || This averaging has shown to improve model performance on validation\/test || sets whilst possibly causing a small increase in loss on the training || set. || \""\""\""",https://github.com/tensorflow/addons/commit/8868537159be8d1a326f32fbc70d8242647b6cc4,No
3639,tensorflow/addons,tensorflow_addons/register.py,1458f7f6cb44b1e5fe6b033c11b49ee6bbbda3ab,TODO: once layer_test is replaced by a public API,https://github.com/tensorflow/addons/commit/1458f7f6cb44b1e5fe6b033c11b49ee6bbbda3ab,No
3640,tensorflow/addons,tensorflow_addons/optimizers/tests/lazy_adam_test.py,82ed39808bd286c1f9f5115c61d3f27a7f1fc186,TODO: remove v1 tests (keep pace with adam_test.py in keras).,https://github.com/tensorflow/addons/commit/82ed39808bd286c1f9f5115c61d3f27a7f1fc186,Yes
3641,menpo/menpo,pybug/align/lucaskanade/residual.py,e559cccae78d2fcce81d4d86c2914b562bf58f0d,TODO think this is a bug fix; should be tested,https://github.com/menpo/menpo/commit/e559cccae78d2fcce81d4d86c2914b562bf58f0d,Yes
3642,menpo/menpo,pybug/transform/piecewiseaffine.py,c534c37c8f377d9e1651777ddda27d7a0f376ced,todo this could be cached if tri_containment is being tested at,https://github.com/menpo/menpo/commit/c534c37c8f377d9e1651777ddda27d7a0f376ced,Yes
3643,menpo/menpo,pybug/image/base.py,df493ef8fd5c99c7150d00e9b830fdd10bb093ba,TODO make a unit test for gradient of masked images (inc_masked_pixels),https://github.com/menpo/menpo/commit/df493ef8fd5c99c7150d00e9b830fdd10bb093ba,Yes
3644,menpo/menpo,pybug/transform/piecewiseaffine.py,2d90f9d3a4497234232a497909c15126dbd290f3,todo this could be cached if tri_containment is being tested at,https://github.com/menpo/menpo/commit/2d90f9d3a4497234232a497909c15126dbd290f3,Yes
3645,menpo/menpo,pybug/transform/test/tps_test.py,d26fcecb4e1d4495971956f3621a9a682ef0842b,TODO: test the kernel,https://github.com/menpo/menpo/commit/d26fcecb4e1d4495971956f3621a9a682ef0842b,No
3646,feedly/transfer-nlp,transfer_nlp/runners/runnersABC.py,5770f658a72c31c646854a10d69fed99c81e3c31,Register useful parameters and objects useful for model instantiation #TODO: do proper testing on this part,https://github.com/feedly/transfer-nlp/commit/5770f658a72c31c646854a10d69fed99c81e3c31,No
3647,feedly/transfer-nlp,transfer_nlp/predictors/predictor.py,8c483e1a28f37259b68a2f1a946ffaeb2e4fe0f3,Register useful parameters and objects useful for model instantiation #TODO: do proper testing on this part,https://github.com/feedly/transfer-nlp/commit/8c483e1a28f37259b68a2f1a946ffaeb2e4fe0f3,No
3648,chainer/chainer-chemistry,examples/qm9/train_qm9.py,c426e08351a5bf2e8989aaf8d79b76fceb177e05,TODO: Not test yet; check behavior,https://github.com/chainer/chainer-chemistry/commit/c426e08351a5bf2e8989aaf8d79b76fceb177e05,No
3649,chainer/chainer-chemistry,examples/qm9/train_qm9_features.py,c426e08351a5bf2e8989aaf8d79b76fceb177e05,TODO: Not test yet; check behavior,https://github.com/chainer/chainer-chemistry/commit/c426e08351a5bf2e8989aaf8d79b76fceb177e05,No
3650,chainer/chainer-chemistry,examples/tox21/train_tox21.py,c426e08351a5bf2e8989aaf8d79b76fceb177e05,TODO: Not test yet; check behavior,https://github.com/chainer/chainer-chemistry/commit/c426e08351a5bf2e8989aaf8d79b76fceb177e05,No
3651,chainer/chainer-chemistry,examples/qm9/train_qm9.py,7784bb5d8dce8fa868ef9445759d4a015c7c0575,TODO: Not test yet; check behavior,https://github.com/chainer/chainer-chemistry/commit/7784bb5d8dce8fa868ef9445759d4a015c7c0575,No
3652,chainer/chainer-chemistry,chainer_chemistry/saliency/calculator/occlusion_calculator.py,03ab3945a686e89e57a05dd5c4761786bee87ada,TODO: test,https://github.com/chainer/chainer-chemistry/commit/03ab3945a686e89e57a05dd5c4761786bee87ada,No
3653,chainer/chainer-chemistry,chainer_chemistry/models/gwm.py,80dad51ce270c73e886055d8c1493fa8917455f2,TODO: fail backward test,https://github.com/chainer/chainer-chemistry/commit/80dad51ce270c73e886055d8c1493fa8917455f2,No
3654,chainer/chainer-chemistry,tests/dataset_tests/preprocessors_tests/test_weavenet_preprocessor.py,4bd35912e53d875330d2792911f6eedee62389d2,TODO (nakago): test feature extraction behavior...,https://github.com/chainer/chainer-chemistry/commit/4bd35912e53d875330d2792911f6eedee62389d2,Yes
3655,chainer/chainer-chemistry,chainer_chemistry/dataset/preprocessors/test_neighbor_node_expansion.py,2e83d8955e81e626f000c31d7620225d76576f50,ToDo: wrie a more formal test...,https://github.com/chainer/chainer-chemistry/commit/2e83d8955e81e626f000c31d7620225d76576f50,Yes
3656,online-ml/river,skmultiflow/classification/trees/hoeffding_adaptive_tree.py,1b29c74f81bd018cdbe0e7332b8130e1d7c0ba35,Example TODO Create a demo\/test from this,https://github.com/online-ml/river/commit/1b29c74f81bd018cdbe0e7332b8130e1d7c0ba35,Yes
3657,online-ml/river,creme/tree/tree.py,873aed0fd929ace64fccc1fd627eaf67e13df9d4,TODO: Test Naive Bayes prediction using MOA paper (from page 79 onwards of https:\/\/www.cs.waikato.ac.nz\/~abifet\/MOA\/StreamMining.pdf),https://github.com/online-ml/river/commit/873aed0fd929ace64fccc1fd627eaf67e13df9d4,Yes
3658,online-ml/river,river/datasets/test_datasets.py,736a6def97efd3f6a606119cbab5453c0be30558,TODO: test the following synth datasets also,https://github.com/online-ml/river/commit/736a6def97efd3f6a606119cbab5453c0be30558,No
3659,onnx/onnx-tensorflow,test/test_node.py,43063583a884a78951c9152aa13ced9a33d829db,TODO: better testing for RNN. For now; we are just making sure,https://github.com/onnx/onnx-tensorflow/commit/43063583a884a78951c9152aa13ced9a33d829db,Yes
3660,onnx/onnx-tensorflow,test/test_node.py,c14b7eb2e582079894f8d7f22423cad0d1793041,TODO: better testing for RNN. For now; we are just making sure,https://github.com/onnx/onnx-tensorflow/commit/c14b7eb2e582079894f8d7f22423cad0d1793041,Yes
3661,onnx/onnx-tensorflow,test/test_node.py,0a2e8beac36bf6c797bf45cdfa27cdd6d2924796,TODO: fix this test,https://github.com/onnx/onnx-tensorflow/commit/0a2e8beac36bf6c797bf45cdfa27cdd6d2924796,Yes
3662,onnx/onnx-tensorflow,test/test_node.py,a8ec204de582d580313374a394dc5de3b589fe80,TODO: test obsolete,https://github.com/onnx/onnx-tensorflow/commit/a8ec204de582d580313374a394dc5de3b589fe80,Yes
3663,onnx/onnx-tensorflow,test/backend/test_node.py,4ca2541496d2113d7a02c2b3f4180c7d2b451afe,TODO: fix this test,https://github.com/onnx/onnx-tensorflow/commit/4ca2541496d2113d7a02c2b3f4180c7d2b451afe,Yes
3664,onnx/onnx-tensorflow,test/backend/test_node.py,3d246d625d0db308aaa0922c8812d49964eab753,TODO: fix this test,https://github.com/onnx/onnx-tensorflow/commit/3d246d625d0db308aaa0922c8812d49964eab753,Yes
3665,onnx/onnx-tensorflow,test/backend/test_node.py,9edb176ecb0b6f061c2f7c196ac1f9563664baff,TODO: fix this test,https://github.com/onnx/onnx-tensorflow/commit/9edb176ecb0b6f061c2f7c196ac1f9563664baff,Yes
3666,CPJKU/madmom,madmom/ml/rnn.py,d684dbace033638eba432509ca480f3cabaf30a9,"\""\""\"" || This file contains recurrent neural network (RNN) related functionality. ||  || It's main purpose is to serve as a substitute for testing neural networks || which were trained by other ML packages or programs without requiring these || packages or programs as dependencies. ||  || The only allowed dependencies are Python + numpy + scipy. ||  || The structure reflects just the needed functionality for testing networks. This || module is not meant to be a general purpose RNN with lots of functionality. || Just use one of the many NN\/ML packages out there if you need training or any || other stuff. ||  || @author: Sebastian B\u00F6ck <sebastian.boeck@jku.at> ||  || \""\""\""",https://github.com/CPJKU/madmom/commit/d684dbace033638eba432509ca480f3cabaf30a9,No
3667,CPJKU/madmom,madmom/test/test_utils.py,ae7e374c8077ef467079974da98b2dea3a22208d,TODO: write a test for speed,https://github.com/CPJKU/madmom/commit/ae7e374c8077ef467079974da98b2dea3a22208d,No
3668,CPJKU/madmom,madmom/ml/rnnlib.py,437ba0413caa6729236b9f195291b33821a72f4b,TODO: unify this with RnnlibConfig.test(),https://github.com/CPJKU/madmom/commit/437ba0413caa6729236b9f195291b33821a72f4b,No
3669,CPJKU/madmom,madmom/test/test_audio_filters.py,27ca8712bf976dbbded4274087ef246d972e7276,TODO: why can't we test the inherited constants? it does not matter,https://github.com/CPJKU/madmom/commit/27ca8712bf976dbbded4274087ef246d972e7276,No
3670,CPJKU/madmom,madmom/test/test_audio_spectrogram.py,d6730264dad0df6e3e0ad1b085554994851d3dca,TODO: write a test which catches the warning about the circular_shift,https://github.com/CPJKU/madmom/commit/d6730264dad0df6e3e0ad1b085554994851d3dca,No
3671,CPJKU/madmom,madmom/test/test_audio_stft.py,b1fba67e480970d6e67b5b21a6082ff94c737747,TODO: write a test which catches the warning about the circular_shift,https://github.com/CPJKU/madmom/commit/b1fba67e480970d6e67b5b21a6082ff94c737747,No
3672,CPJKU/madmom,madmom/test/test_audio_spectrogram.py,b8fa6f1b3f5daa0fb262d4a5728a1f17988d3d95,TODO: write a test which catches the warning about the circular_shift,https://github.com/CPJKU/madmom/commit/b8fa6f1b3f5daa0fb262d4a5728a1f17988d3d95,No
3673,CPJKU/madmom,tests/test_bin.py,9be5595820f0bf72fec9fafe40a824f608b04ad0,TODO: parametrize tests; don't know how to do with nose; should be simple,https://github.com/CPJKU/madmom/commit/9be5595820f0bf72fec9fafe40a824f608b04ad0,Yes
3674,CPJKU/madmom,tests/test_bin.py,9be5595820f0bf72fec9fafe40a824f608b04ad0,TODO: can we speed up these tests?,https://github.com/CPJKU/madmom/commit/9be5595820f0bf72fec9fafe40a824f608b04ad0,No
3675,CPJKU/madmom,tests/test_ml_hmm.py,c895d7c6fa07e63e96a8b2c19466112d398d8ac7,TODO: assertWarns exist only for Python 3.2+; test in all versions,https://github.com/CPJKU/madmom/commit/c895d7c6fa07e63e96a8b2c19466112d398d8ac7,Yes
3676,ottogroup/palladium,palladium/tests/test_wsgi.py,56054ce07d07e633f70846f6477e9a2e12e39468,needed to avoid config postprocessing side effects in this test,https://github.com/ottogroup/palladium/commit/56054ce07d07e633f70846f6477e9a2e12e39468,No
3677,modAL-python/modAL,active_learning/models.py,8fa11142ad914d3335342f78297fa89bb76928b6,TODO: test if this works with multiple shapes and types of data,https://github.com/modAL-python/modAL/commit/8fa11142ad914d3335342f78297fa89bb76928b6,No
3678,EducationalTestingService/skll,skll/learner.py,2f1062df7acafc258009e4252f69d6cd2095866d,columns for the test set.,https://github.com/EducationalTestingService/skll/commit/2f1062df7acafc258009e4252f69d6cd2095866d,Yes
3679,scikit-learn-contrib/metric-learn,test/test_sklearn_compat.py,580d38d12d01af755dc2cb9a3cf0d81d1f633cf9,we subsample the data for the test to be more efficient,https://github.com/scikit-learn-contrib/metric-learn/commit/580d38d12d01af755dc2cb9a3cf0d81d1f633cf9,Yes
3680,oracle/Skater,pyinterpret/tests/test_partial_dependence.py,5fe98231a26e08d81f9684a681af43bc2fb2b847,TODO: Add tests for various kinds of kwargs like sampling for pdp funcs,https://github.com/oracle/Skater/commit/5fe98231a26e08d81f9684a681af43bc2fb2b847,No
3681,Accenture/AmpliGraph,tests/ampligraph/evaluation/test_protocol.py,3de385c15a8f21e816f3ef1958e1607882fed6eb,"@pytest.mark.skip(reason=\""excluded to try out jenkins.\"")   # TODO: re-enable this",https://github.com/Accenture/AmpliGraph/commit/3de385c15a8f21e816f3ef1958e1607882fed6eb,Yes
3682,Accenture/AmpliGraph,tests/ampligraph/temporal/test_models_temp.py,3de385c15a8f21e816f3ef1958e1607882fed6eb,TODO: TEC-1819: Enable coexisting eager and graph tf tests:,https://github.com/Accenture/AmpliGraph/commit/3de385c15a8f21e816f3ef1958e1607882fed6eb,Yes
3683,Accenture/AmpliGraph,ampligraph/latent_features/models.py,c6c7ca5a66c1e709cf0770212b7e50f3e7952e3f,TODO: Update _test_generator; maybe include scores_filter (?),https://github.com/Accenture/AmpliGraph/commit/c6c7ca5a66c1e709cf0770212b7e50f3e7952e3f,No
3684,tensorflow/privacy,research/pate_2017/train_student.py,93e9585f185a2b9b10f6f4c98433526bb7300a78,Store unused part of test set for use as a test set after student training,https://github.com/tensorflow/privacy/commit/93e9585f185a2b9b10f6f4c98433526bb7300a78,No
3685,tensorflow/privacy,tensorflow_privacy/research/pate_2017/train_student.py,313edfc80c10f1ae770f9528f641da7662911f40,Store unused part of test set for use as a test set after student training,https://github.com/tensorflow/privacy/commit/313edfc80c10f1ae770f9528f641da7662911f40,No
3686,awslabs/sockeye,test/unit/test_scoring.py,16dba0131432e5db01f3ee92550554dae60192c5,TODO: make this a useful test,https://github.com/awslabs/sockeye/commit/16dba0131432e5db01f3ee92550554dae60192c5,No
3687,awslabs/sockeye,test/unit/test_beam_search.py,3271ecefa3a7104d51b425369038578765dc5dba,TODO make this a useful test,https://github.com/awslabs/sockeye/commit/3271ecefa3a7104d51b425369038578765dc5dba,Yes
3688,awslabs/sockeye,test/unit/test_beam_search.py,3271ecefa3a7104d51b425369038578765dc5dba,TODO: add vocabulary selection test,https://github.com/awslabs/sockeye/commit/3271ecefa3a7104d51b425369038578765dc5dba,No
3689,optuna/optuna,tests/test_trial.py,97836d5f74beebc5557e7a76806ec50a3020180e,TODO: more tests for _check_distribution,https://github.com/optuna/optuna/commit/97836d5f74beebc5557e7a76806ec50a3020180e,Yes
3690,optuna/optuna,tests/importance_tests/fanova_tests/test_fanova.py,2680a5ff28ed65873acee0debcb6d6548e1a3e1d,Create test data with 5 columns with the following types of features.,https://github.com/optuna/optuna/commit/2680a5ff28ed65873acee0debcb6d6548e1a3e1d,Yes
3691,optuna/optuna,tests/multi_objective/samplers_tests/test_nsga2.py,3c9514e9c8d91eae20a27bd078e1f1a13ca87ad1,TODO(ohta): Consider to move this utility function to `optuna.testing` module.,https://github.com/optuna/optuna/commit/3c9514e9c8d91eae20a27bd078e1f1a13ca87ad1,Yes
3692,optuna/optuna,tutorial/20_recipes/006_user_defined_sampler.py,df77d7c3a3aabc6d5002cdabf71d4a9106d2aeec,"\""\""\"" || .. _ud_pruner: ||  || User-Defined Pruner || =================== ||  || This tutorial walks you through how :class:`~optuna.pruners.ThresholdPruner` is implemented to give you || a big picture of how you can implement your own pruners. ||  || As you can see in the :class:`~optuna.pruner.BasePruner`; || what you need to implement is :func:`~optuna.pruner.BasePruner.prune` || which takes :class:`~optuna.study.Study` and currently being evaluated || :class:`~optuna.trial.FrozenTrial`. || This means that you can have the access to the annals of :class:`~optuna.trial.FrozenTrial`\\\\'s. || :class:`~optuna.pruners.SuccessiveHalvingPruner` utilizes this feature. ||  || So; for the illustration purpose; I walk through you the implementation of :class:`~optuna.pruners.ThresholdPruner`\\\\'s :func:`~optuna.pruners.ThresholdPruner.prune`. ||  || .. code:: python ||  ||     class ThresholdProuner(BasePruner): ||  ||         ... ||  ||         def pruner( ||             self; ||             study: optuna.study.Study; ||             trial: optuna.trial.FrozenTrial ||         ) -> bool: ||  ||             # `step` generally represents the iteration or epoch. ||             step = trail.last_step ||  ||             # ``False`` means not pruned. ||             if step is None: ||                 return False ||  ||             # Check whether the ``trial`` has run the enough ``steps``. ||             if not _is_first_in_interval_step( ||                 step; trial.intermediate_values.keys(); n_warmup_steps; self._interval_steps ||             ): ||                 return False ||  ||             latest_value = trial.intermediate_values[step] ||             if math.isnan(latest_value): ||                 return True ||  ||             if latest_value < self._lower: ||                 return True ||  ||             if latest_value > self._upper: ||                 return True ||  ||             return False ||  ||  || \""\""\""",https://github.com/optuna/optuna/commit/df77d7c3a3aabc6d5002cdabf71d4a9106d2aeec,Yes
3693,optuna/optuna,tests/test_transform.py,6c11ae1db6b13a94a002983f6bc2b4750c3fbd5b,Create test data with 5 columns with the following types of parameters.,https://github.com/optuna/optuna/commit/6c11ae1db6b13a94a002983f6bc2b4750c3fbd5b,No
3694,optuna/optuna,tests/samplers_tests/test_nsga2.py,a8df87cd388d8000ed0892cae8ea87983fe48307,TODO(ohta): Consider to move this utility function to `optuna.testing` module.,https://github.com/optuna/optuna/commit/a8df87cd388d8000ed0892cae8ea87983fe48307,Yes
3695,explosion/thinc,tests/test_model.py,0a3b498f3c0f30d0c6544f605455b7c71ac0ec76,TODO: Need a test that exercises multiple lines. Example bug:,https://github.com/explosion/thinc/commit/0a3b498f3c0f30d0c6544f605455b7c71ac0ec76,Yes
3696,explosion/thinc,thinc/ids2id/tests/test_avgtron.py,7c0eb51c3d3ec715b4d7e48fb89addf1596a8a5e,## TODO: Need a test that exercises multiple lines. Example bug:,https://github.com/explosion/thinc/commit/7c0eb51c3d3ec715b4d7e48fb89addf1596a8a5e,Yes
3697,explosion/thinc,thinc/tests/linear/test_avgtron.py,3d422f894d86d60d04ce1670430eb5feb08100f4,## TODO: Need a test that exercises multiple lines. Example bug:,https://github.com/explosion/thinc/commit/3d422f894d86d60d04ce1670430eb5feb08100f4,Yes
3698,explosion/thinc,thinc/tests/linear/test_linear.py,5468beaaf18ea8ac1b955b25bcea0aea1c650af0,## TODO: Need a test that exercises multiple lines. Example bug:,https://github.com/explosion/thinc/commit/5468beaaf18ea8ac1b955b25bcea0aea1c650af0,Yes
3699,explosion/thinc,thinc/tests/layers/test_lstm.py,377ddfeaf0a020887521ce5afda994a2bed7e8be,"@pytest.mark.xfail(reason=\""validation; TODO: fix\"")",https://github.com/explosion/thinc/commit/377ddfeaf0a020887521ce5afda994a2bed7e8be,Yes
3700,explosion/thinc,thinc/tests/layers/test_uniqued.py,6909185e9401a71f6014a007b102c4063aa90e5d,TODO: This test is a problem; because we exceed the embedding table.,https://github.com/explosion/thinc/commit/6909185e9401a71f6014a007b102c4063aa90e5d,No
3701,tensorflow/ranking,tensorflow_ranking/python/model.py,368cff0bd610920c067a355d87e6078e8c0274d4,unittest purpose. We can find a better way to avoid setting this seed,https://github.com/tensorflow/ranking/commit/368cff0bd610920c067a355d87e6078e8c0274d4,No
3702,microsoft/dowhy,tests/causal_estimators/test_econml_estimator.py,a92ac859ea79fa4b69b60633c4c7f6e2225b10a8,TODO: Test IntentToTreatDRIV when EconML v0.7 comes out,https://github.com/microsoft/dowhy/commit/a92ac859ea79fa4b69b60633c4c7f6e2225b10a8,Yes
3703,chartbeat-labs/textacy,tests/test_dataset_wikipedia.py,0841f2ba3de88922e061e0c966e5befd6ac58278,TODO: test individual parsing functions,https://github.com/chartbeat-labs/textacy/commit/0841f2ba3de88922e061e0c966e5befd6ac58278,No
3704,chartbeat-labs/textacy,tests/test_doc.py,772759cd2a83f87ea288eabeb6167baf6cd0d22b,TODO: re-add this test if count() gets implemented,https://github.com/chartbeat-labs/textacy/commit/772759cd2a83f87ea288eabeb6167baf6cd0d22b,Yes
3705,chartbeat-labs/textacy,tests/test_doc.py,2d81f7bda58297ec717e4a7bbe2b57867115dcd7,TODO: add more combos; if you can convince pytest to not hang forever,https://github.com/chartbeat-labs/textacy/commit/2d81f7bda58297ec717e4a7bbe2b57867115dcd7,Yes
3706,chartbeat-labs/textacy,tests/spacier/test_doc_extensions.py,a9a37a6fbd7874d2578196c9d9fcee74e4f03865,TODO: re-add this test if count() gets implemented,https://github.com/chartbeat-labs/textacy/commit/a9a37a6fbd7874d2578196c9d9fcee74e4f03865,Yes
3707,EducationalTestingService/skll,tests/test_voting_learners.py,298a42d29d3adce9b18c3ca527823f106897b2e8,define some constants needed for testing,https://github.com/EducationalTestingService/skll/commit/298a42d29d3adce9b18c3ca527823f106897b2e8,Yes
3708,EducationalTestingService/skll,tests/test_voting_learners_api_1.py,21a82f8921e422f8a9193239799fcf4e1bf4558e,define some constants needed for testing,https://github.com/EducationalTestingService/skll/commit/21a82f8921e422f8a9193239799fcf4e1bf4558e,Yes
3709,EducationalTestingService/skll,tests/test_voting_learners_api_2.py,21a82f8921e422f8a9193239799fcf4e1bf4558e,define some constants needed for testing,https://github.com/EducationalTestingService/skll/commit/21a82f8921e422f8a9193239799fcf4e1bf4558e,Yes
3710,EducationalTestingService/skll,tests/test_voting_learners_api_3.py,21a82f8921e422f8a9193239799fcf4e1bf4558e,define some constants needed for testing,https://github.com/EducationalTestingService/skll/commit/21a82f8921e422f8a9193239799fcf4e1bf4558e,Yes
3711,EducationalTestingService/skll,tests/test_voting_learners_api_4.py,21a82f8921e422f8a9193239799fcf4e1bf4558e,define some constants needed for testing,https://github.com/EducationalTestingService/skll/commit/21a82f8921e422f8a9193239799fcf4e1bf4558e,Yes
3712,EducationalTestingService/skll,tests/test_voting_learners_api_5.py,21a82f8921e422f8a9193239799fcf4e1bf4558e,define some constants needed for testing,https://github.com/EducationalTestingService/skll/commit/21a82f8921e422f8a9193239799fcf4e1bf4558e,Yes
3713,pytorch/translate,pytorch_translate/ensemble_export.py,589fffa9254cd8413362a87204c039cb3681bf02,TODO: This method hasn't been tested for now.,https://github.com/pytorch/translate/commit/589fffa9254cd8413362a87204c039cb3681bf02,No
3714,nilearn/nilearn,nisl/signals.py,fd0ae4e922f86fb7910d1d70318dbda4d14238c3,# FIXME: test this detrend implementation; improve; benchmark.,https://github.com/nilearn/nilearn/commit/fd0ae4e922f86fb7910d1d70318dbda4d14238c3,Yes
3715,nilearn/nilearn,nisl/tests/test_roi.py,f89d396b8a18b1cec4f9a1639d9efbb0238729c5,FIXME: test dtype argument,https://github.com/nilearn/nilearn/commit/f89d396b8a18b1cec4f9a1639d9efbb0238729c5,No
3716,nilearn/nilearn,nisl/tests/test_roi.py,f89d396b8a18b1cec4f9a1639d9efbb0238729c5,FIXME: test labels argument,https://github.com/nilearn/nilearn/commit/f89d396b8a18b1cec4f9a1639d9efbb0238729c5,Yes
3717,nilearn/nilearn,nisl/tests/test_region.py,99fdce6a8ab311eda37eaf3eb07ae77c193426a2,FIXME: Test reversibility of img_from_maps() and signals_from_maps();,https://github.com/nilearn/nilearn/commit/99fdce6a8ab311eda37eaf3eb07ae77c193426a2,Yes
3718,nilearn/nilearn,nisl/decomposition/tests/test_multi_pca.py,41ec4c8ef89fbcbf1edbaa92ba65874fc5396074,XXX: this is mostly a smoke test,https://github.com/nilearn/nilearn/commit/41ec4c8ef89fbcbf1edbaa92ba65874fc5396074,Yes
3719,nilearn/nilearn,nisl/honorio_samaras.py,7d19b206fd66bffd2f47ae3ebc67b8f61c106a45,FIXME: there's something wrong with this test.,https://github.com/nilearn/nilearn/commit/7d19b206fd66bffd2f47ae3ebc67b8f61c106a45,Yes
3720,nilearn/nilearn,nisl/honorio_samaras.py,6ec0d156811feec1ac48c11471e874840e333e33,FIXME: there's something wrong with this test.,https://github.com/nilearn/nilearn/commit/6ec0d156811feec1ac48c11471e874840e333e33,Yes
3721,nilearn/nilearn,nilearn/_utils/fixes/sklearn_f_regression.py,b792740c3a441ada48f483844366fcb615ab2b89,XXX could use corr \/= row_norms(X.T) here; but the test doesn't pass,https://github.com/nilearn/nilearn/commit/b792740c3a441ada48f483844366fcb615ab2b89,Yes
3722,nilearn/nilearn,nilearn/_utils/fixes/sklearn_f_regression_nosparse.py,b7728d5b71ae5aca9ad06f0a4298b19db8b74f0d,XXX could use corr \/= row_norms(X.T) here; but the test doesn't pass,https://github.com/nilearn/nilearn/commit/b7728d5b71ae5aca9ad06f0a4298b19db8b74f0d,Yes
3723,nilearn/nilearn,_utils/fixes/sklearn_f_regression.py,204c3dc2dfcb78cfe478e50fd81182252222d58f,XXX could use corr \/= row_norms(X.T) here; but the test doesn't pass,https://github.com/nilearn/nilearn/commit/204c3dc2dfcb78cfe478e50fd81182252222d58f,Yes
3724,nilearn/nilearn,_utils/fixes/sklearn_f_regression_nosparse.py,204c3dc2dfcb78cfe478e50fd81182252222d58f,XXX could use corr \/= row_norms(X.T) here; but the test doesn't pass,https://github.com/nilearn/nilearn/commit/204c3dc2dfcb78cfe478e50fd81182252222d58f,Yes
3725,nilearn/nilearn,doc/sphinxext/sphinxgallery/gen_gallery.py,aca637208dc967a433598d7080458d3e327d38a7,HACK: Stop nosetests running setup() above,https://github.com/nilearn/nilearn/commit/aca637208dc967a433598d7080458d3e327d38a7,Yes
3726,nilearn/nilearn,nilearn/sparse_models/cv.py,ce9e8b0b9c93f980648c0d7a9ae6f001edb91fdf,XXX uncovered \/ untested code!,https://github.com/nilearn/nilearn/commit/ce9e8b0b9c93f980648c0d7a9ae6f001edb91fdf,Yes
3727,nilearn/nilearn,nilearn/sparse_models/tests/test_smooth_lasso.py,ce9e8b0b9c93f980648c0d7a9ae6f001edb91fdf,XXX A small dataset here (this test is very lengthy),https://github.com/nilearn/nilearn/commit/ce9e8b0b9c93f980648c0d7a9ae6f001edb91fdf,No
3728,nilearn/nilearn,nilearn/sparse_models/tests/test_smooth_lasso.py,ce9e8b0b9c93f980648c0d7a9ae6f001edb91fdf,XXX This test is senseless as we solver reports even history of,https://github.com/nilearn/nilearn/commit/ce9e8b0b9c93f980648c0d7a9ae6f001edb91fdf,No
3729,nilearn/nilearn,nilearn/sparse_models/_cv_tricks.py,d2413d9a01eff57c08c908514e3d2f1bd6dc11f5,XXX uncovered \/ untested code!,https://github.com/nilearn/nilearn/commit/d2413d9a01eff57c08c908514e3d2f1bd6dc11f5,Yes
3730,nilearn/nilearn,nilearn/sparse_models/tests/test_cv.py,67cd5f0eec00c1da6d9b60959b5b0d745f4a6f0d,XXX test fails with early stopping in CV,https://github.com/nilearn/nilearn/commit/67cd5f0eec00c1da6d9b60959b5b0d745f4a6f0d,Yes
3731,nilearn/nilearn,nilearn/decoding/cv.py,b69f7dd1620a5bfc8c6711b0a8344614d3cf3fcc,XXX uncovered \/ untested code!,https://github.com/nilearn/nilearn/commit/b69f7dd1620a5bfc8c6711b0a8344614d3cf3fcc,Yes
3732,nilearn/nilearn,nilearn/sparse_models/cv.py,5caa0e45e9523192e55bf8394f6c8fb0ad773369,XXX uncovered \/ untested code!,https://github.com/nilearn/nilearn/commit/5caa0e45e9523192e55bf8394f6c8fb0ad773369,Yes
3733,nilearn/nilearn,nilearn/sparse_models/tests/test_smooth_lasso.py,5caa0e45e9523192e55bf8394f6c8fb0ad773369,XXX A small dataset here (this test is very lengthy),https://github.com/nilearn/nilearn/commit/5caa0e45e9523192e55bf8394f6c8fb0ad773369,No
3734,nilearn/nilearn,nilearn/sparse_models/tests/test_smooth_lasso.py,5caa0e45e9523192e55bf8394f6c8fb0ad773369,XXX This test is senseless as we solver reports even history of,https://github.com/nilearn/nilearn/commit/5caa0e45e9523192e55bf8394f6c8fb0ad773369,No
3735,nilearn/nilearn,nilearn/sparse_models/_cv_tricks.py,91776d94113f4dfa4bcdb2b4a5a7baec758bda67,XXX uncovered \/ untested code!,https://github.com/nilearn/nilearn/commit/91776d94113f4dfa4bcdb2b4a5a7baec758bda67,Yes
3736,nilearn/nilearn,nilearn/sparse_models/tests/test_cv.py,0e68d70b52626fdfd4883c9f147dc3987ec5d017,XXX test fails with early stopping in CV,https://github.com/nilearn/nilearn/commit/0e68d70b52626fdfd4883c9f147dc3987ec5d017,Yes
3737,nilearn/nilearn,nilearn/decoding/cv.py,a4875ae4b8f312aeeb7193b15f2862b16a940ff0,XXX uncovered \/ untested code!,https://github.com/nilearn/nilearn/commit/a4875ae4b8f312aeeb7193b15f2862b16a940ff0,Yes
3738,nilearn/nilearn,nilearn/decoding/space_net.py,1d8192c79ab58d098a1925ad1457af78bbc37c74,XXX uncovered \/ untested code!,https://github.com/nilearn/nilearn/commit/1d8192c79ab58d098a1925ad1457af78bbc37c74,Yes
3739,nilearn/nilearn,nilearn/decoding/tests/test_space_net.py,59edf53452a10b55102db2872940dd42549103f7,XXX test fails with early stopping in CV,https://github.com/nilearn/nilearn/commit/59edf53452a10b55102db2872940dd42549103f7,Yes
3740,nilearn/nilearn,doc/sphinxext/sphinx_gallery/gen_rst.py,307e54f5e15a20079e3be97b865b97af929b2636,XXX This check can break during testing e.g. if you uncomment the,https://github.com/nilearn/nilearn/commit/307e54f5e15a20079e3be97b865b97af929b2636,Yes
3741,nilearn/nilearn,doc/sphinxext/sphinx_gallery/gen_gallery.py,0429387cd8e61fdbe15e6927a12d3827acf7980c,TODO: Test this behavior.,https://github.com/nilearn/nilearn/commit/0429387cd8e61fdbe15e6927a12d3827acf7980c,Yes
3742,nilearn/nilearn,nilearn/__init__.py,65009b93329d8b290751771545b94b1dc60c33d2,Temporary work around to address formatting issues in doc tests,https://github.com/nilearn/nilearn/commit/65009b93329d8b290751771545b94b1dc60c33d2,No
3743,nilearn/nilearn,examples/06_second_level_models_non_parametric_tests/plot_second_level_one_sample_test.py,e2b99002e25c4aad8abf42795fca11fe32a244c9,"\""\""\"" || Second-level fMRI model: one sample test || ======================================== ||  || Full step-by-step example of fitting a GLM to perform a second-level analysis (one-sample test) || and visualizing the results. ||  || More specifically: ||  || 1. A sequence of subject fMRI button press contrasts is downloaded. || 2. a mask of the useful brain volume is computed || 3. A one-sample t-test is applied to the brain maps ||  || We focus on a given contrast of the localizer dataset: the motor response to left versus right button press. Both at the ndividual and group level; this is expected to elicit activity in the motor cortex (positive in the right hemisphere; negative in the left hemisphere). ||  || \""\""\""",https://github.com/nilearn/nilearn/commit/e2b99002e25c4aad8abf42795fca11fe32a244c9,No
3744,nilearn/nilearn,nistats/examples/03_second_level_models/plot_second_level_one_sample_test.py,7df3c3f60bd20c70732c69518eed600b6908ec44,"\""\""\"" || Second-level fMRI model: one sample test || ======================================== ||  || Full step-by-step example of fitting a GLM to perform a second-level analysis || (one-sample test) || and visualizing the results. ||  || More specifically: ||  || 1. A sequence of subject fMRI button press contrasts is downloaded. || 2. a mask of the useful brain volume is computed || 3. A one-sample t-test is applied to the brain maps ||  || We focus on a given contrast of the localizer dataset: the motor response to || left versus right button press. Both at the ndividual and group level; this is || expected to elicit activity in the motor cortex (positive in the right || hemisphere; negative in the left hemisphere). ||  || \""\""\""",https://github.com/nilearn/nilearn/commit/7df3c3f60bd20c70732c69518eed600b6908ec44,Yes
3745,nilearn/nilearn,nistats/nistats/__init__.py,7df3c3f60bd20c70732c69518eed600b6908ec44,"\""\""\"" || functional MRI module for NeuroImaging in python || -------------------------------------------------- ||  || Documentation is available in the docstrings and online at || http:\/\/nistats.github.io. ||  || Contents || -------- || Nistats is a Python module for fast and easy functional MRI statistical || analysis. ||  || Submodules || --------- || datasets                --- Utilities to download NeuroImaging datasets || hemodynamic_models      --- Hemodyanmic response function specification || design_matrix           --- Design matrix creation for fMRI analysis || experimental_paradigm   --- Experimental paradigm files checks and utils || model                   --- Statistical tests on likelihood models || regression              --- Standard regression models || first_level_model       --- API for first level fMRI model estimation || second_level_model      --- API for second level fMRI model estimation || contrasts               --- API for contrast computation and manipulations || thresholding            --- Utilities for cluster-level statistical results || reporting               --- Utilities for creating reports & plotting data || utils                   --- Miscellaneous utilities || \""\""\""",https://github.com/nilearn/nilearn/commit/7df3c3f60bd20c70732c69518eed600b6908ec44,Yes
3746,nilearn/nilearn,nilearn/stats/__init__.py,8a9438634cbf2acb5bce4e7a10aa5e7ce5d6aef5,"\""\""\"" || functional MRI module for NeuroImaging in python || -------------------------------------------------- ||  || Documentation is available in the docstrings and online at || http:\/\/nistats.github.io. ||  || Contents || -------- || Nistats is a Python module for fast and easy functional MRI statistical || analysis. ||  || Submodules || --------- || datasets                --- Utilities to download NeuroImaging datasets || hemodynamic_models      --- Hemodyanmic response function specification || design_matrix           --- Design matrix creation for fMRI analysis || experimental_paradigm   --- Experimental paradigm files checks and utils || model                   --- Statistical tests on likelihood models || regression              --- Standard regression models || first_level_model       --- API for first level fMRI model estimation || second_level_model      --- API for second level fMRI model estimation || contrasts               --- API for contrast computation and manipulations || thresholding            --- Utilities for cluster-level statistical results || reporting               --- Utilities for creating reports & plotting data || utils                   --- Miscellaneous utilities || \""\""\""",https://github.com/nilearn/nilearn/commit/8a9438634cbf2acb5bce4e7a10aa5e7ce5d6aef5,Yes
3747,nilearn/nilearn,examples/02_decoding/plot_oasis_vbm.py,a2726e4adf24705bd56b53c932152906b8d1d53f,Sort test data for better visualization (trend; etc.),https://github.com/nilearn/nilearn/commit/a2726e4adf24705bd56b53c932152906b8d1d53f,No
3748,cltk/cltk,cltk/corpus/sanskrit/itrans/itrans_transliterator.py,1e8e9a75c19df8770ba4781880f0f8dbcbe838f1,"\""\""\"" Transliterate texts between unicode and standard transliteration schemes. ||  || Transliterate texts between non-latin scripts and commonly-used latin || transliteration schemes. Uses standard Unicode character blocks --  || e.g. DEVANAGARI U+0900 ... U+097F -- and transliteration schemes --  || e.g. the IAST convention for transliteration of Sanskrit to latin-with-dots. ||  || The following character blocks and transliteration schemes are included: ||  || DEVANAGARI ||     IAST ||     ITRANS -- http:\/\/www.aczoom.com\/itrans\/#itransencoding (Sanskrit only) ||     Harvard Kyoto ||      || CYRILLIC ||     ISO 9:1995 (Russian only) ||      || New character blocks and transliteration schemes can be added by creating || new CharacterBlock and TransliterationScheme objects. ||  || USAGE || -------- || Transliterate a text: ||  || >>> import transliterator || >>> transliterator.transliterate('yogazcittavRttinirodhaH'; 'harvardkyoto'; || ...     'devanagari'; {'outputASCIIEncoded' : True}) || '&#x92f;&#x94b;&#x917;&#x936;&#x94d;&#x91a;&#x93f;&#x924;&#x94d;&#x924;&#x935;&#x943;&#x924;&#x94d;&#x924;&#x93f;&#x928;&#x93f;&#x930;&#x94b;&#x927;&#x903;' ||  || Create a new CharacterBlock and TransliterationScheme: ||  || >>> import transliterator || >>> cb = transliterator.CharacterBlock('NEWBLOCK'; range(0x901; 0x9FF)) || >>> scheme = transliterator.TransliterationScheme(cb.name; 'NEWSCHEME'; || ...                          {'ab': 0x901; 'cd': 0x902}) || >>> transliterator.transliterate('abcd'; scheme; cb; {'outputASCIIEncoded' : True}) || '&#x901;&#x902;' ||  || COPYRIGHT AND DISCLAIMER || ------------------------------------ || Transliterator is: ||  || version 0.1 software  - use at your own risk. ||  || The IAST; ITRANS and Harvard-Kyoto transliteration schemes have been || tested for classical Sanskrit; not for any other language. ||  || The Cyrillic alphabet and ISO 9:1995 transliteration (for Russian only) || are included but have been even more lightly tested than Devanagari. ||  || Copyright (c) 2005 by Alan Little ||  || By obtaining; using; and\/or copying this software and\/or its || associated documentation; you agree that you have read; understood; || and will comply with the following terms and conditions: ||  || Permission to use; copy; modify; and distribute this software and || its associated documentation for any purpose and without fee is || hereby granted; provided that the above copyright notice appears in || all copies; and that both that copyright notice and this permission || notice appear in supporting documentation; and that the name of  || the author not be used in advertising or publicity pertaining to  || distribution of the software without specific; written prior permission. ||  || THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE;  || INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS.   || IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL; INDIRECT OR  || CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM  || LOSS OF USE; DATA OR PROFITS; WHETHER IN AN ACTION OF CONTRACT;  || NEGLIGENCE OR OTHER TORTIOUS ACTION; ARISING OUT OF OR IN CONNECTION  || WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. ||         || \""\""\""",https://github.com/cltk/cltk/commit/1e8e9a75c19df8770ba4781880f0f8dbcbe838f1,No
3749,cltk/cltk,cltk/tests/test_corpus.py,7413fbae0825a7f7ef7e2ae21e9eda8df723198f,"\""\""\""Test cltk.corpus. ||  || TODO: Consider whether to import the very large Word2Vec corpora for Greek and Latin. || \""\""\""",https://github.com/cltk/cltk/commit/7413fbae0825a7f7ef7e2ae21e9eda8df723198f,No
3750,cltk/cltk,cltk/tests/test_corpus.py,cbf67f1e019483ed6e7ed71545f5a249e281ac0d,"\""\""\""Test cltk.corpus. ||  || TODO: Consider whether to import the very large Word2Vec corpora for Greek and Latin. || \""\""\""",https://github.com/cltk/cltk/commit/cbf67f1e019483ed6e7ed71545f5a249e281ac0d,No
3751,cltk/cltk,cltk/tests/test_arabic_utils.py,101faecbdb6ec22ad627f602e249f5f42d161d5f,is_arabicrange TODO: add test,https://github.com/cltk/cltk/commit/101faecbdb6ec22ad627f602e249f5f42d161d5f,Yes
3752,cltk/cltk,cltk/tests/test_arabic_utils.py,101faecbdb6ec22ad627f602e249f5f42d161d5f,is_arabicword TODO: test other cases,https://github.com/cltk/cltk/commit/101faecbdb6ec22ad627f602e249f5f42d161d5f,Yes
3753,cltk/cltk,cltk/tests/test_arabic_utils.py,101faecbdb6ec22ad627f602e249f5f42d161d5f,test separate function  TODO: testme,https://github.com/cltk/cltk/commit/101faecbdb6ec22ad627f602e249f5f42d161d5f,Yes
3754,cltk/cltk,cltk/corpus/readers.py,320e810184204d9171e683241adea4c5c1c73a04,TODO add other languages and write tests for each corpus,https://github.com/cltk/cltk/commit/320e810184204d9171e683241adea4c5c1c73a04,Yes
3755,cltk/cltk,cltk/tests/test_nlp/test_lemmatize.py,aaa5c2a78e07dd9b3305a6ba9effc7c4a1539a1b,test_str = 'i ii iii iv v vi vii vii ix x xx xxx xl l lx c cc',https://github.com/cltk/cltk/commit/aaa5c2a78e07dd9b3305a6ba9effc7c4a1539a1b,No
3756,cltk/cltk,cltk/tests/test_corpus/test_corpus.py,ed1a2414287bb5bf121950528a1a7b752a8e3d71,Need a additional instance because tests below change internals #TO-DO Fix,https://github.com/cltk/cltk/commit/ed1a2414287bb5bf121950528a1a7b752a8e3d71,No
3757,cltk/cltk,src/cltkv1/wrappers/stanford.py,31584e4c12153612511fb4c984c7db50df751dd7,TODO: Write tests for all treebanks,https://github.com/cltk/cltk/commit/31584e4c12153612511fb4c984c7db50df751dd7,No
3758,cltk/cltk,src/cltkv1/utils/utils.py,db3fd85f5086e94d7f52cbeb9f5ac09d3b9f3864,TODO: Run tests with a defined `$CLTK_DATA` environment variable),https://github.com/cltk/cltk/commit/db3fd85f5086e94d7f52cbeb9f5ac09d3b9f3864,No
3759,cltk/cltk,src/cltkv1/embeddings/embeddings.py,e50e90af4ba40d005648b67450d079c68a2dc6b9,TODO: Do better than test for just name. Try trimming up to user home dir.,https://github.com/cltk/cltk/commit/e50e90af4ba40d005648b67450d079c68a2dc6b9,Yes
3760,cltk/cltk,src/cltkv1/alphabet/egy.py,e92bd756e5c5e824e2bfa2e7ff4d777a40d049f5,"\""\""\""Convert MdC transliterated text to Unicode. ||  || TODO: Add tests and clean up. || \""\""\""",https://github.com/cltk/cltk/commit/e92bd756e5c5e824e2bfa2e7ff4d777a40d049f5,No
3761,cltk/cltk,src/cltkv1/alphabet/non.py,e92bd756e5c5e824e2bfa2e7ff4d777a40d049f5,"\""\""\""Old Norse runes; Unicode block: 16A0\u201316FF. || Source: *Viking Language 1*; Jessie L. Byock ||  || TODO: Document and test better. || \""\""\""",https://github.com/cltk/cltk/commit/e92bd756e5c5e824e2bfa2e7ff4d777a40d049f5,No
3762,cltk/cltk,src/cltkv1/alphabet/pes.py,e92bd756e5c5e824e2bfa2e7ff4d777a40d049f5,"\""\""\""The Persian alphabet. ||  || TODO: Write tests. || \""\""\""",https://github.com/cltk/cltk/commit/e92bd756e5c5e824e2bfa2e7ff4d777a40d049f5,Yes
3763,cltk/cltk,src/cltkv1/alphabet/pli.py,e92bd756e5c5e824e2bfa2e7ff4d777a40d049f5,"\""\""\""The Pali alphabet. ||  || TODO: Add tests. ||  || \""\""\""",https://github.com/cltk/cltk/commit/e92bd756e5c5e824e2bfa2e7ff4d777a40d049f5,Yes
3764,cltk/cltk,src/cltkv1/alphabet/grc/beta_to_unicode.py,3d602feb7ddbb9881ee362ff4898263e05647e70,"\""\""\""Converts legacy encodings into Unicode. ||  || TODO: Rm regex dependency || TODO: Add tests || \""\""\""",https://github.com/cltk/cltk/commit/3d602feb7ddbb9881ee362ff4898263e05647e70,Yes
3765,cltk/cltk,src/cltkv1/alphabet/tel.py,18e719e21a8a3a45e14d2ae053686afd7522877a,"\""\""\""Telugu alphabet ||  || TODO: Add tests. || \""\""\""",https://github.com/cltk/cltk/commit/18e719e21a8a3a45e14d2ae053686afd7522877a,No
3766,cltk/cltk,src/cltkv1/alphabet/urd.py,18e719e21a8a3a45e14d2ae053686afd7522877a,"\""\""\""Urdu alphabet ||  || TODO: Add tests. || \""\""\""",https://github.com/cltk/cltk/commit/18e719e21a8a3a45e14d2ae053686afd7522877a,No
3767,cltk/cltk,src/cltkv1/tests/test_corpus.py,64e16fcaa4918ba72def1d3a5dccc9f4b9988fff,# Need a additional instance because tests below change internals #TO-DO Fix,https://github.com/cltk/cltk/commit/64e16fcaa4918ba72def1d3a5dccc9f4b9988fff,Yes
3768,cltk/cltk,src/cltkv1/tests/test_tokenize_and_sent.py,8a01afdec7974f173135e0467241488382dd3f9f,"\""\""\""Test cltk.tokenize. ||  || TODO: Mk different file for sentence tests. || \""\""\""",https://github.com/cltk/cltk/commit/8a01afdec7974f173135e0467241488382dd3f9f,Yes
3769,cltk/cltk,src/cltkv1/dependency/stanza.py,13fff8524c52cc4fca2caa04836ecde7d024a9d8,TODO: Write tests for all treebanks,https://github.com/cltk/cltk/commit/13fff8524c52cc4fca2caa04836ecde7d024a9d8,No
3770,cltk/cltk,src/cltkv1/readers/readers.py,ff79291b015b63b23036397b6050cc632f706cda,TODO add other languages and write tests for each corpus,https://github.com/cltk/cltk/commit/ff79291b015b63b23036397b6050cc632f706cda,Yes
3771,cltk/cltk,tests/test_embeddings.py,cbc0a834ab95049336a08d8a3b0892b64ba41811,TODO: Add Arabic test; fails with `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 97: invalid continuation byte`,https://github.com/cltk/cltk/commit/cbc0a834ab95049336a08d8a3b0892b64ba41811,No
3772,cltk/cltk,tests/test_tag.py,b1d723c41baa17d1123dc768e9a35003f7904c2d,TODO: Re-enable; see ``test_pos_crf_tagger_latin`` above,https://github.com/cltk/cltk/commit/b1d723c41baa17d1123dc768e9a35003f7904c2d,Yes
3773,cltk/cltk,src/cltk/phonology/__init__.py,04d433e340b6edea284c197ac8e83d3a40efe5d6,"\""\""\"" || The phonology module aims to provide tools that: ||  || - phonetically\/phonologically transcribe words of a given language; || - syllabify words. ||  || For some specific languages; there exist; for example; a word stresser || (i.e. a function that gives which syllable is stressed). ||  || These tasks are interesting in themselves for historical linguists or teachers. || They are also essential for more high-level tasks such as prosody analyzers. ||  || Like for all CLTK modules; the phonology module may be extended and improved if a set of features does not suit || your needs because they are insufficient or they do not follow rules you want to test || (agreement on phonology of extinct languages is often weak). || \""\""\""",https://github.com/cltk/cltk/commit/04d433e340b6edea284c197ac8e83d3a40efe5d6,Yes
3774,cltk/cltk,tests/test_corpora.py,7fa08d2ac4f09af1224789a4592290fbb81987f3,# Need a additional instance because tests below change internals #TO-DO Fix,https://github.com/cltk/cltk/commit/7fa08d2ac4f09af1224789a4592290fbb81987f3,Yes
3775,IntelPython/sdc,hpat/tests/test_hiframes.py,b937bcb3d7d229fe127df9c392c3c422bf426a21,XXX: test actual output,https://github.com/IntelPython/sdc/commit/b937bcb3d7d229fe127df9c392c3c422bf426a21,No
3776,IntelPython/sdc,hpat/hiframes_join.py,e1d4113053656d5fc07460c6bd785312917f1acb,XXX: create dummy output arrays to allow testing for now,https://github.com/IntelPython/sdc/commit/e1d4113053656d5fc07460c6bd785312917f1acb,Yes
3777,IntelPython/sdc,hpat/tests/test_hiframes.py,61cf292f660149b41d9f38a2718f4aca2a50d46b,XXX: test actual output,https://github.com/IntelPython/sdc/commit/61cf292f660149b41d9f38a2718f4aca2a50d46b,No
3778,IntelPython/sdc,hpat/tests/test_hiframes.py,57b45d80c5eadd2ad9d2ad5fea76b3760292881e,TODO: test without file,https://github.com/IntelPython/sdc/commit/57b45d80c5eadd2ad9d2ad5fea76b3760292881e,No
3779,IntelPython/sdc,hpat/distributed.py,ea25c0895af3142e235864d8a4116fbd9ee81d5f,TODO: add unittest,https://github.com/IntelPython/sdc/commit/ea25c0895af3142e235864d8a4116fbd9ee81d5f,Yes
3780,IntelPython/sdc,hpat/distributed.py,4432e99af36486b6612132317e4ff148b93077b9,TODO: add group unittest,https://github.com/IntelPython/sdc/commit/4432e99af36486b6612132317e4ff148b93077b9,No
3781,IntelPython/sdc,hpat/hiframes_aggregate.py,c89b3fd9c38e7cbd921bebe4ad64a6ed815874ac,TODO: test agg remove,https://github.com/IntelPython/sdc/commit/c89b3fd9c38e7cbd921bebe4ad64a6ed815874ac,Yes
3782,IntelPython/sdc,hpat/hiframes_aggregate.py,06eaeac416c4f1079a8815dd35a0964fa7500f79,XXX dummy test code,https://github.com/IntelPython/sdc/commit/06eaeac416c4f1079a8815dd35a0964fa7500f79,No
3783,IntelPython/sdc,hpat/hiframes.py,04c722541a852013d687523b43f5646a19a10397,FIXME: see why this breaks test_kmeans,https://github.com/IntelPython/sdc/commit/04c722541a852013d687523b43f5646a19a10397,Yes
3784,IntelPython/sdc,hpat/distributed_api.py,5a0e699129882d0520d4faccc05d1929e298ca1f,TODO: test,https://github.com/IntelPython/sdc/commit/5a0e699129882d0520d4faccc05d1929e298ca1f,No
3785,IntelPython/sdc,hpat/distributed_api.py,6fb97482e6626bd7a2f0f92cbd043acceb727425,TODO: test,https://github.com/IntelPython/sdc/commit/6fb97482e6626bd7a2f0f92cbd043acceb727425,No
3786,IntelPython/sdc,hpat/distributed_api.py,60519fb75200d61cf9e0afc1cf9d563ddbfc8597,TODO: test,https://github.com/IntelPython/sdc/commit/60519fb75200d61cf9e0afc1cf9d563ddbfc8597,No
3787,IntelPython/sdc,hpat/tests/test_hiframes.py,12dc0226a5a0f3300f5d1bf2896ba306e664b05b,TODO: better parallel sort test,https://github.com/IntelPython/sdc/commit/12dc0226a5a0f3300f5d1bf2896ba306e664b05b,Yes
3788,IntelPython/sdc,hpat/hiframes_join.py,a2a4f4f02158b325b9fcfa2010f3bcd6f6e5d227,TODO: corner case test,https://github.com/IntelPython/sdc/commit/a2a4f4f02158b325b9fcfa2010f3bcd6f6e5d227,No
3789,IntelPython/sdc,hpat/tests/test_hiframes.py,3a882b602070ee1d17214b67f25fa1fc101f59ff,TODO: test without file,https://github.com/IntelPython/sdc/commit/3a882b602070ee1d17214b67f25fa1fc101f59ff,No
3790,IntelPython/sdc,hpat/tests/test_rolling.py,4a3334e4ed2127576e8116c9cc9786f13d24c25d,XXX: skipping min\/max for this test since the behavior of Pandas,https://github.com/IntelPython/sdc/commit/4a3334e4ed2127576e8116c9cc9786f13d24c25d,No
3791,IntelPython/sdc,hpat/hiframes_typed.py,ee199e8688542cd40cc6c0f4f1e4f08755532b4d,TODO: why doesn't empty_inferred work for t4 mortgage test?,https://github.com/IntelPython/sdc/commit/ee199e8688542cd40cc6c0f4f1e4f08755532b4d,No
3792,IntelPython/sdc,hpat/pd_categorical_ext.py,4b37ea2861d38a249e70563548f08ab50eab6cb0,TODO: why does list_pack crash for test_csv_cat2?,https://github.com/IntelPython/sdc/commit/4b37ea2861d38a249e70563548f08ab50eab6cb0,No
3793,IntelPython/sdc,hpat/ml/d4p.py,9b90e784e157fe28f2d7c901ffc82964ef41e8ee,TODO: fix and test Numba unicode_type,https://github.com/IntelPython/sdc/commit/9b90e784e157fe28f2d7c901ffc82964ef41e8ee,Yes
3794,IntelPython/sdc,hpat/hiframes/hiframes_typed.py,635888b78950e4dbb282125c1b3310b7d74998bc,FIXME e.g. test_series_nlargest_parallel1 np.int32(),https://github.com/IntelPython/sdc/commit/635888b78950e4dbb282125c1b3310b7d74998bc,No
3795,IntelPython/sdc,hpat/hiframes/hiframes_typed.py,cd97ea885faf259c5230a2338bcfca9ae5592758,XXX remove when df pass is typed? (test_pass_series2),https://github.com/IntelPython/sdc/commit/cd97ea885faf259c5230a2338bcfca9ae5592758,No
3796,IntelPython/sdc,hpat/hiframes/hiframes_typed.py,056b61677ddbc452c9ecf03b9b90bab73b6a63bd,TODO: test ndim and T,https://github.com/IntelPython/sdc/commit/056b61677ddbc452c9ecf03b9b90bab73b6a63bd,Yes
3797,IntelPython/sdc,hpat/hiframes/hiframes_typed.py,8005c5863bd3bf844709ee1146291b127f3301d9,TODO: test,https://github.com/IntelPython/sdc/commit/8005c5863bd3bf844709ee1146291b127f3301d9,No
3798,IntelPython/sdc,hpat/tests/test_series.py,1a6946cc3479a1cb06924b721f4b772fa9880f37,TODO: use 2 for test int casting,https://github.com/IntelPython/sdc/commit/1a6946cc3479a1cb06924b721f4b772fa9880f37,No
3799,IntelPython/sdc,hpat/hiframes/pd_dataframe_ext.py,6ee182022fdacd2f0a1cb7c282ad838abf111dfd,Numba to fail. See TestDataFrame.test_unbox1; TODO: find root cause in Numba,https://github.com/IntelPython/sdc/commit/6ee182022fdacd2f0a1cb7c282ad838abf111dfd,Yes
3800,IntelPython/sdc,hpat/hiframes/dataframe_pass.py,b7b06f692e3570c37fa2e23efc38fb356d4c2864,TODO: test this case,https://github.com/IntelPython/sdc/commit/b7b06f692e3570c37fa2e23efc38fb356d4c2864,No
3801,IntelPython/sdc,hpat/hiframes/dataframe_pass.py,3e327efb8d98a494f69f53e1bca41cf57ea4dc1c,TODO: test non-df case,https://github.com/IntelPython/sdc/commit/3e327efb8d98a494f69f53e1bca41cf57ea4dc1c,Yes
3802,IntelPython/sdc,hpat/tests/test_dataframe.py,ad5a53d5bda5e551bc311de9f37ff1b46fe48b56,XXX: test actual output,https://github.com/IntelPython/sdc/commit/ad5a53d5bda5e551bc311de9f37ff1b46fe48b56,No
3803,IntelPython/sdc,hpat/hiframes/api.py,7455bdae04c94910daecb855d66b99fc4f826e83,TODO: FloatLiteral e.g. test_fillna,https://github.com/IntelPython/sdc/commit/7455bdae04c94910daecb855d66b99fc4f826e83,Yes
3804,IntelPython/sdc,hpat/hiframes/hiframes_untyped.py,088665c3b527977d4a13bab5a7957b5717f857f8,if lhs changed; TODO: test,https://github.com/IntelPython/sdc/commit/088665c3b527977d4a13bab5a7957b5717f857f8,No
3805,IntelPython/sdc,hpat/hiframes/hiframes_untyped.py,135b3c387d8792cae45223468fd8faa7dc48037a,if rhs.op in ('build_list'; 'build_tuple'): TODO: test tuple,https://github.com/IntelPython/sdc/commit/135b3c387d8792cae45223468fd8faa7dc48037a,No
3806,IntelPython/sdc,hpat/distributed.py,1c34f97215dec05cb9e21ce33948ef403199d99e,TODO: test parallel,https://github.com/IntelPython/sdc/commit/1c34f97215dec05cb9e21ce33948ef403199d99e,Yes
3807,IntelPython/sdc,hpat/hiframes/pd_dataframe_ext.py,1de529bdb3047ef74df684c63e9bd1d04542bc94,TODO: test,https://github.com/IntelPython/sdc/commit/1de529bdb3047ef74df684c63e9bd1d04542bc94,No
3808,IntelPython/sdc,hpat/str_arr_ext.py,6c0701323a277bcb7938a40a4dad14d1ccfb8569,TODO: test,https://github.com/IntelPython/sdc/commit/6c0701323a277bcb7938a40a4dad14d1ccfb8569,No
3809,IntelPython/sdc,hpat/hiframes/series_kernels.py,552628ab42778b70755631cbf0092d7295f47810,TODO: test,https://github.com/IntelPython/sdc/commit/552628ab42778b70755631cbf0092d7295f47810,No
3810,IntelPython/sdc,hpat/tests/test_dataframe.py,3fc4b5ed561fd6d11ee2154e542786f9587cb658,TODO: add column with datetime values when test_series_datetime_isna1 is fixed,https://github.com/IntelPython/sdc/commit/3fc4b5ed561fd6d11ee2154e542786f9587cb658,No
3811,IntelPython/sdc,hpat/tests/test_dataframe.py,e9730e1484591de5b5e59d86e8cc1e1e499d202d,TODO: uncomment column with string values when test_series_astype_str_to_float64 is fixed,https://github.com/IntelPython/sdc/commit/e9730e1484591de5b5e59d86e8cc1e1e499d202d,Yes
3812,IntelPython/sdc,hpat/tests/test_dataframe.py,e9730e1484591de5b5e59d86e8cc1e1e499d202d,TODO: uncomment column with string values when test_series_astype_str_to_int32 is fixed,https://github.com/IntelPython/sdc/commit/e9730e1484591de5b5e59d86e8cc1e1e499d202d,Yes
3813,IntelPython/sdc,sdc/datatypes/hpat_pandas_dataframe_pass.py,bca334dfe32973ecb726a54bfcfe7747be825a71,FIXME: see why this breaks test_kmeans,https://github.com/IntelPython/sdc/commit/bca334dfe32973ecb726a54bfcfe7747be825a71,Yes
3814,IntelPython/sdc,sdc/datatypes/hpat_pandas_dataframe_pass.py,bca334dfe32973ecb726a54bfcfe7747be825a71,if lhs changed; TODO: test,https://github.com/IntelPython/sdc/commit/bca334dfe32973ecb726a54bfcfe7747be825a71,No
3815,IntelPython/sdc,docs/source/buildscripts/sdc_object_utils.py,0c9d320baac7624e031754fd7ee31a7f829c8453,Customizable test for skipping objects as needed,https://github.com/IntelPython/sdc/commit/0c9d320baac7624e031754fd7ee31a7f829c8453,Yes
3816,IntelPython/sdc,sdc/tests/test_series.py,81208c78abc8ae9caec4ac0b461e39455e7632ed,FIXME: skip the sub-test if one of the dtypes is float and the other is integer,https://github.com/IntelPython/sdc/commit/81208c78abc8ae9caec4ac0b461e39455e7632ed,Yes
3817,IntelPython/sdc,sdc/tests/test_series.py,34f214484ae3162e2db9bf324f3e55fddbe034e3,FIXME: skip the sub-test if one of the dtypes is float and the other is integer,https://github.com/IntelPython/sdc/commit/34f214484ae3162e2db9bf324f3e55fddbe034e3,Yes
3818,IntelPython/sdc,sdc/tests/test_series.py,9b5c64bba0bdb09d9472e88eb367ffe66d316b26,FIXME: skip the sub-test if one of the dtypes is float and the other is integer,https://github.com/IntelPython/sdc/commit/9b5c64bba0bdb09d9472e88eb367ffe66d316b26,Yes
3819,IntelPython/sdc,sdc/tests/test_indexes.py,4c855983d9cb8e45414af0b4ad64d52fd8853d68,FIXME: replace with pd.testing.assert_index_equal when Int64Index is supported,https://github.com/IntelPython/sdc/commit/4c855983d9cb8e45414af0b4ad64d52fd8853d68,No
3820,IntelPython/sdc,sdc/tests/test_series_ops.py,c7889893b49f7b251cd9f0a0889107593d8f1c4a,TODO: use 2 for test int casting,https://github.com/IntelPython/sdc/commit/c7889893b49f7b251cd9f0a0889107593d8f1c4a,No
3821,IntelPython/sdc,sdc/tests/tests_perf/test_perf_series.py,c7889893b49f7b251cd9f0a0889107593d8f1c4a,TO-DO: fix below test that hangs due to inefficient impl,https://github.com/IntelPython/sdc/commit/c7889893b49f7b251cd9f0a0889107593d8f1c4a,Yes
3822,undertheseanlp/underthesea,setup.py,2ce3f0ba9294ae708fb7796036c47fed8fb049de,TODO: put package test requirements here,https://github.com/undertheseanlp/underthesea/commit/2ce3f0ba9294ae708fb7796036c47fed8fb049de,No
3823,fastnlp/fastNLP,reproduction/chinese_word_segment/train_context.py,de3feeaf5aca2529585b7572cd1d16d4dfcf4865,TODO \u8FD9\u91CC\u8C8C\u4F3C\u9700\u8981\u533A\u5206test pipeline\u4E0Edev pipeline,https://github.com/fastnlp/fastNLP/commit/de3feeaf5aca2529585b7572cd1d16d4dfcf4865,No
3824,fastnlp/fastNLP,fastNLP/core/trainer.py,3d91f2f024207c8bfc0dae62cdaead227f4558c7,TODO \u8FD9\u91CC\u4FEE\u6539\u4E3A\u4F7F\u7528tester,https://github.com/fastnlp/fastNLP/commit/3d91f2f024207c8bfc0dae62cdaead227f4558c7,No
3825,fastnlp/fastNLP,test/core/test_dataset.py,f4e64906d46a66ea2e12e24fe29ce0e19614c26e,TODO test failed because 'fastNLP\\core\\fieldarray.py:143: RuntimeError',https://github.com/fastnlp/fastNLP/commit/f4e64906d46a66ea2e12e24fe29ce0e19614c26e,No
3826,fastnlp/fastNLP,test/core/test_dataset.py,dffd9b96cd2b070de3550113338ee904ee1c3e10,TODO test failed because 'fastNLP\\core\\fieldarray.py:143: RuntimeError',https://github.com/fastnlp/fastNLP/commit/dffd9b96cd2b070de3550113338ee904ee1c3e10,No
3827,fastnlp/fastNLP,test/core/test_dataset.py,c344f7a2f9f637d0c5d6b2b059d59a69d7fb885f,# TODO test failed because 'fastNLP\\core\\fieldarray.py:143: RuntimeError',https://github.com/fastnlp/fastNLP/commit/c344f7a2f9f637d0c5d6b2b059d59a69d7fb885f,No
3828,kornia/kornia,docs/source/conf.py,5f55bed2572ed76144e09357d511d03123722b85,@jpchen's hack to get rtd builder to install latest pytorch,https://github.com/kornia/kornia/commit/5f55bed2572ed76144e09357d511d03123722b85,No
3829,kornia/kornia,torchgeometry/contrib/spatial_soft_argmax2d.py,acc83eaa5084b4237ba2eeb06c9ea31296a59180,TODO: use utils.create_meshgrid and test,https://github.com/kornia/kornia/commit/acc83eaa5084b4237ba2eeb06c9ea31296a59180,Yes
3830,kornia/kornia,test/color/test_yuv.py,7616e92175363e56922f7788cf773b9c3cd5655d,TODO add cv2 comparision test,https://github.com/kornia/kornia/commit/7616e92175363e56922f7788cf773b9c3cd5655d,Yes
3831,kornia/kornia,kornia/feature/laf.py,d0b43268a0b496192331b1e12529d1afb92dd1a7,TODO: Refactor doctest,https://github.com/kornia/kornia/commit/d0b43268a0b496192331b1e12529d1afb92dd1a7,Yes
3832,kornia/kornia,kornia/geometry/camera/pinhole.py,ee45c6c2da78ae944d6bb0043d6414122590299e,TODO: Add doctest once having `rtvec_to_pose`.,https://github.com/kornia/kornia/commit/ee45c6c2da78ae944d6bb0043d6414122590299e,No
3833,kornia/kornia,test/augmentation/test_augmentation.py,74cc0cfd6406179570b06ca4ef8423142e7eaa0b,TODO same_on_batch tests?,https://github.com/kornia/kornia/commit/74cc0cfd6406179570b06ca4ef8423142e7eaa0b,Yes
3834,kornia/kornia,test/augmentation/test_augmentation.py,a828315185a9dc8b21ec8e5dbead9044caf0d3a2,TODO same_on_batch tests?,https://github.com/kornia/kornia/commit/a828315185a9dc8b21ec8e5dbead9044caf0d3a2,Yes
3835,kornia/kornia,test/augmentation/test_augmentation.py,ab3ff67f6d34cedbb06dd888c202198392b892b1,TODO: improve and implement more meaningful smoke tests e.g check for a consistent,https://github.com/kornia/kornia/commit/ab3ff67f6d34cedbb06dd888c202198392b892b1,No
3836,kornia/kornia,test/enhance/test_equalization.py,169af9babe5dbcb7f73f9b34a8a6cd420b9fae36,TODO: test with a more realistic pattern,https://github.com/kornia/kornia/commit/169af9babe5dbcb7f73f9b34a8a6cd420b9fae36,Yes
3837,ebu/benchmarkstt,tests/benchmarkstt/test_docblock.py,a0142e1493a69772d103fa4cb5f88111d5b518be,todo: test the other Docblock properties as well,https://github.com/ebu/benchmarkstt/commit/a0142e1493a69772d103fa4cb5f88111d5b518be,Yes
3838,ecohealthalliance/EpiTator,annotator/annotator.py,ca1d01680fbc076b4306a137e90e97015b011e76,TODO needs testing,https://github.com/ecohealthalliance/EpiTator/commit/ca1d01680fbc076b4306a137e90e97015b011e76,Yes
3839,ecohealthalliance/EpiTator,annotator/annotator.py,08e4ed0341eab02874b2af6bc23a8d5b488a7b72,TODO needs extensive testing,https://github.com/ecohealthalliance/EpiTator/commit/08e4ed0341eab02874b2af6bc23a8d5b488a7b72,Yes
3840,ecohealthalliance/EpiTator,annotator/annotator.py,be32890c1addbb399343a56ea13bed5a912e5110,TODO needs extensive testing,https://github.com/ecohealthalliance/EpiTator/commit/be32890c1addbb399343a56ea13bed5a912e5110,Yes
3841,ecohealthalliance/EpiTator,annotator/annotator.py,92780db203cdbe6ee36c6158390f8cb9d528bbc4,TODO needs testing,https://github.com/ecohealthalliance/EpiTator/commit/92780db203cdbe6ee36c6158390f8cb9d528bbc4,Yes
3842,ecohealthalliance/EpiTator,annotator/annotator.py,172e2c47bfa76ff96acaf58b54908c0a4f68175c,TODO needs testing,https://github.com/ecohealthalliance/EpiTator/commit/172e2c47bfa76ff96acaf58b54908c0a4f68175c,Yes
3843,ecohealthalliance/EpiTator,tests/annotator/test_infection_annotator.py,25f17dcae7bae4465ce2cd1465d84054d1d5eb36,TODO: This test currently fails because it stops looking after,https://github.com/ecohealthalliance/EpiTator/commit/25f17dcae7bae4465ce2cd1465d84054d1d5eb36,No
3844,openopt/copt,tests/test_stochastic.py,957b7fb639f5373ffe2fdf48a2a29ad756cfbdff,XXX test with L1,https://github.com/openopt/copt/commit/957b7fb639f5373ffe2fdf48a2a29ad756cfbdff,No
3845,DIVA-DIA/DeepDIVA,template/CIFAR_CNN_classifier.py,7875d579a8aeffd60b5ab70ded47cc9d74353145,TODO being testing,https://github.com/DIVA-DIA/DeepDIVA/commit/7875d579a8aeffd60b5ab70ded47cc9d74353145,No
3846,DIVA-DIA/DeepDIVA,template/standard.py,81906fc71337ac21282ed2a5afa1147264c767a4,TODO being testing,https://github.com/DIVA-DIA/DeepDIVA/commit/81906fc71337ac21282ed2a5afa1147264c767a4,No
3847,DIVA-DIA/DeepDIVA,template/runner/semantic_segmentation/evaluate.py,c7092400bceebc2c8bff5d5939dfed4e06fe312c,needed for test phase output generation,https://github.com/DIVA-DIA/DeepDIVA/commit/c7092400bceebc2c8bff5d5939dfed4e06fe312c,Yes
3848,DIVA-DIA/DeepDIVA,template/runner/divahisdb_semantic_segmentation/evaluate.py,db503214d9c9154e496bcf8299822d6157db1665,needed for test phase output generation,https://github.com/DIVA-DIA/DeepDIVA/commit/db503214d9c9154e496bcf8299822d6157db1665,Yes
3849,neuronets/nobrainer,nobrainer/models/highres3dnet.py,488c915368edcd8bd9d753997fe5a512c633b654,TODO: use `tensorflow.python.framework.test_util.is_gpu_available` to,https://github.com/neuronets/nobrainer/commit/488c915368edcd8bd9d753997fe5a512c633b654,Yes
3850,neuronets/nobrainer,nobrainer/layers/variational_convolution.py,393e9d269ed9364173effdc2738340b588c8b3f1,TODO: test the channels first implementation.,https://github.com/neuronets/nobrainer/commit/393e9d269ed9364173effdc2738340b588c8b3f1,Yes
3851,neuronets/nobrainer,nobrainer/training.py,393e9d269ed9364173effdc2738340b588c8b3f1,TODO: can we test if the model is compiled? We lose the optimizer,https://github.com/neuronets/nobrainer/commit/393e9d269ed9364173effdc2738340b588c8b3f1,Yes
3852,rosette-api/python,tests/test_rosette_api.py,aa4581d3cd1f5407ff5f1d1611b0e297688c857c,"Set user key as filename as a workaround - tests don\""t require user key",https://github.com/rosette-api/python/commit/aa4581d3cd1f5407ff5f1d1611b0e297688c857c,Yes
3853,mercury-ml-team/mercury-ml,mercury_ml/keras/providers/image_generators/multi_input.py,1d48c3a9104cf12cd63868623ea0b3cc1f6e50df,returns a list; sorted on the class number. #TODO test this,https://github.com/mercury-ml-team/mercury-ml/commit/1d48c3a9104cf12cd63868623ea0b3cc1f6e50df,No
3854,mercury-ml-team/mercury-ml,mercury_ml/keras/providers/image_generators/single_input.py,1d48c3a9104cf12cd63868623ea0b3cc1f6e50df,returns a list; sorted on the class number. #TODO test this,https://github.com/mercury-ml-team/mercury-ml/commit/1d48c3a9104cf12cd63868623ea0b3cc1f6e50df,No
3855,mercury-ml-team/mercury-ml,examples/tensorflow/fit_minimal_example.py,46860e593da82a868207b9c1967a9852ebe9a4d1,"Here we set the parameters needed to reading the source data; and then proceed to use the task \""read_train_valid_test_data_bunch\"" from the mercury_ml.common.tasks API",https://github.com/mercury-ml-team/mercury-ml/commit/46860e593da82a868207b9c1967a9852ebe9a4d1,Yes
3856,sdv-dev/Copulas,tests/large_scale_evaluation.py,018d4906a468892f59bc5bc41679c18f234540dd,"\""\""\"" || Large Scale Copulas Evaluation. ||  || This script is a command line module that evaluates multiple MultiVariate models || from the Copulas library over a collection of real world datasets stored in an || S3 Bucket as CSV files. ||  || Usage: ||  ||     python large_scale_evaluation.py [-h] [-v] [-o OUTPUT_PATH] [-s SAMPLE] ||                                      [-r MAX_ROWS] [-c MAX_COLUMNS] ||                                      [-m MODEL [MODEL ...]] ||                                      [datasets [datasets ...]] ||  ||     positional arguments: ||       datasets              Name of the datasets\/s to test. ||  ||     optional arguments: ||       -h; --help            show this help message and exit ||       -v; --verbose         Be verbose. Use -vv for increased verbosity. ||       -o OUTPUT_PATH; --output-path OUTPUT_PATH ||                             Path to the CSV file where the report will be dumped ||       -s SAMPLE; --sample SAMPLE ||                             Limit the test to a sample of datasets for the given ||                             size. ||       -r MAX_ROWS; --max-rows MAX_ROWS ||                             Limit the number of rows per dataset. ||       -c MAX_COLUMNS; --max-columns MAX_COLUMNS ||                             Limit the number of columns per dataset. ||       -m MODEL [MODEL ...]; --model MODEL [MODEL ...] ||                             Name of the model to test. Can be passed multiple ||                             times to evaluate more than one model. || \""\""\""",https://github.com/sdv-dev/Copulas/commit/018d4906a468892f59bc5bc41679c18f234540dd,No
3857,comic/grand-challenge.org,django/comic/settings.py,da4de68e00a4a397778ddcf6a190884a5b71a735,FIXME: put site source root here for testing purposes. This should be a real data drive,https://github.com/comic/grand-challenge.org/commit/da4de68e00a4a397778ddcf6a190884a5b71a735,Yes
3858,comic/grand-challenge.org,django/comicsite/tests.py,9b8f28ab5d7b6216e5357267ecbd1bd0a95729ab,TODO: these test fail; but are not very important now. fix this later.,https://github.com/comic/grand-challenge.org/commit/9b8f28ab5d7b6216e5357267ecbd1bd0a95729ab,Yes
3859,comic/grand-challenge.org,app/evaluation/tests/test_models.py,4945d709e7f145c42bcbfa7eb4191b53cd3bd2ad,TODO: Add some model tests,https://github.com/comic/grand-challenge.org/commit/4945d709e7f145c42bcbfa7eb4191b53cd3bd2ad,Yes
3860,comic/grand-challenge.org,app/ckeditor/tests.py,53f006b0dc46f042a80686ac23a4cd1f18e10b87,TODO: These tests do not work as you cannot dynamically mess with,https://github.com/comic/grand-challenge.org/commit/53f006b0dc46f042a80686ac23a4cd1f18e10b87,Yes
3861,comic/grand-challenge.org,app/evaluation/signals.py,30f21d1e101217227a56cec1aea1e9bf8da8daf6,TODO: Create Timeout tests,https://github.com/comic/grand-challenge.org/commit/30f21d1e101217227a56cec1aea1e9bf8da8daf6,Yes
3862,comic/grand-challenge.org,app/tests/evaluation_tests/test_views.py,646ff0021aabcf4c5d1f9b4e0d9c1db9cc778ebb,TODO: we need a submission to test,https://github.com/comic/grand-challenge.org/commit/646ff0021aabcf4c5d1f9b4e0d9c1db9cc778ebb,Yes
3863,comic/grand-challenge.org,app/tests/evaluation_tests/test_views.py,646ff0021aabcf4c5d1f9b4e0d9c1db9cc778ebb,TODO: we need a job to test,https://github.com/comic/grand-challenge.org/commit/646ff0021aabcf4c5d1f9b4e0d9c1db9cc778ebb,No
3864,comic/grand-challenge.org,app/tests/evaluation_tests/test_views.py,646ff0021aabcf4c5d1f9b4e0d9c1db9cc778ebb,TODO: we need a result to test,https://github.com/comic/grand-challenge.org/commit/646ff0021aabcf4c5d1f9b4e0d9c1db9cc778ebb,Yes
3865,comic/grand-challenge.org,app/tests/evaluation_tests/test_views.py,702e79d8b9c3abdfd7c72c0786ce0c8fa78a93a0,TODO: Test creation with forms.,https://github.com/comic/grand-challenge.org/commit/702e79d8b9c3abdfd7c72c0786ce0c8fa78a93a0,Yes
3866,comic/grand-challenge.org,app/tests/evaluation_tests/test_views.py,6f86b2f881706c8536b9c8b05321d1d011575838,TODO: test that private results cannot be seen,https://github.com/comic/grand-challenge.org/commit/6f86b2f881706c8536b9c8b05321d1d011575838,Yes
3867,comic/grand-challenge.org,app/tests/pages_tests/test_pages.py,63526e3960cf7b9cf74abe11ca84cccee7a3516f,TODO: Test page moving,https://github.com/comic/grand-challenge.org/commit/63526e3960cf7b9cf74abe11ca84cccee7a3516f,Yes
3868,comic/grand-challenge.org,app/uploads/views.py,9199f7e9eff799a719727285586957b18abc9823,TODO: test created filename,https://github.com/comic/grand-challenge.org/commit/9199f7e9eff799a719727285586957b18abc9823,Yes
3869,comic/grand-challenge.org,app/uploads/views.py,e1ec03165b1f47fac773a977ca6b1d18abb8463d,TODO: Write a selenium test to check this.,https://github.com/comic/grand-challenge.org/commit/e1ec03165b1f47fac773a977ca6b1d18abb8463d,No
3870,comic/grand-challenge.org,app/grandchallenge/cases/urls.py,cf1b6abbda2f93a0236e7d709bb395743869ff09,TODO: Remove this - for testing purposes only!,https://github.com/comic/grand-challenge.org/commit/cf1b6abbda2f93a0236e7d709bb395743869ff09,Yes
3871,comic/grand-challenge.org,app/grandchallenge/core/permissions/mixins.py,e83e36a52a463c45c2922d7449cc564c13f765f0,TODO: add a test for this,https://github.com/comic/grand-challenge.org/commit/e83e36a52a463c45c2922d7449cc564c13f765f0,No
3872,comic/grand-challenge.org,app/tests/retina_api_tests/test_views.py,9749e55d36e83cc08230ee540b63d89bd16d32e2,TODO fix this failing test (fix authentication check for is_retina_user,https://github.com/comic/grand-challenge.org/commit/9749e55d36e83cc08230ee540b63d89bd16d32e2,No
3873,comic/grand-challenge.org,app/tests/retina_api_tests/test_views.py,9749e55d36e83cc08230ee540b63d89bd16d32e2,TODO add tests for polygonAnnotationSetViewset queryset and singlepolygonviewset queryset\uFFFF,https://github.com/comic/grand-challenge.org/commit/9749e55d36e83cc08230ee540b63d89bd16d32e2,Yes
3874,comic/grand-challenge.org,app/tests/retina_api_tests/test_views.py,f8f94128c8b1cddf6975ce0a4457419f4cd09761,TODO reenable test after Archive permission filtering is implemented correctly,https://github.com/comic/grand-challenge.org/commit/f8f94128c8b1cddf6975ce0a4457419f4cd09761,No
3875,openAGI/tefla,tests/test_losses.py,9fc3f0b2bc9567fba5044cda7b4eac4b1d671c45,TODO: Include weights in the lagrange multiplier update tests.,https://github.com/openAGI/tefla/commit/9fc3f0b2bc9567fba5044cda7b4eac4b1d671c45,No
3876,PaddlePaddle/X2Paddle,onnx2fluid/onnx2fluid/writer.py,7c3e9379ce62b6c144671e1a5a8b878baf0f192e,TODO: test all items,https://github.com/PaddlePaddle/X2Paddle/commit/7c3e9379ce62b6c144671e1a5a8b878baf0f192e,No
3877,openml/automlbenchmark,automl/openml.py,47ef33cfa63c3e8a9cc9a0a75e9064dae5f21d9d,todo: make auto split 80% train; 20% test (make this configurable; also random vs sequential) and save it to disk,https://github.com/openml/automlbenchmark/commit/47ef33cfa63c3e8a9cc9a0a75e9064dae5f21d9d,Yes
3878,SPFlow/SPFlow,src/spn/algorithms/Inference.py,758c72bd7c70e35f2f0bc106d7e9cab224a90e49,TODO: test this function super thorougly,https://github.com/SPFlow/SPFlow/commit/758c72bd7c70e35f2f0bc106d7e9cab224a90e49,Yes
3879,SPFlow/SPFlow,src/spn/algorithms/Inference.py,aadafe1be9d26b12f002107c4bf88ab044abd7f5,TODO: test this function super thorougly,https://github.com/SPFlow/SPFlow/commit/aadafe1be9d26b12f002107c4bf88ab044abd7f5,Yes
3880,SPFlow/SPFlow,src/spn/structure/leaves/conditional/Sampling.py,5222b5bcc2a62aa32731fb965bc14cb75794eb4a,todo tmp test,https://github.com/SPFlow/SPFlow/commit/5222b5bcc2a62aa32731fb965bc14cb75794eb4a,No
3881,SPFlow/SPFlow,src/spn/structure/leaves/conditional/Sampling.py,efe5f587b4535a9fa389d2883dcd826545283197,todo tmp test,https://github.com/SPFlow/SPFlow/commit/efe5f587b4535a9fa389d2883dcd826545283197,No
3882,SPFlow/SPFlow,src/spn/structure/leaves/conditional/Sampling.py,f997a256291b6ee90a75ba68bc6156da33f6e46a,todo tmp test,https://github.com/SPFlow/SPFlow/commit/f997a256291b6ee90a75ba68bc6156da33f6e46a,No
3883,SPFlow/SPFlow,src/spn/tests/test_text.py,94e0e55b44297a4c2f893f369a7d76d7b4eeaedd,TODO: add test for spn to json,https://github.com/SPFlow/SPFlow/commit/94e0e55b44297a4c2f893f369a7d76d7b4eeaedd,Yes
3884,SPFlow/SPFlow,src/spn/tests/test_pwl.py,d529a7bab12c383845f5c4981761b544f455f866,TODO: add more test to the PWL,https://github.com/SPFlow/SPFlow/commit/d529a7bab12c383845f5c4981761b544f455f866,Yes
3885,Rostlab/nalaf,tests/test_features.py,5fde23fe506f88f8b1a5513dc60bd6167eea5325,TODO implement separate test functions for each feature that is already implemented in test_generate,https://github.com/Rostlab/nalaf/commit/5fde23fe506f88f8b1a5513dc60bd6167eea5325,No
3886,Rostlab/nalaf,tests/test_features.py,3a38b34cb332007524ebf47fbcdcd8fc65973e43,TODO implement separate test functions for each feature that is already implemented in test_generate,https://github.com/Rostlab/nalaf/commit/3a38b34cb332007524ebf47fbcdcd8fc65973e43,No
3887,Rostlab/nalaf,tests/test_mutationFinderReader.py,c6a789b0a795431787059be6b3fcdd719a392154,TODO Figure out if this should be a test class,https://github.com/Rostlab/nalaf/commit/c6a789b0a795431787059be6b3fcdd719a392154,No
3888,Rostlab/nalaf,nala/preprocessing/definers.py,9aed54cf35ee5376fbb86b4bda2a39ba5d136a3e,TODO implement test class,https://github.com/Rostlab/nalaf/commit/9aed54cf35ee5376fbb86b4bda2a39ba5d136a3e,Yes
3889,Rostlab/nalaf,nala/preprocessing/definers.py,9aed54cf35ee5376fbb86b4bda2a39ba5d136a3e,TODO correct test class for renamed function,https://github.com/Rostlab/nalaf/commit/9aed54cf35ee5376fbb86b4bda2a39ba5d136a3e,Yes
3890,Rostlab/nalaf,nala/utils/qmath.py,06e73ce6958e15d620df13ea37eedee98282acc2,todo implement test functions,https://github.com/Rostlab/nalaf/commit/06e73ce6958e15d620df13ea37eedee98282acc2,No
3891,Rostlab/nalaf,nala/preprocessing/definers.py,636f476c6051926ea31f06825e96405b92984e92,TODO test function,https://github.com/Rostlab/nalaf/commit/636f476c6051926ea31f06825e96405b92984e92,No
3892,Rostlab/nalaf,nala/bootstrapping/__init__.py,8e8931ab047bcd6d6c00ad0a12a717974309bba9,todo has to be tested,https://github.com/Rostlab/nalaf/commit/8e8931ab047bcd6d6c00ad0a12a717974309bba9,No
3893,Rostlab/nalaf,nala/bootstrapping/iteration.py,74bfdc2fbf03fb745f01ddf07a18c19293457821,todo has to be tested,https://github.com/Rostlab/nalaf/commit/74bfdc2fbf03fb745f01ddf07a18c19293457821,No
3894,Rostlab/nalaf,nala/utils/ncbi_utils.py,5f1fee7ed3d3dadc03e515a70928f566a6aa0715,todo test whether really working...,https://github.com/Rostlab/nalaf/commit/5f1fee7ed3d3dadc03e515a70928f566a6aa0715,Yes
3895,Rostlab/nalaf,nala/utils/uniprot_utils.py,b0ba24f7a0c49417731e93d3d9209ad9266d390a,todo test this part again,https://github.com/Rostlab/nalaf/commit/b0ba24f7a0c49417731e93d3d9209ad9266d390a,Yes
3896,Rostlab/nalaf,nala/structures/data.py,eb0d45b0e6ae09676109584d0cfd8a90f6c0fab9,todo test method,https://github.com/Rostlab/nalaf/commit/eb0d45b0e6ae09676109584d0cfd8a90f6c0fab9,Yes
3897,Rostlab/nalaf,nala/structures/data.py,1982abe7456fe89edaecbcf4228b650ee130188d,todo test method,https://github.com/Rostlab/nalaf/commit/1982abe7456fe89edaecbcf4228b650ee130188d,Yes
3898,Rostlab/nalaf,nala/structures/data.py,c945308f15d64fb070d74c21855ae2f1b9870dfc,todo testing,https://github.com/Rostlab/nalaf/commit/c945308f15d64fb070d74c21855ae2f1b9870dfc,Yes
3899,Rostlab/nalaf,tests/utils/test_Tagger.py,c75ca82ceb7cfa7901b7b5e37ac91f1bff58761e,todo major merge into tests\/learning\/test_taggers.py,https://github.com/Rostlab/nalaf/commit/c75ca82ceb7cfa7901b7b5e37ac91f1bff58761e,No
3900,Rostlab/nalaf,nalaf/learning/evaluators.py,395e98c407018243b9eef978ac06aaf9d0b3c6fd,TODO test if there is no normalization,https://github.com/Rostlab/nalaf/commit/395e98c407018243b9eef978ac06aaf9d0b3c6fd,Yes
3901,graknlabs/kglib,kgcn/src/models/manager.py,8c88e5373abc2add048dc5cf1184c7d679b68f66,TODO Made predict do the exact same as evaluate to use as test; change back,https://github.com/graknlabs/kglib/commit/8c88e5373abc2add048dc5cf1184c7d679b68f66,No
3902,graknlabs/kglib,kglib/kgcn/core/model_test.py,095cc36d72473e26f6e445c786c948f1ebe620e9,TODO No idea why this test fails,https://github.com/graknlabs/kglib/commit/095cc36d72473e26f6e445c786c948f1ebe620e9,Yes
3903,DigitalSlideArchive/HistomicsTK,tests/htk_test_utilities.py,16d7ec86dbc8a9049ae802749489fa4a3efb1506,TODO -- refactor to session scope by figuring out pytest issue (bug?),https://github.com/DigitalSlideArchive/HistomicsTK/commit/16d7ec86dbc8a9049ae802749489fa4a3efb1506,Yes
3904,DigitalSlideArchive/HistomicsTK,tests/htk_test_utilities.py,c44b8011c1bdc71ac8cdd37261bbdebbff1f1214,TODO -- refactor to session scope by figuring out pytest issue (bug?),https://github.com/DigitalSlideArchive/HistomicsTK/commit/c44b8011c1bdc71ac8cdd37261bbdebbff1f1214,Yes
3905,DigitalSlideArchive/HistomicsTK,tests/htk_test_utilities.py,4ad127d5da81068d48985e17abd76b90a91099e6,TODO -- refactor to session scope by figuring out pytest issue (bug?),https://github.com/DigitalSlideArchive/HistomicsTK/commit/4ad127d5da81068d48985e17abd76b90a91099e6,Yes
3906,modelhub-ai/modelhub-engine,framework/modelhubapi_tests/restapi_test.py,a964a599f0c2e5763d787790f91d141191e7f003,TODO this is not so nice yet; test should not require a download from the inet,https://github.com/modelhub-ai/modelhub-engine/commit/a964a599f0c2e5763d787790f91d141191e7f003,Yes
3907,modelhub-ai/modelhub-engine,framework/modelhubapi_tests/restapi_test.py,e3c501f365d94822b920fb26b196509a47608adc,TODO this is not so nice yet; test should not require a download from the inet,https://github.com/modelhub-ai/modelhub-engine/commit/e3c501f365d94822b920fb26b196509a47608adc,Yes
3908,thehyve/tmtk,tests/skinny_loader_tests.py,8dc05bbb1841e8f9b3727041cc468d6792b13f2b,TODO: Extend TEST_17_1 with modifiers with empty reference column (so row wide),https://github.com/thehyve/tmtk/commit/8dc05bbb1841e8f9b3727041cc468d6792b13f2b,No
3909,neptune-ai/neptune-client,tests/neptune/test_handler.py,08fec730e1b0c0250623bfc243aa02ed3a331ba4,TODO: Test download,https://github.com/neptune-ai/neptune-client/commit/08fec730e1b0c0250623bfc243aa02ed3a331ba4,Yes
3910,neptune-ai/neptune-client,alpha_integration_dev/old_client.py,2f58ea743303a5bb2f82c88d37a95098663e8b12,notebook_id='test1';  # TODO: Error 500 when wrong value,https://github.com/neptune-ai/neptune-client/commit/2f58ea743303a5bb2f82c88d37a95098663e8b12,No
3911,a2i2/surround,surround/split_data.py,7eba40075de5cf111568f2b4cf48b6a44fa2ab53,"\""\""\""A script to randomly move files in a directory to a test; train and || validate folder. ||  || This script is intended to be used on projects where the data is made || up of multiple files e.g. images or email files. ||  || Usage: python3 split_data.py <directory> <file extension> ||  || TODO: Add a flag to sort data into sub-folders based on a prefix for file names || TODO: Make sure reset works for all flags || TODO: Modify script to work on a CSV file. In this case split the CSV file into multiple files under each folder. ||  || \""\""\""",https://github.com/a2i2/surround/commit/7eba40075de5cf111568f2b4cf48b6a44fa2ab53,No
3912,Aifred-Health/Vulcan,vulcanai2/models/BaseNetwork.py,343f3f3196ca62a1237c48fa6521d005dc6ec49f,TODO: this is for the test data,https://github.com/Aifred-Health/Vulcan/commit/343f3f3196ca62a1237c48fa6521d005dc6ec49f,Yes
3913,Aifred-Health/Vulcan,vulcanai2/models/metrics.py,448d32b3bbfa2f047b2f00874a420e2b98e63557,TODO: Move components into run_test since a majority of things calculated are already there,https://github.com/Aifred-Health/Vulcan/commit/448d32b3bbfa2f047b2f00874a420e2b98e63557,Yes
3914,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,21acea06419eed157ee71f7bb7db75e0447d846c,TODO: this may break on different devices?? test.,https://github.com/Aifred-Health/Vulcan/commit/21acea06419eed157ee71f7bb7db75e0447d846c,No
3915,Aifred-Health/Vulcan,vulcanai2/models/basenetwork.py,10880bcd1ff5b1d68104db3f070c634b5699d689,TODO: this may break on different devices?? test.,https://github.com/Aifred-Health/Vulcan/commit/10880bcd1ff5b1d68104db3f070c634b5699d689,No
3916,Aifred-Health/Vulcan,vulcanai2/models/metrics.py,73f1b9de7f8fdae8fddd5bcd1cd3472a9cd9ee52,TODO: this may break on different devices?? test.,https://github.com/Aifred-Health/Vulcan/commit/73f1b9de7f8fdae8fddd5bcd1cd3472a9cd9ee52,No
3917,Aifred-Health/Vulcan,vulcanai2/models/metrics.py,e3b9ac7129c1c406eb171bb112c6351cfe6fc201,TODO: this may break on different devices?? test.,https://github.com/Aifred-Health/Vulcan/commit/e3b9ac7129c1c406eb171bb112c6351cfe6fc201,No
3918,Aifred-Health/Vulcan,vulcanai2/tests/models/test_device.py,9a4cf0df071e0412c24bedd39c944532aaea2371,TODO: more tests yet to come,https://github.com/Aifred-Health/Vulcan/commit/9a4cf0df071e0412c24bedd39c944532aaea2371,No
3919,Aifred-Health/Vulcan,vulcanai2/tests/datasets/test_tabulardataset.py,000120f5f0a634a3338f353c01a56654aff6031d,# TODO: test all possible combinations of merging,https://github.com/Aifred-Health/Vulcan/commit/000120f5f0a634a3338f353c01a56654aff6031d,No
3920,Aifred-Health/Vulcan,vulcanai2/tests/datasets/test_tabulardataset.py,000120f5f0a634a3338f353c01a56654aff6031d,#TODO: test what Jospeh updates,https://github.com/Aifred-Health/Vulcan/commit/000120f5f0a634a3338f353c01a56654aff6031d,No
3921,Aifred-Health/Vulcan,vulcanai2/tests/models/test_dnn.py,64ef2dd67dbc359bd007358848cab934c435fbb2,TODO: Failing! Fix the test,https://github.com/Aifred-Health/Vulcan/commit/64ef2dd67dbc359bd007358848cab934c435fbb2,Yes
3922,Aifred-Health/Vulcan,vulcanai2/tests/models/test_dnn.py,64ef2dd67dbc359bd007358848cab934c435fbb2,TODO: Failing fit! Fix the test,https://github.com/Aifred-Health/Vulcan/commit/64ef2dd67dbc359bd007358848cab934c435fbb2,Yes
3923,Aifred-Health/Vulcan,vulcanai2/tests/models/test_dnn.py,f67166039134b17cf90714014547b1f42799d5fe,TODO: Failing fit! Fix the test,https://github.com/Aifred-Health/Vulcan/commit/f67166039134b17cf90714014547b1f42799d5fe,Yes
3924,Aifred-Health/Vulcan,vulcanai/models/metrics.py,39b5ad778d0f672b2c763b878b0ed159e8d69479,TODO: this may break on different devices?? test.,https://github.com/Aifred-Health/Vulcan/commit/39b5ad778d0f672b2c763b878b0ed159e8d69479,No
3925,Aifred-Health/Vulcan,vulcanai/models/metrics.py,f4f9d896bc546b3904ce874aa69fe37cea1c1d54,TODO: this may break on different devices?? test.,https://github.com/Aifred-Health/Vulcan/commit/f4f9d896bc546b3904ce874aa69fe37cea1c1d54,No
3926,Aifred-Health/Vulcan,vulcanai/models/metrics.py,b6762cfb8220da0d70885d6cb280e4607e4cd1ab,TODO: this may break on different devices?? test.,https://github.com/Aifred-Health/Vulcan/commit/b6762cfb8220da0d70885d6cb280e4607e4cd1ab,No
3927,Aifred-Health/Vulcan,vulcanai/models/metrics.py,6f42bbe62dbbd84f4b02475b105f976340cfee1d,TODO: this may break on different devices?? test.,https://github.com/Aifred-Health/Vulcan/commit/6f42bbe62dbbd84f4b02475b105f976340cfee1d,No
3928,Aifred-Health/Vulcan,vulcanai/tests/datasets/test_tabulardataset.py,acf19fcb6e685f847cf0f333e5dddc1c71471f45,def test_list_columns(self; my_test_dataset):,https://github.com/Aifred-Health/Vulcan/commit/acf19fcb6e685f847cf0f333e5dddc1c71471f45,Yes
3929,Aifred-Health/Vulcan,vulcanai/tests/datasets/test_tabulardataset.py,acf19fcb6e685f847cf0f333e5dddc1c71471f45,def test_delete_columns(self; my_test_dataset):,https://github.com/Aifred-Health/Vulcan/commit/acf19fcb6e685f847cf0f333e5dddc1c71471f45,Yes
3930,Aifred-Health/Vulcan,vulcanai/tests/datasets/test_tabulardataset.py,acf19fcb6e685f847cf0f333e5dddc1c71471f45,def test_identify_unbalanced_columns(self; my_test_dataset):,https://github.com/Aifred-Health/Vulcan/commit/acf19fcb6e685f847cf0f333e5dddc1c71471f45,No
3931,Aifred-Health/Vulcan,vulcanai/tests/datasets/test_tabulardataset.py,acf19fcb6e685f847cf0f333e5dddc1c71471f45,res = my_test_dataset.identify_unbalanced_columns(0.5),https://github.com/Aifred-Health/Vulcan/commit/acf19fcb6e685f847cf0f333e5dddc1c71471f45,Yes
3932,Aifred-Health/Vulcan,vulcanai/tests/datasets/test_utils.py,acf19fcb6e685f847cf0f333e5dddc1c71471f45,def test_no_merge_on_columns(self; my_test_dataset_one):,https://github.com/Aifred-Health/Vulcan/commit/acf19fcb6e685f847cf0f333e5dddc1c71471f45,Yes
3933,Aifred-Health/Vulcan,vulcanai/tests/datasets/test_utils.py,acf19fcb6e685f847cf0f333e5dddc1c71471f45,def test_single_merge_on_columns(self; my_test_dataset_two):,https://github.com/Aifred-Health/Vulcan/commit/acf19fcb6e685f847cf0f333e5dddc1c71471f45,Yes
3934,Aifred-Health/Vulcan,vulcanai/tests/datasets/test_utils.py,acf19fcb6e685f847cf0f333e5dddc1c71471f45,def test_two_merge_on_columns(self; my_test_dataset_three):,https://github.com/Aifred-Health/Vulcan/commit/acf19fcb6e685f847cf0f333e5dddc1c71471f45,Yes
3935,Aifred-Health/Vulcan,vulcanai/tests/datasets/test_utils.py,acf19fcb6e685f847cf0f333e5dddc1c71471f45,def test_three_merge_on_columns(self; my_test_dataset_four):,https://github.com/Aifred-Health/Vulcan/commit/acf19fcb6e685f847cf0f333e5dddc1c71471f45,Yes
3936,ResponsiblyAI/responsibly,ethically/we/core.py,43e270a4a5ea622fac2727a762c31f7a8e50369b,TODO: write unitest for when it is False,https://github.com/ResponsiblyAI/responsibly/commit/43e270a4a5ea622fac2727a762c31f7a8e50369b,No
3937,ResponsiblyAI/responsibly,ethically/tests/test_we.py,72b79277138233929944849e9947ddd5d7be7940,TODO deeper testing,https://github.com/ResponsiblyAI/responsibly/commit/72b79277138233929944849e9947ddd5d7be7940,No
3938,ResponsiblyAI/responsibly,ethically/we/bias.py,898ed376fa961f47f903741c46406d46b917f9ed,TODO: write unitest for when it is False,https://github.com/ResponsiblyAI/responsibly/commit/898ed376fa961f47f903741c46406d46b917f9ed,No
3939,ResponsiblyAI/responsibly,ethically/we/bias.py,8af2aa8768bd49de604c75f88596b8088018ccee,TODO: write unitest for when it is False,https://github.com/ResponsiblyAI/responsibly/commit/8af2aa8768bd49de604c75f88596b8088018ccee,No
3940,biolab/orange3-educational,orangecontrib/educational/widgets/tests/test_owunivariateregression.py,40cb031a4cafc07eb93722ff12e2e6ac8ef3fec5,TODO: output will be checked when it available in GuiTest,https://github.com/biolab/orange3-educational/commit/40cb031a4cafc07eb93722ff12e2e6ac8ef3fec5,Yes
3941,biolab/orange3-educational,orangecontrib/educational/widgets/owrandomdata.py,94a276a2a32d9b089d84745520266475bd1f5811,self.add_combo is needed so that tests can manipulate it,https://github.com/biolab/orange3-educational/commit/94a276a2a32d9b089d84745520266475bd1f5811,Yes
3942,nltk/nltk,nltk.py,8b673cc65bd1aedc1f583d080037b71da4898e20,"\""\""\""## || The Natural Language Toolkit is a package intended to simplify the || task of programming natural language systems.  It is intended to be || used as a teaching tool; not as a basis for building production || systems. <P> ||  || <H1> Interfaces <\/H1> ||  || The Natural Language Toolkit is implemented as a set of interfaces and || classes.  Interfaces are a concept loosely borrowed from Java.  They || are essentially a specification of a set of methods.  Any class that || implements all of an interface's methods according to the interface's || specification are said to \\\""implement\\\"" that interface. <P> ||  || In the context of this toolkit; an interface is implemented as a || class; all of whose methods simply raise AssertionError.  The || __doc__ strings of these methods; together with the methods' || declarations;  provide specifications for the methods. <P> ||  || Interface classes are named with a trailing \\\""I\\\""; such as || <CODE>TokenizerI<\/CODE> or <CODE>EventI<\/CODE>. ||  || <H1> Interface and Class Hierarchy <\/H1> ||  || The classes defined by the Natural Language Toolkit can be divided || into two basic categories: Data classes; and Processing (or || Task-Oriented) Classes. ||  || <H2> Data Classes <\/H2> ||  || Data classes are used to store several different types of information || that are relavant to natural language processing.  Data classes can || generally be grouped into small clusters; with minimal interaction || between the clusters.  The clusters that are currently defined by the || Natural Language Toolkit are listed below.  Under each cluster; the || top-level classes and interfaces contained in that cluster are given. ||  || <UL> ||   <LI> <B>Sets<\/B>: Encodes the mathmatical notion of a \\\""finite set\\\"". ||   <UL> ||     <LI> <A coderef='nltk.Set'>Set<\/A>: A finite set. ||   <\/UL> ||   <LI> <B>Tokens<\/B>: Encodes units of text such as words. ||   <UL> ||     <LI> <A coderef='nltk.TokenTypeI'>TokenTypeI<\/A>: ||          A unit of text. ||     <LI> <A coderef='nltk.TextLocationI'>TextLocationI<\/A>: ||          A location within a text. ||     <LI> <A coderef='nltk.Token'>Token<\/A>: ||          An occurance of a unit of text. ||          Consists of a TokenType and a TextLocation. ||   <\/UL> ||   <LI> <B>Syntax Trees<\/B>: Encodes syntax trees.  Not fully designed  ||         yet. ||   <LI> <B>Probability<\/B>: Encodes data structures associated with ||         the mathmatical notion of probability; such as events and ||         frequency distributions. ||   <UL> ||     <LI> Sample: Encodes the mathmatical notion of a ||           \\\""sample\\\"".  This is actually not implemented as a class or ||           an interface -- (almost) anything can be a Sample. ||     <LI> <A coderef='nltk.EventI'>EventI<\/A>: ||          A (possibly infinite) set of samples. ||     <LI> <A coderef='nltk.FreqDistI'>FreqDistI<\/A>: ||           The frequency distribution of a collection of samples. ||     <LI> <A coderef='nltk.ProbDistI'>ProbDistI<\/A>: ||           A probability distribution; typically derived from a ||           frequency distribution (e.g.; using ELE). ||   <\/UL> || <\/UL> ||  || <H2> Processing Classes <\/H2> ||  || Processing classes are used to perform a variety of tasks that are || relavant to natural language processing.  Processing classes can || generally be grouped into small clusters; with minimial interaction || between the clusters.  Each cluster typically makes use of several || data-class clusters.  The processing clusters that are currently || defined by the Natural Language Toolkit are listed below.  Under each || cluster; the interfaces contained in that cluster are given. ||  || <UL> ||   <LI> <B>Tokenizers<\/B>: Separate a string of text into a list of ||        Tokens.  ||   <UL> ||      <LI> <A coderef='nltk.TokenizerI'>TokenizerI<\/A> ||   <\/UL> ||   <LI> <B>Taggers<\/B>: Assign tags to each Token in a list of Tokens. ||   <UL> ||      <LI> <A coderef='nltk.TaggerI'>TaggerI<\/A> ||   <\/UL> ||   <LI> <B>Language Model<\/B>: (not yet designed\/implemented) ||   <LI> <B>Parser<\/B>: (not yet designed\/implemented) || <\/UL> ||  || <H1> Open Questions <\/H1> ||  || The following is a list of currently unresolved questions; pertaining || to currently implemented interfaces and classes. || <UL> ||   <LI> Terminology\/Naming Questions ||   <UL> ||     <LI> Is \\\""Token Type\\\"" too easily confusable with the notion of ||          type in python?  E.g.; names like SimpleTokenType suggest ||          that they are similar to StringType; IntType; etc. when they ||          are very different.  I could use \\\""TokenTyp\\\"" to distinguish;  ||          but this also seems somewhat confusing to the uninitiated. ||     <LI> What name can be used for the \\\""word content\\\"" of a token ||          type?  Currently; <CODE>name<\/CODE> is used; but that's not a  ||          very intuitive name.  <CODE>word<\/CODE> might be used; ||          although often times the string is not a word (e.g.; \\\"".\\\""). ||   <\/UL> ||   <LI> Is the token\/token type\/text location system too complex? ||        Often; one only cares about the token type.  E.g.; a tokenizer ||        could be defined as mapping string to list of TokenType; and a ||        tagger as mapping list of SimpleTokenType to TaggedTokenType. ||        But sometimes we really need to be able to distinguish tokens; ||        not just token types.. e.g.; to test the chunk parser from the ||        chunk parsing problem set. ||   <LI> How should text locations be represented?  character index? ||        token index?  To some extent; it dosen't matter; as long as ||        __cmp__ is properly defined.  Should text locations have ranges  ||        or just starting points?  etc. || <\/UL> ||  || @exclude .*(?!Token).....Type || @exclude ....Type || @exclude ...Type || @exclude _typemsg || @exclude _Old.* ||  || @variable _type_safety_level The level of type safety to use when || checking the input parameters to methods defined by the Natural || Language Toolkit.  Currently defined values are: || <UL> ||   <LI> 0: no type checking ||   <LI> 1: check types only ||   <LI> 2: check types and classes ||   <LI> 3: check types; classes; list contents; and tuple contents ||   <LI> 4: check types; classes; list contents; tuple contents; and ||        dictionary contents. || <\/UL> || Higher levels of type safety (3-4) can result in signifigant loss of || efficiency.  || \""\""\""",https://github.com/nltk/nltk/commit/8b673cc65bd1aedc1f583d080037b71da4898e20,Yes
3943,nltk/nltk,src/nltk/__init__.py,337aeac06f968dbc83cf6dc98ac61bcd047a4c2b,"\""\""\""## || The Natural Language Toolkit is a package intended to simplify the || task of programming natural language systems.  It is intended to be || used as a teaching tool; not as a basis for building production || systems. <P> ||  || This is a prototype implementation for the natural language toolkit. || The natural language toolkit is still under development. ||  || <H1> Interfaces <\/H1> ||  || The Natural Language Toolkit is implemented as a set of interfaces and || classes.  Interfaces are a concept loosely borrowed from Java.  They || are essentially a specification of a set of methods.  Any class that || implements all of an interface's methods according to the interface's || specification are said to \\\""implement\\\"" that interface. <P> ||  || In the context of this toolkit; an interface is implemented as a || class; all of whose methods simply raise AssertionError.  The || __doc__ strings of these methods; together with the methods' || declarations;  provide specifications for the methods. <P> ||  || Interface classes are named with a trailing \\\""I\\\""; such as || <CODE>TokenizerI<\/CODE> or <CODE>EventI<\/CODE>. ||  || <H1> Interface and Class Hierarchy <\/H1> ||  || The classes defined by the Natural Language Toolkit can be divided || into two basic categories: Data classes; and Processing (or || Task-Oriented) Classes.  The Natural Language Toolkit is still under || development; and any definitions are subject to change. ||  || <H2> Data Classes <\/H2> ||  || Data classes are used to store several different types of information || that are relavant to natural language processing.  Data classes can || generally be grouped into small clusters; with minimal interaction || between the clusters.  The clusters that are currently defined by the || Natural Language Toolkit are listed below.  Under each cluster; the || top-level classes and interfaces contained in that cluster are given. ||  || <UL> ||   <LI> <B>Sets<\/B>: Encodes the mathmatical notion of a \\\""finite set\\\"". ||   <UL> ||     <LI> <A coderef='nltk.Set'>Set<\/A>: A finite set. ||   <\/UL> ||   <LI> <B>Tokens<\/B>: Encodes units of text such as words. ||   <UL> ||     <LI> <A coderef='nltk.TokenTypeI'>TokenTypeI<\/A>: ||          A unit of text. ||     <LI> <A coderef='nltk.TextLocationI'>TextLocationI<\/A>: ||          A location within a text. ||     <LI> <A coderef='nltk.Token'>Token<\/A>: ||          An occurance of a unit of text. ||          Consists of a TokenType and a TextLocation. ||   <\/UL> ||   <LI> <B>Syntax Trees<\/B>: Encodes syntax trees.  Not fully designed  ||         yet. ||   <LI> <B>Probability<\/B>: Encodes data structures associated with ||         the mathmatical notion of probability; such as events and ||         frequency distributions. ||   <UL> ||     <LI> Sample: Encodes the mathmatical notion of a ||           \\\""sample\\\"".  This is actually not implemented as a class or ||           an interface -- (almost) anything can be a Sample. ||     <LI> <A coderef='nltk.EventI'>EventI<\/A>: ||          A (possibly infinite) set of samples. ||     <LI> <A coderef='nltk.FreqDistI'>FreqDistI<\/A>: ||           The frequency distribution of a collection of samples. ||     <LI> <A coderef='nltk.ProbDistI'>ProbDistI<\/A>: ||           A probability distribution; typically derived from a ||           frequency distribution (e.g.; using ELE). ||   <\/UL> || <\/UL> ||  || <H2> Processing Classes <\/H2> ||  || Processing classes are used to perform a variety of tasks that are || relavant to natural language processing.  Processing classes can || generally be grouped into small clusters; with minimial interaction || between the clusters.  Each cluster typically makes use of several || data-class clusters.  The processing clusters that are currently || defined by the Natural Language Toolkit are listed below.  Under each || cluster; the interfaces contained in that cluster are given. ||  || <UL> ||   <LI> <B>Tokenizers<\/B>: Separate a string of text into a list of ||        Tokens.  ||   <UL> ||      <LI> <A coderef='nltk.TokenizerI'>TokenizerI<\/A> ||   <\/UL> ||   <LI> <B>Taggers<\/B>: Assign tags to each Token in a list of Tokens. ||   <UL> ||      <LI> <A coderef='nltk.TaggerI'>TaggerI<\/A> ||   <\/UL> ||   <LI> <B>Language Model<\/B>: (not yet designed\/implemented) ||   <LI> <B>Parser<\/B>: (not yet designed\/implemented) || <\/UL> ||  || <H1> Open Questions <\/H1> ||  || The following is a list of currently unresolved questions; pertaining || to currently implemented interfaces and classes. || <UL> ||   <LI> Terminology\/Naming Questions ||   <UL> ||     <LI> Is \\\""Token Type\\\"" too easily confusable with the notion of ||          type in python?  E.g.; names like SimpleTokenType suggest ||          that they are similar to StringType; IntType; etc. when they ||          are very different.  I could use \\\""TokenTyp\\\"" to distinguish;  ||          but this also seems somewhat confusing to the uninitiated. ||     <LI> What name can be used for the \\\""word content\\\"" of a token ||          type?  Currently; <CODE>name<\/CODE> is used; but that's not a  ||          very intuitive name.  <CODE>word<\/CODE> might be used; ||          although often times the string is not a word (e.g.; \\\"".\\\""). ||     <LI> Better name than \\\""SimpleTagger\\\""? ||   <\/UL> ||   <LI> Is the token\/token type\/text location system too complex? ||        Often; one only cares about the token type.  E.g.; a tokenizer ||        could be defined as mapping string to list of TokenType; and a ||        tagger as mapping list of SimpleTokenType to TaggedTokenType. ||        But sometimes we really need to be able to distinguish tokens; ||        not just token types.. e.g.; to test the chunk parser from the ||        chunk parsing problem set. ||   <LI> How should text locations be represented?  character index? ||        token index?  To some extent; it dosen't matter; as long as ||        __cmp__ is properly defined.  Should text locations have ranges  ||        or just starting points?  etc. ||   <LI> Should FreqDist.max() and FreqDist.cond_max() be merged; with ||        the condition as an optional argument?  Same for ||        FreqDist.freq() and FreqDist.cond_freq(). ||   <LI> Should I implement cross-toolkit policies on how to use __str__  ||        and __repr__?  If so; what should they be? ||   <LI> How should I split the toolkit into modules?  Should I use a ||        multi-layer structure (e.g.; <CODE>nltk.probability<\/CODE>)? || <\/UL> ||  || @exclude .*(?!Token).....Type || @exclude ....Type || @exclude ...Type || @exclude _typemsg || @exclude _Old.* ||  || @author Edward Loper || @version 0.1 || \""\""\""",https://github.com/nltk/nltk/commit/337aeac06f968dbc83cf6dc98ac61bcd047a4c2b,No
3944,nltk/nltk,src/nltk/__init__.py,da8de8018a6ff213f81754d58b15da28626d17cc,"\""\""\"" || Open Questions || ============== ||  || The following is a list of currently unresolved questions; pertaining || to currently implemented interfaces and classes. ||   - Terminology\/Naming Questions ||     - Is \\\""Token Type\\\"" too easily confusable with the notion of ||          type in python?  E.g.; names like SimpleTokenType suggest ||          that they are similar to StringType; IntType; etc. when they ||          are very different.  I could use \\\""TokenTyp\\\"" to distinguish;  ||          but this also seems somewhat confusing to the uninitiated. ||   - Is the token\/token type\/text location system too complex? ||        Often; one only cares about the token type.  E.g.; a tokenizer ||        could be defined as mapping string to list of TokenType; and a ||        tagger as mapping list of SimpleTokenType to TaggedTokenType. ||        But sometimes we really need to be able to distinguish tokens; ||        not just token types.. e.g.; to test the chunk parser from the ||        chunk parsing problem set. ||   - Should FreqDist.max() and FreqDist.cond_max() be merged; with ||        the condition as an optional argument?  Same for ||        FreqDist.freq() and FreqDist.cond_freq(). ||   - Should I implement cross-toolkit policies on how to use __str__  ||        and __repr__?  If so; what should they be? ||  || \""\""\""",https://github.com/nltk/nltk/commit/da8de8018a6ff213f81754d58b15da28626d17cc,Yes
3945,nltk/nltk,src/nltk/rechunkparser.py,47adbdb6bf9e284efb3f933c4edc9d355be688ba,"\""\""\"" || Terms\/representations that should be defined in module docstring: ||   - chunk = a list of tagged tokens ||   - ttoklist = a list of tagged tokens (=chunk; but used in different ||     contexts)  ||   - chunklist = a list of tagged tokens and chunks ||   - ChunkString = string rep of chunked tags ||   - tag pattern = a regexp pattern over tags.  Has slightly different ||     rules than normal regexps (since it gets translated): <> act like ||     parens; . gets \\w; etc. ||  || High priority: ||   - More checking? ||       - _verify ||       - check the tag patterns ||  || Medium priority: ||   - generalize ChunkString constructor to accept a chunk list? ||   - rename ChunkString? ||   - rename \""chunklist\"" to \""chunkstruct\""; to better cohere with the ||     terminology used in chunkparser.py? ||   - should ChunkString have a str-ish method? (that returns its ||     internal rep) ||   - Add more documentation explaining precompiled regexps. ||   - In order to conform to interfaces; we might want to change reps: ||     - chunkparser output as a tree ||     - chunkedtaggedtokenizer should produce a list of tokens ||   - IN_CHINK_PATTERN; IN_CHUNK_PATTERN exceed maximum recursion depth ||     when we try to do chunking above the sentence level (~1000-2000 ||     words).  Grr. :)  So maybe we need a sentence tokenizer?  Or ||     something?   ||  || Low priority: ||   - Efficiency issues? (currently we do ~500-1500 tokens\/sec; faster ||     when we chunk more text at a time) ||  || Questions: ||   - Should ChunkString be made immutable? ||  || Indication of current efficiency:: ||  ||   TIMING TEST (3 rules: chunk\/unchunk\/merge) ||   1 x 10008 words: ||       Time = 15.3654409647 ||   8 x 1260 words: ||       Time = 10.0115730762 ||   27 x 372 words: ||       Time = 9.67810499668 ||   64 x 168 words: ||       Time = 10.1981619596 ||   125 x 84 words: ||       Time = 10.1149849892 ||   216 x 48 words: ||       Time = 10.5362759829 ||   343 x 36 words: ||       Time = 12.9556429386 ||   512 x 24 words: ||       Time = 13.6545710564 ||   729 x 24 words: ||       Time = 19.4766739607 ||   1000 x 12 words: ||       Time = 16.0188289881 ||  || \""\""\""",https://github.com/nltk/nltk/commit/47adbdb6bf9e284efb3f933c4edc9d355be688ba,No
3946,nltk/nltk,src/nltk/parser/chunk.py,b04ead36e7bc4e84c0fc8fef6cb240e446ab445d,"\""\""\"" || Issues: ||   - maximum recursion depth issues! grr... ||   - generalize ChunkString constructor to accept a chunk struct ||     instead of a tagged text? ||   - Add more comments\/docs; explaining precompiled regexps. ||   - In order to conform to interfaces; we might eventually want to ||     change reps:  ||     - chunkparser output as a tree ||     - chunkedtaggedtokenizer should produce a list of tokens ||   - Efficiency issues? (currently we do ~500-1500 tokens\/sec; faster ||     when we chunk more text at a time) ||  || Questions: ||   - Should ChunkString be made immutable? ||  || Indication of current efficiency:: ||  ||   TIMING TEST (3 rules: chunk\/unchunk\/merge) ||   1 x 10008 words: ||       Time = 15.36 seconds ||   8 x 1260 words: ||       Time = 10.01 seconds ||   27 x 372 words: ||       Time = 9.678 seconds ||   64 x 168 words: ||       Time = 10.19 seconds ||   125 x 84 words: ||       Time = 10.11 seconds ||   216 x 48 words: ||       Time = 10.53 seconds ||   343 x 36 words: ||       Time = 12.95 seconds ||   512 x 24 words: ||       Time = 13.65 seconds ||   729 x 24 words: ||       Time = 19.47 seconds ||   1000 x 12 words: ||       Time = 16.01 seconds ||  || \""\""\""",https://github.com/nltk/nltk/commit/b04ead36e7bc4e84c0fc8fef6cb240e446ab445d,Yes
3947,nltk/nltk,src/nltk/test/imports.py,0ab0040a9419b99da4b747725cac91f7af0944ad,"\""\""\"" || A test suite that checks to make sure that all imports of nltk modules || use absolute paths. ||  || In particular; check that all nltk modules import each other with || statements that have the forms: ||  ||     >>> import nltk.token ||     >>> from nltk.token import * ||  || and not the forms: ||  ||     >>> import token ||     >>> from token import * ||  || This convention ensures that the same module is not imported twice; || even if it is imported with both relative and absolute paths.  It also || reduces confusion if two modules have the same relative name (eg || C{nltk.token} and C{nltk.speech.token}). || \""\""\""",https://github.com/nltk/nltk/commit/0ab0040a9419b99da4b747725cac91f7af0944ad,Yes
3948,nltk/nltk,src/nltk/test/token.py,e24a59e7d7fe0d1348c2e1348645b540a727f4ce,"\""\""\"" || Unit testing for L{nltk.token}. ||  || @todo: Test L{nltk.token.CharTokenizer} || @todo: Test L{nltk.token.LineTokenizer} || \""\""\""",https://github.com/nltk/nltk/commit/e24a59e7d7fe0d1348c2e1348645b540a727f4ce,No
3949,nltk/nltk,src/nltk/test/tree.py,dde61caa86c4999359d58ce171a8986fbfdbd5b0,"\""\""\"" || Unit testing for L{nltk.tree}. ||  || @todo: Test L{nltk.tree.ProbabilisticTreeToken} || @todo: Test L{nltk.tree.parse_treebank} || @todo: Test L{nltk.tree.TreebankTokenizer} || \""\""\""",https://github.com/nltk/nltk/commit/dde61caa86c4999359d58ce171a8986fbfdbd5b0,Yes
3950,nltk/nltk,src/nltk/test/tokenizer.py,6810fb69adf141bf192712dbfbfb1e042aff825c,"\""\""\"" || Unit testing for L{nltk.tokenizer}. ||  || @todo: Test L{nltk.token.CharTokenizer} || @todo: Test L{nltk.token.LineTokenizer} || \""\""\""",https://github.com/nltk/nltk/commit/6810fb69adf141bf192712dbfbfb1e042aff825c,Yes
3951,nltk/nltk,src/nltk/test/__init__.py,52a25991db07295e8ef0e835c603ae641dd7c270,This is something of a hack; so trace.Trace can get at test().,https://github.com/nltk/nltk/commit/52a25991db07295e8ef0e835c603ae641dd7c270,Yes
3952,nltk/nltk,lite/nltk_lite/parse/chunk.py,3240a10e9894cce3dbb7584ba1310e44a5dae665,"\""\""\"" || Classes and interfaces for identifying non-overlapping linguistic || groups (such as base noun phrases) in unrestricted text.  This task is || called X{chunk parsing} or X{chunking}; and the identified groups are || called X{chunks}.  The chunked text is represented using a shallow || tree called a \""chunk structure.\""  A X{chunk structure} is a tree || containing tokens and chunks; where each chunk is a subtree containing || only tokens.  For example; the chunk structure for base noun phrase || chunks in the sentence \""I saw the big dog on the hill\"" is:: ||  ||   (SENTENCE: ||     (NP: <I>) ||     <saw> ||     (NP: <the> <big> <dog>) ||     <on> ||     (NP: <the> <hill>)) ||  || To convert a chunk structure back to a list of tokens; simply use the || chunk structure's L{leaves<Tree.leaves>} method. ||  || The C{parser.chunk} module defines L{ChunkParserI}; a standard || interface for chunking texts; and L{RegexpChunkParser}; a || regular-expression based implementation of that interface.  It also || defines the L{ChunkedTaggedTokenizer} and L{ConllChunkedTokenizer} || classes; which tokenize strings containing chunked and tagged texts; || and L{ChunkScore}; a utility class for scoring chunk parsers. ||  || RegexpChunkParser || ============= ||   C{RegexpChunkParser} is an implementation of the chunk parser interface ||   that uses regular-expressions over tags to chunk a text.  Its ||   C{parse} method first constructs a C{ChunkString}; which encodes a ||   particular chunking of the input text.  Initially; nothing is ||   chunked.  C{RegexpChunkParser} then applies a sequence of ||   C{RegexpChunkParserRule}s to the C{ChunkString}; each of which modifies ||   the chunking that it encodes.  Finally; the C{ChunkString} is ||   transformed back into a chunk structure; which is returned. ||  ||   C{RegexpChunkParser} can only be used to chunk a single kind of phrase. ||   For example; you can use an C{RegexpChunkParser} to chunk the noun ||   phrases in a text; or the verb phrases in a text; but you can not ||   use it to simultaneously chunk both noun phrases and verb phrases in ||   the same text.  (This is a limitation of C{RegexpChunkParser}; not of ||   chunk parsers in general.) ||  ||   RegexpChunkParserRules ||   ------------------ ||     C{RegexpChunkParserRule}s are transformational rules that update the ||     chunking of a text by modifying its C{ChunkString}.  Each ||     C{RegexpChunkParserRule} defines the C{apply} method; which modifies ||     the chunking encoded by a C{ChunkString}.  The ||     L{RegexpChunkParserRule} class itself can be used to implement any ||     transformational rule based on regular expressions.  There are ||     also a number of subclasses; which can be used to implement ||     simpler types of rules: ||  ||       - L{ChunkRule} chunks anything that matches a given regular ||         expression. ||       - L{ChinkRule} chinks anything that matches a given regular ||         expression. ||       - L{UnChunkRule} will un-chunk any chunk that matches a given ||         regular expression. ||       - L{MergeRule} can be used to merge two contiguous chunks. ||       - L{SplitRule} can be used to split a single chunk into two ||         smaller chunks. ||  ||     Tag Patterns ||     ~~~~~~~~~~~~ ||       C{RegexpChunkParserRule}s use a modified version of regular ||       expression patterns; called X{tag patterns}.  Tag patterns are ||       used to match sequences of tags.  Examples of tag patterns are:: ||  ||          r'(<DT>|<JJ>|<NN>)+' ||          r'<NN>+' ||          r'<NN.*>' ||  ||       The differences between regular expression patterns and tag ||       patterns are: ||  ||         - In tag patterns; C{'<'} and C{'>'} act as parenthases; so ||           C{'<NN>+'} matches one or more repetitions of C{'<NN>'}; not ||           C{'<NN'} followed by one or more repetitions of C{'>'}. ||         - Whitespace in tag patterns is ignored.  So ||           C{'<DT> | <NN>'} is equivalant to C{'<DT>|<NN>'} ||         - In tag patterns; C{'.'} is equivalant to C{'[^{}<>]'}; so ||           C{'<NN.*>'} matches any single tag starting with C{'NN'}. ||  ||       The function L{tag_pattern2re_pattern} can be used to transform ||       a tag pattern to an equivalent regular expression pattern. ||  ||   Efficiency ||   ---------- ||     Preliminary tests indicate that C{RegexpChunkParser} can chunk at a ||     rate of about 300 tokens\/second; with a moderately complex rule ||     set. ||  ||     There may be problems if C{RegexpChunkParser} is used with more than ||     5;000 tokens at a time.  In particular; evaluation of some regular ||     expressions may cause the Python regular expression engine to ||     exceed its maximum recursion depth.  We have attempted to minimize ||     these problems; but it is impossible to avoid them completely.  We ||     therefore recommend that you apply the chunk parser to a single ||     sentence at a time. ||  ||   Emacs Tip ||   --------- ||     If you evaluate the following elisp expression in emacs; it will ||     colorize C{ChunkString}s when you use an interactive python shell ||     with emacs or xemacs (\""C-c !\""):: ||  ||       (let () ||         (defconst comint-mode-font-lock-keywords  ||           '((\""<[^>]+>\"" 0 'font-lock-reference-face) ||             (\""[{}]\"" 0 'font-lock-function-name-face))) ||         (add-hook 'comint-mode-hook (lambda () (turn-on-font-lock)))) ||  ||     You can evaluate this code by copying it to a temporary buffer; ||     placing the cursor after the last close parenthasis; and typing ||     \""C{C-x C-e}\"".  You should evaluate it before running the interactive ||     session.  The change will last until you close emacs. ||  ||   Unresolved Issues ||   ----------------- ||     If we use the C{re} module for regular expressions; Python's ||     regular expression engine generates \""maximum recursion depth ||     exceeded\"" errors when processing very large texts; even for ||     regular expressions that should not require any recursion.  We ||     therefore use the C{pre} module instead.  But note that C{pre} ||     does not include Unicode support; so this module will not work ||     with unicode strings.  Note also that C{pre} regular expressions ||     are not quite as advanced as C{re} ones (e.g.; no leftward ||     zero-length assertions). ||  || @type _VALID_CHUNK_STRING: C{regexp} || @var _VALID_CHUNK_STRING: A regular expression to test whether a chunk ||      string is valid. || @type _VALID_TAG_PATTERN: C{regexp} || @var _VALID_TAG_PATTERN: A regular expression to test whether a tag ||      pattern is valid. || \""\""\""",https://github.com/nltk/nltk/commit/3240a10e9894cce3dbb7584ba1310e44a5dae665,Yes
3953,nltk/nltk,nltk_lite/parse/chunk.py,603384c5576ae07862766cd332c9ab5059a029ae,"\""\""\"" || Classes and interfaces for identifying non-overlapping linguistic || groups (such as base noun phrases) in unrestricted text.  This task is || called X{chunk parsing} or X{chunking}; and the identified groups are || called X{chunks}.  The chunked text is represented using a shallow || tree called a \""chunk structure.\""  A X{chunk structure} is a tree || containing tokens and chunks; where each chunk is a subtree containing || only tokens.  For example; the chunk structure for base noun phrase || chunks in the sentence \""I saw the big dog on the hill\"" is:: ||  ||   (SENTENCE: ||     (NP: <I>) ||     <saw> ||     (NP: <the> <big> <dog>) ||     <on> ||     (NP: <the> <hill>)) ||  || To convert a chunk structure back to a list of tokens; simply use the || chunk structure's L{leaves<Tree.leaves>} method. ||  || The C{parser.chunk} module defines L{ChunkI}; a standard interface for || chunking texts; and L{RegexpChunk}; a regular-expression based || implementation of that interface.  It uses the L{tree.chunk} and || L{tree.conll_chunk} methods; which tokenize strings containing chunked || and tagged texts.  It defines L{ChunkScore}; a utility class for || scoring chunk parsers. ||  || RegexpChunk || =========== ||  ||   C{parse.RegexpChunk} is an implementation of the chunk parser interface ||   that uses regular-expressions over tags to chunk a text.  Its ||   C{parse} method first constructs a C{ChunkString}; which encodes a ||   particular chunking of the input text.  Initially; nothing is ||   chunked.  C{parse.RegexpChunk} then applies a sequence of ||   C{RegexpChunkRule}s to the C{ChunkString}; each of which modifies ||   the chunking that it encodes.  Finally; the C{ChunkString} is ||   transformed back into a chunk structure; which is returned. ||  ||   C{RegexpChunk} can only be used to chunk a single kind of phrase. ||   For example; you can use an C{RegexpChunk} to chunk the noun ||   phrases in a text; or the verb phrases in a text; but you can not ||   use it to simultaneously chunk both noun phrases and verb phrases in ||   the same text.  (This is a limitation of C{RegexpChunk}; not of ||   chunk parsers in general.) ||  ||   RegexpChunkRules ||   ------------------ ||     C{RegexpChunkRule}s are transformational rules that update the ||     chunking of a text by modifying its C{ChunkString}.  Each ||     C{RegexpChunkRule} defines the C{apply} method; which modifies ||     the chunking encoded by a C{ChunkString}.  The ||     L{RegexpChunkRule} class itself can be used to implement any ||     transformational rule based on regular expressions.  There are ||     also a number of subclasses; which can be used to implement ||     simpler types of rules: ||  ||       - L{ChunkRule} chunks anything that matches a given regular ||         expression. ||       - L{ChinkRule} chinks anything that matches a given regular ||         expression. ||       - L{UnChunkRule} will un-chunk any chunk that matches a given ||         regular expression. ||       - L{MergeRule} can be used to merge two contiguous chunks. ||       - L{SplitRule} can be used to split a single chunk into two ||         smaller chunks. ||  ||     Tag Patterns ||     ~~~~~~~~~~~~ ||       C{RegexpChunkRule}s use a modified version of regular ||       expression patterns; called X{tag patterns}.  Tag patterns are ||       used to match sequences of tags.  Examples of tag patterns are:: ||  ||          r'(<DT>|<JJ>|<NN>)+' ||          r'<NN>+' ||          r'<NN.*>' ||  ||       The differences between regular expression patterns and tag ||       patterns are: ||  ||         - In tag patterns; C{'<'} and C{'>'} act as parenthases; so ||           C{'<NN>+'} matches one or more repetitions of C{'<NN>'}; not ||           C{'<NN'} followed by one or more repetitions of C{'>'}. ||         - Whitespace in tag patterns is ignored.  So ||           C{'<DT> | <NN>'} is equivalant to C{'<DT>|<NN>'} ||         - In tag patterns; C{'.'} is equivalant to C{'[^{}<>]'}; so ||           C{'<NN.*>'} matches any single tag starting with C{'NN'}. ||  ||       The function L{tag_pattern2re_pattern} can be used to transform ||       a tag pattern to an equivalent regular expression pattern. ||  ||   Efficiency ||   ---------- ||     Preliminary tests indicate that C{RegexpChunk} can chunk at a ||     rate of about 300 tokens\/second; with a moderately complex rule ||     set. ||  ||     There may be problems if C{RegexpChunk} is used with more than ||     5;000 tokens at a time.  In particular; evaluation of some regular ||     expressions may cause the Python regular expression engine to ||     exceed its maximum recursion depth.  We have attempted to minimize ||     these problems; but it is impossible to avoid them completely.  We ||     therefore recommend that you apply the chunk parser to a single ||     sentence at a time. ||  ||   Emacs Tip ||   --------- ||     If you evaluate the following elisp expression in emacs; it will ||     colorize C{ChunkString}s when you use an interactive python shell ||     with emacs or xemacs (\""C-c !\""):: ||  ||       (let () ||         (defconst comint-mode-font-lock-keywords  ||           '((\""<[^>]+>\"" 0 'font-lock-reference-face) ||             (\""[{}]\"" 0 'font-lock-function-name-face))) ||         (add-hook 'comint-mode-hook (lambda () (turn-on-font-lock)))) ||  ||     You can evaluate this code by copying it to a temporary buffer; ||     placing the cursor after the last close parenthasis; and typing ||     \""C{C-x C-e}\"".  You should evaluate it before running the interactive ||     session.  The change will last until you close emacs. ||  ||   Unresolved Issues ||   ----------------- ||     If we use the C{re} module for regular expressions; Python's ||     regular expression engine generates \""maximum recursion depth ||     exceeded\"" errors when processing very large texts; even for ||     regular expressions that should not require any recursion.  We ||     therefore use the C{pre} module instead.  But note that C{pre} ||     does not include Unicode support; so this module will not work ||     with unicode strings.  Note also that C{pre} regular expressions ||     are not quite as advanced as C{re} ones (e.g.; no leftward ||     zero-length assertions). ||  || @type _VALID_CHUNK_STRING: C{regexp} || @var _VALID_CHUNK_STRING: A regular expression to test whether a chunk ||      string is valid. || @type _VALID_TAG_PATTERN: C{regexp} || @var _VALID_TAG_PATTERN: A regular expression to test whether a tag ||      pattern is valid. || \""\""\""",https://github.com/nltk/nltk/commit/603384c5576ae07862766cd332c9ab5059a029ae,No
3954,nltk/nltk,nltk_lite/test/__init__.py,6f4f49bd3ce287d29e477b27d885cb515500131a,This is something of a hack; so trace.Trace can get at test().,https://github.com/nltk/nltk/commit/6f4f49bd3ce287d29e477b27d885cb515500131a,Yes
3955,nltk/nltk,nltk_lite/test/tree.py,6f4f49bd3ce287d29e477b27d885cb515500131a,"\""\""\"" || Unit testing for L{nltk.tree}. ||  || @todo: Test L{nltk.tree.ProbabilisticTreeToken} || @todo: Test L{nltk.tree.parse_treebank} || \""\""\""",https://github.com/nltk/nltk/commit/6f4f49bd3ce287d29e477b27d885cb515500131a,Yes
3956,nltk/nltk,nltk_lite/wordnet/wordnet.py,571fb266e92714546c5b8e125e342f90e69b2383,TODO: Add some comments\/doctest strings.,https://github.com/nltk/nltk/commit/571fb266e92714546c5b8e125e342f90e69b2383,Yes
3957,nltk/nltk,nltk_lite/chunk/__init__.py,c3abd766eb501e0595fee08b3467bb2a819bccc5,"\""\""\"" || Classes and interfaces for identifying non-overlapping linguistic || groups (such as base noun phrases) in unrestricted text.  This task is || called X{chunk parsing} or X{chunking}; and the identified groups are || called X{chunks}.  The chunked text is represented using a shallow || tree called a \""chunk structure.\""  A X{chunk structure} is a tree || containing tokens and chunks; where each chunk is a subtree containing || only tokens.  For example; the chunk structure for base noun phrase || chunks in the sentence \""I saw the big dog on the hill\"" is:: ||  ||   (SENTENCE: ||     (NP: <I>) ||     <saw> ||     (NP: <the> <big> <dog>) ||     <on> ||     (NP: <the> <hill>)) ||  || To convert a chunk structure back to a list of tokens; simply use the || chunk structure's L{leaves<Tree.leaves>} method. ||  || The C{parser.chunk} module defines L{ChunkI}; a standard interface for || chunking texts; and L{RegexpChunk}; a regular-expression based || implementation of that interface.  It uses the L{tree.chunk} and || L{tree.conll_chunk} methods; which tokenize strings containing chunked || and tagged texts.  It defines L{ChunkScore}; a utility class for || scoring chunk parsers. ||  || RegexpChunk || =========== ||  ||   C{parse.RegexpChunk} is an implementation of the chunk parser interface ||   that uses regular-expressions over tags to chunk a text.  Its ||   C{parse} method first constructs a C{ChunkString}; which encodes a ||   particular chunking of the input text.  Initially; nothing is ||   chunked.  C{parse.RegexpChunk} then applies a sequence of ||   C{RegexpChunkRule}s to the C{ChunkString}; each of which modifies ||   the chunking that it encodes.  Finally; the C{ChunkString} is ||   transformed back into a chunk structure; which is returned. ||  ||   C{RegexpChunk} can only be used to chunk a single kind of phrase. ||   For example; you can use an C{RegexpChunk} to chunk the noun ||   phrases in a text; or the verb phrases in a text; but you can not ||   use it to simultaneously chunk both noun phrases and verb phrases in ||   the same text.  (This is a limitation of C{RegexpChunk}; not of ||   chunk parsers in general.) ||  ||   RegexpChunkRules ||   ------------------ ||     C{RegexpChunkRule}s are transformational rules that update the ||     chunking of a text by modifying its C{ChunkString}.  Each ||     C{RegexpChunkRule} defines the C{apply} method; which modifies ||     the chunking encoded by a C{ChunkString}.  The ||     L{RegexpChunkRule} class itself can be used to implement any ||     transformational rule based on regular expressions.  There are ||     also a number of subclasses; which can be used to implement ||     simpler types of rules: ||  ||       - L{ChunkRule} chunks anything that matches a given regular ||         expression. ||       - L{ChinkRule} chinks anything that matches a given regular ||         expression. ||       - L{UnChunkRule} will un-chunk any chunk that matches a given ||         regular expression. ||       - L{MergeRule} can be used to merge two contiguous chunks. ||       - L{SplitRule} can be used to split a single chunk into two ||         smaller chunks. ||       - L{ExpandLeftRule} will expand a chunk to incorporate new ||         unchunked material on the left. ||       - L{ExpandRightRule} will expand a chunk to incorporate new ||         unchunked material on the right. ||  ||     Tag Patterns ||     ~~~~~~~~~~~~ ||       C{RegexpChunkRule}s use a modified version of regular ||       expression patterns; called X{tag patterns}.  Tag patterns are ||       used to match sequences of tags.  Examples of tag patterns are:: ||  ||          r'(<DT>|<JJ>|<NN>)+' ||          r'<NN>+' ||          r'<NN.*>' ||  ||       The differences between regular expression patterns and tag ||       patterns are: ||  ||         - In tag patterns; C{'<'} and C{'>'} act as parentheses; so ||           C{'<NN>+'} matches one or more repetitions of C{'<NN>'}; not ||           C{'<NN'} followed by one or more repetitions of C{'>'}. ||         - Whitespace in tag patterns is ignored.  So ||           C{'<DT> | <NN>'} is equivalant to C{'<DT>|<NN>'} ||         - In tag patterns; C{'.'} is equivalant to C{'[^{}<>]'}; so ||           C{'<NN.*>'} matches any single tag starting with C{'NN'}. ||  ||       The function L{tag_pattern2re_pattern} can be used to transform ||       a tag pattern to an equivalent regular expression pattern. ||  ||   Efficiency ||   ---------- ||     Preliminary tests indicate that C{RegexpChunk} can chunk at a ||     rate of about 300 tokens\/second; with a moderately complex rule ||     set. ||  ||     There may be problems if C{RegexpChunk} is used with more than ||     5;000 tokens at a time.  In particular; evaluation of some regular ||     expressions may cause the Python regular expression engine to ||     exceed its maximum recursion depth.  We have attempted to minimize ||     these problems; but it is impossible to avoid them completely.  We ||     therefore recommend that you apply the chunk parser to a single ||     sentence at a time. ||  ||   Emacs Tip ||   --------- ||     If you evaluate the following elisp expression in emacs; it will ||     colorize C{ChunkString}s when you use an interactive python shell ||     with emacs or xemacs (\""C-c !\""):: ||  ||       (let () ||         (defconst comint-mode-font-lock-keywords  ||           '((\""<[^>]+>\"" 0 'font-lock-reference-face) ||             (\""[{}]\"" 0 'font-lock-function-name-face))) ||         (add-hook 'comint-mode-hook (lambda () (turn-on-font-lock)))) ||  ||     You can evaluate this code by copying it to a temporary buffer; ||     placing the cursor after the last close parenthesis; and typing ||     \""C{C-x C-e}\"".  You should evaluate it before running the interactive ||     session.  The change will last until you close emacs. ||  ||   Unresolved Issues ||   ----------------- ||     If we use the C{re} module for regular expressions; Python's ||     regular expression engine generates \""maximum recursion depth ||     exceeded\"" errors when processing very large texts; even for ||     regular expressions that should not require any recursion.  We ||     therefore use the C{pre} module instead.  But note that C{pre} ||     does not include Unicode support; so this module will not work ||     with unicode strings.  Note also that C{pre} regular expressions ||     are not quite as advanced as C{re} ones (e.g.; no leftward ||     zero-length assertions). ||  || @type CHUNK_TAG_PATTERN: C{regexp} || @var CHUNK_TAG_PATTERN: A regular expression to test whether a tag ||      pattern is valid. || \""\""\""",https://github.com/nltk/nltk/commit/c3abd766eb501e0595fee08b3467bb2a819bccc5,Yes
3958,nltk/nltk,nltk/classify/api.py,6deaa7da3bd97694fbc2b9bc12fb8ba36cf4d766,"''' ||  || - training tokens ...... train_toks \/ labeled_toks || - testing tokens ....... test_toks \/ toks || - training features .... train_feats \/ labeled_feat_dicts? || - testing features ..... test_feats \/ feat_dict? ||  || feat_dict?  featdict?  featuredict? || feature_dict?  labeled_feature_dict? || labeled_featuredict? || labeled_featuredicts? ||  || a better name that \""feature dictionary\"" would be nice!! ||  || featureset? ||  || featureset \/ labeled_featureset ||  || '''",https://github.com/nltk/nltk/commit/6deaa7da3bd97694fbc2b9bc12fb8ba36cf4d766,No
3959,nltk/nltk,nltk/classify/api.py,9b2edcda127dd61986dde5b3d72db7af9c112f68,"''' ||  || - training tokens ...... train_toks \/ labeled_toks || - testing tokens ....... test_toks \/ toks || - training features .... train_feats \/ labeled_feat_dicts? || - testing features ..... test_feats \/ feat_dict? ||  || feat_dict?  featdict?  featuredict? || feature_dict?  labeled_feature_dict? || labeled_featuredict? || labeled_featuredicts? ||  || a better name that \""feature dictionary\"" would be nice!! ||  || featureset? ||  || featureset \/ labeled_featureset ||  || '''",https://github.com/nltk/nltk/commit/9b2edcda127dd61986dde5b3d72db7af9c112f68,No
3960,nltk/nltk,nltk/test/coverage.py,53783e79881b7922d822aa3e3f9e402082379d8f,2006-08-23 NMB Refactorings to improve testability.  Fixes to command-line,https://github.com/nltk/nltk/commit/53783e79881b7922d822aa3e3f9e402082379d8f,Yes
3961,nltk/nltk,nltk/test/doctest_driver.py,233307f2599415ce67451ac6e679fd9b9b28dc25,Monkey-Patch to fix Doctest,https://github.com/nltk/nltk/commit/233307f2599415ce67451ac6e679fd9b9b28dc25,Yes
3962,nltk/nltk,nltk/test/testrunner.py,fd588fd614983be009634a39846e5c5579dba8a3,TODO: Run the tests if the relevant dependeices are present on the system,https://github.com/nltk/nltk/commit/fd588fd614983be009634a39846e5c5579dba8a3,Yes
3963,nltk/nltk,nltk/test/testrunner.py,fd588fd614983be009634a39846e5c5579dba8a3,TODO: create an option for the doctest driver to disable output,https://github.com/nltk/nltk/commit/fd588fd614983be009634a39846e5c5579dba8a3,Yes
3964,nltk/nltk,nltk/test/doctest_nose_plugin.py,ee1416e72de74f5c64600f728d4539b09917d19e,FIXME this breaks the id plugin somehow (tests probably don't,https://github.com/nltk/nltk/commit/ee1416e72de74f5c64600f728d4539b09917d19e,Yes
3965,nltk/nltk,nltk/test/runtests.py,ee1416e72de74f5c64600f728d4539b09917d19e,TODO: Run the tests if the relevant dependeices are present on the system,https://github.com/nltk/nltk/commit/ee1416e72de74f5c64600f728d4539b09917d19e,Yes
3966,nltk/nltk,nltk/test/discourse_fixt.py,1a21e972e674444be125fdccfee07f0173051f18,FIXME: the entire discourse.doctest is skipped if Prover9\/Mace4 is,https://github.com/nltk/nltk/commit/1a21e972e674444be125fdccfee07f0173051f18,Yes
3967,nltk/nltk,nltk/corpus/util.py,6b337267cd964c5e7c558956ae539604a7f39df7,Without this fix tests may take extra 1.5GB RAM,https://github.com/nltk/nltk/commit/6b337267cd964c5e7c558956ae539604a7f39df7,No
3968,nltk/nltk,nltk/data.py,97ef63b51a0bf66374467b519df65449ef036cd5,XXX: ``path`` must be a bytestring under Python 2.x because,https://github.com/nltk/nltk/commit/97ef63b51a0bf66374467b519df65449ef036cd5,Yes
3969,nltk/nltk,nltk/tag/brill/trainer/fast.py,9acaec8c7aa4e8bee59c30f2e873db3d116e1dbc,!! FIXME: several tests are a bit too dependent on tracing format,https://github.com/nltk/nltk/commit/9acaec8c7aa4e8bee59c30f2e873db3d116e1dbc,No
3970,nltk/nltk,nltk/tag/brill/trainer/fast.py,9acaec8c7aa4e8bee59c30f2e873db3d116e1dbc,!! FIXME: tests in trainer.fast and trainer.brillorig are exact duplicates,https://github.com/nltk/nltk/commit/9acaec8c7aa4e8bee59c30f2e873db3d116e1dbc,No
3971,tensorflow/tensor2tensor,tensor2tensor/utils/multistep_optimizer_test.py,64e1df15e649c6db97856f7ea6aea6508d52b0cf,TODO: Remove version check once 1.5 is not tested anymore,https://github.com/tensorflow/tensor2tensor/commit/64e1df15e649c6db97856f7ea6aea6508d52b0cf,Yes
3972,tensorflow/tensor2tensor,tensor2tensor/utils/multistep_optimizer_test.py,1d6cb552c4d637a6cb241f6ba7e19a10a0a632bc,TODO: Remove version check once 1.5 is not tested anymore,https://github.com/tensorflow/tensor2tensor/commit/1d6cb552c4d637a6cb241f6ba7e19a10a0a632bc,Yes
3973,tensorflow/tensor2tensor,tensor2tensor/rl/batch_dqn_agent_test.py,150aad3be93c68f9424c0769e9e710c754ebaaea,TODO: maybe add testStepTrain (and possibly some other tests) from dopamine,https://github.com/tensorflow/tensor2tensor/commit/150aad3be93c68f9424c0769e9e710c754ebaaea,No
3974,tensorflow/tensor2tensor,tensor2tensor/rl/batch_runner_test.py,150aad3be93c68f9424c0769e9e710c754ebaaea,TODO: decide if we want to use and modify more tests from,https://github.com/tensorflow/tensor2tensor/commit/150aad3be93c68f9424c0769e9e710c754ebaaea,Yes
3975,tensorflow/tensor2tensor,tensor2tensor/rl/batch_runner_test.py,150aad3be93c68f9424c0769e9e710c754ebaaea,TODO: We probably do not need this test; dopamine test for Runner is enugh,https://github.com/tensorflow/tensor2tensor/commit/150aad3be93c68f9424c0769e9e710c754ebaaea,Yes
3976,tensorflow/tensor2tensor,tensor2tensor/rl/batch_dqn_agent_test.py,f28a5e9cec63fb2e0af575fdca754318b0cbdbca,TODO: maybe add testStepTrain (and possibly some other tests) from dopamine,https://github.com/tensorflow/tensor2tensor/commit/f28a5e9cec63fb2e0af575fdca754318b0cbdbca,No
3977,tensorflow/tensor2tensor,tensor2tensor/rl/batch_runner_test.py,f28a5e9cec63fb2e0af575fdca754318b0cbdbca,TODO: decide if we want to use and modify more tests from,https://github.com/tensorflow/tensor2tensor/commit/f28a5e9cec63fb2e0af575fdca754318b0cbdbca,Yes
3978,tensorflow/tensor2tensor,tensor2tensor/rl/batch_runner_test.py,f28a5e9cec63fb2e0af575fdca754318b0cbdbca,TODO: We probably do not need this test; dopamine test for Runner is enugh,https://github.com/tensorflow/tensor2tensor/commit/f28a5e9cec63fb2e0af575fdca754318b0cbdbca,Yes
3979,tensorflow/tensor2tensor,setup.py,4289bcfa12b75f434bfa2f73ca5adc088ea35731,Needed to fix a Travis pytest error.,https://github.com/tensorflow/tensor2tensor/commit/4289bcfa12b75f434bfa2f73ca5adc088ea35731,No
3980,RaRe-Technologies/gensim,DocumentCollection.py,7dfc5ce04437298d4aa3f69ef7cd6e033d7feaf8,TODO pridat DocumentCollection.check(): test na prazdne dokumenty; konzistenci; chybove stavy atd,https://github.com/RaRe-Technologies/gensim/commit/7dfc5ce04437298d4aa3f69ef7cd6e033d7feaf8,No
3981,RaRe-Technologies/gensim,matutils.py,7dfc5ce04437298d4aa3f69ef7cd6e033d7feaf8,FIXME better test against numpy.allclose?,https://github.com/RaRe-Technologies/gensim/commit/7dfc5ce04437298d4aa3f69ef7cd6e033d7feaf8,Yes
3982,RaRe-Technologies/gensim,src/gensim/models/lsimodel.py,1c8fbde7b8b79da86250a644c4eac66526659e59,FIXME test=>update True!!!,https://github.com/RaRe-Technologies/gensim/commit/1c8fbde7b8b79da86250a644c4eac66526659e59,Yes
3983,RaRe-Technologies/gensim,src/gensim/corpora/textcorpus.py,956fc79db9fcfef18a3eb1dfa96df824bb220a47,"\""\""\"" ||  || The original idea of Gensim was to delegate parsing etc to external tools. || For our usage scenario we do need to have a representation fo a text corpus. || We need basic agreement on what memory structures define a vorpus. || To do functional testing; tests will use a corpus; if we serialize the memmory || s || We assume the input text is one single file with documents separated by blanks. ||  || A text corpus is defined by a parsing and several output files (serializations). || The serializations are both in cpickle format (faster to load into memory) and in text format \\ || for easier human supervision. ||  || This class has methods to read and write all those serializations. ||  || It creates three files: ||  || * `OUTPUT_PREFIX_wordids.txt`: mapping between words and their integer ids || * `OUTPUT_PREFIX_idwords.txt`: mapping between words and their integer ids || * `OUTPUT_PREFIX_bow.mm`: bag-of-words (word counts) representation; in Matrix Matrix format || * `OUTPUT_PREFIX_tfidf.mm`: TF-IDF representation ||  || 1. corpusFilename: a single file with one doc per line. example: wiki2008 ||  || 2. filters (if none; using DEFAULT_FILTERS). This is a file too: corpusFilename.filters ||  || 3. the txt serializations ||  || 4. the cpickle serializations ||  || 5. a flag to optionally create txt files. by default it doesn't. ||  || To instantiate from txt or cpickle; we assume a convention for representations: ||  || mm ||  || bow ||  || tf_variation ||  || wordids ||  || idword ||  || The name is: corpusFilename + parsingParameters + representation + '.txt' OR 'cpickle' ||  || We can have either txt OR cpickle or both. txt is only for human debugging. If both are present;\\ ||  load should use the cpickle ||  || 1.1 methods ||  || for every representation: ||  || 1. loadtxt; loadCpickle ||  || 2. save; saveCpickle ||  || 3. getArticles. Yields an article (for the model object) ||  ||  || USAGE: %(program)s WIKI_XML_DUMP OUTPUT_PREFIX [VOCABULARY_SIZE] ||  ||     Convert articles from a Wikipedia dump to (sparse) vectors. The input is a bz2-compressed \\ || dump of Wikipedia articles; in XML format. ||  ||  || The output Matrix Market files can then be compressed (e.g.; by bzip2) to save \\ || disk space; gensim's corpus iterators can work with compressed input; too. ||  || `VOCABULARY_SIZE` controls how many of the most frequent words to keep (after || removing all tokens that appear in more than 10 percent documents). Defaults to 100;000. ||  || Example: .\/wikicorpus.py ~\/gensim\/results\/enwiki-20100622-pages-articles.xml.bz2 ~\/gensim\/results\/wiki_en || \""\""\""",https://github.com/RaRe-Technologies/gensim/commit/956fc79db9fcfef18a3eb1dfa96df824bb220a47,No
3984,RaRe-Technologies/gensim,gensim/test/test_simserver.py,a01f1a4fe5552950a0d7b8a7a56a550a2ec27baf,TODO how to test that it's faster? just read and verify server.status()?,https://github.com/RaRe-Technologies/gensim/commit/a01f1a4fe5552950a0d7b8a7a56a550a2ec27baf,Yes
3985,RaRe-Technologies/gensim,gensim/corpora/dictionary.py,aafe506fb6c3794ab1dd670e09ffd686d80cc5ff,TODO: test function,https://github.com/RaRe-Technologies/gensim/commit/aafe506fb6c3794ab1dd670e09ffd686d80cc5ff,Yes
3986,RaRe-Technologies/gensim,gensim/test/test_corpora_dictionary.py,aafe506fb6c3794ab1dd670e09ffd686d80cc5ff,TODO: Fix test this can't be the same; because there are no information,https://github.com/RaRe-Technologies/gensim/commit/aafe506fb6c3794ab1dd670e09ffd686d80cc5ff,Yes
3987,RaRe-Technologies/gensim,gensim/models/word2vec.py,da48ba9f05316fa48238f96dde4b2dd04a882dd9,TODO: remove this log when done with batching tests.,https://github.com/RaRe-Technologies/gensim/commit/da48ba9f05316fa48238f96dde4b2dd04a882dd9,No
3988,RaRe-Technologies/gensim,gensim/models/word2vec.py,da48ba9f05316fa48238f96dde4b2dd04a882dd9,TODO: remove these logs when done with batching tests.,https://github.com/RaRe-Technologies/gensim/commit/da48ba9f05316fa48238f96dde4b2dd04a882dd9,Yes
3989,RaRe-Technologies/gensim,gensim/test/test_hdpmodel.py,098be5fb22e5e6dbdb30354a3814938fa8061c69,TODO create show_topic in HdpModel and then test,https://github.com/RaRe-Technologies/gensim/commit/098be5fb22e5e6dbdb30354a3814938fa8061c69,Yes
3990,RaRe-Technologies/gensim,gensim/models/atmodel.py,739f34ee29381ebb4b005934b60b7f264d08e01c,"TODO: if deepcopy is not used here; something goes wrong. When unit tests are run (specifically \""testPasses\"");",https://github.com/RaRe-Technologies/gensim/commit/739f34ee29381ebb4b005934b60b7f264d08e01c,Yes
3991,RaRe-Technologies/gensim,gensim/test/test_atmodel.py,739f34ee29381ebb4b005934b60b7f264d08e01c,"\""\""\"" || Automated tests for the author-topic model (AuthorTopicModel class). These tests || are based on the unit tests of LDA; the classes are quite similar; and the tests || needed are thus quite similar. || \""\""\""",https://github.com/RaRe-Technologies/gensim/commit/739f34ee29381ebb4b005934b60b7f264d08e01c,Yes
3992,RaRe-Technologies/gensim,gensim/test/test_textcorpus.py,10e5429706879ffe2d7d6948a9ff8529338b4a6b,TODO add tests for other methods,https://github.com/RaRe-Technologies/gensim/commit/10e5429706879ffe2d7d6948a9ff8529338b4a6b,Yes
3993,RaRe-Technologies/gensim,gensim/models/translation_matrix.py,37bc8d45f5b1a5b702e70e3f48d8073508ee22b1,"\""\""\""Produce translation matrix to translate the word from one language to another language; using either || standard nearest neighbour method or globally corrected neighbour retrieval method [1]_. ||  || This method can be used to augment the existing phrase tables with more candidate translations; or || filter out errors from the translation tables and known dictionaries [2]_. What's more; It also work || for any two sets of named-vectors where there are some paired-guideposts to learn the transformation. ||  || Examples || -------- || **How to make translation between two set of word-vectors** ||  || Initialize a word-vector models ||  || >>> from gensim.models import KeyedVectors || >>> from gensim.test.utils import datapath; temporary_file || >>> from gensim.models import TranslationMatrix || >>> || >>> model_en = KeyedVectors.load_word2vec_format(datapath(\""EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt\"")) || >>> model_it = KeyedVectors.load_word2vec_format(datapath(\""IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt\"")) ||  || Define word pairs (that will be used for construction of translation matrix ||  || >>> word_pairs = [ || ...     (\""one\""; \""uno\""); (\""two\""; \""due\""); (\""three\""; \""tre\""); (\""four\""; \""quattro\""); (\""five\""; \""cinque\""); || ...     (\""seven\""; \""sette\""); (\""eight\""; \""otto\""); || ...     (\""dog\""; \""cane\""); (\""pig\""; \""maiale\""); (\""fish\""; \""cavallo\""); (\""birds\""; \""uccelli\""); || ...     (\""apple\""; \""mela\""); (\""orange\""; \""arancione\""); (\""grape\""; \""acino\""); (\""banana\""; \""banana\"") || ... ] ||  || Fit :class:`~gensim.models.translation_matrix.TranslationMatrix` ||  || >>> trans_model = TranslationMatrix(model_en; model_it; word_pairs=word_pairs) ||  || Apply model (translate words \""dog\"" and \""one\"") ||  || >>> trans_model.translate([\""dog\""; \""one\""]; topn=3) || OrderedDict([('dog'; [u'cane'; u'gatto'; u'cavallo']); ('one'; [u'uno'; u'due'; u'tre'])]) ||  ||  || Save \/ load model ||  || >>> with temporary_file(\""model_file\"") as fname: || ...     trans_model.save(fname)  # save model to file || ...     loaded_trans_model = TranslationMatrix.load(fname)  # load model ||  ||  || **How to make translation between two :class:`~gensim.models.doc2vec.Doc2Vec` models** ||  || Prepare data and models ||  || >>> from gensim.test.utils import datapath || >>> from gensim.test.test_translation_matrix import read_sentiment_docs || >>> from gensim.models import Doc2Vec; BackMappingTranslationMatrix || >>> || >>> data = read_sentiment_docs(datapath(\""alldata-id-10.txt\""))[:5] || >>> src_model = Doc2Vec.load(datapath(\""small_tag_doc_5_iter50\"")) || >>> dst_model = Doc2Vec.load(datapath(\""large_tag_doc_10_iter50\"")) ||  || Train backward translation ||  || >>> model_trans = BackMappingTranslationMatrix(data; src_model; dst_model) || >>> trans_matrix = model_trans.train(data) ||  ||  || Apply model ||  || >>> result = model_trans.infer_vector(dst_model.docvecs[data[3].tags]) ||  ||  || References || ---------- || .. [1] Dinu; Georgiana; Angeliki Lazaridou; and Marco Baroni. \""Improving zero-shot learning by mitigating the ||        hubness problem\""; https:\/\/arxiv.org\/abs\/1412.6568 || .. [2] Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg Corrado; and Jeffrey Dean. ||        \""Distributed Representations of Words and Phrases and their Compositionality\""; https:\/\/arxiv.org\/abs\/1310.4546 ||  || \""\""\""",https://github.com/RaRe-Technologies/gensim/commit/37bc8d45f5b1a5b702e70e3f48d8073508ee22b1,Yes
3994,RaRe-Technologies/gensim,gensim/test/test_corpora.py,1648ac2e4299ccdfde2ac1b535faf2c5eaf6c022,Needed for the test_custom_tokenizer is the TestWikiCorpus class.,https://github.com/RaRe-Technologies/gensim/commit/1648ac2e4299ccdfde2ac1b535faf2c5eaf6c022,Yes
3995,RaRe-Technologies/gensim,gensim/test/test_utils_any2vec.py,1052b9bd890761f89bcfd6c340304ad448f4a187,"\""\""\"" || Automated tests for checking utils_any2vec functionality. || \""\""\""",https://github.com/RaRe-Technologies/gensim/commit/1052b9bd890761f89bcfd6c340304ad448f4a187,Yes
3996,RaRe-Technologies/gensim,gensim/test/test_doc2vec.py,c0e0169565116854993b22efef29e3c402ec6c69,TODO: test saving in word2vec format,https://github.com/RaRe-Technologies/gensim/commit/c0e0169565116854993b22efef29e3c402ec6c69,Yes
3997,RaRe-Technologies/gensim,gensim/test/test_fasttext.py,c0e0169565116854993b22efef29e3c402ec6c69,TODO: add semantic tests; where appropriate,https://github.com/RaRe-Technologies/gensim/commit/c0e0169565116854993b22efef29e3c402ec6c69,Yes
3998,RaRe-Technologies/gensim,gensim/models/keyedvectors.py,1edbb4c3f9432eef9c2aee499d1c861b33fc30bc,FIXME: `uniform` passes all tests; but generates temporary double-sized np.float64 array;,https://github.com/RaRe-Technologies/gensim/commit/1edbb4c3f9432eef9c2aee499d1c861b33fc30bc,Yes
3999,RaRe-Technologies/gensim,gensim/models/keyedvectors.py,1edbb4c3f9432eef9c2aee499d1c861b33fc30bc,sticking with the RAM-wasteful-but-all-tests-passing approach above; TODO debug\/fix ASAP.,https://github.com/RaRe-Technologies/gensim/commit/1edbb4c3f9432eef9c2aee499d1c861b33fc30bc,Yes
4000,explosion/spaCy,tests/test_token_api.py,0962ffc0955ead4f377470498e2f026ce46469d0,TODO: Test more of these; esp. if a bug is found,https://github.com/explosion/spaCy/commit/0962ffc0955ead4f377470498e2f026ce46469d0,No
4001,explosion/spaCy,spacy/tests/conftest.py,9b3f8f9ec3c98c94581694060cfb60be0b8bf2f3,TODO: re-implement generic tokenizer tests,https://github.com/explosion/spaCy/commit/9b3f8f9ec3c98c94581694060cfb60be0b8bf2f3,Yes
4002,explosion/spaCy,spacy/tests/conftest.py,18ec6610ddc53272eeada6aab1aeb6b8f26b9b05,TODO: re-implement generic tokenizer tests,https://github.com/explosion/spaCy/commit/18ec6610ddc53272eeada6aab1aeb6b8f26b9b05,Yes
4003,explosion/spaCy,spacy/tests/conftest.py,facf77e54136e16a1808f064a984ae8bb9da8d66,TODO: re-implement generic tokenizer tests,https://github.com/explosion/spaCy/commit/facf77e54136e16a1808f064a984ae8bb9da8d66,Yes
4004,explosion/spaCy,examples/training/ner_multitask_objective.py,00557c5fddd865aa767d9e09a0cd454e70e79f6e,'''This example shows how to add a multi-task objective that is trained || alongside the entity recognizer. This is an alternative to adding features || to the model. ||  || The multi-task idea is to train an auxiliary model to predict some attribute; || with weights shared between the auxiliary model and the main model. In this || example; we're predicting the position of the word in the document. ||  || The model that predicts the position of the word encourages the convolutional || layers to include the position information in their representation. The || information is then available to the main model; as a feature. ||  || The overall idea is that we might know something about what sort of features || we'd like the CNN to extract. The multi-task objectives can encourage the || extraction of this type of feature. The multi-task objective is only used || during training. We discard the auxiliary model before run-time. ||  || The specific example here is not necessarily a good idea --- but it shows || how an arbitrary objective function for some word can be used. ||  || Developed and tested for spaCy 2.0.6 || ''',https://github.com/explosion/spaCy/commit/00557c5fddd865aa767d9e09a0cd454e70e79f6e,Yes
4005,zhanghang1989/PyTorch-Encoding,encoding/__init__.py,76eeba5ddde3d1cb4fc40f1f6b8491b03d0535bd,TODO FIXME this is test only,https://github.com/zhanghang1989/PyTorch-Encoding/commit/76eeba5ddde3d1cb4fc40f1f6b8491b03d0535bd,No
4006,zhanghang1989/PyTorch-Encoding,test/test.py,76eeba5ddde3d1cb4fc40f1f6b8491b03d0535bd,TODO cpu test,https://github.com/zhanghang1989/PyTorch-Encoding/commit/76eeba5ddde3d1cb4fc40f1f6b8491b03d0535bd,Yes
4007,PetrochukM/PyTorch-NLP,tests/unit_test/nn/test_seq_decoder.py,0cdbcfbe4a5508e902d60152001b29fb0f331fe4,TODO: Test if `max_length=none` explicitly. This is challenging because the SeqDecoder only,https://github.com/PetrochukM/PyTorch-NLP/commit/0cdbcfbe4a5508e902d60152001b29fb0f331fe4,Yes
4008,PetrochukM/PyTorch-NLP,tests/optim/common.py,fd46926c5d6b8eeee3e19bf704f55a1f92eae7af,a hack for JIT tests,https://github.com/PetrochukM/PyTorch-NLP/commit/fd46926c5d6b8eeee3e19bf704f55a1f92eae7af,Yes
4009,PetrochukM/PyTorch-NLP,tests/datasets/test_simple_qa.py,83f92e8fa41c675bb0d723422ee978df16357831,TODO: Consider mocking the simple_qa dataset for faster tests,https://github.com/PetrochukM/PyTorch-NLP/commit/83f92e8fa41c675bb0d723422ee978df16357831,Yes
4010,PetrochukM/PyTorch-NLP,tests/test_pretrained_embeddings.py,83f92e8fa41c675bb0d723422ee978df16357831,TODO: Consider making the tests faster by mocking the download,https://github.com/PetrochukM/PyTorch-NLP/commit/83f92e8fa41c675bb0d723422ee978df16357831,No
4011,PetrochukM/PyTorch-NLP,torchnlp/datasets/wikitext_2.py,62ba88d154c209a0aa56588fa1f4ed910a40af14,TODO: Add test,https://github.com/PetrochukM/PyTorch-NLP/commit/62ba88d154c209a0aa56588fa1f4ed910a40af14,Yes
4012,PetrochukM/PyTorch-NLP,tests/test_random.py,887f3d2bc4a57e4c33935ab139219f745da73d8b,TODO: Test `torch.cuda` random as well.,https://github.com/PetrochukM/PyTorch-NLP/commit/887f3d2bc4a57e4c33935ab139219f745da73d8b,Yes
4013,yzhao062/pyod,pyador/util/tests/test_data_prep.py,499b0cc828958a341e4ca7473b566892c03821ae,TODO: supplement more testcases for util functions,https://github.com/yzhao062/pyod/commit/499b0cc828958a341e4ca7473b566892c03821ae,No
4014,yzhao062/pyod,models/hbos.py,4638cbc8d106450f1e821750b4af0ea88408a6ee,TODO: move to testing code,https://github.com/yzhao062/pyod/commit/4638cbc8d106450f1e821750b4af0ea88408a6ee,Yes
4015,yzhao062/pyod,models/knn.py,4638cbc8d106450f1e821750b4af0ea88408a6ee,TODO: move to testing code,https://github.com/yzhao062/pyod/commit/4638cbc8d106450f1e821750b4af0ea88408a6ee,Yes
4016,yzhao062/pyod,models/lof.py,4638cbc8d106450f1e821750b4af0ea88408a6ee,TODO: move to testing code,https://github.com/yzhao062/pyod/commit/4638cbc8d106450f1e821750b4af0ea88408a6ee,Yes
4017,yzhao062/pyod,models/ocsvm.py,4638cbc8d106450f1e821750b4af0ea88408a6ee,TODO: move to testing code,https://github.com/yzhao062/pyod/commit/4638cbc8d106450f1e821750b4af0ea88408a6ee,Yes
4018,yzhao062/pyod,models/abod.py,e1e06512a9d6496e3bf8097d8a78acd5afa9f425,# TODO: move to testing code,https://github.com/yzhao062/pyod/commit/e1e06512a9d6496e3bf8097d8a78acd5afa9f425,No
4019,yzhao062/pyod,pyod/models/abod.py,d1a7e4754f983c1dda78c14495c4604afaa6af0a,# TODO: move to testing code,https://github.com/yzhao062/pyod/commit/d1a7e4754f983c1dda78c14495c4604afaa6af0a,No
4020,yzhao062/pyod,pyod/models/hbos.py,d1a7e4754f983c1dda78c14495c4604afaa6af0a,TODO: move to testing code,https://github.com/yzhao062/pyod/commit/d1a7e4754f983c1dda78c14495c4604afaa6af0a,Yes
4021,yzhao062/pyod,pyod/models/knn.py,d1a7e4754f983c1dda78c14495c4604afaa6af0a,TODO: move to testing code,https://github.com/yzhao062/pyod/commit/d1a7e4754f983c1dda78c14495c4604afaa6af0a,Yes
4022,yzhao062/pyod,pyod/models/lof.py,d1a7e4754f983c1dda78c14495c4604afaa6af0a,TODO: move to testing code,https://github.com/yzhao062/pyod/commit/d1a7e4754f983c1dda78c14495c4604afaa6af0a,Yes
4023,yzhao062/pyod,pyod/models/ocsvm.py,d1a7e4754f983c1dda78c14495c4604afaa6af0a,TODO: move to testing code,https://github.com/yzhao062/pyod/commit/d1a7e4754f983c1dda78c14495c4604afaa6af0a,Yes
4024,yzhao062/pyod,pyod/models/temp_abod.py,d1a7e4754f983c1dda78c14495c4604afaa6af0a,# TODO: move to testing code,https://github.com/yzhao062/pyod/commit/d1a7e4754f983c1dda78c14495c4604afaa6af0a,No
4025,yzhao062/pyod,pyod/pyador/util/tests/test_data_prep.py,d1a7e4754f983c1dda78c14495c4604afaa6af0a,TODO: supplement more testcases for util functions,https://github.com/yzhao062/pyod/commit/d1a7e4754f983c1dda78c14495c4604afaa6af0a,No
4026,yzhao062/pyod,pyod/test/test_base.py,798882c255701943d530d7024d0da2db22e33cc0,TODO: add more testcases,https://github.com/yzhao062/pyod/commit/798882c255701943d530d7024d0da2db22e33cc0,No
4027,yzhao062/pyod,pyod/test/test_base.py,798882c255701943d530d7024d0da2db22e33cc0,TODO: create uniform testcases,https://github.com/yzhao062/pyod/commit/798882c255701943d530d7024d0da2db22e33cc0,Yes
4028,yzhao062/pyod,pyod/models/base.py,ad5beccf2b74dc39314843e8a970a891cbd52267,XXX: should we rather test if instance of estimator?,https://github.com/yzhao062/pyod/commit/ad5beccf2b74dc39314843e8a970a891cbd52267,No
4029,yzhao062/pyod,pyod/test/test_base.py,ad5beccf2b74dc39314843e8a970a891cbd52267,TODO: add more testcases,https://github.com/yzhao062/pyod/commit/ad5beccf2b74dc39314843e8a970a891cbd52267,No
4030,yzhao062/pyod,pyod/test/test_feat_bagging.py,e2487c560433686e337be99e6389a7f728d2fdce,TODO: finish the tests once the main model is ready,https://github.com/yzhao062/pyod/commit/e2487c560433686e337be99e6389a7f728d2fdce,No
4031,yzhao062/pyod,pyod/test/test_base.py,71723b53d3c29d46d5b317e79ea074320cb1021e,TODO: create uniform testcases,https://github.com/yzhao062/pyod/commit/71723b53d3c29d46d5b317e79ea074320cb1021e,Yes
4032,yzhao062/pyod,examples/temp_do_not_use_lscp.py,5818ed1abe7fc48186339cf9a23327fadc79cc13,TODO: Design unit tests,https://github.com/yzhao062/pyod/commit/5818ed1abe7fc48186339cf9a23327fadc79cc13,Yes
4033,yzhao062/pyod,pyod/models/lscp.py,9f784a04364e67eaf87a499557945b0f021f7521,TODO: Design unit tests,https://github.com/yzhao062/pyod/commit/9f784a04364e67eaf87a499557945b0f021f7521,Yes
4034,yzhao062/pyod,pyod/test/test_knn.py,1d51313cbac69426b538c00298be3115fb04e19b,TODO: add a testcase for #158,https://github.com/yzhao062/pyod/commit/1d51313cbac69426b538c00298be3115fb04e19b,No
4035,chrislit/abydos,tests/distance/test_distance__token_distance.py,3264baf8b48ffd31542ceeccadb30533206a3819,Some additional constructors needed to complete test coverage,https://github.com/chrislit/abydos/commit/3264baf8b48ffd31542ceeccadb30533206a3819,No
4036,prihoda/golem,golem/core/context.py,424ffea8579cce6b7f97346cd1df2ede9926cd98,TODO for testing,https://github.com/prihoda/golem/commit/424ffea8579cce6b7f97346cd1df2ede9926cd98,No
4037,prihoda/golem,golem/core/entity_query.py,44e896aef8de5af92a326b4c2cb46a15faa86200,TODO for testing,https://github.com/prihoda/golem/commit/44e896aef8de5af92a326b4c2cb46a15faa86200,No
4038,lambdaloop/anipose,triangulate.py,bee5865e362621900cdfd7b978da83961342d1ef,# hack for hdf5 for testing,https://github.com/lambdaloop/anipose/commit/bee5865e362621900cdfd7b978da83961342d1ef,No
4039,rsokl/MyGrad,tests/tensor_base/test_augmented_updates.py,4f93d09723f69289a98de5182ed4c22f7a8f520d,TODO: These tests need to be modified to check that tensor-identity,https://github.com/rsokl/MyGrad/commit/4f93d09723f69289a98de5182ed4c22f7a8f520d,No
4040,raamana/neuropredict,rhst.py,f0c40cb6b51fc981fc69606d3e4d78ab5e0e23b1,TODO test if the gathering of prob data is consistent across multiple calls to this method,https://github.com/raamana/neuropredict/commit/f0c40cb6b51fc981fc69606d3e4d78ab5e0e23b1,Yes
4041,raamana/neuropredict,neuropredict/rhst.py,ea8604b6967062a2d60bb8aa2f3d633bb0265890,TODO test if the gathering of prob data is consistent across multiple calls to this method,https://github.com/raamana/neuropredict/commit/ea8604b6967062a2d60bb8aa2f3d633bb0265890,Yes
4042,raamana/neuropredict,build/lib/neuropredict/posthoc.py,2d9b868edbf24bd3bb804eba5bf41a7695158b73,TODO later: as the implementation will need significant testing!,https://github.com/raamana/neuropredict/commit/2d9b868edbf24bd3bb804eba5bf41a7695158b73,No
4043,raamana/neuropredict,neuropredict/rhst.py,ace60b754a7c6439ccbfe2cd3e622497607cbd0e,TODO NOW test if the gathering of prob data is consistent across multiple calls to this method,https://github.com/raamana/neuropredict/commit/ace60b754a7c6439ccbfe2cd3e622497607cbd0e,Yes
4044,lozuwa/impy,unitTest.py,741f9539e370ebf856025a1b978b2e8aabe21450,"\""\""\"" || package: Images2Dataset || class: unitTests || Author: Rodrigo Loza || Description: Unit tests || \""\""\""",https://github.com/lozuwa/impy/commit/741f9539e370ebf856025a1b978b2e8aabe21450,Yes
4045,lozuwa/impy,ImagePreprocessing.py,98f12e9d18bf5f3914ada8deb819122953724ad2,"\""\""\"" || package: Images2Dataset || class: ImagePreprocessing || Author: Rodrigo Loza || Description: Common pre-processing operations  || for an image. || Log: || \tAugust; 2017 -> Created class and added most relevant functions. || \tSeptember; 2017 -> Created more features for the DivideIntoPatches method. || \tNovember; 2017 -> Refactored all the code to Google style and more OOP paradigm. || \tDecember; 2017 -> Tested refactor to avoid breaking external code. || \tMarch; 2018 -> Refactored to another structure. || \""\""\""",https://github.com/lozuwa/impy/commit/98f12e9d18bf5f3914ada8deb819122953724ad2,No
4046,williamSYSU/TextGAN-PyTorch,instructor/real_data/catgan_instructor.py,d8a7827e4e1f1fe669aa8ca5a7ce192629ed57bd,self.clas_data.reset(self.test_samples_list)  # TODO: bug: have to reset,https://github.com/williamSYSU/TextGAN-PyTorch/commit/d8a7827e4e1f1fe669aa8ca5a7ce192629ed57bd,No
4047,bhargavvader/pycobra,pycobra/visualisation.py,2822360217b1bb78cc76ef7265f914c36f8b902a,note: for default colors to be assigned; the latest version of matplotlib is needed.,https://github.com/bhargavvader/pycobra/commit/2822360217b1bb78cc76ef7265f914c36f8b902a,No
4048,silvandeleemput/memcnn,memcnn/models/tests/test_revop.py,ea16dea05708f8375de83c94b5176026927a9ea7,TODO: add normal test for affine coupling,https://github.com/silvandeleemput/memcnn/commit/ea16dea05708f8375de83c94b5176026927a9ea7,Yes
4049,silvandeleemput/memcnn,memcnn/models/revop.py,100f4d05aa0942b31731ca8be60bd68143faa384,TODO add ensure non-zero inputs for testing otherwise resample? (repeat for n times??? \/ or raise warnings),https://github.com/silvandeleemput/memcnn/commit/100f4d05aa0942b31731ca8be60bd68143faa384,Yes
4050,csxeba/brainforge,ops/activations.py,b5e76caafe213e024644443a04fb1c5112a8c433,TODO: test this with numerical gradient testing!,https://github.com/csxeba/brainforge/commit/b5e76caafe213e024644443a04fb1c5112a8c433,No
4051,nschaetti/EchoTorch,echotorch/nn/reservoir/FreeRunESN.py,256363e1e65c1c9cef12eddad1e244dc06d29007,TODO: Test,https://github.com/nschaetti/EchoTorch/commit/256363e1e65c1c9cef12eddad1e244dc06d29007,Yes
4052,nschaetti/EchoTorch,echotorch/nn/reservoir/FreeRunESNCell.py,256363e1e65c1c9cef12eddad1e244dc06d29007,TODO: Test,https://github.com/nschaetti/EchoTorch/commit/256363e1e65c1c9cef12eddad1e244dc06d29007,Yes
4053,nschaetti/EchoTorch,echotorch/nn/conceptors/ConceptorSet.py,2bb7c0852256f96d96ff11111dbc0b29c28dc383,TODO: Test,https://github.com/nschaetti/EchoTorch/commit/2bb7c0852256f96d96ff11111dbc0b29c28dc383,Yes
4054,nschaetti/EchoTorch,test/test_narma10_prediction.py,6159b46ed280df738b10e0aa3d290dc4314b883f,TODO: Check why the CUDA version does 3.7291 (train_nrmse32); 0.16 (test_mse32) and 3.42 (test_nrmse32),https://github.com/nschaetti/EchoTorch/commit/6159b46ed280df738b10e0aa3d290dc4314b883f,No
4055,r9y9/nnmnkwii,tests/test_preprocessing.py,cf349c272afa2e83506fecf9a73dd37549fa1f58,TODO: better test,https://github.com/r9y9/nnmnkwii/commit/cf349c272afa2e83506fecf9a73dd37549fa1f58,Yes
4056,keiffster/program-y,src/test/parser/template/test_nodes.py,76bdb32600141297febb8aa566ff63cd5fc1fe3d,TODO Better unit testing of external service calls required,https://github.com/keiffster/program-y/commit/76bdb32600141297febb8aa566ff63cd5fc1fe3d,Yes
4057,keiffster/program-y,src/test/parser/template/test_nodes.py,76bdb32600141297febb8aa566ff63cd5fc1fe3d,TODO More unit tests for different variations here please,https://github.com/keiffster/program-y/commit/76bdb32600141297febb8aa566ff63cd5fc1fe3d,No
4058,keiffster/program-y,src/test/parser/template/test_nodes.py,76bdb32600141297febb8aa566ff63cd5fc1fe3d,TODO Add unit testing here,https://github.com/keiffster/program-y/commit/76bdb32600141297febb8aa566ff63cd5fc1fe3d,No
4059,keiffster/program-y,src/test/test_dialog.py,f13c34d87c2a6b7eaa29662c0a1c4f28b0faa9bd,TODO dialog needs proper unit testing,https://github.com/keiffster/program-y/commit/f13c34d87c2a6b7eaa29662c0a1c4f28b0faa9bd,No
4060,keiffster/program-y,src/test/parser/template/test_nodes/test_condtype2.py,5c25761be7a56e7636de9e06230e73e371b7183a,TODO Add unit tests for <loop \/> construct,https://github.com/keiffster/program-y/commit/5c25761be7a56e7636de9e06230e73e371b7183a,Yes
4061,keiffster/program-y,src/test/parser/template/test_nodes/test_condtype3.py,5c25761be7a56e7636de9e06230e73e371b7183a,TODO Add unit tests for <loop \/> construct,https://github.com/keiffster/program-y/commit/5c25761be7a56e7636de9e06230e73e371b7183a,Yes
4062,keiffster/program-y,src/test/parser/template/test_nodes/test_mixed.py,5c25761be7a56e7636de9e06230e73e371b7183a,TODO Add unit testing here,https://github.com/keiffster/program-y/commit/5c25761be7a56e7636de9e06230e73e371b7183a,No
4063,keiffster/program-y,src/test/parser/template/test_nodes/test_sraix.py,5c25761be7a56e7636de9e06230e73e371b7183a,TODO Better unit testing of external service calls required,https://github.com/keiffster/program-y/commit/5c25761be7a56e7636de9e06230e73e371b7183a,Yes
4064,keiffster/program-y,src/test/parser/template/test_nodes/test_think.py,5c25761be7a56e7636de9e06230e73e371b7183a,TODO More unit tests for different variations here please,https://github.com/keiffster/program-y/commit/5c25761be7a56e7636de9e06230e73e371b7183a,No
4065,keiffster/program-y,bots/y-bot/src/test/extensions/geocode/test_geocode.py,c3ed6a8ebe357c240c9186a8c7ea6b34d254a932,TODO Mock out responses; these tests currently call live Google maps API,https://github.com/keiffster/program-y/commit/c3ed6a8ebe357c240c9186a8c7ea6b34d254a932,No
4066,keiffster/program-y,bots/y-bot/src/test/extensions/maps/test_maps.py,c3ed6a8ebe357c240c9186a8c7ea6b34d254a932,TODO Mock out responses; these tests currently call live Google maps API,https://github.com/keiffster/program-y/commit/c3ed6a8ebe357c240c9186a8c7ea6b34d254a932,No
4067,keiffster/program-y,src/programy/parser/template/nodes/get.py,2726298a11ac073be56db6f1ed9f6a007e1b0168,TODO Why would you need this test; when is get_conversation(clientid) == None ?,https://github.com/keiffster/program-y/commit/2726298a11ac073be56db6f1ed9f6a007e1b0168,Yes
4068,keiffster/program-y,test/programytest/aiml_tests/sraix_tests/test_sraix_aiml.py,4b7c063e2d7b93e616fb823751cda83b53647c4a,TODO Add tests here !!!,https://github.com/keiffster/program-y/commit/4b7c063e2d7b93e616fb823751cda83b53647c4a,No
4069,keiffster/program-y,test/programytest/test_brain.py,4b7c063e2d7b93e616fb823751cda83b53647c4a,TODO Add tests here,https://github.com/keiffster/program-y/commit/4b7c063e2d7b93e616fb823751cda83b53647c4a,Yes
4070,achaiah/pywick,pywick/models/classification/xception.py,afbbd54fecaf6aaa5ae25162c8fe2c1af821d865,"\""\""\"" || Ported to pytorch thanks to [tstandley](https:\/\/github.com\/tstandley\/Xception-PyTorch) ||  || @author: tstandley || Adapted by cadene ||  || Creates an Xception Model as defined in: ||  || Francois Chollet || Xception: Deep Learning with Depthwise Separable Convolutions || https:\/\/arxiv.org\/pdf\/1610.02357.pdf ||  || This weights ported from the Keras implementation. Achieves the following performance on the validation set: ||  || Loss:0.9173 Prec@1:78.892 Prec@5:94.292 ||  || REMEMBER to set your image size to 3x299x299 for both test and validation ||  || normalize = transforms.Normalize(mean=[0.5; 0.5; 0.5]; ||                                   std=[0.5; 0.5; 0.5]) ||  || The resize parameter of the validation transform should be 333; and make sure to center crop at 299x299 || \""\""\""",https://github.com/achaiah/pywick/commit/afbbd54fecaf6aaa5ae25162c8fe2c1af821d865,Yes
4071,achaiah/pywick,pywick/models/__init__.py,8b1167c64e8926eeb6930be3c30e290e6a2378c6,"\""\""\"" || Neural network models is what deep learning is all about! While you can download some standard models from || `torchvision <https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html\/>`_; we strive to create a library of models || that are on the cutting edge of AI. Whenever possible; `we provide pretrained solutions as well!`\ ||  || That said; we didn't come up with any of these on our own so we owe a huge debt of gratitude to the many researchers who have shared || their models and weights on github.\ ||  || **Caution:** While we strive to ensure that all models can be used out of the box; sometimes things become broken due to Pytorch updates || or misalignment of the planets. Please don't yell at us. Gently point out what's broken; or even better; submit a pull request to fix it!\ ||  || **Here Be Dragons:** Aaand one more thing - we constantly plumb the depths of github for new models or tweaks to existing ones. While we don't || list this in the docs; there is a special `testnets` directory with tons of probably broken; semi-working; and at times crazy awesome || models and model-variations. If you're interested in the bleeding edge; that's where you'd look (see ``models.__init__.py`` for what's available) || \""\""\""",https://github.com/achaiah/pywick/commit/8b1167c64e8926eeb6930be3c30e290e6a2378c6,Yes
4072,PyTorchLightning/pytorch-lightning,tests/test_models.py,6e2bf991f0f2491901ce82c043797cd9a5a89081,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/6e2bf991f0f2491901ce82c043797cd9a5a89081,Yes
4073,PyTorchLightning/pytorch-lightning,tests/test_models.py,18ce3e5a23c9d1bb72cb36a347766b816a0b81c7,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/18ce3e5a23c9d1bb72cb36a347766b816a0b81c7,Yes
4074,PyTorchLightning/pytorch-lightning,tests/debug.py,4104a0fc4726021142b520374e770c209e25d838,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/4104a0fc4726021142b520374e770c209e25d838,Yes
4075,PyTorchLightning/pytorch-lightning,tests/test_models.py,481aa2497439e51bd6545fd18d45f898ffe47335,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/481aa2497439e51bd6545fd18d45f898ffe47335,Yes
4076,PyTorchLightning/pytorch-lightning,tests/test_models.py,bf09060fef2e3c60b1e6539570071cb104a82257,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/bf09060fef2e3c60b1e6539570071cb104a82257,Yes
4077,PyTorchLightning/pytorch-lightning,tests/test_z_amp.py,5afae59715d2e2107e145c626bb7367b0f89a3d6,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/5afae59715d2e2107e145c626bb7367b0f89a3d6,Yes
4078,PyTorchLightning/pytorch-lightning,pytorch_lightning/core/__init__.py,d71556e7a15a82dc3f47294bcbdb176f882f0c44,"\""\""\"" || Lightning Module interface || ========================== ||  || A lightning module is a strict superclass of nn.Module; it provides a standard interface ||  for the trainer to interact with the model. ||  || The easiest thing to do is copy the minimal example below and modify accordingly. ||  || Otherwise; to Define a Lightning Module; implement the following methods: ||  ||  || Minimal example || --------------- ||  || .. code-block:: python ||  ||     import os ||     import torch ||     from torch.nn import functional as F ||     from torch.utils.data import DataLoader ||     from torchvision.datasets import MNIST ||     import torchvision.transforms as transforms ||  ||     import pytorch_lightning as pl ||  ||     class CoolModel(pl.LightningModule): ||  ||         def __init__(self): ||             super(CoolModel; self).__init__() ||             # not the best model... ||             self.l1 = torch.nn.Linear(28 * 28; 10) ||  ||         def forward(self; x): ||             return torch.relu(self.l1(x.view(x.size(0); -1))) ||  ||         def training_step(self; batch; batch_nb): ||             # REQUIRED ||             x; y = batch ||             y_hat = self.forward(x) ||             return {'loss': F.cross_entropy(y_hat; y)} ||  ||         def validation_step(self; batch; batch_nb): ||             # OPTIONAL ||             x; y = batch ||             y_hat = self.forward(x) ||             return {'val_loss': F.cross_entropy(y_hat; y)} ||  ||         def validation_end(self; outputs): ||             # OPTIONAL ||             avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean() ||             return {'avg_val_loss': avg_loss} ||  ||         def test_step(self; batch; batch_nb): ||             # OPTIONAL ||             x; y = batch ||             y_hat = self.forward(x) ||             return {'test_loss': F.cross_entropy(y_hat; y)} ||  ||         def test_end(self; outputs): ||             # OPTIONAL ||             avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean() ||             return {'avg_test_loss': avg_loss} ||  ||         def configure_optimizers(self): ||             # REQUIRED ||             return torch.optim.Adam(self.parameters(); lr=0.02) ||  ||         @pl.data_loader ||         def train_dataloader(self): ||             return DataLoader(MNIST(os.getcwd(); train=True; download=True; ||                               transform=transforms.ToTensor()); batch_size=32) ||  ||         @pl.data_loader ||         def val_dataloader(self): ||             # OPTIONAL ||             # can also return a list of val dataloaders ||             return DataLoader(MNIST(os.getcwd(); train=True; download=True; ||                               transform=transforms.ToTensor()); batch_size=32) ||  ||         @pl.data_loader ||         def test_dataloader(self): ||             # OPTIONAL ||             # can also return a list of test dataloaders ||             return DataLoader(MNIST(os.getcwd(); train=False; download=True; ||                               transform=transforms.ToTensor()); batch_size=32) ||  ||  || How do these methods fit into the broader training? || --------------------------------------------------- ||  || The LightningModule interface is on the right. Each method corresponds ||  to a part of a research project. Lightning automates everything not in blue. ||  || .. figure::  docs\/source\/_static\/images\/overview_flat.jpg ||    :align:   center ||  ||    Overview. ||  ||  || Optional Methods || ---------------- ||  || **add_model_specific_args** ||  || .. code-block:: python ||  ||     @staticmethod ||     def add_model_specific_args(parent_parser; root_dir) ||  || Lightning has a list of default argparse commands. ||  This method is your chance to add or modify commands specific to your model. ||  The `hyperparameter argument parser ||   <https:\/\/williamfalcon.github.io\/test-tube\/hyperparameter_optimization\/HyperOptArgumentParser>`_ ||  is available anywhere in your model by calling self.hparams. ||  || **Return** || An argument parser ||  || **Example** ||  || .. code-block:: python ||  ||     @staticmethod ||     def add_model_specific_args(parent_parser; root_dir): ||         parser = HyperOptArgumentParser(strategy=parent_parser.strategy; parents=[parent_parser]) ||  ||         # param overwrites ||         # parser.set_defaults(gradient_clip_val=5.0) ||  ||         # network params ||         parser.opt_list('--drop_prob'; default=0.2; options=[0.2; 0.5]; type=float; tunable=False) ||         parser.add_argument('--in_features'; default=28*28) ||         parser.add_argument('--out_features'; default=10) ||         # use 500 for CPU; 50000 for GPU to see speed difference ||         parser.add_argument('--hidden_dim'; default=50000) ||  ||         # data ||         parser.add_argument('--data_root'; default=os.path.join(root_dir; 'mnist'); type=str) ||  ||         # training params (opt) ||         parser.opt_list('--learning_rate'; default=0.001; type=float; ||                         options=[0.0001; 0.0005; 0.001; 0.005]; tunable=False) ||         parser.opt_list('--batch_size'; default=256; type=int; ||                         options=[32; 64; 128; 256]; tunable=False) ||         parser.opt_list('--optimizer_name'; default='adam'; type=str; ||                         options=['adam']; tunable=False) ||         return parser ||  || \""\""\""",https://github.com/PyTorchLightning/pytorch-lightning/commit/d71556e7a15a82dc3f47294bcbdb176f882f0c44,No
4079,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/ddp_mixin.py,d71556e7a15a82dc3f47294bcbdb176f882f0c44,"\""\""\"" || Lightning supports model training on a cluster managed by SLURM in the following cases: ||  || 1. Training on a single cpu or single GPU. || 2. Train on multiple GPUs on the same node using DataParallel or DistributedDataParallel || 3. Training across multiple GPUs on multiple different nodes via DistributedDataParallel. ||  || .. note:: A node means a machine with multiple GPUs ||  || Running grid search on a cluster || -------------------------------- ||  || To use lightning to run a hyperparameter search (grid-search or random-search) on a cluster do 4 things: ||  || (1). Define the parameters for the grid search ||  || .. code-block:: python ||  ||     from test_tube import HyperOptArgumentParser ||  ||     # subclass of argparse ||     parser = HyperOptArgumentParser(strategy='random_search') ||     parser.add_argument('--learning_rate'; default=0.002; type=float; help='the learning rate') ||  ||     # let's enable optimizing over the number of layers in the network ||     parser.opt_list('--nb_layers'; default=2; type=int; tunable=True; options=[2; 4; 8]) ||  ||     hparams = parser.parse_args() ||  || .. note:: You must set `Tunable=True` for that argument to be considered in the permutation set. ||  Otherwise test-tube will use the default value. This flag is useful when you don't want ||  to search over an argument and want to use the default instead. ||  || (2). Define the cluster options in the ||  `SlurmCluster object <https:\/\/williamfalcon.github.io\/test-tube\/hpc\/SlurmCluster>`_ (over 5 nodes and 8 gpus) ||  || .. code-block:: python ||  ||     from test_tube.hpc import SlurmCluster ||  ||     # hyperparameters is a test-tube hyper params object ||     # see https:\/\/williamfalcon.github.io\/test-tube\/hyperparameter_optimization\/HyperOptArgumentParser\/ ||     hyperparams = args.parse() ||  ||     # init cluster ||     cluster = SlurmCluster( ||         hyperparam_optimizer=hyperparams; ||         log_path='\/path\/to\/log\/results\/to'; ||         python_cmd='python3' ||     ) ||  ||     # let the cluster know where to email for a change in job status (ie: complete; fail; etc...) ||     cluster.notify_job_status(email='some@email.com'; on_done=True; on_fail=True) ||  ||     # set the job options. In this instance; we'll run 20 different models ||     # each with its own set of hyperparameters giving each one 1 GPU (ie: taking up 20 GPUs) ||     cluster.per_experiment_nb_gpus = 8 ||     cluster.per_experiment_nb_nodes = 5 ||  ||     # we'll request 10GB of memory per node ||     cluster.memory_mb_per_node = 10000 ||  ||     # set a walltime of 10 minues ||     cluster.job_time = '10:00' ||  ||  || (3). Make a main function with your model and trainer. Each job will call this function with a particular || hparams configuration.:: ||  ||     from pytorch_lightning import Trainer ||  ||     def train_fx(trial_hparams; cluster_manager; _): ||         # hparams has a specific set of hyperparams ||  ||         my_model = MyLightningModel() ||  ||         # give the trainer the cluster object ||         trainer = Trainer() ||         trainer.fit(my_model) ||  ||     ` ||  || (4). Start the grid\/random search:: ||  ||     # run the models on the cluster ||     cluster.optimize_parallel_cluster_gpu( ||         train_fx; ||         nb_trials=20; ||         job_name='my_grid_search_exp_name'; ||         job_display_name='my_exp') ||  || .. note:: `nb_trials` specifies how many of the possible permutations to use. If using `grid_search` it will use ||  the depth first ordering. If using `random_search` it will use the first k shuffled options. FYI; random search ||  has been shown to be just as good as any Bayesian optimization method when using a reasonable number of samples (60); ||  see this `paper <http:\/\/www.jmlr.org\/papers\/volume13\/bergstra12a\/bergstra12a.pdf>`_  for more information. ||  || Walltime auto-resubmit || ---------------------- ||  || Lightning automatically resubmits jobs when they reach the walltime. Make sure to set the SIGUSR1 signal in || your SLURM script.:: ||  ||     # 90 seconds before training ends ||     #SBATCH --signal=SIGUSR1@90 ||  || When lightning receives the SIGUSR1 signal it will: || 1. save a checkpoint with 'hpc_ckpt' in the name. || 2. resubmit the job using the SLURM_JOB_ID ||  || When the script starts again; Lightning will: || 1. search for a 'hpc_ckpt' checkpoint. || 2. restore the model; optimizers; schedulers; epoch; etc... ||  || \""\""\""",https://github.com/PyTorchLightning/pytorch-lightning/commit/d71556e7a15a82dc3f47294bcbdb176f882f0c44,No
4080,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/trainer_io.py,d71556e7a15a82dc3f47294bcbdb176f882f0c44,"\""\""\"" || Lightning can automate saving and loading checkpoints || ===================================================== ||  || Checkpointing is enabled by default to the current working directory. || To change the checkpoint path pass in:: ||  ||     Trainer(default_save_path='\/your\/path\/to\/save\/checkpoints') ||  ||  || To modify the behavior of checkpointing pass in your own callback. ||  || .. code-block:: python ||  ||     from pytorch_lightning.callbacks import ModelCheckpoint ||  ||     # DEFAULTS used by the Trainer ||     checkpoint_callback = ModelCheckpoint( ||         filepath=os.getcwd(); ||         save_best_only=True; ||         verbose=True; ||         monitor='val_loss'; ||         mode='min'; ||         prefix='' ||     ) ||  ||     trainer = Trainer(checkpoint_callback=checkpoint_callback) ||  ||  || Restoring training session || -------------------------- ||  || You might want to not only load a model but also continue training it. Use this method to || restore the trainer state as well. This will continue from the epoch and global step you last left off. || However; the dataloaders will start from the first batch again (if you shuffled it shouldn't matter). ||  || Lightning will restore the session if you pass a logger with the same version and there's a saved checkpoint. ||  || .. code-block:: python ||  ||     from pytorch_lightning import Trainer ||     from pytorch_lightning.logging import TestTubeLogger ||  ||     logger = TestTubeLogger( ||         save_dir='.\/savepath'; ||         version=1  # An existing version with a saved checkpoint ||     ) ||     trainer = Trainer( ||         logger=logger; ||         default_save_path='.\/savepath' ||     ) ||  ||     # this fit call loads model weights and trainer state ||     # the trainer continues seamlessly from where you left off ||     # without having to do anything else. ||     trainer.fit(model) ||  ||  || The trainer restores: ||  || - global_step || - current_epoch || - All optimizers || - All lr_schedulers || - Model weights ||  || You can even change the logic of your model as long as the weights and \""architecture\"" of || the system isn't different. If you add a layer; for instance; it might not work. ||  || At a rough level; here's what happens inside Trainer :py:mod:`pytorch_lightning.base_module.model_saving.py`: ||  || .. code-block:: python ||  ||     self.global_step = checkpoint['global_step'] ||     self.current_epoch = checkpoint['epoch'] ||  ||     # restore the optimizers ||     optimizer_states = checkpoint['optimizer_states'] ||     for optimizer; opt_state in zip(self.optimizers; optimizer_states): ||         optimizer.load_state_dict(opt_state) ||  ||     # restore the lr schedulers ||     lr_schedulers = checkpoint['lr_schedulers'] ||     for scheduler; lrs_state in zip(self.lr_schedulers; lr_schedulers): ||         scheduler.load_state_dict(lrs_state) ||  ||     # uses the model you passed into trainer ||     model.load_state_dict(checkpoint['state_dict']) ||  || \""\""\""",https://github.com/PyTorchLightning/pytorch-lightning/commit/d71556e7a15a82dc3f47294bcbdb176f882f0c44,No
4081,PyTorchLightning/pytorch-lightning,pytorch_lightning/utilities/debugging.py,d71556e7a15a82dc3f47294bcbdb176f882f0c44,"\""\""\"" || These flags are useful to help debug a model. ||  || Fast dev run || ------------ ||  || This flag is meant for debugging a full train\/val\/test loop. ||  It'll activate callbacks; everything but only with 1 training and 1 validation batch. ||  Use this to debug a full run of your program quickly ||  || .. code-block:: python ||  ||     # DEFAULT ||     trainer = Trainer(fast_dev_run=False) ||  ||  || Inspect gradient norms || ---------------------- ||  || Looking at grad norms can help you figure out where training might be going wrong. ||  || .. code-block:: python ||  ||     # DEFAULT (-1 doesn't track norms) ||     trainer = Trainer(track_grad_norm=-1) ||  ||     # track the LP norm (P=2 here) ||     trainer = Trainer(track_grad_norm=2) ||  ||  || Make model overfit on subset of data || ------------------------------------ ||  || A useful debugging trick is to make your model overfit a tiny fraction of the data. ||  || setting `overfit_pct > 0` will overwrite train_percent_check; val_percent_check; test_percent_check ||  || .. code-block:: python ||  ||     # DEFAULT don't overfit (ie: normal training) ||     trainer = Trainer(overfit_pct=0.0) ||  ||     # overfit on 1% of data ||     trainer = Trainer(overfit_pct=0.01) ||  ||  || Print the parameter count by layer || ---------------------------------- ||  || By default lightning prints a list of parameters *and submodules* when it starts training. ||  || .. code-block:: python ||  ||     # DEFAULT print a full list of all submodules and their parameters. ||     trainer = Trainer(weights_summary='full') ||  ||     # only print the top-level modules (i.e. the children of LightningModule). ||     trainer = Trainer(weights_summary='top') ||  || Print which gradients are nan || ----------------------------- ||  || This option prints a list of tensors with nan gradients:: ||  ||     # DEFAULT ||     trainer = Trainer(print_nan_grads=False) ||  || Log GPU usage || ------------- ||  || Lightning automatically logs gpu usage to the test tube logs. ||  It'll only do it at the metric logging interval; so it doesn't slow down training. ||  || \""\""\""",https://github.com/PyTorchLightning/pytorch-lightning/commit/d71556e7a15a82dc3f47294bcbdb176f882f0c44,Yes
4082,PyTorchLightning/pytorch-lightning,tests/test_trainer.py,62f6f92fdf6343e17336f0a28247ff95d35bac3a,TODO split this up into multiple tests,https://github.com/PyTorchLightning/pytorch-lightning/commit/62f6f92fdf6343e17336f0a28247ff95d35bac3a,Yes
4083,PyTorchLightning/pytorch-lightning,tests/test_trainer.py,63717e8fdaa6d2cda8a5d0ca1afe6f3ab82ab117,TODO split this up into multiple tests,https://github.com/PyTorchLightning/pytorch-lightning/commit/63717e8fdaa6d2cda8a5d0ca1afe6f3ab82ab117,Yes
4084,PyTorchLightning/pytorch-lightning,pytorch_lightning/logging/tensorboard.py,5d00e62047b73bbad51c89c01b9bb91508f0391d,in case converting from namespace; todo: rather test if it is namespace,https://github.com/PyTorchLightning/pytorch-lightning/commit/5d00e62047b73bbad51c89c01b9bb91508f0391d,Yes
4085,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/__init__.py,bc67689068a0db11adaf10b32a41bcd33b8ae88e,"\""\""\"" ||  || The trainer de-couples the engineering code (16-bit; early stopping; GPU distribution; etc...) from the || science code (GAN; BERT; your project; etc...). It uses many assumptions which are best practices in || AI research today. ||  || The trainer automates all parts of training except: ||  || - what happens in training ; test; val loop || - where the data come from || - which optimizers to use || - how to do the computations ||  || The Trainer delegates those calls to your LightningModule which defines how to do those parts. ||  || This is the basic use of the trainer: ||  || .. code-block:: python ||  ||     from pytorch_lightning import Trainer ||  ||     model = MyLightningModule() ||  ||     trainer = Trainer() ||     trainer.fit(model) || \""\""\""",https://github.com/PyTorchLightning/pytorch-lightning/commit/bc67689068a0db11adaf10b32a41bcd33b8ae88e,Yes
4086,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/trainer.py,2a04be038624bd8867b01b4b7bfe967b010fb941,TODO: remove self.testing condition because model.summarize() is wiping out the weights,https://github.com/PyTorchLightning/pytorch-lightning/commit/2a04be038624bd8867b01b4b7bfe967b010fb941,Yes
4087,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/training_io.py,969e929a482b3384c0c23777389db866bcc5c5ed,"\""\""\"" || Lightning can automate saving and loading checkpoints || ===================================================== ||  || Checkpointing is enabled by default to the current working directory. || To change the checkpoint path pass in:: ||  ||     Trainer(default_save_path='\/your\/path\/to\/save\/checkpoints') ||  ||  || To modify the behavior of checkpointing pass in your own callback. ||  || .. code-block:: python ||  ||     from pytorch_lightning.callbacks import ModelCheckpoint ||  ||     # DEFAULTS used by the Trainer ||     checkpoint_callback = ModelCheckpoint( ||         filepath=os.getcwd(); ||         save_best_only=True; ||         verbose=True; ||         monitor='val_loss'; ||         mode='min'; ||         prefix='' ||     ) ||  ||     trainer = Trainer(checkpoint_callback=checkpoint_callback) ||  ||  || Restoring training session || -------------------------- ||  || You might want to not only load a model but also continue training it. Use this method to || restore the trainer state as well. This will continue from the epoch and global step you last left off. || However; the dataloaders will start from the first batch again (if you shuffled it shouldn't matter). ||  || Lightning will restore the session if you pass a logger with the same version and there's a saved checkpoint. ||  || .. code-block:: python ||  ||     from pytorch_lightning import Trainer ||     from pytorch_lightning.loggers import TestTubeLogger ||  ||     logger = TestTubeLogger( ||         save_dir='.\/savepath'; ||         version=1  # An existing version with a saved checkpoint ||     ) ||     trainer = Trainer( ||         logger=logger; ||         default_save_path='.\/savepath' ||     ) ||  ||     # this fit call loads model weights and trainer state ||     # the trainer continues seamlessly from where you left off ||     # without having to do anything else. ||     trainer.fit(model) ||  ||  || The trainer restores: ||  || - global_step || - current_epoch || - All optimizers || - All lr_schedulers || - Model weights ||  || You can even change the logic of your model as long as the weights and \""architecture\"" of || the system isn't different. If you add a layer; for instance; it might not work. ||  || At a rough level; here's what happens inside Trainer :py:mod:`pytorch_lightning.base_module.model_saving.py`: ||  || .. code-block:: python ||  ||     self.global_step = checkpoint['global_step'] ||     self.current_epoch = checkpoint['epoch'] ||  ||     # restore the optimizers ||     optimizer_states = checkpoint['optimizer_states'] ||     for optimizer; opt_state in zip(self.optimizers; optimizer_states): ||         optimizer.load_state_dict(opt_state) ||  ||     # restore the lr schedulers ||     lr_schedulers = checkpoint['lr_schedulers'] ||     for scheduler; lrs_state in zip(self.lr_schedulers; lr_schedulers): ||         scheduler['scheduler'].load_state_dict(lrs_state) ||  ||     # uses the model you passed into trainer ||     model.load_state_dict(checkpoint['state_dict']) ||  || \""\""\""",https://github.com/PyTorchLightning/pytorch-lightning/commit/969e929a482b3384c0c23777389db866bcc5c5ed,Yes
4088,PyTorchLightning/pytorch-lightning,tests/models/data/horovod/train_default_model.py,7024177f7d4163e2af57c7dfa5550585748cbc0d,"\""\""\"" || This script is meant to be executed from `..\/..\/test_horovod.py`. ||  || Because Horovod uses a parallel programming model similar to MPI; unit tests for collective || ops like allreduce need to be run in parallel. The most common approach for running parallel || Horovod workers is to launch multiple replicas of the training script via the `horovodrun` || command-line tool: ||  || .. code-block:: bash ||  ||     horovodrun -np 2 python train_default_model.py ... ||  || Individual test parameters are configured by the serialized `--trainer-options` JSON object. ||  || An non-zero exit code from this script on any rank will indicate failure; while a zero exit code || across all ranks indicates success. || \""\""\""",https://github.com/PyTorchLightning/pytorch-lightning/commit/7024177f7d4163e2af57c7dfa5550585748cbc0d,Yes
4089,PyTorchLightning/pytorch-lightning,tests/trainer/test_trainer.py,6d58fb1353b04b85beb4c71b61e8b317bc456276,todo: check duplicated tests against trainer_checks,https://github.com/PyTorchLightning/pytorch-lightning/commit/6d58fb1353b04b85beb4c71b61e8b317bc456276,Yes
4090,PyTorchLightning/pytorch-lightning,tests/trainer/test_trainer_cli.py,bee0392c372936567b2bbe6e7ed5828cb3078354,"todo: add also testing for \""gpus\""",https://github.com/PyTorchLightning/pytorch-lightning/commit/bee0392c372936567b2bbe6e7ed5828cb3078354,Yes
4091,PyTorchLightning/pytorch-lightning,tests/metrics/functional/test_classification.py,3436d00230000ebdf33cc87ea5c62b2e31dbc5e9,TODO: move back the pred and target to test func arguments,https://github.com/PyTorchLightning/pytorch-lightning/commit/3436d00230000ebdf33cc87ea5c62b2e31dbc5e9,No
4092,PyTorchLightning/pytorch-lightning,tests/trainer/test_trainer_tricks.py,04c794ca72f45dd492e49baff773c2b8e274b9ed,test limit_xxx_batches as percent AND int,https://github.com/PyTorchLightning/pytorch-lightning/commit/04c794ca72f45dd492e49baff773c2b8e274b9ed,Yes
4093,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/trainer.py,25ee51bc570503f331dceecc610d0eb355e22327,TODO refactor codebase (tests) to not directly reach into these callbacks,https://github.com/PyTorchLightning/pytorch-lightning/commit/25ee51bc570503f331dceecc610d0eb355e22327,Yes
4094,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/callback_connector.py,deb82d9c086b339925467e8a9ee2fdced5691613,TODO refactor codebase (tests) to not directly reach into these callbacks,https://github.com/PyTorchLightning/pytorch-lightning/commit/deb82d9c086b339925467e8a9ee2fdced5691613,Yes
4095,PyTorchLightning/pytorch-lightning,tests/metrics/test_aggregation.py,fe290280be18a689a53bdf56419a76987ed6aeed,TODO: update remaining metrics and uncomment the corresponding test cases,https://github.com/PyTorchLightning/pytorch-lightning/commit/fe290280be18a689a53bdf56419a76987ed6aeed,No
4096,PyTorchLightning/pytorch-lightning,tests/trainer/optimization/test_backward_calls.py,0ec410769744843726a140b7efabbeeb1c4e2929,TODO: this test fails on master; max_steps seems to fail together with accumulation,https://github.com/PyTorchLightning/pytorch-lightning/commit/0ec410769744843726a140b7efabbeeb1c4e2929,No
4097,PyTorchLightning/pytorch-lightning,tests/metrics/regression/test_ssim.py,a93739431299871f29d8014a343be9ad10162377,TODO: ideally tests should pass with lower tolerance,https://github.com/PyTorchLightning/pytorch-lightning/commit/a93739431299871f29d8014a343be9ad10162377,Yes
4098,PyTorchLightning/pytorch-lightning,tests/trainer/legacy_deprecate_flow_log_tests/test_trainer_steps_scalar_return.py,4a01fd048cebb65405021d3f24ae4dc07cb735e6,TODO: Move this test to DDP. pbar_added_metrics is empty with ddp_spawn for some reasons,https://github.com/PyTorchLightning/pytorch-lightning/commit/4a01fd048cebb65405021d3f24ae4dc07cb735e6,Yes
4099,PyTorchLightning/pytorch-lightning,tests/trainer/test_trainer.py,ce9179591dc3414aa0ff59007ea46f30fa9ddbee,Todo: mock nb Gpus so all these tests can run on any device,https://github.com/PyTorchLightning/pytorch-lightning/commit/ce9179591dc3414aa0ff59007ea46f30fa9ddbee,Yes
4100,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/deprecated_api.py,7449ce216de8786b4370e1a39faa84b96cd6de42,todo: consider rename as `is_testing`,https://github.com/PyTorchLightning/pytorch-lightning/commit/7449ce216de8786b4370e1a39faa84b96cd6de42,Yes
4101,PyTorchLightning/pytorch-lightning,tests/core/test_results.py,29bcf309848b9c5ff95ab0d6dd0f12b7ae28f9b4,TODO: add end-to-end test,https://github.com/PyTorchLightning/pytorch-lightning/commit/29bcf309848b9c5ff95ab0d6dd0f12b7ae28f9b4,Yes
4102,PyTorchLightning/pytorch-lightning,tests/accelerators/legacy/__init__.py,7e2e874d9507e6b6f7b96dcf7f44406640381d74,"todo: feel free to move any of these \""legacy\"" tests up...",https://github.com/PyTorchLightning/pytorch-lightning/commit/7e2e874d9507e6b6f7b96dcf7f44406640381d74,Yes
4103,PyTorchLightning/pytorch-lightning,tests/plugins/legacy/__init__.py,7e2e874d9507e6b6f7b96dcf7f44406640381d74,"todo: feel free to move any of these \""legacy\"" tests up...",https://github.com/PyTorchLightning/pytorch-lightning/commit/7e2e874d9507e6b6f7b96dcf7f44406640381d74,Yes
4104,PyTorchLightning/pytorch-lightning,tests/base/develop_pipelines.py,82943515dc60e00d8e62eafc2a5cf52891381c63,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/82943515dc60e00d8e62eafc2a5cf52891381c63,Yes
4105,PyTorchLightning/pytorch-lightning,tests/helpers/pipelines.py,bd920b4102083635be49ed21cc354bfa1a274869,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/bd920b4102083635be49ed21cc354bfa1a274869,Yes
4106,PyTorchLightning/pytorch-lightning,tests/helpers/pipelines.py,a0f7831278bb8d40d1b23331537ffb7ec653ae80,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/a0f7831278bb8d40d1b23331537ffb7ec653ae80,Yes
4107,PyTorchLightning/pytorch-lightning,tests/helpers/pipelines.py,8e9a026bc34d8409faa572a7144c2d96a7c039ed,on hpc this would work fine... but need to hack it for the purpose of the test,https://github.com/PyTorchLightning/pytorch-lightning/commit/8e9a026bc34d8409faa572a7144c2d96a7c039ed,Yes
4108,PyTorchLightning/pytorch-lightning,tests/helpers/pipelines.py,8e9a026bc34d8409faa572a7144c2d96a7c039ed,TODO: This test compares a loss value with a min accuracy - complete non-sense!,https://github.com/PyTorchLightning/pytorch-lightning/commit/8e9a026bc34d8409faa572a7144c2d96a7c039ed,Yes
4109,PyTorchLightning/pytorch-lightning,tests/callbacks/test_callbacks.py,f4cc7451a94010a572480c43ad5f0af7ad52cd21,TODO: add callback tests for predict and tune,https://github.com/PyTorchLightning/pytorch-lightning/commit/f4cc7451a94010a572480c43ad5f0af7ad52cd21,Yes
4110,proycon/pynlpl,tests/search.py,153ebc4c786642075faeb527a6247059fb6be92c,TODO: this is not a test!,https://github.com/proycon/pynlpl/commit/153ebc4c786642075faeb527a6247059fb6be92c,No
4111,proycon/pynlpl,clients/cornetto.py,c16dd550369505d18e2a1531fb5fd564d29c5c38,"\""\""\"" || -------------------------------------------------------------------------------- || Original Author:     Fons Laan; ILPS-ISLA; University of Amsterdam || Original Project:    DutchSemCor || Original Name:        cornettodb\/views.py || Original Version:    0.2 || Goal:        Cornetto views definitions ||  || Original functions: ||     index( request ) ||     local_open() ||     remote_open( self.debug ) ||     search( request ) ||     search_local( dict_in; search_query ) ||     search_remote( dict_in; search_query ) ||     cornet_check_lusyn( utf8_lemma ) ||     query_remote_lusyn_id( syn_id_self.debug; http; utf8_lemma; syn_id ) ||     query_cornet( keyword; category ) ||     query_remote_syn_lemma( self.debug; http; utf8_lemma ) ||     query_remote_syn_id( self.debug; http; utf8_lemma; syn_id; domains_abbrev ) ||     query_remote_lu_lemma( self.debug; http; utf8_lemma ) ||     query_remote_lu_id( self.debug; http; lu_id ) || FL-04-Sep-2009: Created || FL-03-Nov-2009: Removed http global: sometimes it was None; missed initialization? || FL-01-Feb-2010: Added Category filtering || FL-15-Feb-2010: Tag counts -> separate qx query || FL-07-Apr-2010: Merge canonical + textual examples || FL-10-Jun-2010: Latest Change || MvG-29-Sep-2010: Turned into minimal CornettoClient class; some new functions added; many old ones disabled until necessary || \""\""\""",https://github.com/proycon/pynlpl/commit/c16dd550369505d18e2a1531fb5fd564d29c5c38,Yes
4112,proycon/pynlpl,search.py,35c04eb0d90469acf8c3b0c8519857ba75d56760,TODO: TEST,https://github.com/proycon/pynlpl/commit/35c04eb0d90469acf8c3b0c8519857ba75d56760,No
4113,proycon/pynlpl,tests/folia.py,7afe90865fecac390804f035c6e9adebdc54c211,TODO: THIS TEST FAILS; SHOULD BE FIXED; TEST MAY CHANGE,https://github.com/proycon/pynlpl/commit/7afe90865fecac390804f035c6e9adebdc54c211,Yes
4114,proycon/pynlpl,tests/folia.py,7afe90865fecac390804f035c6e9adebdc54c211,TODO: THIS TEST FAILS NOW; BUT SHOULDN'T,https://github.com/proycon/pynlpl/commit/7afe90865fecac390804f035c6e9adebdc54c211,Yes
4115,PythonOT/POT,ot/da.py,bd7c7d2534980d3105d060dd24a444433422134d,XXX: should we rather test if instance of estimator?,https://github.com/PythonOT/POT/commit/bd7c7d2534980d3105d060dd24a444433422134d,No
4116,PythonOT/POT,ot/utils.py,55840f6bccadd79caf722d86f06da857e3045453,XXX: should we rather test if instance of estimator?,https://github.com/PythonOT/POT/commit/55840f6bccadd79caf722d86f06da857e3045453,No
4117,PythonOT/POT,ot/da.py,5ab50354e60ed94d9d799927fd4b680fb8447304,XXX: should we rather test if instance of estimator?,https://github.com/PythonOT/POT/commit/5ab50354e60ed94d9d799927fd4b680fb8447304,No
4118,ljvmiranda921/pyswarms,setup.py,1a222717ab6fb05711173486e09d3404771e579a,TODO: put package test requirements here,https://github.com/ljvmiranda921/pyswarms/commit/1a222717ab6fb05711173486e09d3404771e579a,No
4119,flennerhag/mlens,mlens/ensemble/ensemble.py,0ec8f8829056a9dc7a4ec2b87a71c22238c50737,TODO: add option to store base estimator cv test scores during fit,https://github.com/flennerhag/mlens/commit/0ec8f8829056a9dc7a4ec2b87a71c22238c50737,No
4120,flennerhag/mlens,mlens/test/test_simple.py,0921843b5c9cece580856fb714b5238b8b394add,"\""\""\"" || author: Sebastian Flennerhag || date: 10\/01\/2017 || An initial test suite for ML-Ensemble. The test covers the Ensemble class; || both as integrated with grid search and stand-alone; and the Evaluator class. || The test relies on fitting a dummy problem using a fixed seed and ensuring || that the optimized estimator \/ ensemble gives the right score. Since the || problem is deterministic; if the ensemble finds another score; the || learning algorithm has changed. || \""\""\""",https://github.com/flennerhag/mlens/commit/0921843b5c9cece580856fb714b5238b8b394add,No
4121,flennerhag/mlens,mlens/tests/test_simple.py,8b738347595d55096966b9847ee76b80784c6f50,"\""\""\"" || author: Sebastian Flennerhag || date: 10\/01\/2017 || An initial test suite for ML-Ensemble. The test covers the Ensemble class; || both as integrated with grid search and stand-alone; and the Evaluator class. || The test relies on fitting a dummy problem using a fixed seed and ensuring || that the optimized estimator \/ ensemble gives the right score. Since the || problem is deterministic; if the ensemble finds another score; the || learning algorithm has changed. || \""\""\""",https://github.com/flennerhag/mlens/commit/8b738347595d55096966b9847ee76b80784c6f50,No
4122,flennerhag/mlens,mlens/utils/checks.py,7b759f67914e67ee5fd3be53e98fcdff33e1e166,"\""\""\""ML-ENSEMBLE ||  || author: Sebastian Flennerhag || date: 19\/02\/2017 || licence: MIT || Suite for checking valid estimation and informative error traceback. || This is a light version of the Scikit-learn test suite; to pre-check || conditions for estimation before making estimator function calls in parallel || jobs. || \""\""\""",https://github.com/flennerhag/mlens/commit/7b759f67914e67ee5fd3be53e98fcdff33e1e166,Yes
4123,flennerhag/mlens,mlens/externals/base.py,156a5557ae2d57fc77f3e752424ddc4446683544,XXX: should we rather test if instance of estimator?,https://github.com/flennerhag/mlens/commit/156a5557ae2d57fc77f3e752424ddc4446683544,No
4124,flennerhag/mlens,benchmarks/mnist.py,e6eb78d2ef5a5beade18dbf238130510adb13ef7,"\""\""\"" || ======================= || MNIST dataset benchmark || ======================= ||  || Benchmark on the MNIST dataset.  The dataset comprises 70;000 samples || and 784 features. Here; we consider the task of predicting || 10 classes -  digits from 0 to 9 from their raw images. By contrast to the || covertype dataset; the feature space is homogenous. ||  || Example of output : ||     [..] ||  ||     Classification performance: ||     =========================== ||     Classifier               train-time   test-time   error-rate ||     ------------------------------------------------------------ ||     Subsemble                   343.31s       3.17s       0.0210 ||     MLP_adam                     53.46s       0.11s       0.0224 ||     Nystroem-SVM                112.97s       0.92s       0.0228 ||     MultilayerPerceptron         24.33s       0.14s       0.0287 ||     ExtraTrees                   42.99s       0.57s       0.0294 ||     RandomForest                 42.70s       0.49s       0.0318 ||     SampledRBF-SVM              135.81s       0.56s       0.0486 ||     LinearRegression-SAG         16.67s       0.06s       0.0824 ||     CART                         20.69s       0.02s       0.1219 ||     dummy                         0.00s       0.01s       0.8973 || \""\""\""",https://github.com/flennerhag/mlens/commit/e6eb78d2ef5a5beade18dbf238130510adb13ef7,No
4125,flennerhag/mlens,mlens/parallel/tests/test_a_base_functions.py,e605f734f5cb182ef8fdc4a55daa28e945ec1e11,TODO: Write tests,https://github.com/flennerhag/mlens/commit/e605f734f5cb182ef8fdc4a55daa28e945ec1e11,Yes
4126,annoviko/pyclustering,nnet/plsom/tests.py,c862d06119afffa4bf6b5bccfcc35c9b6dfa5c8f,TODO: This test is fail. Is this test applicable for PLSOM?,https://github.com/annoviko/pyclustering/commit/c862d06119afffa4bf6b5bccfcc35c9b6dfa5c8f,No
4127,annoviko/pyclustering,docs/breathe/renderer/rst/doxygen/filter.py,8bd0a906bafa55b45547a51f40facb67e84a3276,"\""\""\"" || Filters || ------- ||  || Filters are an interesting and somewhat challenging part of the code base. They are used for || two different purposes: ||  ||  - To figure out which nodes in the xml hierarchy to start rendering from. These are called ||    'finder filters' or 'content filters'. This is done before rendering starts. ||  - To figure out which nodes under a selected nodes in the xml hierarchy should be rendered. These ||    are called 'render filters'. This is done during the render process with a test in the ||    DoxygenToRstRendererFactory. ||  ||  || General Implementation || ~~~~~~~~~~~~~~~~~~~~~~ ||  || Filters are essential just tests to see if a node matches certain parameters that are needed to || decide whether or not to include it in some output. ||  || As these filters are declared once and then used on multiple nodes; we model them as object || hierarchies that encapsulate the required test and take a node (with its context) and return True or || False. ||  || If you wanted a test which figures out if a node has the node_type 'memberdef' you might create the || following object hierarchy: ||  ||     node_is_memberdef = InFilter(AttributeAccessor(Node(); 'node_type'); ['memberdef']) ||  || This reads from the inside out; as get the node; then get the node_type attribute from it; and see || if the value of the attribute is in the list ['memberdef']. ||  || The Node() is called a 'Selector'. Parent() is also a selector. It means given the current context; || work with the parent of the current node rather than the node itself. This allows you to frame tests || in terms of a node's parent as well as the node which helps when we want nodes with particular || parents and not others. ||  || The AttributeAccessor() is called an 'Accessor'. It wraps up an attempt to access a particular || attribute on the selected node. There are quite a few different specific accessors but they can || mostly be generalised with the AttributeAccessor. This code has evolved over time and initially the || implementation involved specific accessor classes (which are still used in large parts of it.) ||  || The InFilter() is unsurprisingly called a 'Filter'. There are lots of different filters. Filters || either act on the results of Accessors or on the results of other Filters and they always return || True or False. The AndFilter and the OrFilter can be used to combine the outputs of other Filters || with logical 'and' and 'or' operations. ||  || You can build up some pretty complex expressions with this level of freedom as you || might imagine. The complexity is unfortunate but necessary as the nature of filtering the xml is || quite complex. ||  ||  || Finder Filters || ~~~~~~~~~~~~~~ ||  || The implementation of the filters can change a little depending on how they are called. Finder || filters are called from the breathe.finder.doxygen.index and breathe.finder.doxygen.compound files. || They are called like this: ||  ||     # Descend down the hierarchy ||     # ... ||  ||     if filter_.allow(node_stack): ||         matches.append(self.data_object) ||  ||     # Keep on descending ||     # ... ||  || This means that the result of the filter does not stop us descending down the hierarchy and testing || more nodes. This simplifies the filters as they only have to return true for the exact nodes they || are interested in and they don't have to worry about allowing the iteration down the hierarchy to || continue for nodes which don't match. ||  || An example of a finder filter is: ||  ||     AndFilter( ||         InFilter(NodeTypeAccessor(Node()); [\""compound\""]); ||         InFilter(KindAccessor(Node()); [\""group\""]); ||         InFilter(NameAccessor(Node()); [\""mygroup\""]) ||         ) ||  || This says; return True for all the nodes of node_type 'compound' with 'kind' set to 'group' which || have the name 'mygroup'. It returns false for everything else; but when a node matching this is || found then it is added to the matches list by the code above. ||  || It is therefore relatively easy to write finder filters. If you have two separate node filters like || the one above and you want to match on both of them then you can do: ||  ||     OrFilter( ||         node_filter_1; ||         node_filter_2 ||         ) ||  || To combine them. ||  ||  || Content Filters || ~~~~~~~~~~~~~~~ ||  || Content filters are harder than the finder filters as they are responsible for halting the iteration || down the hierarchy if they return false. This means that if you're interested in memberdef nodes || with a particular attribute then you have to check for that but also include a clause which allows || all other non-memberdef nodes to pass through as you don't want to interrupt them. ||  || This means you end up with filters like this: ||  ||     OrFilter( ||         AndFilter( ||             InFilter(NodeTypeAccessor(Node()); [\""compound\""]); ||             InFilter(KindAccessor(Node()); [\""group\""]); ||             InFilter(NameAccessor(Node()); [\""mygroup\""]) ||             ); ||         NotFilter( ||             AndFilter( ||                 InFilter(NodeTypeAccessor(Node()); [\""compound\""]); ||                 InFilter(KindAccessor(Node()); [\""group\""]); ||                 ) ||             ) ||         ) ||  || Which is to say that we want to let through a compound; with kind group; with name 'mygroup' but || we're also happy if the node is **not** a compund with kind group. Really we just don't want to let || through any compounds with kind group with name other than 'mygroup'. As such; we can rephrase this || as: ||  ||     NotFilter( ||         AndFilter( ||             InFilter(NodeTypeAccessor(Node()); [\""compound\""]); ||             InFilter(KindAccessor(Node()); [\""group\""]); ||             NotFilter(InFilter(NameAccessor(Node()); [\""mygroup\""])) ||             ) ||         ) ||  || Using logical manipulation we can rewrite this as: ||  ||     OrFilter( ||         NotFilter(InFilter(NodeTypeAccessor(Node()); [\""compound\""])); ||         NotFilter(InFilter(KindAccessor(Node()); [\""group\""])); ||         InFilter(NameAccessor(Node()); [\""mygroup\""]) ||         ) ||  || We reads: allow if it isn't a compound; or if it is a compound but doesn't have a 'kind' of 'group'; || but if it is a compound and has a 'kind' of 'group then only allow it if it is named 'mygroup'. ||  ||  || Helper Syntax || ~~~~~~~~~~~~~ ||  || Some of these filter declarations get a little awkward to read and write. They are not laid out in || manner which reads smoothly. Additional helper methods and operator overloads have been introduced || to help with this. ||  || AttributeAccessor objects are created in property methods on the Selector classes so: ||  ||     node.kind ||  || Where node has been declared as a Node() instance. Results in: ||  ||     AttributeAccessor(Node(); 'kind') ||  || The '==' and '!=' operators on the Accessors have been overloaded to return the appropriate filters || so that: ||  ||     node.kind == 'group' ||  || Results in: ||  ||     InFilter(AttributeAccessor(Node(); 'kind'); ['kind']) ||  || We also override the binary 'and' (&); 'or' (|) and 'not' (~) operators in Python to apply || AndFilters; OrFilters and NotFilters respectively. We have to override the binary operators as they || actual 'and'; 'or' and 'not' operators cannot be overridden. So: ||  ||     (node.node_type == 'compound') & (node.name == 'mygroup') ||  || Translates to: ||  ||     AndFilter( ||         InFilter(NodeTypeAccessor(Node()); [\""compound\""])); ||         InFilter(NameAccessor(Node()); [\""mygroup\""]) ||         ) ||  || Where the former is hopefully more readable without sacrificing too much to the abstract magic of || operator overloads. ||  ||  || Operator Precedences & Extra Parenthesis || '''''''''''''''''''''''''''''''''''''''' ||  || As the binary operators have a lower operator precedence than '==' and '!=' and some other operators || we have to include additional parenthesis in the expressions to group them as we want. So instead of || writing: ||  ||     node.node_type == 'compound' & node.name == 'mygroup' ||  || We have to write: ||  ||     (node.node_type == 'compound') & (node.name == 'mygroup') ||  || \""\""\""",https://github.com/annoviko/pyclustering/commit/8bd0a906bafa55b45547a51f40facb67e84a3276,No
4128,annoviko/pyclustering,docs/breathe/renderer/rst/doxygen/filter.py,8bd0a906bafa55b45547a51f40facb67e84a3276,I can't fully explain the filtering of descriptions here. More testing needed to figure,https://github.com/annoviko/pyclustering/commit/8bd0a906bafa55b45547a51f40facb67e84a3276,Yes
4129,annoviko/pyclustering,pyclustering/tests/tests_runner.py,941fee703090fd3b1d2de82c1b74e959c3b4c300,Add path to pyclustering package (much better to set PYTHONPATH); but just to be sure that at least unit-tests can be run.,https://github.com/annoviko/pyclustering/commit/941fee703090fd3b1d2de82c1b74e959c3b4c300,Yes
4130,kengz/SLM-Lab,unity_lab/lib/util.py,dbd416214da3049cd1ba80649f07b973d070bb28,TODO unit tests,https://github.com/kengz/SLM-Lab/commit/dbd416214da3049cd1ba80649f07b973d070bb28,Yes
4131,kengz/SLM-Lab,test/test_memory.py,7740b54028a8ebef504ea4e554446d35e0d3994b,TODO: test_sample_dist,https://github.com/kengz/SLM-Lab/commit/7740b54028a8ebef504ea4e554446d35e0d3994b,Yes
4132,kengz/SLM-Lab,test/test_memory.py,7740b54028a8ebef504ea4e554446d35e0d3994b,TODO: implement test_update_priorities,https://github.com/kengz/SLM-Lab/commit/7740b54028a8ebef504ea4e554446d35e0d3994b,Yes
4133,kengz/SLM-Lab,test/experiment/test_control.py,f70c83bd318fd3381957f6e5e570d9ee4c362d50,TODO test control steps in detail when complete,https://github.com/kengz/SLM-Lab/commit/f70c83bd318fd3381957f6e5e570d9ee4c362d50,Yes
4134,kengz/SLM-Lab,test/conftest.py,d670e94d3f8e938858e938b1d3efeac5389a26d3,TODO properly use in tests,https://github.com/kengz/SLM-Lab/commit/d670e94d3f8e938858e938b1d3efeac5389a26d3,Yes
4135,kengz/SLM-Lab,slm_lab/agent/memory/replay.py,8c17debc9dc6552da8c317c0c852b55f03c80c61,TODO if latest; return unused. implement,https://github.com/kengz/SLM-Lab/commit/8c17debc9dc6552da8c317c0c852b55f03c80c61,Yes
4136,kengz/SLM-Lab,test/experiment/test_monitor.py,8c17debc9dc6552da8c317c0c852b55f03c80c61,TODO add these tests,https://github.com/kengz/SLM-Lab/commit/8c17debc9dc6552da8c317c0c852b55f03c80c61,No
4137,kengz/SLM-Lab,slm_lab/lib/viz.py,60d3325b8551cc6f7592e1266878818ad07d3b3a,TODO tmp; switch back to == 'test' later when we have free plot,https://github.com/kengz/SLM-Lab/commit/60d3325b8551cc6f7592e1266878818ad07d3b3a,Yes
4138,kengz/SLM-Lab,test/agent/algo/test_algo.py,5d79fcc2055c22eb71e7cd8d5b75ecb279443dc6,TODO Fix pytest seg faults on Ubuntu,https://github.com/kengz/SLM-Lab/commit/5d79fcc2055c22eb71e7cd8d5b75ecb279443dc6,No
4139,kengz/SLM-Lab,slm_lab/lib/tf_util.py,99f54b4f5d398727527b83c720a42156c61d592c,TODO test this below,https://github.com/kengz/SLM-Lab/commit/99f54b4f5d398727527b83c720a42156c61d592c,No
4140,kengz/SLM-Lab,slm_lab/agent/memory/replay.py,70db35897c3a4683e485d4343d8021e68afd1668,TODO if latest; return unused. implement,https://github.com/kengz/SLM-Lab/commit/70db35897c3a4683e485d4343d8021e68afd1668,Yes
4141,tslearn-team/tslearn,setup.py,ba1182eeaaadb1805319166022128f4823f491f9,TODO: test package_data option on PyPI deployment\uFFFF,https://github.com/tslearn-team/tslearn/commit/ba1182eeaaadb1805319166022128f4823f491f9,No
4142,tslearn-team/tslearn,tslearn/tests/test_estimators.py,c46ad4e48e88027ab99f3010d2aec53d34580f9a,TODO: Create a test similar to check_clustering (check_ts_clustering),https://github.com/tslearn-team/tslearn/commit/c46ad4e48e88027ab99f3010d2aec53d34580f9a,Yes
4143,tslearn-team/tslearn,tslearn/tests/test_estimators.py,c46ad4e48e88027ab99f3010d2aec53d34580f9a,TODO: that generates a simple timeseries example and tests whether,https://github.com/tslearn-team/tslearn/commit/c46ad4e48e88027ab99f3010d2aec53d34580f9a,Yes
4144,jmwoloso/pychattr,dev/python_scripts/setup_markov.py,25b8d88585680219c7ec9925f89fc9544d19cb3f,TODO: figure out how to 1) test for windows and 2) set the compiler,https://github.com/jmwoloso/pychattr/commit/25b8d88585680219c7ec9925f89fc9544d19cb3f,No
4145,JohnVinyard/zounds,analyze/audiostream.py,30375cebfe4a7944d253dae78f30e5afbdfdd819,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/30375cebfe4a7944d253dae78f30e5afbdfdd819,Yes
4146,JohnVinyard/zounds,tests/analyze_test.py,30375cebfe4a7944d253dae78f30e5afbdfdd819,TODO: How do I cleanup wave files for failed tests?,https://github.com/JohnVinyard/zounds/commit/30375cebfe4a7944d253dae78f30e5afbdfdd819,No
4147,JohnVinyard/zounds,util.py,d3f3f12ce513b79a3f37065f0fa630ecc0e91116,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/d3f3f12ce513b79a3f37065f0fa630ecc0e91116,Yes
4148,JohnVinyard/zounds,analyze/extractor.py,f0241ce5c2934240c17a986ac8560accae81e1fa,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/f0241ce5c2934240c17a986ac8560accae81e1fa,Yes
4149,JohnVinyard/zounds,analyze/util.py,9a4fce7c4d843c5d1fa325ee66b6bde3e5cdc495,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/9a4fce7c4d843c5d1fa325ee66b6bde3e5cdc495,Yes
4150,JohnVinyard/zounds,analyze/audiostream.py,13ef571a03fe87926c30e7b3aa506d7a360ef244,TODO: Write test for encoding bug,https://github.com/JohnVinyard/zounds/commit/13ef571a03fe87926c30e7b3aa506d7a360ef244,Yes
4151,JohnVinyard/zounds,model/frame.py,3e141751900cbf9ac8d058fe221b7bec8f3a7489,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/3e141751900cbf9ac8d058fe221b7bec8f3a7489,Yes
4152,JohnVinyard/zounds,model/frame.py,76b2c4f7c7734a417e30c82fa55db9dcefe0340b,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/76b2c4f7c7734a417e30c82fa55db9dcefe0340b,Yes
4153,JohnVinyard/zounds,analyze/extractor.py,e5b4dbdc449d9823a8271b8ecfebd3c6cbb676b8,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/e5b4dbdc449d9823a8271b8ecfebd3c6cbb676b8,Yes
4154,JohnVinyard/zounds,model/frame.py,0790f510f81fb5b94bf761db8f8a69db1924c53a,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/0790f510f81fb5b94bf761db8f8a69db1924c53a,Yes
4155,JohnVinyard/zounds,data/frame.py,24e8531afaa2e38a7c065a4b638426b4b1a21d51,TODO: Write tests for this method!,https://github.com/JohnVinyard/zounds/commit/24e8531afaa2e38a7c065a4b638426b4b1a21d51,Yes
4156,JohnVinyard/zounds,data/frame.py,21463cd7c0750fcc5d7f7d178dde1c5c27d3827f,TODO: Make sure there are tests,https://github.com/JohnVinyard/zounds/commit/21463cd7c0750fcc5d7f7d178dde1c5c27d3827f,Yes
4157,JohnVinyard/zounds,data/frame.py,a37a8afe63e812f05275bd2c6a11013bfec8036f,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/a37a8afe63e812f05275bd2c6a11013bfec8036f,Yes
4158,JohnVinyard/zounds,data/frame.py,a6c77ee1ab58e7258279e9c3401c0d410a9af7ba,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/a6c77ee1ab58e7258279e9c3401c0d410a9af7ba,Yes
4159,JohnVinyard/zounds,model/frame_test.py,b26ca47e6b3b212dde66668b35419bef5588341d,TODO: factor out common code in the following two tests,https://github.com/JohnVinyard/zounds/commit/b26ca47e6b3b212dde66668b35419bef5588341d,Yes
4160,JohnVinyard/zounds,analyze/extractor.py,619d6ba8cd8b5ad7d8d20bf26bfa7fb1443d1ac9,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/619d6ba8cd8b5ad7d8d20bf26bfa7fb1443d1ac9,Yes
4161,JohnVinyard/zounds,model/frame.py,619d6ba8cd8b5ad7d8d20bf26bfa7fb1443d1ac9,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/619d6ba8cd8b5ad7d8d20bf26bfa7fb1443d1ac9,Yes
4162,JohnVinyard/zounds,model/framesearch.py,db0ad8902f5e009e598dea387ece8b29d6556d94,TODO: ExtractorChain.prune() that can take multiple features and tests,https://github.com/JohnVinyard/zounds/commit/db0ad8902f5e009e598dea387ece8b29d6556d94,Yes
4163,JohnVinyard/zounds,analyze/feature/learned.py,47ca02358d1dd4f97beba7ea739056b5e0d7622e,TODO: Write tests!,https://github.com/JohnVinyard/zounds/commit/47ca02358d1dd4f97beba7ea739056b5e0d7622e,Yes
4164,JohnVinyard/zounds,data/frame.py,f6529a00244e733d5962b24bcf5c6fbd6c19635d,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/f6529a00244e733d5962b24bcf5c6fbd6c19635d,Yes
4165,JohnVinyard/zounds,learn/fetch.py,c047c6e848946d32338f2370cc7357ec9ec4dc12,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/c047c6e848946d32338f2370cc7357ec9ec4dc12,Yes
4166,JohnVinyard/zounds,model/frame.py,d2948a9d94f03f3c75a9886b38f373c0e030acaa,TODO: Write tests for overlapping and non-overlapping windows,https://github.com/JohnVinyard/zounds/commit/d2948a9d94f03f3c75a9886b38f373c0e030acaa,Yes
4167,JohnVinyard/zounds,zounds/analyze/test_analyze.py,e57e55b5799c5a1b67b8012fa62f871e33b8b7ba,TODO: How do I cleanup wave files for failed tests?,https://github.com/JohnVinyard/zounds/commit/e57e55b5799c5a1b67b8012fa62f871e33b8b7ba,No
4168,JohnVinyard/zounds,zounds/data/test_frame.py,86289b3b6541d07e0a9810cffe08da5f52af5d55,TODO: The following two tests need to be adapted to concern themselves,https://github.com/JohnVinyard/zounds/commit/86289b3b6541d07e0a9810cffe08da5f52af5d55,Yes
4169,JohnVinyard/zounds,zounds/nputil/npx.py,c9d48d3b7323cfb36f4ef81aa4e2d03a0d49feea,TODO: Write tests for this!,https://github.com/JohnVinyard/zounds/commit/c9d48d3b7323cfb36f4ef81aa4e2d03a0d49feea,No
4170,JohnVinyard/zounds,zounds/data/frame/pytables.py,65c588f06ae3387cb2676598e34484603f150595,TODO: Write tests for this method!,https://github.com/JohnVinyard/zounds/commit/65c588f06ae3387cb2676598e34484603f150595,Yes
4171,JohnVinyard/zounds,zounds/data/frame/pytables.py,65c588f06ae3387cb2676598e34484603f150595,TODO: Make sure there are tests,https://github.com/JohnVinyard/zounds/commit/65c588f06ae3387cb2676598e34484603f150595,Yes
4172,JohnVinyard/zounds,zounds/data/test_frame.py,478944077843ff94622a5dc95780966ce15d7ea4,TODO: Get rid of this file once all tests have been moved to data.frame.test_frame,https://github.com/JohnVinyard/zounds/commit/478944077843ff94622a5dc95780966ce15d7ea4,Yes
4173,JohnVinyard/zounds,zounds/data/test_frame.py,352c7ddc1b228143c94f953b51d9fa44950c75f0,# TODO: Get rid of this file once all tests have been moved to data.frame.test_frame,https://github.com/JohnVinyard/zounds/commit/352c7ddc1b228143c94f953b51d9fa44950c75f0,Yes
4174,JohnVinyard/zounds,zounds/analyze/test_analyze.py,0301225e013c17093e6f482c2688e2e16c0c7be2,TODO: Switch to unittest2; so this test can be ignored,https://github.com/JohnVinyard/zounds/commit/0301225e013c17093e6f482c2688e2e16c0c7be2,Yes
4175,JohnVinyard/zounds,zounds/data/frame/test_frame.py,0301225e013c17093e6f482c2688e2e16c0c7be2,TODO: Switch to unittest2; so this test can be ignored,https://github.com/JohnVinyard/zounds/commit/0301225e013c17093e6f482c2688e2e16c0c7be2,Yes
4176,JohnVinyard/zounds,zounds/model/pattern.py,8e821cfff82302813879e806837abb1adce3e6f8,TODO: Tests,https://github.com/JohnVinyard/zounds/commit/8e821cfff82302813879e806837abb1adce3e6f8,No
4177,JohnVinyard/zounds,zounds/model/pattern.py,5d1bb452eb34d5db4f0b40af59c7814eaf36e474,TODO: Test,https://github.com/JohnVinyard/zounds/commit/5d1bb452eb34d5db4f0b40af59c7814eaf36e474,Yes
4178,JohnVinyard/zounds,zounds/model/pattern.py,97d4022f1692e3f4cbd30240864a361900851a15,TODO: Test,https://github.com/JohnVinyard/zounds/commit/97d4022f1692e3f4cbd30240864a361900851a15,Yes
4179,JohnVinyard/zounds,zounds/model/pattern.py,97d4022f1692e3f4cbd30240864a361900851a15,TODO: Stress test\/profile the play method and find out what an,https://github.com/JohnVinyard/zounds/commit/97d4022f1692e3f4cbd30240864a361900851a15,Yes
4180,JohnVinyard/zounds,zounds/data/test_pattern.py,02bf684259b83f0fce5f25a59d277148372cae81,TODO: Implement the __getitem__ method; and\/or switch to unittest2 so,https://github.com/JohnVinyard/zounds/commit/02bf684259b83f0fce5f25a59d277148372cae81,Yes
4181,JohnVinyard/zounds,zounds/model/pattern/zound.py,6e5ce9d2261f8e62e7ff6a2b13511eb6fc2b3eea,TODO: Test,https://github.com/JohnVinyard/zounds/commit/6e5ce9d2261f8e62e7ff6a2b13511eb6fc2b3eea,Yes
4182,JohnVinyard/zounds,zounds/model/pattern/zound.py,6e5ce9d2261f8e62e7ff6a2b13511eb6fc2b3eea,TODO: Stress test\/profile the play method and find out what an,https://github.com/JohnVinyard/zounds/commit/6e5ce9d2261f8e62e7ff6a2b13511eb6fc2b3eea,Yes
4183,JohnVinyard/zounds,setup.py,7e29e0fe9bd0466f1a93742db232ab8451f24383,TODO: run tests from here,https://github.com/JohnVinyard/zounds/commit/7e29e0fe9bd0466f1a93742db232ab8451f24383,Yes
4184,JohnVinyard/zounds,setup.py,9eb27842d06cded92511f3bf27295f9a47f6460b,TODO: run tests from here,https://github.com/JohnVinyard/zounds/commit/9eb27842d06cded92511f3bf27295f9a47f6460b,Yes
4185,JohnVinyard/zounds,zounds/nputil/npx.py,0b8b3766fbce2c8e39e9db5d596654d23f2c0ee0,TODO: Write tests!!,https://github.com/JohnVinyard/zounds/commit/0b8b3766fbce2c8e39e9db5d596654d23f2c0ee0,Yes
4186,JohnVinyard/zounds,zounds/nputil/npx.py,9f8c9eec41fde7f21a57030bf561fe45c265f0a2,TODO: Write tests,https://github.com/JohnVinyard/zounds/commit/9f8c9eec41fde7f21a57030bf561fe45c265f0a2,Yes
4187,JohnVinyard/zounds,zounds/node/sliding_window.py,fd3c30fc5ed66aef9897689447bf4b0dfbff5fe5,TODO: Steal AudioStream tests and use them here,https://github.com/JohnVinyard/zounds/commit/fd3c30fc5ed66aef9897689447bf4b0dfbff5fe5,Yes
4188,JohnVinyard/zounds,zounds/node/ogg_vorbis.py,72102408a77e41fd3c11cd42f7469e1b4eaabfdf,TODO: test to ensure that decoded ogg vorbis is the same length,https://github.com/JohnVinyard/zounds/commit/72102408a77e41fd3c11cd42f7469e1b4eaabfdf,No
4189,JohnVinyard/zounds,zounds/node/onset.py,46d1d8a0bd6012b602dad012cd6cefa194da9a8c,TODO: Encode\/decode tests,https://github.com/JohnVinyard/zounds/commit/46d1d8a0bd6012b602dad012cd6cefa194da9a8c,Yes
4190,ryfeus/gcf-packs,pandas_numpy/sources/dateutil/parser/_parser.py,255a05a5980efb8b096c283d79872d0695886161,TODO: not hit in tests,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4191,ryfeus/gcf-packs,pandas_numpy/sources/numpy/conftest.py,255a05a5980efb8b096c283d79872d0695886161,FIXME when yield tests are gone.,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4192,ryfeus/gcf-packs,pandas_numpy/sources/numpy/core/tests/test_multiarray.py,255a05a5980efb8b096c283d79872d0695886161,TODO: test for multidimensional,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4193,ryfeus/gcf-packs,pandas_numpy/sources/numpy/core/tests/test_nditer.py,255a05a5980efb8b096c283d79872d0695886161,Test that the inner loop grows when no buffering is needed,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4194,ryfeus/gcf-packs,pandas_numpy/sources/numpy/core/tests/test_regression.py,255a05a5980efb8b096c283d79872d0695886161,This didn't require a fix; but it's worth testing anyway; because,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4195,ryfeus/gcf-packs,pandas_numpy/sources/numpy/core/tests/test_umath.py,255a05a5980efb8b096c283d79872d0695886161,FIXME cinf not tested.,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4196,ryfeus/gcf-packs,pandas_numpy/sources/numpy/core/tests/test_umath.py,255a05a5980efb8b096c283d79872d0695886161,test fix for bug #1026:,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4197,ryfeus/gcf-packs,pandas_numpy/sources/numpy/core/tests/test_umath.py,255a05a5980efb8b096c283d79872d0695886161,test fix for bug #826:,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4198,ryfeus/gcf-packs,pandas_numpy/sources/numpy/doc/basics.py,255a05a5980efb8b096c283d79872d0695886161,"\""\""\"" || ============ || Array basics || ============ ||  || Array types and conversions between types || ========================================= ||  || NumPy supports a much greater variety of numerical types than Python does. || This section shows which are available; and how to modify an array's data-type. ||  || ============ ========================================================== || Data type    Description || ============ ========================================================== || ``bool_``    Boolean (True or False) stored as a byte || ``int_``     Default integer type (same as C ``long``; normally either ||              ``int64`` or ``int32``) || intc         Identical to C ``int`` (normally ``int32`` or ``int64``) || intp         Integer used for indexing (same as C ``ssize_t``; normally ||              either ``int32`` or ``int64``) || int8         Byte (-128 to 127) || int16        Integer (-32768 to 32767) || int32        Integer (-2147483648 to 2147483647) || int64        Integer (-9223372036854775808 to 9223372036854775807) || uint8        Unsigned integer (0 to 255) || uint16       Unsigned integer (0 to 65535) || uint32       Unsigned integer (0 to 4294967295) || uint64       Unsigned integer (0 to 18446744073709551615) || ``float_``   Shorthand for ``float64``. || float16      Half precision float: sign bit; 5 bits exponent; ||              10 bits mantissa || float32      Single precision float: sign bit; 8 bits exponent; ||              23 bits mantissa || float64      Double precision float: sign bit; 11 bits exponent; ||              52 bits mantissa || ``complex_`` Shorthand for ``complex128``. || complex64    Complex number; represented by two 32-bit floats (real ||              and imaginary components) || complex128   Complex number; represented by two 64-bit floats (real ||              and imaginary components) || ============ ========================================================== ||  || Additionally to ``intc`` the platform dependent C integer types ``short``; || ``long``; ``longlong`` and their unsigned versions are defined. ||  || NumPy numerical types are instances of ``dtype`` (data-type) objects; each || having unique characteristics.  Once you have imported NumPy using ||  ||   :: ||  ||     >>> import numpy as np ||  || the dtypes are available as ``np.bool_``; ``np.float32``; etc. ||  || Advanced types; not listed in the table above; are explored in || section :ref:`structured_arrays`. ||  || There are 5 basic numerical types representing booleans (bool); integers (int); || unsigned integers (uint) floating point (float) and complex. Those with numbers || in their name indicate the bitsize of the type (i.e. how many bits are needed || to represent a single value in memory).  Some types; such as ``int`` and || ``intp``; have differing bitsizes; dependent on the platforms (e.g. 32-bit || vs. 64-bit machines).  This should be taken into account when interfacing || with low-level code (such as C or Fortran) where the raw memory is addressed. ||  || Data-types can be used as functions to convert python numbers to array scalars || (see the array scalar section for an explanation); python sequences of numbers || to arrays of that type; or as arguments to the dtype keyword that many numpy || functions or methods accept. Some examples:: ||  ||     >>> import numpy as np ||     >>> x = np.float32(1.0) ||     >>> x ||     1.0 ||     >>> y = np.int_([1;2;4]) ||     >>> y ||     array([1; 2; 4]) ||     >>> z = np.arange(3; dtype=np.uint8) ||     >>> z ||     array([0; 1; 2]; dtype=uint8) ||  || Array types can also be referred to by character codes; mostly to retain || backward compatibility with older packages such as Numeric.  Some || documentation may still refer to these; for example:: ||  ||   >>> np.array([1; 2; 3]; dtype='f') ||   array([ 1.;  2.;  3.]; dtype=float32) ||  || We recommend using dtype objects instead. ||  || To convert the type of an array; use the .astype() method (preferred) or || the type itself as a function. For example: :: ||  ||     >>> z.astype(float)                 #doctest: +NORMALIZE_WHITESPACE ||     array([  0.;  1.;  2.]) ||     >>> np.int8(z) ||     array([0; 1; 2]; dtype=int8) ||  || Note that; above; we use the *Python* float object as a dtype.  NumPy knows || that ``int`` refers to ``np.int_``; ``bool`` means ``np.bool_``; || that ``float`` is ``np.float_`` and ``complex`` is ``np.complex_``. || The other data-types do not have Python equivalents. ||  || To determine the type of an array; look at the dtype attribute:: ||  ||     >>> z.dtype ||     dtype('uint8') ||  || dtype objects also contain information about the type; such as its bit-width || and its byte-order.  The data type can also be used indirectly to query || properties of the type; such as whether it is an integer:: ||  ||     >>> d = np.dtype(int) ||     >>> d ||     dtype('int32') ||  ||     >>> np.issubdtype(d; np.integer) ||     True ||  ||     >>> np.issubdtype(d; np.floating) ||     False ||  ||  || Array Scalars || ============= ||  || NumPy generally returns elements of arrays as array scalars (a scalar || with an associated dtype).  Array scalars differ from Python scalars; but || for the most part they can be used interchangeably (the primary || exception is for versions of Python older than v2.x; where integer array || scalars cannot act as indices for lists and tuples).  There are some || exceptions; such as when code requires very specific attributes of a scalar || or when it checks specifically whether a value is a Python scalar. Generally; || problems are easily fixed by explicitly converting array scalars || to Python scalars; using the corresponding Python type function || (e.g.; ``int``; ``float``; ``complex``; ``str``; ``unicode``). ||  || The primary advantage of using array scalars is that || they preserve the array type (Python may not have a matching scalar type || available; e.g. ``int16``).  Therefore; the use of array scalars ensures || identical behaviour between arrays and scalars; irrespective of whether the || value is inside an array or not.  NumPy scalars also have many of the same || methods arrays do. ||  || Extended Precision || ================== ||  || Python's floating-point numbers are usually 64-bit floating-point numbers; || nearly equivalent to ``np.float64``. In some unusual situations it may be || useful to use floating-point numbers with more precision. Whether this || is possible in numpy depends on the hardware and on the development || environment: specifically; x86 machines provide hardware floating-point || with 80-bit precision; and while most C compilers provide this as their || ``long double`` type; MSVC (standard for Windows builds) makes || ``long double`` identical to ``double`` (64 bits). NumPy makes the || compiler's ``long double`` available as ``np.longdouble`` (and || ``np.clongdouble`` for the complex numbers). You can find out what your || numpy provides with ``np.finfo(np.longdouble)``. ||  || NumPy does not provide a dtype with more precision than C || ``long double``\\\\s; in particular; the 128-bit IEEE quad precision || data type (FORTRAN's ``REAL*16``\\\\) is not available. ||  || For efficient memory alignment; ``np.longdouble`` is usually stored || padded with zero bits; either to 96 or 128 bits. Which is more efficient || depends on hardware and development environment; typically on 32-bit || systems they are padded to 96 bits; while on 64-bit systems they are || typically padded to 128 bits. ``np.longdouble`` is padded to the system || default; ``np.float96`` and ``np.float128`` are provided for users who || want specific padding. In spite of the names; ``np.float96`` and || ``np.float128`` provide only as much precision as ``np.longdouble``; || that is; 80 bits on most x86 machines and 64 bits in standard || Windows builds. ||  || Be warned that even if ``np.longdouble`` offers more precision than || python ``float``; it is easy to lose that extra precision; since || python often forces values to pass through ``float``. For example; || the ``%`` formatting operator requires its arguments to be converted || to standard python types; and it is therefore impossible to preserve || extended precision even if many decimal places are requested. It can || be useful to test your code with the value || ``1 + np.finfo(np.longdouble).eps``. ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4199,ryfeus/gcf-packs,pandas_numpy/sources/numpy/doc/glossary.py,255a05a5980efb8b096c283d79872d0695886161,"\""\""\"" || ======== || Glossary || ======== ||  || .. glossary:: ||  ||    along an axis ||        Axes are defined for arrays with more than one dimension.  A ||        2-dimensional array has two corresponding axes: the first running ||        vertically downwards across rows (axis 0); and the second running ||        horizontally across columns (axis 1). ||  ||        Many operations can take place along one of these axes.  For example; ||        we can sum each row of an array; in which case we operate along ||        columns; or axis 1:: ||  ||          >>> x = np.arange(12).reshape((3;4)) ||  ||          >>> x ||          array([[ 0;  1;  2;  3]; ||                 [ 4;  5;  6;  7]; ||                 [ 8;  9; 10; 11]]) ||  ||          >>> x.sum(axis=1) ||          array([ 6; 22; 38]) ||  ||    array ||        A homogeneous container of numerical elements.  Each element in the ||        array occupies a fixed amount of memory (hence homogeneous); and ||        can be a numerical element of a single type (such as float; int ||        or complex) or a combination (such as ``(float; int; float)``).  Each ||        array has an associated data-type (or ``dtype``); which describes ||        the numerical type of its elements:: ||  ||          >>> x = np.array([1; 2; 3]; float) ||  ||          >>> x ||          array([ 1.;  2.;  3.]) ||  ||          >>> x.dtype # floating point number; 64 bits of memory per element ||          dtype('float64') ||  ||  ||          # More complicated data type: each array element is a combination of ||          # and integer and a floating point number ||          >>> np.array([(1; 2.0); (3; 4.0)]; dtype=[('x'; int); ('y'; float)]) ||          array([(1; 2.0); (3; 4.0)]; ||                dtype=[('x'; '<i4'); ('y'; '<f8')]) ||  ||        Fast element-wise operations; called a :term:`ufunc`; operate on arrays. ||  ||    array_like ||        Any sequence that can be interpreted as an ndarray.  This includes ||        nested lists; tuples; scalars and existing arrays. ||  ||    attribute ||        A property of an object that can be accessed using ``obj.attribute``; ||        e.g.; ``shape`` is an attribute of an array:: ||  ||          >>> x = np.array([1; 2; 3]) ||          >>> x.shape ||          (3;) ||  ||    big-endian ||        When storing a multi-byte value in memory as a sequence of bytes; the ||        sequence addresses\/sends\/stores the most significant byte first (lowest ||        address) and the least significant byte last (highest address). Common in ||        micro-processors and used for transmission of data over network protocols. ||  ||    BLAS ||        `Basic Linear Algebra Subprograms <http:\/\/en.wikipedia.org\/wiki\/BLAS>`_ ||  ||    broadcast ||        NumPy can do operations on arrays whose shapes are mismatched:: ||  ||          >>> x = np.array([1; 2]) ||          >>> y = np.array([[3]; [4]]) ||  ||          >>> x ||          array([1; 2]) ||  ||          >>> y ||          array([[3]; ||                 [4]]) ||  ||          >>> x + y ||          array([[4; 5]; ||                 [5; 6]]) ||  ||        See `numpy.doc.broadcasting` for more information. ||  ||    C order ||        See `row-major` ||  ||    column-major ||        A way to represent items in a N-dimensional array in the 1-dimensional ||        computer memory. In column-major order; the leftmost index \""varies the ||        fastest\"": for example the array:: ||  ||             [[1; 2; 3]; ||              [4; 5; 6]] ||  ||        is represented in the column-major order as:: ||  ||            [1; 4; 2; 5; 3; 6] ||  ||        Column-major order is also known as the Fortran order; as the Fortran ||        programming language uses it. ||  ||    decorator ||        An operator that transforms a function.  For example; a ``log`` ||        decorator may be defined to print debugging information upon ||        function execution:: ||  ||          >>> def log(f): ||          ...     def new_logging_func(*args; **kwargs): ||          ...         print(\""Logging call with parameters:\""; args; kwargs) ||          ...         return f(*args; **kwargs) ||          ... ||          ...     return new_logging_func ||  ||        Now; when we define a function; we can \""decorate\"" it using ``log``:: ||  ||          >>> @log ||          ... def add(a; b): ||          ...     return a + b ||  ||        Calling ``add`` then yields: ||  ||        >>> add(1; 2) ||        Logging call with parameters: (1; 2) {} ||        3 ||  ||    dictionary ||        Resembling a language dictionary; which provides a mapping between ||        words and descriptions thereof; a Python dictionary is a mapping ||        between two objects:: ||  ||          >>> x = {1: 'one'; 'two': [1; 2]} ||  ||        Here; `x` is a dictionary mapping keys to values; in this case ||        the integer 1 to the string \""one\""; and the string \""two\"" to ||        the list ``[1; 2]``.  The values may be accessed using their ||        corresponding keys:: ||  ||          >>> x[1] ||          'one' ||  ||          >>> x['two'] ||          [1; 2] ||  ||        Note that dictionaries are not stored in any specific order.  Also; ||        most mutable (see *immutable* below) objects; such as lists; may not ||        be used as keys. ||  ||        For more information on dictionaries; read the ||        `Python tutorial <http:\/\/docs.python.org\/tut>`_. ||  ||    field ||        In a :term:`structured data type`; each sub-type is called a `field`. ||        The `field` has a name (a string); a type (any valid :term:`dtype`; and ||        an optional `title`. See :ref:`arrays.dtypes` ||  ||    Fortran order ||        See `column-major` ||  ||    flattened ||        Collapsed to a one-dimensional array. See `numpy.ndarray.flatten` ||        for details. ||  ||    homogenous ||        Describes a block of memory comprised of blocks; each block comprised of  ||        items and of the same size; and blocks are interpreted in exactly the ||        same way. In the simplest case each block contains a single item; for ||        instance int32 or float64. ||  ||    immutable ||        An object that cannot be modified after execution is called ||        immutable.  Two common examples are strings and tuples. ||  ||    instance ||        A class definition gives the blueprint for constructing an object:: ||  ||          >>> class House(object): ||          ...     wall_colour = 'white' ||  ||        Yet; we have to *build* a house before it exists:: ||  ||          >>> h = House() # build a house ||  ||        Now; ``h`` is called a ``House`` instance.  An instance is therefore ||        a specific realisation of a class. ||  ||    iterable ||        A sequence that allows \""walking\"" (iterating) over items; typically ||        using a loop such as:: ||  ||          >>> x = [1; 2; 3] ||          >>> [item**2 for item in x] ||          [1; 4; 9] ||  ||        It is often used in combination with ``enumerate``:: ||          >>> keys = ['a';'b';'c'] ||          >>> for n; k in enumerate(keys): ||          ...     print(\""Key %d: %s\"" % (n; k)) ||          ... ||          Key 0: a ||          Key 1: b ||          Key 2: c ||  ||    list ||        A Python container that can hold any number of objects or items. ||        The items do not have to be of the same type; and can even be ||        lists themselves:: ||  ||          >>> x = [2; 2.0; \""two\""; [2; 2.0]] ||  ||        The list `x` contains 4 items; each which can be accessed individually:: ||  ||          >>> x[2] # the string 'two' ||          'two' ||  ||          >>> x[3] # a list; containing an integer 2 and a float 2.0 ||          [2; 2.0] ||  ||        It is also possible to select more than one item at a time; ||        using *slicing*:: ||  ||          >>> x[0:2] # or; equivalently; x[:2] ||          [2; 2.0] ||  ||        In code; arrays are often conveniently expressed as nested lists:: ||  ||  ||          >>> np.array([[1; 2]; [3; 4]]) ||          array([[1; 2]; ||                 [3; 4]]) ||  ||        For more information; read the section on lists in the `Python ||        tutorial <http:\/\/docs.python.org\/tut>`_.  For a mapping ||        type (key-value); see *dictionary*. ||  ||    little-endian ||        When storing a multi-byte value in memory as a sequence of bytes; the ||        sequence addresses\/sends\/stores the least significant byte first (lowest ||        address) and the most significant byte last (highest address). Common in ||        x86 processors. ||  ||    mask ||        A boolean array; used to select only certain elements for an operation:: ||  ||          >>> x = np.arange(5) ||          >>> x ||          array([0; 1; 2; 3; 4]) ||  ||          >>> mask = (x > 2) ||          >>> mask ||          array([False; False; False; True;  True]) ||  ||          >>> x[mask] = -1 ||          >>> x ||          array([ 0;  1;  2;  -1; -1]) ||  ||    masked array ||        Array that suppressed values indicated by a mask:: ||  ||          >>> x = np.ma.masked_array([np.nan; 2; np.nan]; [True; False; True]) ||          >>> x ||          masked_array(data = [-- 2.0 --]; ||                       mask = [ True False  True]; ||                 fill_value = 1e+20) ||          <BLANKLINE> ||  ||          >>> x + [1; 2; 3] ||          masked_array(data = [-- 4.0 --]; ||                       mask = [ True False  True]; ||                 fill_value = 1e+20) ||          <BLANKLINE> ||  ||  ||        Masked arrays are often used when operating on arrays containing ||        missing or invalid entries. ||  ||    matrix ||        A 2-dimensional ndarray that preserves its two-dimensional nature ||        throughout operations.  It has certain special operations; such as ``*`` ||        (matrix multiplication) and ``**`` (matrix power); defined:: ||  ||          >>> x = np.mat([[1; 2]; [3; 4]]) ||          >>> x ||          matrix([[1; 2]; ||                  [3; 4]]) ||  ||          >>> x**2 ||          matrix([[ 7; 10]; ||                [15; 22]]) ||  ||    method ||        A function associated with an object.  For example; each ndarray has a ||        method called ``repeat``:: ||  ||          >>> x = np.array([1; 2; 3]) ||          >>> x.repeat(2) ||          array([1; 1; 2; 2; 3; 3]) ||  ||    ndarray ||        See *array*. ||  ||    record array ||        An :term:`ndarray` with :term:`structured data type` which has been ||        subclassed as ``np.recarray`` and whose dtype is of type ``np.record``; ||        making the fields of its data type to be accessible by attribute. ||  ||    reference ||        If ``a`` is a reference to ``b``; then ``(a is b) == True``.  Therefore; ||        ``a`` and ``b`` are different names for the same Python object. ||  ||    row-major ||        A way to represent items in a N-dimensional array in the 1-dimensional ||        computer memory. In row-major order; the rightmost index \""varies ||        the fastest\"": for example the array:: ||  ||             [[1; 2; 3]; ||              [4; 5; 6]] ||  ||        is represented in the row-major order as:: ||  ||            [1; 2; 3; 4; 5; 6] ||  ||        Row-major order is also known as the C order; as the C programming ||        language uses it. New NumPy arrays are by default in row-major order. ||  ||    self ||        Often seen in method signatures; ``self`` refers to the instance ||        of the associated class.  For example: ||  ||          >>> class Paintbrush(object): ||          ...     color = 'blue' ||          ... ||          ...     def paint(self): ||          ...         print(\""Painting the city %s!\"" % self.color) ||          ... ||          >>> p = Paintbrush() ||          >>> p.color = 'red' ||          >>> p.paint() # self refers to 'p' ||          Painting the city red! ||  ||    slice ||        Used to select only certain elements from a sequence:: ||  ||          >>> x = range(5) ||          >>> x ||          [0; 1; 2; 3; 4] ||  ||          >>> x[1:3] # slice from 1 to 3 (excluding 3 itself) ||          [1; 2] ||  ||          >>> x[1:5:2] # slice from 1 to 5; but skipping every second element ||          [1; 3] ||  ||          >>> x[::-1] # slice a sequence in reverse ||          [4; 3; 2; 1; 0] ||  ||        Arrays may have more than one dimension; each which can be sliced ||        individually:: ||  ||          >>> x = np.array([[1; 2]; [3; 4]]) ||          >>> x ||          array([[1; 2]; ||                 [3; 4]]) ||  ||          >>> x[:; 1] ||          array([2; 4]) ||  ||    structure ||        See :term:`structured data type` ||  ||    structured data type ||        A data type composed of other datatypes ||  ||    tuple ||        A sequence that may contain a variable number of types of any ||        kind.  A tuple is immutable; i.e.; once constructed it cannot be ||        changed.  Similar to a list; it can be indexed and sliced:: ||  ||          >>> x = (1; 'one'; [1; 2]) ||          >>> x ||          (1; 'one'; [1; 2]) ||  ||          >>> x[0] ||          1 ||  ||          >>> x[:2] ||          (1; 'one') ||  ||        A useful concept is \""tuple unpacking\""; which allows variables to ||        be assigned to the contents of a tuple:: ||  ||          >>> x; y = (1; 2) ||          >>> x; y = 1; 2 ||  ||        This is often used when a function returns multiple values: ||  ||          >>> def return_many(): ||          ...     return 1; 'alpha'; None ||  ||          >>> a; b; c = return_many() ||          >>> a; b; c ||          (1; 'alpha'; None) ||  ||          >>> a ||          1 ||          >>> b ||          'alpha' ||  ||    ufunc ||        Universal function.  A fast element-wise array operation.  Examples include ||        ``add``; ``sin`` and ``logical_or``. ||  ||    view ||        An array that does not own its data; but refers to another array's ||        data instead.  For example; we may create a view that only shows ||        every second element of another array:: ||  ||          >>> x = np.arange(5) ||          >>> x ||          array([0; 1; 2; 3; 4]) ||  ||          >>> y = x[::2] ||          >>> y ||          array([0; 2; 4]) ||  ||          >>> x[0] = 3 # changing x changes y as well; since y is a view on x ||          >>> y ||          array([3; 2; 4]) ||  ||    wrapper ||        Python is a high-level (highly abstracted; or English-like) language. ||        This abstraction comes at a price in execution speed; and sometimes ||        it becomes necessary to use lower level languages to do fast ||        computations.  A wrapper is code that provides a bridge between ||        high and the low level languages; allowing; e.g.; Python to execute ||        code written in C or Fortran. ||  ||        Examples include ctypes; SWIG and Cython (which wraps C and C++) ||        and f2py (which wraps Fortran). ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4200,ryfeus/gcf-packs,pandas_numpy/sources/numpy/doc/misc.py,255a05a5980efb8b096c283d79872d0695886161,"\""\""\"" || ============= || Miscellaneous || ============= ||  || IEEE 754 Floating Point Special Values || -------------------------------------- ||  || Special values defined in numpy: nan; inf; ||  || NaNs can be used as a poor-man's mask (if you don't care what the || original value was) ||  || Note: cannot use equality to test NaNs. E.g.: :: ||  ||  >>> myarr = np.array([1.; 0.; np.nan; 3.]) ||  >>> np.nonzero(myarr == np.nan) ||  (array([]; dtype=int64);) ||  >>> np.nan == np.nan  # is always False! Use special numpy functions instead. ||  False ||  >>> myarr[myarr == np.nan] = 0. # doesn't work ||  >>> myarr ||  array([  1.;   0.;  NaN;   3.]) ||  >>> myarr[np.isnan(myarr)] = 0. # use this instead find ||  >>> myarr ||  array([ 1.;  0.;  0.;  3.]) ||  || Other related special value functions: :: ||  ||  isinf():    True if value is inf ||  isfinite(): True if not nan or inf ||  nan_to_num(): Map nan to 0; inf to max float; -inf to min float ||  || The following corresponds to the usual functions except that nans are excluded || from the results: :: ||  ||  nansum() ||  nanmax() ||  nanmin() ||  nanargmax() ||  nanargmin() ||  ||  >>> x = np.arange(10.) ||  >>> x[3] = np.nan ||  >>> x.sum() ||  nan ||  >>> np.nansum(x) ||  42.0 ||  || How numpy handles numerical exceptions || -------------------------------------- ||  || The default is to ``'warn'`` for ``invalid``; ``divide``; and ``overflow`` || and ``'ignore'`` for ``underflow``.  But this can be changed; and it can be || set individually for different kinds of exceptions. The different behaviors || are: ||  ||  - 'ignore' : Take no action when the exception occurs. ||  - 'warn'   : Print a `RuntimeWarning` (via the Python `warnings` module). ||  - 'raise'  : Raise a `FloatingPointError`. ||  - 'call'   : Call a function specified using the `seterrcall` function. ||  - 'print'  : Print a warning directly to ``stdout``. ||  - 'log'    : Record error in a Log object specified by `seterrcall`. ||  || These behaviors can be set for all kinds of errors or specific ones: ||  ||  - all       : apply to all numeric exceptions ||  - invalid   : when NaNs are generated ||  - divide    : divide by zero (for integers as well!) ||  - overflow  : floating point overflows ||  - underflow : floating point underflows ||  || Note that integer divide-by-zero is handled by the same machinery. || These behaviors are set on a per-thread basis. ||  || Examples || -------- ||  || :: ||  ||  >>> oldsettings = np.seterr(all='warn') ||  >>> np.zeros(5;dtype=np.float32)\/0. ||  invalid value encountered in divide ||  >>> j = np.seterr(under='ignore') ||  >>> np.array([1.e-100])**10 ||  >>> j = np.seterr(invalid='raise') ||  >>> np.sqrt(np.array([-1.])) ||  FloatingPointError: invalid value encountered in sqrt ||  >>> def errorhandler(errstr; errflag): ||  ...      print(\""saw stupid error!\"") ||  >>> np.seterrcall(errorhandler) ||  <function err_handler at 0x...> ||  >>> j = np.seterr(all='call') ||  >>> np.zeros(5; dtype=np.int32)\/0 ||  FloatingPointError: invalid value encountered in divide ||  saw stupid error! ||  >>> j = np.seterr(**oldsettings) # restore previous ||  ...                              # error-handling settings ||  || Interfacing to C || ---------------- || Only a survey of the choices. Little detail on how each works. ||  || 1) Bare metal; wrap your own C-code manually. ||  ||  - Plusses: ||  ||    - Efficient ||    - No dependencies on other tools ||  ||  - Minuses: ||  ||    - Lots of learning overhead: ||  ||      - need to learn basics of Python C API ||      - need to learn basics of numpy C API ||      - need to learn how to handle reference counting and love it. ||  ||    - Reference counting often difficult to get right. ||  ||      - getting it wrong leads to memory leaks; and worse; segfaults ||  ||    - API will change for Python 3.0! ||  || 2) Cython ||  ||  - Plusses: ||  ||    - avoid learning C API's ||    - no dealing with reference counting ||    - can code in pseudo python and generate C code ||    - can also interface to existing C code ||    - should shield you from changes to Python C api ||    - has become the de-facto standard within the scientific Python community ||    - fast indexing support for arrays ||  ||  - Minuses: ||  ||    - Can write code in non-standard form which may become obsolete ||    - Not as flexible as manual wrapping ||  || 3) ctypes ||  ||  - Plusses: ||  ||    - part of Python standard library ||    - good for interfacing to existing sharable libraries; particularly ||      Windows DLLs ||    - avoids API\/reference counting issues ||    - good numpy support: arrays have all these in their ctypes ||      attribute: :: ||  ||        a.ctypes.data              a.ctypes.get_strides ||        a.ctypes.data_as           a.ctypes.shape ||        a.ctypes.get_as_parameter  a.ctypes.shape_as ||        a.ctypes.get_data          a.ctypes.strides ||        a.ctypes.get_shape         a.ctypes.strides_as ||  ||  - Minuses: ||  ||    - can't use for writing code to be turned into C extensions; only a wrapper ||      tool. ||  || 4) SWIG (automatic wrapper generator) ||  ||  - Plusses: ||  ||    - around a long time ||    - multiple scripting language support ||    - C++ support ||    - Good for wrapping large (many functions) existing C libraries ||  ||  - Minuses: ||  ||    - generates lots of code between Python and the C code ||    - can cause performance problems that are nearly impossible to optimize ||      out ||    - interface files can be hard to write ||    - doesn't necessarily avoid reference counting issues or needing to know ||      API's ||  || 5) scipy.weave ||  ||  - Plusses: ||  ||    - can turn many numpy expressions into C code ||    - dynamic compiling and loading of generated C code ||    - can embed pure C code in Python module and have weave extract; generate ||      interfaces and compile; etc. ||  ||  - Minuses: ||  ||    - Future very uncertain: it's the only part of Scipy not ported to Python 3 ||      and is effectively deprecated in favor of Cython. ||  || 6) Psyco ||  ||  - Plusses: ||  ||    - Turns pure python into efficient machine code through jit-like ||      optimizations ||    - very fast when it optimizes well ||  ||  - Minuses: ||  ||    - Only on intel (windows?) ||    - Doesn't do much for numpy? ||  || Interfacing to Fortran: || ----------------------- || The clear choice to wrap Fortran code is || `f2py <http:\/\/docs.scipy.org\/doc\/numpy\/f2py\/>`_. ||  || Pyfort is an older alternative; but not supported any longer. || Fwrap is a newer project that looked promising but isn't being developed any || longer. ||  || Interfacing to C++: || ------------------- ||  1) Cython ||  2) CXX ||  3) Boost.python ||  4) SWIG ||  5) SIP (used mainly in PyQT) ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4201,ryfeus/gcf-packs,pandas_numpy/sources/numpy/doc/structured_arrays.py,255a05a5980efb8b096c283d79872d0695886161,"\""\""\"" || ================= || Structured Arrays || ================= ||  || Introduction || ============ ||  || Structured arrays are ndarrays whose datatype is a composition of simpler || datatypes organized as a sequence of named :term:`fields <field>`. For example; || :: ||  ||  >>> x = np.array([('Rex'; 9; 81.0); ('Fido'; 3; 27.0)]; ||  ...              dtype=[('name'; 'U10'); ('age'; 'i4'); ('weight'; 'f4')]) ||  >>> x ||  array([('Rex'; 9; 81.0); ('Fido'; 3; 27.0)]; ||        dtype=[('name'; 'S10'); ('age'; '<i4'); ('weight'; '<f4')]) ||  || Here ``x`` is a one-dimensional array of length two whose datatype is a || structure with three fields: 1. A string of length 10 or less named 'name'; 2. || a 32-bit integer named 'age'; and 3. a 32-bit float named 'weight'. ||  || If you index ``x`` at position 1 you get a structure:: ||  ||  >>> x[1] ||  ('Fido'; 3; 27.0) ||  || You can access and modify individual fields of a structured array by indexing || with the field name:: ||  ||  >>> x['age'] ||  array([9; 3]; dtype=int32) ||  >>> x['age'] = 5 ||  >>> x ||  array([('Rex'; 5; 81.0); ('Fido'; 5; 27.0)]; ||        dtype=[('name'; 'S10'); ('age'; '<i4'); ('weight'; '<f4')]) ||  || Structured arrays are designed for low-level manipulation of structured data; || for example; for interpreting binary blobs. Structured datatypes are || designed to mimic 'structs' in the C language; making them also useful for || interfacing with C code. For these purposes; numpy supports specialized || features such as subarrays and nested datatypes; and allows manual control over || the memory layout of the structure. ||  || For simple manipulation of tabular data other pydata projects; such as pandas; || xarray; or DataArray; provide higher-level interfaces that may be more || suitable. These projects may also give better performance for tabular data || analysis because the C-struct-like memory layout of structured arrays can lead || to poor cache behavior. ||  || .. _defining-structured-types: ||  || Structured Datatypes || ==================== ||  || To use structured arrays one first needs to define a structured datatype. ||  || A structured datatype can be thought of as a sequence of bytes of a certain || length (the structure's :term:`itemsize`) which is interpreted as a collection || of fields. Each field has a name; a datatype; and a byte offset within the || structure. The datatype of a field may be any numpy datatype including other || structured datatypes; and it may also be a :term:`sub-array` which behaves like || an ndarray of a specified shape. The offsets of the fields are arbitrary; and || fields may even overlap. These offsets are usually determined automatically by || numpy; but can also be specified. ||  || Structured Datatype Creation || ---------------------------- ||  || Structured datatypes may be created using the function :func:`numpy.dtype`. || There are 4 alternative forms of specification which vary in flexibility and || conciseness. These are further documented in the || :ref:`Data Type Objects <arrays.dtypes.constructing>` reference page; and in || summary they are: ||  || 1.   A list of tuples; one tuple per field ||  ||      Each tuple has the form ``(fieldname; datatype; shape)`` where shape is ||      optional. ``fieldname`` is a string (or tuple if titles are used; see ||      :ref:`Field Titles <titles>` below); ``datatype`` may be any object ||      convertible to a datatype; and ``shape`` is a tuple of integers specifying ||      subarray shape. ||  ||       >>> np.dtype([('x'; 'f4'); ('y'; np.float32); ('z'; 'f4'; (2;2))]) ||       dtype=[('x'; '<f4'); ('y'; '<f4'); ('z'; '<f4'; (2; 2))]) ||  ||      If ``fieldname`` is the empty string ``''``; the field will be given a ||      default name of the form ``f#``; where ``#`` is the integer index of the ||      field; counting from 0 from the left:: ||  ||       >>> np.dtype([('x'; 'f4');(''; 'i4');('z'; 'i8')]) ||       dtype([('x'; '<f4'); ('f1'; '<i4'); ('z'; '<i8')]) ||  ||      The byte offsets of the fields within the structure and the total ||      structure itemsize are determined automatically. ||  || 2.   A string of comma-separated dtype specifications ||  ||      In this shorthand notation any of the :ref:`string dtype specifications ||      <arrays.dtypes.constructing>` may be used in a string and separated by ||      commas. The itemsize and byte offsets of the fields are determined ||      automatically; and the field names are given the default names ``f0``; ||      ``f1``; etc. :: ||  ||       >>> np.dtype('i8;f4;S3') ||       dtype([('f0'; '<i8'); ('f1'; '<f4'); ('f2'; 'S3')]) ||       >>> np.dtype('3int8; float32; (2;3)float64') ||       dtype([('f0'; 'i1'; 3); ('f1'; '<f4'); ('f2'; '<f8'; (2; 3))]) ||  || 3.   A dictionary of field parameter arrays ||  ||      This is the most flexible form of specification since it allows control ||      over the byte-offsets of the fields and the itemsize of the structure. ||  ||      The dictionary has two required keys; 'names' and 'formats'; and four ||      optional keys; 'offsets'; 'itemsize'; 'aligned' and 'titles'. The values ||      for 'names' and 'formats' should respectively be a list of field names and ||      a list of dtype specifications; of the same length. The optional 'offsets' ||      value should be a list of integer byte-offsets; one for each field within ||      the structure. If 'offsets' is not given the offsets are determined ||      automatically. The optional 'itemsize' value should be an integer ||      describing the total size in bytes of the dtype; which must be large ||      enough to contain all the fields. ||      :: ||  ||       >>> np.dtype({'names': ['col1'; 'col2']; 'formats': ['i4';'f4']}) ||       dtype([('col1'; '<i4'); ('col2'; '<f4')]) ||       >>> np.dtype({'names': ['col1'; 'col2']; ||       ...           'formats': ['i4';'f4']; ||       ...           'offsets': [0; 4]; ||       ...           'itemsize': 12}) ||       dtype({'names':['col1';'col2']; 'formats':['<i4';'<f4']; 'offsets':[0;4]; 'itemsize':12}) ||  ||      Offsets may be chosen such that the fields overlap; though this will mean ||      that assigning to one field may clobber any overlapping field's data. As ||      an exception; fields of :class:`numpy.object` type cannot overlap with ||      other fields; because of the risk of clobbering the internal object ||      pointer and then dereferencing it. ||  ||      The optional 'aligned' value can be set to ``True`` to make the automatic ||      offset computation use aligned offsets (see :ref:`offsets-and-alignment`); ||      as if the 'align' keyword argument of :func:`numpy.dtype` had been set to ||      True. ||  ||      The optional 'titles' value should be a list of titles of the same length ||      as 'names'; see :ref:`Field Titles <titles>` below. ||  || 4.   A dictionary of field names ||  ||      The use of this form of specification is discouraged; but documented here ||      because older numpy code may use it. The keys of the dictionary are the ||      field names and the values are tuples specifying type and offset:: ||  ||       >>> np.dtype=({'col1': ('i1';0); 'col2': ('f4';1)}) ||       dtype([(('col1'); 'i1'); (('col2'); '>f4')]) ||  ||      This form is discouraged because Python dictionaries do not preserve order ||      in Python versions before Python 3.6; and the order of the fields in a ||      structured dtype has meaning. :ref:`Field Titles <titles>` may be ||      specified by using a 3-tuple; see below. ||  || Manipulating and Displaying Structured Datatypes || ------------------------------------------------ ||  || The list of field names of a structured datatype can be found in the ``names`` || attribute of the dtype object:: ||  ||  >>> d = np.dtype([('x'; 'i8'); ('y'; 'f4')]) ||  >>> d.names ||  ('x'; 'y') ||  || The field names may be modified by assigning to the ``names`` attribute using a || sequence of strings of the same length. ||  || The dtype object also has a dictionary-like attribute; ``fields``; whose keys || are the field names (and :ref:`Field Titles <titles>`; see below) and whose || values are tuples containing the dtype and byte offset of each field. :: ||  ||  >>> d.fields ||  mappingproxy({'x': (dtype('int64'); 0); 'y': (dtype('float32'); 8)}) ||  || Both the ``names`` and ``fields`` attributes will equal ``None`` for || unstructured arrays. ||  || The string representation of a structured datatype is shown in the \""list of || tuples\"" form if possible; otherwise numpy falls back to using the more general || dictionary form. ||  || .. _offsets-and-alignment: ||  || Automatic Byte Offsets and Alignment || ------------------------------------ ||  || Numpy uses one of two methods to automatically determine the field byte offsets || and the overall itemsize of a structured datatype; depending on whether || ``align=True`` was specified as a keyword argument to :func:`numpy.dtype`. ||  || By default (``align=False``); numpy will pack the fields together such that || each field starts at the byte offset the previous field ended; and the fields || are contiguous in memory. :: ||  ||  >>> def print_offsets(d): ||  ...     print(\""offsets:\""; [d.fields[name][1] for name in d.names]) ||  ...     print(\""itemsize:\""; d.itemsize) ||  >>> print_offsets(np.dtype('u1;u1;i4;u1;i8;u2')) ||  offsets: [0; 1; 2; 6; 7; 15] ||  itemsize: 17 ||  || If ``align=True`` is set; numpy will pad the structure in the same way many C || compilers would pad a C-struct. Aligned structures can give a performance || improvement in some cases; at the cost of increased datatype size. Padding || bytes are inserted between fields such that each field's byte offset will be a || multiple of that field's alignment; which is usually equal to the field's size || in bytes for simple datatypes; see :c:member:`PyArray_Descr.alignment`.  The || structure will also have trailing padding added so that its itemsize is a || multiple of the largest field's alignment. :: ||  ||  >>> print_offsets(np.dtype('u1;u1;i4;u1;i8;u2'; align=True)) ||  offsets: [0; 1; 4; 8; 16; 24] ||  itemsize: 32 ||  || Note that although almost all modern C compilers pad in this way by default; || padding in C structs is C-implementation-dependent so this memory layout is not || guaranteed to exactly match that of a corresponding struct in a C program. Some || work may be needed; either on the numpy side or the C side; to obtain exact || correspondence. ||  || If offsets were specified using the optional ``offsets`` key in the || dictionary-based dtype specification; setting ``align=True`` will check that || each field's offset is a multiple of its size and that the itemsize is a || multiple of the largest field size; and raise an exception if not. ||  || If the offsets of the fields and itemsize of a structured array satisfy the || alignment conditions; the array will have the ``ALIGNED`` :ref:`flag || <numpy.ndarray.flags>` set. ||  || A convenience function :func:`numpy.lib.recfunctions.repack_fields` converts an || aligned dtype or array to a packed one and vice versa. It takes either a dtype || or structured ndarray as an argument; and returns a copy with fields re-packed; || with or without padding bytes. ||  || .. _titles: ||  || Field Titles || ------------ ||  || In addition to field names; fields may also have an associated :term:`title`; || an alternate name; which is sometimes used as an additional description or || alias for the field. The title may be used to index an array; just like a || field name. ||  || To add titles when using the list-of-tuples form of dtype specification; the || field name may be specified as a tuple of two strings instead of a single || string; which will be the field's title and field name respectively. For || example:: ||  ||  >>> np.dtype([(('my title'; 'name'); 'f4')]) ||  || When using the first form of dictionary-based specification; the titles may be || supplied as an extra ``'titles'`` key as described above. When using the second || (discouraged) dictionary-based specification; the title can be supplied by || providing a 3-element tuple ``(datatype; offset; title)`` instead of the usual || 2-element tuple:: ||  ||  >>> np.dtype({'name': ('i4'; 0; 'my title')}) ||  || The ``dtype.fields`` dictionary will contain :term:`titles` as keys; if any || titles are used.  This means effectively that a field with a title will be || represented twice in the fields dictionary. The tuple values for these fields || will also have a third element; the field title. Because of this; and because || the ``names`` attribute preserves the field order while the ``fields`` || attribute may not; it is recommended to iterate through the fields of a dtype || using the ``names`` attribute of the dtype; which will not list titles; as || in:: ||  ||  >>> for name in d.names: ||  ...     print(d.fields[name][:2]) ||  || Union types || ----------- ||  || Structured datatypes are implemented in numpy to have base type || :class:`numpy.void` by default; but it is possible to interpret other numpy || types as structured types using the ``(base_dtype; dtype)`` form of dtype || specification described in || :ref:`Data Type Objects <arrays.dtypes.constructing>`.  Here; ``base_dtype`` is || the desired underlying dtype; and fields and flags will be copied from || ``dtype``. This dtype is similar to a 'union' in C. ||  || Indexing and Assignment to Structured arrays || ============================================ ||  || Assigning data to a Structured Array || ------------------------------------ ||  || There are a number of ways to assign values to a structured array: Using python || tuples; using scalar values; or using other structured arrays. ||  || Assignment from Python Native Types (Tuples) || ```````````````````````````````````````````` ||  || The simplest way to assign values to a structured array is using python tuples. || Each assigned value should be a tuple of length equal to the number of fields || in the array; and not a list or array as these will trigger numpy's || broadcasting rules. The tuple's elements are assigned to the successive fields || of the array; from left to right:: ||  ||  >>> x = np.array([(1;2;3);(4;5;6)]; dtype='i8;f4;f8') ||  >>> x[1] = (7;8;9) ||  >>> x ||  array([(1; 2.; 3.); (7; 8.; 9.)]; ||       dtype=[('f0'; '<i8'); ('f1'; '<f4'); ('f2'; '<f8')]) ||  || Assignment from Scalars || ``````````````````````` ||  || A scalar assigned to a structured element will be assigned to all fields. This || happens when a scalar is assigned to a structured array; or when an || unstructured array is assigned to a structured array:: ||  ||  >>> x = np.zeros(2; dtype='i8;f4;?;S1') ||  >>> x[:] = 3 ||  >>> x ||  array([(3; 3.0; True; b'3'); (3; 3.0; True; b'3')]; ||        dtype=[('f0'; '<i8'); ('f1'; '<f4'); ('f2'; '?'); ('f3'; 'S1')]) ||  >>> x[:] = np.arange(2) ||  >>> x ||  array([(0; 0.0; False; b'0'); (1; 1.0; True; b'1')]; ||        dtype=[('f0'; '<i8'); ('f1'; '<f4'); ('f2'; '?'); ('f3'; 'S1')]) ||  || Structured arrays can also be assigned to unstructured arrays; but only if the || structured datatype has just a single field:: ||  ||  >>> twofield = np.zeros(2; dtype=[('A'; 'i4'); ('B'; 'i4')]) ||  >>> onefield = np.zeros(2; dtype=[('A'; 'i4')]) ||  >>> nostruct = np.zeros(2; dtype='i4') ||  >>> nostruct[:] = twofield ||  ValueError: Can't cast from structure to non-structure; except if the structure only has a single field. ||  >>> nostruct[:] = onefield ||  >>> nostruct ||  array([0; 0]; dtype=int32) ||  || Assignment from other Structured Arrays || ``````````````````````````````````````` ||  || Assignment between two structured arrays occurs as if the source elements had || been converted to tuples and then assigned to the destination elements. That || is; the first field of the source array is assigned to the first field of the || destination array; and the second field likewise; and so on; regardless of || field names. Structured arrays with a different number of fields cannot be || assigned to each other. Bytes of the destination structure which are not || included in any of the fields are unaffected. :: ||  ||  >>> a = np.zeros(3; dtype=[('a'; 'i8'); ('b'; 'f4'); ('c'; 'S3')]) ||  >>> b = np.ones(3; dtype=[('x'; 'f4'); ('y'; 'S3'); ('z'; 'O')]) ||  >>> b[:] = a ||  >>> b ||  array([(0.0; b'0.0'; b''); (0.0; b'0.0'; b''); (0.0; b'0.0'; b'')]; ||        dtype=[('x'; '<f4'); ('y'; 'S3'); ('z'; 'O')]) ||  ||  || Assignment involving subarrays || `````````````````````````````` ||  || When assigning to fields which are subarrays; the assigned value will first be || broadcast to the shape of the subarray. ||  || Indexing Structured Arrays || -------------------------- ||  || Accessing Individual Fields || ``````````````````````````` ||  || Individual fields of a structured array may be accessed and modified by indexing || the array with the field name. :: ||  ||  >>> x = np.array([(1;2);(3;4)]; dtype=[('foo'; 'i8'); ('bar'; 'f4')]) ||  >>> x['foo'] ||  array([1; 3]) ||  >>> x['foo'] = 10 ||  >>> x ||  array([(10; 2.); (10; 4.)]; ||        dtype=[('foo'; '<i8'); ('bar'; '<f4')]) ||  || The resulting array is a view into the original array. It shares the same || memory locations and writing to the view will modify the original array. :: ||  ||  >>> y = x['bar'] ||  >>> y[:] = 10 ||  >>> x ||  array([(10; 5.); (10; 5.)]; ||        dtype=[('foo'; '<i8'); ('bar'; '<f4')]) ||  || This view has the same dtype and itemsize as the indexed field; so it is || typically a non-structured array; except in the case of nested structures. ||  ||  >>> y.dtype; y.shape; y.strides ||  (dtype('float32'); (2;); (12;)) ||  || Accessing Multiple Fields || ``````````````````````````` ||  || One can index and assign to a structured array with a multi-field index; where || the index is a list of field names. ||  || .. warning:: ||     The behavior of multi-field indexes will change from Numpy 1.15 to Numpy ||     1.16. ||  || In Numpy 1.16; the result of indexing with a multi-field index will be a view || into the original array; as follows:: ||  ||  >>> a = np.zeros(3; dtype=[('a'; 'i4'); ('b'; 'i4'); ('c'; 'f4')]) ||  >>> a[['a'; 'c']] ||  array([(0; 0.); (0; 0.); (0; 0.)]; ||       dtype={'names':['a';'c']; 'formats':['<i4';'<f4']; 'offsets':[0;8]; 'itemsize':12}) ||  || Assignment to the view modifies the original array. The view's fields will be || in the order they were indexed. Note that unlike for single-field indexing; the || view's dtype has the same itemsize as the original array; and has fields at the || same offsets as in the original array; and unindexed fields are merely missing. ||  || In Numpy 1.15; indexing an array with a multi-field index returns a copy of || the result above for 1.16; but with fields packed together in memory as if || passed through :func:`numpy.lib.recfunctions.repack_fields`. This is the || behavior since Numpy 1.7. ||  || .. warning:: ||    The new behavior in Numpy 1.16 leads to extra \""padding\"" bytes at the ||    location of unindexed fields. You will need to update any code which depends ||    on the data having a \""packed\"" layout. For instance code such as:: ||  ||     >>> a[['a';'c']].view('i8')  # will fail in Numpy 1.16 ||     ValueError: When changing to a smaller dtype; its size must be a divisor of the size of original dtype ||  ||    will need to be changed. This code has raised a ``FutureWarning`` since ||    Numpy 1.12. ||  ||    The following is a recommended fix; which will behave identically in Numpy ||    1.15 and Numpy 1.16:: ||  ||     >>> from numpy.lib.recfunctions import repack_fields ||     >>> repack_fields(a[['a';'c']]).view('i8')  # supported 1.15 and 1.16 ||     array([0; 0; 0]) ||  || Assigning to an array with a multi-field index will behave the same in Numpy || 1.15 and Numpy 1.16. In both versions the assignment will modify the original || array:: ||  ||  >>> a[['a'; 'c']] = (2; 3) ||  >>> a ||  array([(2; 0; 3.0); (2; 0; 3.0); (2; 0; 3.0)]; ||        dtype=[('a'; '<i8'); ('b'; '<i4'); ('c'; '<f8')]) ||  || This obeys the structured array assignment rules described above. For example; || this means that one can swap the values of two fields using appropriate || multi-field indexes:: ||  ||  >>> a[['a'; 'c']] = a[['c'; 'a']] ||  || Indexing with an Integer to get a Structured Scalar || ``````````````````````````````````````````````````` ||  || Indexing a single element of a structured array (with an integer index) returns || a structured scalar:: ||  ||  >>> x = np.array([(1; 2.; 3.)]; dtype='i;f;f') ||  >>> scalar = x[0] ||  >>> scalar ||  (1; 2.; 3.) ||  >>> type(scalar) ||  numpy.void ||  || Unlike other numpy scalars; structured scalars are mutable and act like views || into the original array; such that modifying the scalar will modify the || original array. Structured scalars also support access and assignment by field || name:: ||  ||  >>> x = np.array([(1;2);(3;4)]; dtype=[('foo'; 'i8'); ('bar'; 'f4')]) ||  >>> s = x[0] ||  >>> s['bar'] = 100 ||  >>> x ||  array([(1; 100.); (3; 4.)]; ||        dtype=[('foo'; '<i8'); ('bar'; '<f4')]) ||  || Similarly to tuples; structured scalars can also be indexed with an integer:: ||  ||  >>> scalar = np.array([(1; 2.; 3.)]; dtype='i;f;f')[0] ||  >>> scalar[0] ||  1 ||  >>> scalar[1] = 4 ||  || Thus; tuples might be thought of as the native Python equivalent to numpy's || structured types; much like native python integers are the equivalent to || numpy's integer types. Structured scalars may be converted to a tuple by || calling :func:`ndarray.item`:: ||  ||  >>> scalar.item(); type(scalar.item()) ||  ((1; 2.0; 3.0); tuple) ||  || Viewing Structured Arrays Containing Objects || -------------------------------------------- ||  || In order to prevent clobbering object pointers in fields of || :class:`numpy.object` type; numpy currently does not allow views of structured || arrays containing objects. ||  || Structure Comparison || -------------------- ||  || If the dtypes of two void structured arrays are equal; testing the equality of || the arrays will result in a boolean array with the dimensions of the original || arrays; with elements set to ``True`` where all fields of the corresponding || structures are equal. Structured dtypes are equal if the field names; || dtypes and titles are the same; ignoring endianness; and the fields are in || the same order:: ||  ||  >>> a = np.zeros(2; dtype=[('a'; 'i4'); ('b'; 'i4')]) ||  >>> b = np.ones(2; dtype=[('a'; 'i4'); ('b'; 'i4')]) ||  >>> a == b ||  array([False; False]) ||  || Currently; if the dtypes of two void structured arrays are not equivalent the || comparison fails; returning the scalar value ``False``. This behavior is || deprecated as of numpy 1.10 and will raise an error or perform elementwise || comparison in the future. ||  || The ``<`` and ``>`` operators always return ``False`` when comparing void || structured arrays; and arithmetic and bitwise operations are not supported. ||  || Record Arrays || ============= ||  || As an optional convenience numpy provides an ndarray subclass; || :class:`numpy.recarray`; and associated helper functions in the || :mod:`numpy.rec` submodule; that allows access to fields of structured arrays || by attribute instead of only by index. Record arrays also use a special || datatype; :class:`numpy.record`; that allows field access by attribute on the || structured scalars obtained from the array. ||  || The simplest way to create a record array is with :func:`numpy.rec.array`:: ||  ||  >>> recordarr = np.rec.array([(1;2.;'Hello');(2;3.;\""World\"")]; ||  ...                    dtype=[('foo'; 'i4');('bar'; 'f4'); ('baz'; 'S10')]) ||  >>> recordarr.bar ||  array([ 2.;  3.]; dtype=float32) ||  >>> recordarr[1:2] ||  rec.array([(2; 3.0; 'World')]; ||        dtype=[('foo'; '<i4'); ('bar'; '<f4'); ('baz'; 'S10')]) ||  >>> recordarr[1:2].foo ||  array([2]; dtype=int32) ||  >>> recordarr.foo[1:2] ||  array([2]; dtype=int32) ||  >>> recordarr[1].baz ||  'World' ||  || :func:`numpy.rec.array` can convert a wide variety of arguments into record || arrays; including structured arrays:: ||  ||  >>> arr = array([(1;2.;'Hello');(2;3.;\""World\"")]; ||  ...             dtype=[('foo'; 'i4'); ('bar'; 'f4'); ('baz'; 'S10')]) ||  >>> recordarr = np.rec.array(arr) ||  || The :mod:`numpy.rec` module provides a number of other convenience functions for || creating record arrays; see :ref:`record array creation routines || <routines.array-creation.rec>`. ||  || A record array representation of a structured array can be obtained using the || appropriate :ref:`view`:: ||  ||  >>> arr = np.array([(1;2.;'Hello');(2;3.;\""World\"")]; ||  ...                dtype=[('foo'; 'i4');('bar'; 'f4'); ('baz'; 'a10')]) ||  >>> recordarr = arr.view(dtype=dtype((np.record; arr.dtype)); ||  ...                      type=np.recarray) ||  || For convenience; viewing an ndarray as type :class:`np.recarray` will || automatically convert to :class:`np.record` datatype; so the dtype can be left || out of the view:: ||  ||  >>> recordarr = arr.view(np.recarray) ||  >>> recordarr.dtype ||  dtype((numpy.record; [('foo'; '<i4'); ('bar'; '<f4'); ('baz'; 'S10')])) ||  || To get back to a plain ndarray both the dtype and type must be reset. The || following view does so; taking into account the unusual case that the || recordarr was not a structured type:: ||  ||  >>> arr2 = recordarr.view(recordarr.dtype.fields or recordarr.dtype; np.ndarray) ||  || Record array fields accessed by index or by attribute are returned as a record || array if the field has a structured type but as a plain ndarray otherwise. :: ||  ||  >>> recordarr = np.rec.array([('Hello'; (1;2));(\""World\""; (3;4))]; ||  ...                 dtype=[('foo'; 'S6');('bar'; [('A'; int); ('B'; int)])]) ||  >>> type(recordarr.foo) ||  <type 'numpy.ndarray'> ||  >>> type(recordarr.bar) ||  <class 'numpy.core.records.recarray'> ||  || Note that if a field has the same name as an ndarray attribute; the ndarray || attribute takes precedence. Such fields will be inaccessible by attribute but || will still be accessible by index. ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4202,ryfeus/gcf-packs,pandas_numpy/sources/numpy/doc/subclassing.py,255a05a5980efb8b096c283d79872d0695886161,"\""\""\""============================= || Subclassing ndarray in python || ============================= ||  || Introduction || ------------ ||  || Subclassing ndarray is relatively simple; but it has some complications || compared to other Python objects.  On this page we explain the machinery || that allows you to subclass ndarray; and the implications for || implementing a subclass. ||  || ndarrays and object creation || ============================ ||  || Subclassing ndarray is complicated by the fact that new instances of || ndarray classes can come about in three different ways.  These are: ||  || #. Explicit constructor call - as in ``MySubClass(params)``.  This is ||    the usual route to Python instance creation. || #. View casting - casting an existing ndarray as a given subclass || #. New from template - creating a new instance from a template ||    instance. Examples include returning slices from a subclassed array; ||    creating return types from ufuncs; and copying arrays.  See ||    :ref:`new-from-template` for more details ||  || The last two are characteristics of ndarrays - in order to support || things like array slicing.  The complications of subclassing ndarray are || due to the mechanisms numpy has to support these latter two routes of || instance creation. ||  || .. _view-casting: ||  || View casting || ------------ ||  || *View casting* is the standard ndarray mechanism by which you take an || ndarray of any subclass; and return a view of the array as another || (specified) subclass: ||  || >>> import numpy as np || >>> # create a completely useless ndarray subclass || >>> class C(np.ndarray): pass || >>> # create a standard ndarray || >>> arr = np.zeros((3;)) || >>> # take a view of it; as our useless subclass || >>> c_arr = arr.view(C) || >>> type(c_arr) || <class 'C'> ||  || .. _new-from-template: ||  || Creating new from template || -------------------------- ||  || New instances of an ndarray subclass can also come about by a very || similar mechanism to :ref:`view-casting`; when numpy finds it needs to || create a new instance from a template instance.  The most obvious place || this has to happen is when you are taking slices of subclassed arrays. || For example: ||  || >>> v = c_arr[1:] || >>> type(v) # the view is of type 'C' || <class 'C'> || >>> v is c_arr # but it's a new instance || False ||  || The slice is a *view* onto the original ``c_arr`` data.  So; when we || take a view from the ndarray; we return a new ndarray; of the same || class; that points to the data in the original. ||  || There are other points in the use of ndarrays where we need such views; || such as copying arrays (``c_arr.copy()``); creating ufunc output arrays || (see also :ref:`array-wrap`); and reducing methods (like || ``c_arr.mean()``. ||  || Relationship of view casting and new-from-template || -------------------------------------------------- ||  || These paths both use the same machinery.  We make the distinction here; || because they result in different input to your methods.  Specifically; || :ref:`view-casting` means you have created a new instance of your array || type from any potential subclass of ndarray.  :ref:`new-from-template` || means you have created a new instance of your class from a pre-existing || instance; allowing you - for example - to copy across attributes that || are particular to your subclass. ||  || Implications for subclassing || ---------------------------- ||  || If we subclass ndarray; we need to deal not only with explicit || construction of our array type; but also :ref:`view-casting` or || :ref:`new-from-template`.  NumPy has the machinery to do this; and this || machinery that makes subclassing slightly non-standard. ||  || There are two aspects to the machinery that ndarray uses to support || views and new-from-template in subclasses. ||  || The first is the use of the ``ndarray.__new__`` method for the main work || of object initialization; rather then the more usual ``__init__`` || method.  The second is the use of the ``__array_finalize__`` method to || allow subclasses to clean up after the creation of views and new || instances from templates. ||  || A brief Python primer on ``__new__`` and ``__init__`` || ===================================================== ||  || ``__new__`` is a standard Python method; and; if present; is called || before ``__init__`` when we create a class instance. See the `python || __new__ documentation || <http:\/\/docs.python.org\/reference\/datamodel.html#object.__new__>`_ for more detail. ||  || For example; consider the following Python code: ||  || .. testcode:: ||  ||   class C(object): ||       def __new__(cls; *args): ||           print('Cls in __new__:'; cls) ||           print('Args in __new__:'; args) ||           return object.__new__(cls; *args) ||  ||       def __init__(self; *args): ||           print('type(self) in __init__:'; type(self)) ||           print('Args in __init__:'; args) ||  || meaning that we get: ||  || >>> c = C('hello') || Cls in __new__: <class 'C'> || Args in __new__: ('hello';) || type(self) in __init__: <class 'C'> || Args in __init__: ('hello';) ||  || When we call ``C('hello')``; the ``__new__`` method gets its own class || as first argument; and the passed argument; which is the string || ``'hello'``.  After python calls ``__new__``; it usually (see below) || calls our ``__init__`` method; with the output of ``__new__`` as the || first argument (now a class instance); and the passed arguments || following. ||  || As you can see; the object can be initialized in the ``__new__`` || method or the ``__init__`` method; or both; and in fact ndarray does || not have an ``__init__`` method; because all the initialization is || done in the ``__new__`` method. ||  || Why use ``__new__`` rather than just the usual ``__init__``?  Because || in some cases; as for ndarray; we want to be able to return an object || of some other class.  Consider the following: ||  || .. testcode:: ||  ||   class D(C): ||       def __new__(cls; *args): ||           print('D cls is:'; cls) ||           print('D args in __new__:'; args) ||           return C.__new__(C; *args) ||  ||       def __init__(self; *args): ||           # we never get here ||           print('In D __init__') ||  || meaning that: ||  || >>> obj = D('hello') || D cls is: <class 'D'> || D args in __new__: ('hello';) || Cls in __new__: <class 'C'> || Args in __new__: ('hello';) || >>> type(obj) || <class 'C'> ||  || The definition of ``C`` is the same as before; but for ``D``; the || ``__new__`` method returns an instance of class ``C`` rather than || ``D``.  Note that the ``__init__`` method of ``D`` does not get || called.  In general; when the ``__new__`` method returns an object of || class other than the class in which it is defined; the ``__init__`` || method of that class is not called. ||  || This is how subclasses of the ndarray class are able to return views || that preserve the class type.  When taking a view; the standard || ndarray machinery creates the new ndarray object with something || like:: ||  ||   obj = ndarray.__new__(subtype; shape; ... ||  || where ``subdtype`` is the subclass.  Thus the returned view is of the || same class as the subclass; rather than being of class ``ndarray``. ||  || That solves the problem of returning views of the same type; but now || we have a new problem.  The machinery of ndarray can set the class || this way; in its standard methods for taking views; but the ndarray || ``__new__`` method knows nothing of what we have done in our own || ``__new__`` method in order to set attributes; and so on.  (Aside - || why not call ``obj = subdtype.__new__(...`` then?  Because we may not || have a ``__new__`` method with the same call signature). ||  || The role of ``__array_finalize__`` || ================================== ||  || ``__array_finalize__`` is the mechanism that numpy provides to allow || subclasses to handle the various ways that new instances get created. ||  || Remember that subclass instances can come about in these three ways: ||  || #. explicit constructor call (``obj = MySubClass(params)``).  This will ||    call the usual sequence of ``MySubClass.__new__`` then (if it exists) ||    ``MySubClass.__init__``. || #. :ref:`view-casting` || #. :ref:`new-from-template` ||  || Our ``MySubClass.__new__`` method only gets called in the case of the || explicit constructor call; so we can't rely on ``MySubClass.__new__`` or || ``MySubClass.__init__`` to deal with the view casting and || new-from-template.  It turns out that ``MySubClass.__array_finalize__`` || *does* get called for all three methods of object creation; so this is || where our object creation housekeeping usually goes. ||  || * For the explicit constructor call; our subclass will need to create a ||   new ndarray instance of its own class.  In practice this means that ||   we; the authors of the code; will need to make a call to ||   ``ndarray.__new__(MySubClass;...)``; a class-hierarchy prepared call to ||   ``super(MySubClass; cls).__new__(cls; ...)``; or do view casting of an ||   existing array (see below) || * For view casting and new-from-template; the equivalent of ||   ``ndarray.__new__(MySubClass;...`` is called; at the C level. ||  || The arguments that ``__array_finalize__`` receives differ for the three || methods of instance creation above. ||  || The following code allows us to look at the call sequences and arguments: ||  || .. testcode:: ||  ||    import numpy as np ||  ||    class C(np.ndarray): ||        def __new__(cls; *args; **kwargs): ||            print('In __new__ with class %s' % cls) ||            return super(C; cls).__new__(cls; *args; **kwargs) ||  ||        def __init__(self; *args; **kwargs): ||            # in practice you probably will not need or want an __init__ ||            # method for your subclass ||            print('In __init__ with class %s' % self.__class__) ||  ||        def __array_finalize__(self; obj): ||            print('In array_finalize:') ||            print('   self type is %s' % type(self)) ||            print('   obj type is %s' % type(obj)) ||  ||  || Now: ||  || >>> # Explicit constructor || >>> c = C((10;)) || In __new__ with class <class 'C'> || In array_finalize: ||    self type is <class 'C'> ||    obj type is <type 'NoneType'> || In __init__ with class <class 'C'> || >>> # View casting || >>> a = np.arange(10) || >>> cast_a = a.view(C) || In array_finalize: ||    self type is <class 'C'> ||    obj type is <type 'numpy.ndarray'> || >>> # Slicing (example of new-from-template) || >>> cv = c[:1] || In array_finalize: ||    self type is <class 'C'> ||    obj type is <class 'C'> ||  || The signature of ``__array_finalize__`` is:: ||  ||     def __array_finalize__(self; obj): ||  || One sees that the ``super`` call; which goes to || ``ndarray.__new__``; passes ``__array_finalize__`` the new object; of our || own class (``self``) as well as the object from which the view has been || taken (``obj``).  As you can see from the output above; the ``self`` is || always a newly created instance of our subclass; and the type of ``obj`` || differs for the three instance creation methods: ||  || * When called from the explicit constructor; ``obj`` is ``None`` || * When called from view casting; ``obj`` can be an instance of any ||   subclass of ndarray; including our own. || * When called in new-from-template; ``obj`` is another instance of our ||   own subclass; that we might use to update the new ``self`` instance. ||  || Because ``__array_finalize__`` is the only method that always sees new || instances being created; it is the sensible place to fill in instance || defaults for new object attributes; among other tasks. ||  || This may be clearer with an example. ||  || Simple example - adding an extra attribute to ndarray || ----------------------------------------------------- ||  || .. testcode:: ||  ||   import numpy as np ||  ||   class InfoArray(np.ndarray): ||  ||       def __new__(subtype; shape; dtype=float; buffer=None; offset=0; ||                   strides=None; order=None; info=None): ||           # Create the ndarray instance of our type; given the usual ||           # ndarray input arguments.  This will call the standard ||           # ndarray constructor; but return an object of our type. ||           # It also triggers a call to InfoArray.__array_finalize__ ||           obj = super(InfoArray; subtype).__new__(subtype; shape; dtype; ||                                                   buffer; offset; strides; ||                                                   order) ||           # set the new 'info' attribute to the value passed ||           obj.info = info ||           # Finally; we must return the newly created object: ||           return obj ||  ||       def __array_finalize__(self; obj): ||           # ``self`` is a new object resulting from ||           # ndarray.__new__(InfoArray; ...); therefore it only has ||           # attributes that the ndarray.__new__ constructor gave it - ||           # i.e. those of a standard ndarray. ||           # ||           # We could have got to the ndarray.__new__ call in 3 ways: ||           # From an explicit constructor - e.g. InfoArray(): ||           #    obj is None ||           #    (we're in the middle of the InfoArray.__new__ ||           #    constructor; and self.info will be set when we return to ||           #    InfoArray.__new__) ||           if obj is None: return ||           # From view casting - e.g arr.view(InfoArray): ||           #    obj is arr ||           #    (type(obj) can be InfoArray) ||           # From new-from-template - e.g infoarr[:3] ||           #    type(obj) is InfoArray ||           # ||           # Note that it is here; rather than in the __new__ method; ||           # that we set the default value for 'info'; because this ||           # method sees all creation of default objects - with the ||           # InfoArray.__new__ constructor; but also with ||           # arr.view(InfoArray). ||           self.info = getattr(obj; 'info'; None) ||           # We do not need to return anything ||  ||  || Using the object looks like this: ||  ||   >>> obj = InfoArray(shape=(3;)) # explicit constructor ||   >>> type(obj) ||   <class 'InfoArray'> ||   >>> obj.info is None ||   True ||   >>> obj = InfoArray(shape=(3;); info='information') ||   >>> obj.info ||   'information' ||   >>> v = obj[1:] # new-from-template - here - slicing ||   >>> type(v) ||   <class 'InfoArray'> ||   >>> v.info ||   'information' ||   >>> arr = np.arange(10) ||   >>> cast_arr = arr.view(InfoArray) # view casting ||   >>> type(cast_arr) ||   <class 'InfoArray'> ||   >>> cast_arr.info is None ||   True ||  || This class isn't very useful; because it has the same constructor as the || bare ndarray object; including passing in buffers and shapes and so on. || We would probably prefer the constructor to be able to take an already || formed ndarray from the usual numpy calls to ``np.array`` and return an || object. ||  || Slightly more realistic example - attribute added to existing array || ------------------------------------------------------------------- ||  || Here is a class that takes a standard ndarray that already exists; casts || as our type; and adds an extra attribute. ||  || .. testcode:: ||  ||   import numpy as np ||  ||   class RealisticInfoArray(np.ndarray): ||  ||       def __new__(cls; input_array; info=None): ||           # Input array is an already formed ndarray instance ||           # We first cast to be our class type ||           obj = np.asarray(input_array).view(cls) ||           # add the new attribute to the created instance ||           obj.info = info ||           # Finally; we must return the newly created object: ||           return obj ||  ||       def __array_finalize__(self; obj): ||           # see InfoArray.__array_finalize__ for comments ||           if obj is None: return ||           self.info = getattr(obj; 'info'; None) ||  ||  || So: ||  ||   >>> arr = np.arange(5) ||   >>> obj = RealisticInfoArray(arr; info='information') ||   >>> type(obj) ||   <class 'RealisticInfoArray'> ||   >>> obj.info ||   'information' ||   >>> v = obj[1:] ||   >>> type(v) ||   <class 'RealisticInfoArray'> ||   >>> v.info ||   'information' ||  || .. _array-ufunc: ||  || ``__array_ufunc__`` for ufuncs || ------------------------------ ||  ||   .. versionadded:: 1.13 ||  || A subclass can override what happens when executing numpy ufuncs on it by || overriding the default ``ndarray.__array_ufunc__`` method. This method is || executed *instead* of the ufunc and should return either the result of the || operation; or :obj:`NotImplemented` if the operation requested is not || implemented. ||  || The signature of ``__array_ufunc__`` is:: ||  ||     def __array_ufunc__(ufunc; method; *inputs; **kwargs): ||  ||     - *ufunc* is the ufunc object that was called. ||     - *method* is a string indicating how the Ufunc was called; either ||       ``\""__call__\""`` to indicate it was called directly; or one of its ||       :ref:`methods<ufuncs.methods>`: ``\""reduce\""``; ``\""accumulate\""``; ||       ``\""reduceat\""``; ``\""outer\""``; or ``\""at\""``. ||     - *inputs* is a tuple of the input arguments to the ``ufunc`` ||     - *kwargs* contains any optional or keyword arguments passed to the ||       function. This includes any ``out`` arguments; which are always ||       contained in a tuple. ||  || A typical implementation would convert any inputs or outputs that are || instances of one's own class; pass everything on to a superclass using || ``super()``; and finally return the results after possible || back-conversion. An example; taken from the test case || ``test_ufunc_override_with_super`` in ``core\/tests\/test_umath.py``; is the || following. ||  || .. testcode:: ||  ||     input numpy as np ||  ||     class A(np.ndarray): ||         def __array_ufunc__(self; ufunc; method; *inputs; **kwargs): ||             args = [] ||             in_no = [] ||             for i; input_ in enumerate(inputs): ||                 if isinstance(input_; A): ||                     in_no.append(i) ||                     args.append(input_.view(np.ndarray)) ||                 else: ||                     args.append(input_) ||  ||             outputs = kwargs.pop('out'; None) ||             out_no = [] ||             if outputs: ||                 out_args = [] ||                 for j; output in enumerate(outputs): ||                     if isinstance(output; A): ||                         out_no.append(j) ||                         out_args.append(output.view(np.ndarray)) ||                     else: ||                         out_args.append(output) ||                 kwargs['out'] = tuple(out_args) ||             else: ||                 outputs = (None;) * ufunc.nout ||  ||             info = {} ||             if in_no: ||                 info['inputs'] = in_no ||             if out_no: ||                 info['outputs'] = out_no ||  ||             results = super(A; self).__array_ufunc__(ufunc; method; ||                                                      *args; **kwargs) ||             if results is NotImplemented: ||                 return NotImplemented ||  ||             if method == 'at': ||                 if isinstance(inputs[0]; A): ||                     inputs[0].info = info ||                 return ||  ||             if ufunc.nout == 1: ||                 results = (results;) ||  ||             results = tuple((np.asarray(result).view(A) ||                              if output is None else output) ||                             for result; output in zip(results; outputs)) ||             if results and isinstance(results[0]; A): ||                 results[0].info = info ||  ||             return results[0] if len(results) == 1 else results ||  || So; this class does not actually do anything interesting: it just || converts any instances of its own to regular ndarray (otherwise; we'd || get infinite recursion!); and adds an ``info`` dictionary that tells || which inputs and outputs it converted. Hence; e.g.; ||  || >>> a = np.arange(5.).view(A) || >>> b = np.sin(a) || >>> b.info || {'inputs': [0]} || >>> b = np.sin(np.arange(5.); out=(a;)) || >>> b.info || {'outputs': [0]} || >>> a = np.arange(5.).view(A) || >>> b = np.ones(1).view(A) || >>> c = a + b || >>> c.info || {'inputs': [0; 1]} || >>> a += b || >>> a.info || {'inputs': [0; 1]; 'outputs': [0]} ||  || Note that another approach would be to to use ``getattr(ufunc; || methods)(*inputs; **kwargs)`` instead of the ``super`` call. For this example; || the result would be identical; but there is a difference if another operand || also defines ``__array_ufunc__``. E.g.; lets assume that we evalulate || ``np.add(a; b)``; where ``b`` is an instance of another class ``B`` that has || an override.  If you use ``super`` as in the example; || ``ndarray.__array_ufunc__`` will notice that ``b`` has an override; which || means it cannot evaluate the result itself. Thus; it will return || `NotImplemented` and so will our class ``A``. Then; control will be passed || over to ``b``; which either knows how to deal with us and produces a result; || or does not and returns `NotImplemented`; raising a ``TypeError``. ||  || If instead; we replace our ``super`` call with ``getattr(ufunc; method)``; we || effectively do ``np.add(a.view(np.ndarray); b)``. Again; ``B.__array_ufunc__`` || will be called; but now it sees an ``ndarray`` as the other argument. Likely; || it will know how to handle this; and return a new instance of the ``B`` class || to us. Our example class is not set up to handle this; but it might well be || the best approach if; e.g.; one were to re-implement ``MaskedArray`` using || ``__array_ufunc__``. ||  || As a final note: if the ``super`` route is suited to a given class; an || advantage of using it is that it helps in constructing class hierarchies. || E.g.; suppose that our other class ``B`` also used the ``super`` in its || ``__array_ufunc__`` implementation; and we created a class ``C`` that depended || on both; i.e.; ``class C(A; B)`` (with; for simplicity; not another || ``__array_ufunc__`` override). Then any ufunc on an instance of ``C`` would || pass on to ``A.__array_ufunc__``; the ``super`` call in ``A`` would go to || ``B.__array_ufunc__``; and the ``super`` call in ``B`` would go to || ``ndarray.__array_ufunc__``; thus allowing ``A`` and ``B`` to collaborate. ||  || .. _array-wrap: ||  || ``__array_wrap__`` for ufuncs and other functions || ------------------------------------------------- ||  || Prior to numpy 1.13; the behaviour of ufuncs could only be tuned using || ``__array_wrap__`` and ``__array_prepare__``. These two allowed one to || change the output type of a ufunc; but; in contrast to || ``__array_ufunc__``; did not allow one to make any changes to the inputs. || It is hoped to eventually deprecate these; but ``__array_wrap__`` is also || used by other numpy functions and methods; such as ``squeeze``; so at the || present time is still needed for full functionality. ||  || Conceptually; ``__array_wrap__`` \""wraps up the action\"" in the sense of || allowing a subclass to set the type of the return value and update || attributes and metadata.  Let's show how this works with an example.  First || we return to the simpler example subclass; but with a different name and || some print statements: ||  || .. testcode:: ||  ||   import numpy as np ||  ||   class MySubClass(np.ndarray): ||  ||       def __new__(cls; input_array; info=None): ||           obj = np.asarray(input_array).view(cls) ||           obj.info = info ||           return obj ||  ||       def __array_finalize__(self; obj): ||           print('In __array_finalize__:') ||           print('   self is %s' % repr(self)) ||           print('   obj is %s' % repr(obj)) ||           if obj is None: return ||           self.info = getattr(obj; 'info'; None) ||  ||       def __array_wrap__(self; out_arr; context=None): ||           print('In __array_wrap__:') ||           print('   self is %s' % repr(self)) ||           print('   arr is %s' % repr(out_arr)) ||           # then just call the parent ||           return super(MySubClass; self).__array_wrap__(self; out_arr; context) ||  || We run a ufunc on an instance of our new array: ||  || >>> obj = MySubClass(np.arange(5); info='spam') || In __array_finalize__: ||    self is MySubClass([0; 1; 2; 3; 4]) ||    obj is array([0; 1; 2; 3; 4]) || >>> arr2 = np.arange(5)+1 || >>> ret = np.add(arr2; obj) || In __array_wrap__: ||    self is MySubClass([0; 1; 2; 3; 4]) ||    arr is array([1; 3; 5; 7; 9]) || In __array_finalize__: ||    self is MySubClass([1; 3; 5; 7; 9]) ||    obj is MySubClass([0; 1; 2; 3; 4]) || >>> ret || MySubClass([1; 3; 5; 7; 9]) || >>> ret.info || 'spam' ||  || Note that the ufunc (``np.add``) has called the ``__array_wrap__`` method || with arguments ``self`` as ``obj``; and ``out_arr`` as the (ndarray) result || of the addition.  In turn; the default ``__array_wrap__`` || (``ndarray.__array_wrap__``) has cast the result to class ``MySubClass``; || and called ``__array_finalize__`` - hence the copying of the ``info`` || attribute.  This has all happened at the C level. ||  || But; we could do anything we wanted: ||  || .. testcode:: ||  ||   class SillySubClass(np.ndarray): ||  ||       def __array_wrap__(self; arr; context=None): ||           return 'I lost your data' ||  || >>> arr1 = np.arange(5) || >>> obj = arr1.view(SillySubClass) || >>> arr2 = np.arange(5) || >>> ret = np.multiply(obj; arr2) || >>> ret || 'I lost your data' ||  || So; by defining a specific ``__array_wrap__`` method for our subclass; || we can tweak the output from ufuncs. The ``__array_wrap__`` method || requires ``self``; then an argument - which is the result of the ufunc - || and an optional parameter *context*. This parameter is returned by || ufuncs as a 3-element tuple: (name of the ufunc; arguments of the ufunc; || domain of the ufunc); but is not set by other numpy functions. Though; || as seen above; it is possible to do otherwise; ``__array_wrap__`` should || return an instance of its containing class.  See the masked array || subclass for an implementation. ||  || In addition to ``__array_wrap__``; which is called on the way out of the || ufunc; there is also an ``__array_prepare__`` method which is called on || the way into the ufunc; after the output arrays are created but before any || computation has been performed. The default implementation does nothing || but pass through the array. ``__array_prepare__`` should not attempt to || access the array data or resize the array; it is intended for setting the || output array type; updating attributes and metadata; and performing any || checks based on the input that may be desired before computation begins. || Like ``__array_wrap__``; ``__array_prepare__`` must return an ndarray or || subclass thereof or raise an error. ||  || Extra gotchas - custom ``__del__`` methods and ndarray.base || ----------------------------------------------------------- ||  || One of the problems that ndarray solves is keeping track of memory || ownership of ndarrays and their views.  Consider the case where we have || created an ndarray; ``arr`` and have taken a slice with ``v = arr[1:]``. || The two objects are looking at the same memory.  NumPy keeps track of || where the data came from for a particular array or view; with the || ``base`` attribute: ||  || >>> # A normal ndarray; that owns its own data || >>> arr = np.zeros((4;)) || >>> # In this case; base is None || >>> arr.base is None || True || >>> # We take a view || >>> v1 = arr[1:] || >>> # base now points to the array that it derived from || >>> v1.base is arr || True || >>> # Take a view of a view || >>> v2 = v1[1:] || >>> # base points to the view it derived from || >>> v2.base is v1 || True ||  || In general; if the array owns its own memory; as for ``arr`` in this || case; then ``arr.base`` will be None - there are some exceptions to this || - see the numpy book for more details. ||  || The ``base`` attribute is useful in being able to tell whether we have || a view or the original array.  This in turn can be useful if we need || to know whether or not to do some specific cleanup when the subclassed || array is deleted.  For example; we may only want to do the cleanup if || the original array is deleted; but not the views.  For an example of || how this can work; have a look at the ``memmap`` class in || ``numpy.core``. ||  || Subclassing and Downstream Compatibility || ---------------------------------------- ||  || When sub-classing ``ndarray`` or creating duck-types that mimic the ``ndarray`` || interface; it is your responsibility to decide how aligned your APIs will be || with those of numpy. For convenience; many numpy functions that have a corresponding || ``ndarray`` method (e.g.; ``sum``; ``mean``; ``take``; ``reshape``) work by checking || if the first argument to a function has a method of the same name. If it exists; the || method is called instead of coercing the arguments to a numpy array. ||  || For example; if you want your sub-class or duck-type to be compatible with || numpy's ``sum`` function; the method signature for this object's ``sum`` method || should be the following: ||  || .. testcode:: ||  ||     def sum(self; axis=None; dtype=None; out=None; keepdims=False): ||     ... ||  || This is the exact same method signature for ``np.sum``; so now if a user calls || ``np.sum`` on this object; numpy will call the object's own ``sum`` method and || pass in these arguments enumerated above in the signature; and no errors will || be raised because the signatures are completely compatible with each other. ||  || If; however; you decide to deviate from this signature and do something like this: ||  || .. testcode:: ||  ||    def sum(self; axis=None; dtype=None): ||    ... ||  || This object is no longer compatible with ``np.sum`` because if you call ``np.sum``; || it will pass in unexpected arguments ``out`` and ``keepdims``; causing a TypeError || to be raised. ||  || If you wish to maintain compatibility with numpy and its subsequent versions (which || might add new keyword arguments) but do not want to surface all of numpy's arguments; || your function's signature should accept ``**kwargs``. For example: ||  || .. testcode:: ||  ||    def sum(self; axis=None; dtype=None; **unused_kwargs): ||    ... ||  || This object is now compatible with ``np.sum`` again because any extraneous arguments || (i.e. keywords that are not ``axis`` or ``dtype``) will be hidden away in the || ``**unused_kwargs`` parameter. ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4203,ryfeus/gcf-packs,pandas_numpy/sources/numpy/f2py/crackfortran.py,255a05a5980efb8b096c283d79872d0695886161,XXX: non-zero reset values need testing,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4204,ryfeus/gcf-packs,pandas_numpy/sources/numpy/f2py/crackfortran.py,255a05a5980efb8b096c283d79872d0695886161,TODO: test .eq.; .neq.; etc replacements.,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4205,ryfeus/gcf-packs,pandas_numpy/sources/numpy/lib/info.py,255a05a5980efb8b096c283d79872d0695886161,"\""\""\"" || Basic functions used by several sub-packages and || useful to have in the main name-space. ||  || Type Handling || ------------- || ================ =================== || iscomplexobj     Test for complex object; scalar result || isrealobj        Test for real object; scalar result || iscomplex        Test for complex elements; array result || isreal           Test for real elements; array result || imag             Imaginary part || real             Real part || real_if_close    Turns complex number with tiny imaginary part to real || isneginf         Tests for negative infinity; array result || isposinf         Tests for positive infinity; array result || isnan            Tests for nans; array result || isinf            Tests for infinity; array result || isfinite         Tests for finite numbers; array result || isscalar         True if argument is a scalar || nan_to_num       Replaces NaN's with 0 and infinities with large numbers || cast             Dictionary of functions to force cast to each type || common_type      Determine the minimum common type code for a group ||                  of arrays || mintypecode      Return minimal allowed common typecode. || ================ =================== ||  || Index Tricks || ------------ || ================ =================== || mgrid            Method which allows easy construction of N-d ||                  'mesh-grids' || ``r_``           Append and construct arrays: turns slice objects into ||                  ranges and concatenates them; for 2d arrays appends rows. || index_exp        Konrad Hinsen's index_expression class instance which ||                  can be useful for building complicated slicing syntax. || ================ =================== ||  || Useful Functions || ---------------- || ================ =================== || select           Extension of where to multiple conditions and choices || extract          Extract 1d array from flattened array according to mask || insert           Insert 1d array of values into Nd array according to mask || linspace         Evenly spaced samples in linear space || logspace         Evenly spaced samples in logarithmic space || fix              Round x to nearest integer towards zero || mod              Modulo mod(x;y) = x % y except keeps sign of y || amax             Array maximum along axis || amin             Array minimum along axis || ptp              Array max-min along axis || cumsum           Cumulative sum along axis || prod             Product of elements along axis || cumprod          Cumluative product along axis || diff             Discrete differences along axis || angle            Returns angle of complex argument || unwrap           Unwrap phase along given axis (1-d algorithm) || sort_complex     Sort a complex-array (based on real; then imaginary) || trim_zeros       Trim the leading and trailing zeros from 1D array. || vectorize        A class that wraps a Python function taking scalar ||                  arguments into a generalized function which can handle ||                  arrays of arguments using the broadcast rules of ||                  numerix Python. || ================ =================== ||  || Shape Manipulation || ------------------ || ================ =================== || squeeze          Return a with length-one dimensions removed. || atleast_1d       Force arrays to be >= 1D || atleast_2d       Force arrays to be >= 2D || atleast_3d       Force arrays to be >= 3D || vstack           Stack arrays vertically (row on row) || hstack           Stack arrays horizontally (column on column) || column_stack     Stack 1D arrays as columns into 2D array || dstack           Stack arrays depthwise (along third dimension) || stack            Stack arrays along a new axis || split            Divide array into a list of sub-arrays || hsplit           Split into columns || vsplit           Split into rows || dsplit           Split along third dimension || ================ =================== ||  || Matrix (2D Array) Manipulations || ------------------------------- || ================ =================== || fliplr           2D array with columns flipped || flipud           2D array with rows flipped || rot90            Rotate a 2D array a multiple of 90 degrees || eye              Return a 2D array with ones down a given diagonal || diag             Construct a 2D array from a vector; or return a given ||                  diagonal from a 2D array. || mat              Construct a Matrix || bmat             Build a Matrix from blocks || ================ =================== ||  || Polynomials || ----------- || ================ =================== || poly1d           A one-dimensional polynomial class || poly             Return polynomial coefficients from roots || roots            Find roots of polynomial given coefficients || polyint          Integrate polynomial || polyder          Differentiate polynomial || polyadd          Add polynomials || polysub          Subtract polynomials || polymul          Multiply polynomials || polydiv          Divide polynomials || polyval          Evaluate polynomial at given argument || ================ =================== ||  || Iterators || --------- || ================ =================== || Arrayterator     A buffered iterator for big arrays. || ================ =================== ||  || Import Tricks || ------------- || ================ =================== || ppimport         Postpone module import until trying to use it || ppimport_attr    Postpone module import until trying to use its attribute || ppresolve        Import postponed module and return it. || ================ =================== ||  || Machine Arithmetics || ------------------- || ================ =================== || machar_single    Single precision floating point arithmetic parameters || machar_double    Double precision floating point arithmetic parameters || ================ =================== ||  || Threading Tricks || ---------------- || ================ =================== || ParallelExec     Execute commands in parallel thread. || ================ =================== ||  || Array Set Operations || ----------------------- || Set operations for numeric arrays based on sort() function. ||  || ================ =================== || unique           Unique elements of an array. || isin             Test whether each element of an ND array is present  ||                  anywhere within a second array. || ediff1d          Array difference (auxiliary function). || intersect1d      Intersection of 1D arrays with unique elements. || setxor1d         Set exclusive-or of 1D arrays with unique elements. || in1d             Test whether elements in a 1D array are also present in ||                  another array. || union1d          Union of 1D arrays with unique elements. || setdiff1d        Set difference of 1D arrays with unique elements. || ================ =================== ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4206,ryfeus/gcf-packs,pandas_numpy/sources/numpy/lib/tests/test_format.py,255a05a5980efb8b096c283d79872d0695886161,"r''' Test the .npy file format. ||  || Set up: ||  ||     >>> import sys ||     >>> from io import BytesIO ||     >>> from numpy.lib import format ||     >>> ||     >>> scalars = [ ||     ...     np.uint8; ||     ...     np.int8; ||     ...     np.uint16; ||     ...     np.int16; ||     ...     np.uint32; ||     ...     np.int32; ||     ...     np.uint64; ||     ...     np.int64; ||     ...     np.float32; ||     ...     np.float64; ||     ...     np.complex64; ||     ...     np.complex128; ||     ...     object; ||     ... ] ||     >>> ||     >>> basic_arrays = [] ||     >>> ||     >>> for scalar in scalars: ||     ...     for endian in '<>': ||     ...         dtype = np.dtype(scalar).newbyteorder(endian) ||     ...         basic = np.arange(15).astype(dtype) ||     ...         basic_arrays.extend([ ||     ...             np.array([]; dtype=dtype); ||     ...             np.array(10; dtype=dtype); ||     ...             basic; ||     ...             basic.reshape((3;5)); ||     ...             basic.reshape((3;5)).T; ||     ...             basic.reshape((3;5))[::-1;::2]; ||     ...         ]) ||     ... ||     >>> ||     >>> Pdescr = [ ||     ...     ('x'; 'i4'; (2;)); ||     ...     ('y'; 'f8'; (2; 2)); ||     ...     ('z'; 'u1')] ||     >>> ||     >>> ||     >>> PbufferT = [ ||     ...     ([3;2]; [[6.;4.];[6.;4.]]; 8); ||     ...     ([4;3]; [[7.;5.];[7.;5.]]; 9); ||     ...     ] ||     >>> ||     >>> ||     >>> Ndescr = [ ||     ...     ('x'; 'i4'; (2;)); ||     ...     ('Info'; [ ||     ...         ('value'; 'c16'); ||     ...         ('y2'; 'f8'); ||     ...         ('Info2'; [ ||     ...             ('name'; 'S2'); ||     ...             ('value'; 'c16'; (2;)); ||     ...             ('y3'; 'f8'; (2;)); ||     ...             ('z3'; 'u4'; (2;))]); ||     ...         ('name'; 'S2'); ||     ...         ('z2'; 'b1')]); ||     ...     ('color'; 'S2'); ||     ...     ('info'; [ ||     ...         ('Name'; 'U8'); ||     ...         ('Value'; 'c16')]); ||     ...     ('y'; 'f8'; (2; 2)); ||     ...     ('z'; 'u1')] ||     >>> ||     >>> ||     >>> NbufferT = [ ||     ...     ([3;2]; (6j; 6.; ('nn'; [6j;4j]; [6.;4.]; [1;2]); 'NN'; True); 'cc'; ('NN'; 6j); [[6.;4.];[6.;4.]]; 8); ||     ...     ([4;3]; (7j; 7.; ('oo'; [7j;5j]; [7.;5.]; [2;1]); 'OO'; False); 'dd'; ('OO'; 7j); [[7.;5.];[7.;5.]]; 9); ||     ...     ] ||     >>> ||     >>> ||     >>> record_arrays = [ ||     ...     np.array(PbufferT; dtype=np.dtype(Pdescr).newbyteorder('<')); ||     ...     np.array(NbufferT; dtype=np.dtype(Ndescr).newbyteorder('<')); ||     ...     np.array(PbufferT; dtype=np.dtype(Pdescr).newbyteorder('>')); ||     ...     np.array(NbufferT; dtype=np.dtype(Ndescr).newbyteorder('>')); ||     ... ] ||  || Test the magic string writing. ||  ||     >>> format.magic(1; 0) ||     '\\x93NUMPY\\x01\\x00' ||     >>> format.magic(0; 0) ||     '\\x93NUMPY\\x00\\x00' ||     >>> format.magic(255; 255) ||     '\\x93NUMPY\\xff\\xff' ||     >>> format.magic(2; 5) ||     '\\x93NUMPY\\x02\\x05' ||  || Test the magic string reading. ||  ||     >>> format.read_magic(BytesIO(format.magic(1; 0))) ||     (1; 0) ||     >>> format.read_magic(BytesIO(format.magic(0; 0))) ||     (0; 0) ||     >>> format.read_magic(BytesIO(format.magic(255; 255))) ||     (255; 255) ||     >>> format.read_magic(BytesIO(format.magic(2; 5))) ||     (2; 5) ||  || Test the header writing. ||  ||     >>> for arr in basic_arrays + record_arrays: ||     ...     f = BytesIO() ||     ...     format.write_array_header_1_0(f; arr)   # XXX: arr is not a dict; items gets called on it ||     ...     print(repr(f.getvalue())) ||     ... ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '|u1'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '|i1'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<u2'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '<u2'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '<u2'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '<u2'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '<u2'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '<u2'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '>u2'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '>u2'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '>u2'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '>u2'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '>u2'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '>u2'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<i2'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '<i2'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '<i2'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '<i2'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '<i2'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '<i2'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '>i2'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '>i2'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '>i2'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '>i2'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '>i2'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '>i2'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<u4'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '<u4'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '<u4'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '<u4'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '<u4'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '<u4'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '>u4'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '>u4'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '>u4'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '>u4'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '>u4'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '>u4'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<i4'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '<i4'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '<i4'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '<i4'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '<i4'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '<i4'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '>i4'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '>i4'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '>i4'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '>i4'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '>i4'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '>i4'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<u8'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '<u8'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '<u8'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '<u8'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '<u8'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '<u8'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '>u8'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '>u8'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '>u8'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '>u8'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '>u8'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '>u8'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<i8'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '<i8'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '<i8'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '<i8'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '<i8'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '<i8'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '>i8'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '>i8'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '>i8'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '>i8'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '>i8'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '>i8'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<f4'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '<f4'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '<f4'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '<f4'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '<f4'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '<f4'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '>f4'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '>f4'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '>f4'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '>f4'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '>f4'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '>f4'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<f8'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '<f8'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '<f8'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '<f8'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '<f8'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '<f8'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '>f8'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '>f8'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '>f8'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '>f8'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '>f8'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '>f8'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<c8'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '<c8'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '<c8'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '<c8'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '<c8'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '<c8'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '>c8'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': '>c8'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': '>c8'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': '>c8'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': '>c8'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': '>c8'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': '<c16'; 'fortran_order': False; 'shape': (0;)}             \ || \"" ||     \""F\\x00{'descr': '<c16'; 'fortran_order': False; 'shape': ()}               \ || \"" ||     \""F\\x00{'descr': '<c16'; 'fortran_order': False; 'shape': (15;)}            \ || \"" ||     \""F\\x00{'descr': '<c16'; 'fortran_order': False; 'shape': (3; 5)}           \ || \"" ||     \""F\\x00{'descr': '<c16'; 'fortran_order': True; 'shape': (5; 3)}            \ || \"" ||     \""F\\x00{'descr': '<c16'; 'fortran_order': False; 'shape': (3; 3)}           \ || \"" ||     \""F\\x00{'descr': '>c16'; 'fortran_order': False; 'shape': (0;)}             \ || \"" ||     \""F\\x00{'descr': '>c16'; 'fortran_order': False; 'shape': ()}               \ || \"" ||     \""F\\x00{'descr': '>c16'; 'fortran_order': False; 'shape': (15;)}            \ || \"" ||     \""F\\x00{'descr': '>c16'; 'fortran_order': False; 'shape': (3; 5)}           \ || \"" ||     \""F\\x00{'descr': '>c16'; 'fortran_order': True; 'shape': (5; 3)}            \ || \"" ||     \""F\\x00{'descr': '>c16'; 'fortran_order': False; 'shape': (3; 3)}           \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': (0;)}              \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': ()}                \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': (15;)}             \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': (3; 5)}            \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': True; 'shape': (5; 3)}             \ || \"" ||     \""F\\x00{'descr': 'O'; 'fortran_order': False; 'shape': (3; 3)}            \ || \"" ||     \""v\\x00{'descr': [('x'; '<i4'; (2;)); ('y'; '<f8'; (2; 2)); ('z'; '|u1')];\ ||  'fortran_order': False;\ ||  'shape': (2;)}         \ || \"" ||     \""\\x16\\x02{'descr': [('x'; '<i4'; (2;));\ ||            ('Info';\ ||             [('value'; '<c16');\ ||              ('y2'; '<f8');\ ||              ('Info2';\ ||               [('name'; '|S2');\ ||                ('value'; '<c16'; (2;));\ ||                ('y3'; '<f8'; (2;));\ ||                ('z3'; '<u4'; (2;))]);\ ||              ('name'; '|S2');\ ||              ('z2'; '|b1')]);\ ||            ('color'; '|S2');\ ||            ('info'; [('Name'; '<U8'); ('Value'; '<c16')]);\ ||            ('y'; '<f8'; (2; 2));\ ||            ('z'; '|u1')];\ ||  'fortran_order': False;\ ||  'shape': (2;)}      \ || \"" ||     \""v\\x00{'descr': [('x'; '>i4'; (2;)); ('y'; '>f8'; (2; 2)); ('z'; '|u1')];\ ||  'fortran_order': False;\ ||  'shape': (2;)}         \ || \"" ||     \""\\x16\\x02{'descr': [('x'; '>i4'; (2;));\ ||            ('Info';\ ||             [('value'; '>c16');\ ||              ('y2'; '>f8');\ ||              ('Info2';\ ||               [('name'; '|S2');\ ||                ('value'; '>c16'; (2;));\ ||                ('y3'; '>f8'; (2;));\ ||                ('z3'; '>u4'; (2;))]);\ ||              ('name'; '|S2');\ ||              ('z2'; '|b1')]);\ ||            ('color'; '|S2');\ ||            ('info'; [('Name'; '>U8'); ('Value'; '>c16')]);\ ||            ('y'; '>f8'; (2; 2));\ ||            ('z'; '|u1')];\ ||  'fortran_order': False;\ ||  'shape': (2;)}      \ || \"" || '''",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4207,ryfeus/gcf-packs,pandas_numpy/sources/numpy/lib/tests/test_mixins.py,255a05a5980efb8b096c283d79872d0695886161,TODO: test div on Python 2; only,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4208,ryfeus/gcf-packs,pandas_numpy/sources/numpy/lib/tests/test_recfunctions.py,255a05a5980efb8b096c283d79872d0695886161,Fixme; this test looks incomplete and broken,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4209,ryfeus/gcf-packs,pandas_numpy/sources/numpy/lib/tests/test_recfunctions.py,255a05a5980efb8b096c283d79872d0695886161,Fixme; this test is broken,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4210,ryfeus/gcf-packs,pandas_numpy/sources/numpy/ma/tests/test_core.py,255a05a5980efb8b096c283d79872d0695886161,TODO: Test masked_object; masked_equal; ...,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4211,ryfeus/gcf-packs,pandas_numpy/sources/numpy/random/tests/test_random.py,255a05a5980efb8b096c283d79872d0695886161,TODO: Include test for randint once it can broadcast,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4212,ryfeus/gcf-packs,pandas_numpy/sources/numpy/testing/_private/pytesttester.py,255a05a5980efb8b096c283d79872d0695886161,FIXME This is no longer needed? Assume it was for use in tests.,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4213,ryfeus/gcf-packs,pandas_numpy/sources/numpy/testing/tests/test_doctesting.py,255a05a5980efb8b096c283d79872d0695886161,FIXME: None of these tests is run; because 'check' is not a recognized,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4214,ryfeus/gcf-packs,pandas_numpy/sources/pandas/core/ops.py,255a05a5980efb8b096c283d79872d0695886161,TODO: I don't think the functions defined by bool_method are tested,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4215,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/extension/json/array.py,255a05a5980efb8b096c283d79872d0695886161,"\""\""\""Test extension array for storing nested data in a pandas container. ||  || The JSONArray stores lists of dictionaries. The storage mechanism is a list; || not an ndarray. ||  || Note: ||  || We currently store lists of UserDicts (Py3 only). Pandas has a few places || internally that specifically check for dicts; and does non-scalar things || in that case. We *want* the dictionaries to be treated as scalars; so we || hack around pandas by using UserDicts. || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4216,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/frame/test_constructors.py,255a05a5980efb8b096c283d79872d0695886161,Test new columns parameter for from_dict that was added to make,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4217,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/frame/test_repr_info.py,255a05a5980efb8b096c283d79872d0695886161,Test a DataFrame with duplicate columns,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4218,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/frame/test_timeseries.py,255a05a5980efb8b096c283d79872d0695886161,TODO: l1 should be a PeriodIndex for testing,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4219,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/frame/test_timeseries.py,255a05a5980efb8b096c283d79872d0695886161,TODO: untested,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4220,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/datetimes/test_arithmetic.py,255a05a5980efb8b096c283d79872d0695886161,TODO: A couple other tests belong in this section.  Move them in,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4221,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/datetimes/test_indexing.py,255a05a5980efb8b096c283d79872d0695886161,TODO: This method came from test_datetime; de-dup with version above,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4222,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/datetimes/test_setops.py,255a05a5980efb8b096c283d79872d0695886161,TODO: moved from test_datetimelike; dedup with version below,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4223,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/datetimes/test_setops.py,255a05a5980efb8b096c283d79872d0695886161,TODO: moved from test_datetimelike; de-duplicate with version below,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4224,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/datetimes/test_timezones.py,255a05a5980efb8b096c283d79872d0695886161,TODO: belongs outside tz_localize tests?,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4225,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/interval/test_interval.py,255a05a5980efb8b096c283d79872d0695886161,TODO: same tests for more index types,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4226,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/period/test_arithmetic.py,255a05a5980efb8b096c283d79872d0695886161,TODO: De-duplicate with test_pi_cmp_nat,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4227,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/test_base.py,255a05a5980efb8b096c283d79872d0695886161,TODO: make this a dedicated test with parametrized methods,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4228,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/test_base.py,255a05a5980efb8b096c283d79872d0695886161,TODO: Parametrize numeric and str tests after self.strIndex fixture,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4229,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/indexes/timedeltas/test_indexing.py,255a05a5980efb8b096c283d79872d0695886161,TODO: This method came from test_timedelta; de-dup with version above,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4230,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/io/parser/common.py,255a05a5980efb8b096c283d79872d0695886161,TODO: ftp testing,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4231,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/io/parser/mangle_dupes.py,255a05a5980efb8b096c283d79872d0695886161,"TODO: add test for condition \""mangle_dupe_cols=False\""",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4232,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/io/test_excel.py,255a05a5980efb8b096c283d79872d0695886161,XXX: this isn't as strong a test as ideal; we should,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4233,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/io/test_pickle.py,255a05a5980efb8b096c283d79872d0695886161,"\""\""\"" || manage legacy pickle tests ||  || How to add pickle tests: ||  || 1. Install pandas version intended to output the pickle. ||  || 2. Execute \""generate_legacy_storage_files.py\"" to create the pickle. || $ python generate_legacy_storage_files.py <output_dir> pickle ||  || 3. Move the created pickle to \""data\/legacy_pickle\/<version>\"" directory. || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4234,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/plotting/test_frame.py,255a05a5980efb8b096c283d79872d0695886161,TODO add MultiIndex test,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4235,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/plotting/test_frame.py,255a05a5980efb8b096c283d79872d0695886161,TODO: need better way to test. This just does existence.,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4236,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/series/test_timezones.py,255a05a5980efb8b096c283d79872d0695886161,TODO: De-duplicate with test below,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4237,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/sparse/frame/test_frame.py,255a05a5980efb8b096c283d79872d0695886161,TODO: test data is copied from inputs,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4238,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/test_panel.py,255a05a5980efb8b096c283d79872d0695886161,TODO: test correctness,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4239,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/test_panel.py,255a05a5980efb8b096c283d79872d0695886161,weird overlap; TODO: test?,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4240,ryfeus/gcf-packs,pandas_numpy/sources/pandas/tests/test_resample.py,255a05a5980efb8b096c283d79872d0695886161,TODO: once GH 14008 is fixed; move these tests into,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4241,ryfeus/gcf-packs,pandas_numpy/sources/pandas/util/testing.py,255a05a5980efb8b096c283d79872d0695886161,ToDo: There are some tests using rhs is sparse,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,No
4242,ryfeus/gcf-packs,pandas_numpy/sources/pandas/util/testing.py,255a05a5980efb8b096c283d79872d0695886161,ToDo: There are some tests using rhs is SparseDataFrame,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4243,ryfeus/gcf-packs,pandas_numpy/sources/pkg_resources/_vendor/packaging/requirements.py,255a05a5980efb8b096c283d79872d0695886161,TODO: Can we test whether something is contained within a requirement?,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4244,ryfeus/gcf-packs,pandas_numpy/sources/setuptools/_vendor/packaging/requirements.py,255a05a5980efb8b096c283d79872d0695886161,TODO: Can we test whether something is contained within a requirement?,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4245,ryfeus/gcf-packs,pandas_numpy/sources/setuptools/command/easy_install.py,255a05a5980efb8b096c283d79872d0695886161,XXX should this check for latest version?,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
4246,ryfeus/gcf-packs,selenium_chrome/source/pkg_resources/_vendor/packaging/requirements.py,307aae04664413c6f4cbd8481d8504cc01d2219a,TODO: Can we test whether something is contained within a requirement?,https://github.com/ryfeus/gcf-packs/commit/307aae04664413c6f4cbd8481d8504cc01d2219a,Yes
4247,ryfeus/gcf-packs,selenium_chrome/source/setuptools/_vendor/packaging/requirements.py,307aae04664413c6f4cbd8481d8504cc01d2219a,TODO: Can we test whether something is contained within a requirement?,https://github.com/ryfeus/gcf-packs/commit/307aae04664413c6f4cbd8481d8504cc01d2219a,Yes
4248,ryfeus/gcf-packs,selenium_chrome/source/setuptools/command/easy_install.py,307aae04664413c6f4cbd8481d8504cc01d2219a,XXX should this check for latest version?,https://github.com/ryfeus/gcf-packs/commit/307aae04664413c6f4cbd8481d8504cc01d2219a,Yes
4249,ryfeus/gcf-packs,tensorflow2.0/source/google/protobuf/internal/message_test.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"\""\""\""Tests python protocol buffers against the golden message. ||  || Note that the golden messages exercise every known field type; thus this || test ends up exercising and verifying nearly all of the parsing and || serialization code in the whole library. ||  || TODO(kenton):  Merge with wire_format_test?  It doesn't make a whole lot of || sense to call this a test of the \""message\"" module; which only declares an || abstract interface. || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,No
4250,ryfeus/gcf-packs,tensorflow2.0/source/numpy/_pytesttester.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,FIXME This is no longer needed? Assume it was for use in tests.,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,No
4251,ryfeus/gcf-packs,tensorflow2.0/source/numpy/conftest.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,FIXME when yield tests are gone.,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,No
4252,ryfeus/gcf-packs,tensorflow2.0/source/numpy/doc/basics.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"\""\""\"" || ============ || Array basics || ============ ||  || Array types and conversions between types || ========================================= ||  || NumPy supports a much greater variety of numerical types than Python does. || This section shows which are available; and how to modify an array's data-type. ||  || The primitive types supported are tied closely to those in C: ||  || .. list-table:: ||     :header-rows: 1 ||  ||     * - Numpy type ||       - C type ||       - Description ||  ||     * - `np.bool` ||       - ``bool`` ||       - Boolean (True or False) stored as a byte ||  ||     * - `np.byte` ||       - ``signed char`` ||       - Platform-defined ||  ||     * - `np.ubyte` ||       - ``unsigned char`` ||       - Platform-defined ||  ||     * - `np.short` ||       - ``short`` ||       - Platform-defined ||  ||     * - `np.ushort` ||       - ``unsigned short`` ||       - Platform-defined ||  ||     * - `np.intc` ||       - ``int`` ||       - Platform-defined ||  ||     * - `np.uintc` ||       - ``unsigned int`` ||       - Platform-defined ||  ||     * - `np.int_` ||       - ``long`` ||       - Platform-defined ||  ||     * - `np.uint` ||       - ``unsigned long`` ||       - Platform-defined ||  ||     * - `np.longlong` ||       - ``long long`` ||       - Platform-defined ||  ||     * - `np.ulonglong` ||       - ``unsigned long long`` ||       - Platform-defined ||  ||     * - `np.half` \/ `np.float16` ||       - ||       - Half precision float: ||         sign bit; 5 bits exponent; 10 bits mantissa ||  ||     * - `np.single` ||       - ``float`` ||       - Platform-defined single precision float: ||         typically sign bit; 8 bits exponent; 23 bits mantissa ||  ||     * - `np.double` ||       - ``double`` ||       - Platform-defined double precision float: ||         typically sign bit; 11 bits exponent; 52 bits mantissa. ||  ||     * - `np.longdouble` ||       - ``long double`` ||       - Platform-defined extended-precision float ||  ||     * - `np.csingle` ||       - ``float complex`` ||       - Complex number; represented by two single-precision floats (real and imaginary components) ||  ||     * - `np.cdouble` ||       - ``double complex`` ||       - Complex number; represented by two double-precision floats (real and imaginary components). ||  ||     * - `np.clongdouble` ||       - ``long double complex`` ||       - Complex number; represented by two extended-precision floats (real and imaginary components). ||  ||  || Since many of these have platform-dependent definitions; a set of fixed-size || aliases are provided: ||  || .. list-table:: ||     :header-rows: 1 ||  ||     * - Numpy type ||       - C type ||       - Description ||  ||     * - `np.int8` ||       - ``int8_t`` ||       - Byte (-128 to 127) ||  ||     * - `np.int16` ||       - ``int16_t`` ||       - Integer (-32768 to 32767) ||  ||     * - `np.int32` ||       - ``int32_t`` ||       - Integer (-2147483648 to 2147483647) ||  ||     * - `np.int64` ||       - ``int64_t`` ||       - Integer (-9223372036854775808 to 9223372036854775807) ||  ||     * - `np.uint8` ||       - ``uint8_t`` ||       - Unsigned integer (0 to 255) ||  ||     * - `np.uint16` ||       - ``uint16_t`` ||       - Unsigned integer (0 to 65535) ||  ||     * - `np.uint32` ||       - ``uint32_t`` ||       - Unsigned integer (0 to 4294967295) ||  ||     * - `np.uint64` ||       - ``uint64_t`` ||       - Unsigned integer (0 to 18446744073709551615) ||  ||     * - `np.intp` ||       - ``intptr_t`` ||       - Integer used for indexing; typically the same as ``ssize_t`` ||  ||     * - `np.uintp` ||       - ``uintptr_t`` ||       - Integer large enough to hold a pointer ||  ||     * - `np.float32` ||       - ``float`` ||       - ||  ||     * - `np.float64` \/ `np.float_` ||       - ``double`` ||       - Note that this matches the precision of the builtin python `float`. ||  ||     * - `np.complex64` ||       - ``float complex`` ||       - Complex number; represented by two 32-bit floats (real and imaginary components) ||  ||     * - `np.complex128` \/ `np.complex_` ||       - ``double complex`` ||       - Note that this matches the precision of the builtin python `complex`. ||  ||  || NumPy numerical types are instances of ``dtype`` (data-type) objects; each || having unique characteristics.  Once you have imported NumPy using ||  ||   :: ||  ||     >>> import numpy as np ||  || the dtypes are available as ``np.bool_``; ``np.float32``; etc. ||  || Advanced types; not listed in the table above; are explored in || section :ref:`structured_arrays`. ||  || There are 5 basic numerical types representing booleans (bool); integers (int); || unsigned integers (uint) floating point (float) and complex. Those with numbers || in their name indicate the bitsize of the type (i.e. how many bits are needed || to represent a single value in memory).  Some types; such as ``int`` and || ``intp``; have differing bitsizes; dependent on the platforms (e.g. 32-bit || vs. 64-bit machines).  This should be taken into account when interfacing || with low-level code (such as C or Fortran) where the raw memory is addressed. ||  || Data-types can be used as functions to convert python numbers to array scalars || (see the array scalar section for an explanation); python sequences of numbers || to arrays of that type; or as arguments to the dtype keyword that many numpy || functions or methods accept. Some examples:: ||  ||     >>> import numpy as np ||     >>> x = np.float32(1.0) ||     >>> x ||     1.0 ||     >>> y = np.int_([1;2;4]) ||     >>> y ||     array([1; 2; 4]) ||     >>> z = np.arange(3; dtype=np.uint8) ||     >>> z ||     array([0; 1; 2]; dtype=uint8) ||  || Array types can also be referred to by character codes; mostly to retain || backward compatibility with older packages such as Numeric.  Some || documentation may still refer to these; for example:: ||  ||   >>> np.array([1; 2; 3]; dtype='f') ||   array([ 1.;  2.;  3.]; dtype=float32) ||  || We recommend using dtype objects instead. ||  || To convert the type of an array; use the .astype() method (preferred) or || the type itself as a function. For example: :: ||  ||     >>> z.astype(float)                 #doctest: +NORMALIZE_WHITESPACE ||     array([  0.;  1.;  2.]) ||     >>> np.int8(z) ||     array([0; 1; 2]; dtype=int8) ||  || Note that; above; we use the *Python* float object as a dtype.  NumPy knows || that ``int`` refers to ``np.int_``; ``bool`` means ``np.bool_``; || that ``float`` is ``np.float_`` and ``complex`` is ``np.complex_``. || The other data-types do not have Python equivalents. ||  || To determine the type of an array; look at the dtype attribute:: ||  ||     >>> z.dtype ||     dtype('uint8') ||  || dtype objects also contain information about the type; such as its bit-width || and its byte-order.  The data type can also be used indirectly to query || properties of the type; such as whether it is an integer:: ||  ||     >>> d = np.dtype(int) ||     >>> d ||     dtype('int32') ||  ||     >>> np.issubdtype(d; np.integer) ||     True ||  ||     >>> np.issubdtype(d; np.floating) ||     False ||  ||  || Array Scalars || ============= ||  || NumPy generally returns elements of arrays as array scalars (a scalar || with an associated dtype).  Array scalars differ from Python scalars; but || for the most part they can be used interchangeably (the primary || exception is for versions of Python older than v2.x; where integer array || scalars cannot act as indices for lists and tuples).  There are some || exceptions; such as when code requires very specific attributes of a scalar || or when it checks specifically whether a value is a Python scalar. Generally; || problems are easily fixed by explicitly converting array scalars || to Python scalars; using the corresponding Python type function || (e.g.; ``int``; ``float``; ``complex``; ``str``; ``unicode``). ||  || The primary advantage of using array scalars is that || they preserve the array type (Python may not have a matching scalar type || available; e.g. ``int16``).  Therefore; the use of array scalars ensures || identical behaviour between arrays and scalars; irrespective of whether the || value is inside an array or not.  NumPy scalars also have many of the same || methods arrays do. ||  || Extended Precision || ================== ||  || Python's floating-point numbers are usually 64-bit floating-point numbers; || nearly equivalent to ``np.float64``. In some unusual situations it may be || useful to use floating-point numbers with more precision. Whether this || is possible in numpy depends on the hardware and on the development || environment: specifically; x86 machines provide hardware floating-point || with 80-bit precision; and while most C compilers provide this as their || ``long double`` type; MSVC (standard for Windows builds) makes || ``long double`` identical to ``double`` (64 bits). NumPy makes the || compiler's ``long double`` available as ``np.longdouble`` (and || ``np.clongdouble`` for the complex numbers). You can find out what your || numpy provides with ``np.finfo(np.longdouble)``. ||  || NumPy does not provide a dtype with more precision than C || ``long double``\\\\s; in particular; the 128-bit IEEE quad precision || data type (FORTRAN's ``REAL*16``\\\\) is not available. ||  || For efficient memory alignment; ``np.longdouble`` is usually stored || padded with zero bits; either to 96 or 128 bits. Which is more efficient || depends on hardware and development environment; typically on 32-bit || systems they are padded to 96 bits; while on 64-bit systems they are || typically padded to 128 bits. ``np.longdouble`` is padded to the system || default; ``np.float96`` and ``np.float128`` are provided for users who || want specific padding. In spite of the names; ``np.float96`` and || ``np.float128`` provide only as much precision as ``np.longdouble``; || that is; 80 bits on most x86 machines and 64 bits in standard || Windows builds. ||  || Be warned that even if ``np.longdouble`` offers more precision than || python ``float``; it is easy to lose that extra precision; since || python often forces values to pass through ``float``. For example; || the ``%`` formatting operator requires its arguments to be converted || to standard python types; and it is therefore impossible to preserve || extended precision even if many decimal places are requested. It can || be useful to test your code with the value || ``1 + np.finfo(np.longdouble).eps``. ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4253,ryfeus/gcf-packs,tensorflow2.0/source/numpy/doc/glossary.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"\""\""\"" || ======== || Glossary || ======== ||  || .. glossary:: ||  ||    along an axis ||        Axes are defined for arrays with more than one dimension.  A ||        2-dimensional array has two corresponding axes: the first running ||        vertically downwards across rows (axis 0); and the second running ||        horizontally across columns (axis 1). ||  ||        Many operations can take place along one of these axes.  For example; ||        we can sum each row of an array; in which case we operate along ||        columns; or axis 1:: ||  ||          >>> x = np.arange(12).reshape((3;4)) ||  ||          >>> x ||          array([[ 0;  1;  2;  3]; ||                 [ 4;  5;  6;  7]; ||                 [ 8;  9; 10; 11]]) ||  ||          >>> x.sum(axis=1) ||          array([ 6; 22; 38]) ||  ||    array ||        A homogeneous container of numerical elements.  Each element in the ||        array occupies a fixed amount of memory (hence homogeneous); and ||        can be a numerical element of a single type (such as float; int ||        or complex) or a combination (such as ``(float; int; float)``).  Each ||        array has an associated data-type (or ``dtype``); which describes ||        the numerical type of its elements:: ||  ||          >>> x = np.array([1; 2; 3]; float) ||  ||          >>> x ||          array([ 1.;  2.;  3.]) ||  ||          >>> x.dtype # floating point number; 64 bits of memory per element ||          dtype('float64') ||  ||  ||          # More complicated data type: each array element is a combination of ||          # and integer and a floating point number ||          >>> np.array([(1; 2.0); (3; 4.0)]; dtype=[('x'; int); ('y'; float)]) ||          array([(1; 2.0); (3; 4.0)]; ||                dtype=[('x'; '<i4'); ('y'; '<f8')]) ||  ||        Fast element-wise operations; called a :term:`ufunc`; operate on arrays. ||  ||    array_like ||        Any sequence that can be interpreted as an ndarray.  This includes ||        nested lists; tuples; scalars and existing arrays. ||  ||    attribute ||        A property of an object that can be accessed using ``obj.attribute``; ||        e.g.; ``shape`` is an attribute of an array:: ||  ||          >>> x = np.array([1; 2; 3]) ||          >>> x.shape ||          (3;) ||  ||    big-endian ||        When storing a multi-byte value in memory as a sequence of bytes; the ||        sequence addresses\/sends\/stores the most significant byte first (lowest ||        address) and the least significant byte last (highest address). Common in ||        micro-processors and used for transmission of data over network protocols. ||  ||    BLAS ||        `Basic Linear Algebra Subprograms <https:\/\/en.wikipedia.org\/wiki\/Basic_Linear_Algebra_Subprograms>`_ ||  ||    broadcast ||        NumPy can do operations on arrays whose shapes are mismatched:: ||  ||          >>> x = np.array([1; 2]) ||          >>> y = np.array([[3]; [4]]) ||  ||          >>> x ||          array([1; 2]) ||  ||          >>> y ||          array([[3]; ||                 [4]]) ||  ||          >>> x + y ||          array([[4; 5]; ||                 [5; 6]]) ||  ||        See `numpy.doc.broadcasting` for more information. ||  ||    C order ||        See `row-major` ||  ||    column-major ||        A way to represent items in a N-dimensional array in the 1-dimensional ||        computer memory. In column-major order; the leftmost index \""varies the ||        fastest\"": for example the array:: ||  ||             [[1; 2; 3]; ||              [4; 5; 6]] ||  ||        is represented in the column-major order as:: ||  ||            [1; 4; 2; 5; 3; 6] ||  ||        Column-major order is also known as the Fortran order; as the Fortran ||        programming language uses it. ||  ||    decorator ||        An operator that transforms a function.  For example; a ``log`` ||        decorator may be defined to print debugging information upon ||        function execution:: ||  ||          >>> def log(f): ||          ...     def new_logging_func(*args; **kwargs): ||          ...         print(\""Logging call with parameters:\""; args; kwargs) ||          ...         return f(*args; **kwargs) ||          ... ||          ...     return new_logging_func ||  ||        Now; when we define a function; we can \""decorate\"" it using ``log``:: ||  ||          >>> @log ||          ... def add(a; b): ||          ...     return a + b ||  ||        Calling ``add`` then yields: ||  ||        >>> add(1; 2) ||        Logging call with parameters: (1; 2) {} ||        3 ||  ||    dictionary ||        Resembling a language dictionary; which provides a mapping between ||        words and descriptions thereof; a Python dictionary is a mapping ||        between two objects:: ||  ||          >>> x = {1: 'one'; 'two': [1; 2]} ||  ||        Here; `x` is a dictionary mapping keys to values; in this case ||        the integer 1 to the string \""one\""; and the string \""two\"" to ||        the list ``[1; 2]``.  The values may be accessed using their ||        corresponding keys:: ||  ||          >>> x[1] ||          'one' ||  ||          >>> x['two'] ||          [1; 2] ||  ||        Note that dictionaries are not stored in any specific order.  Also; ||        most mutable (see *immutable* below) objects; such as lists; may not ||        be used as keys. ||  ||        For more information on dictionaries; read the ||        `Python tutorial <https:\/\/docs.python.org\/tutorial\/>`_. ||  ||    field ||        In a :term:`structured data type`; each sub-type is called a `field`. ||        The `field` has a name (a string); a type (any valid :term:`dtype`; and ||        an optional `title`. See :ref:`arrays.dtypes` ||  ||    Fortran order ||        See `column-major` ||  ||    flattened ||        Collapsed to a one-dimensional array. See `numpy.ndarray.flatten` ||        for details. ||  ||    homogenous ||        Describes a block of memory comprised of blocks; each block comprised of  ||        items and of the same size; and blocks are interpreted in exactly the ||        same way. In the simplest case each block contains a single item; for ||        instance int32 or float64. ||  ||    immutable ||        An object that cannot be modified after execution is called ||        immutable.  Two common examples are strings and tuples. ||  ||    instance ||        A class definition gives the blueprint for constructing an object:: ||  ||          >>> class House(object): ||          ...     wall_colour = 'white' ||  ||        Yet; we have to *build* a house before it exists:: ||  ||          >>> h = House() # build a house ||  ||        Now; ``h`` is called a ``House`` instance.  An instance is therefore ||        a specific realisation of a class. ||  ||    iterable ||        A sequence that allows \""walking\"" (iterating) over items; typically ||        using a loop such as:: ||  ||          >>> x = [1; 2; 3] ||          >>> [item**2 for item in x] ||          [1; 4; 9] ||  ||        It is often used in combination with ``enumerate``:: ||          >>> keys = ['a';'b';'c'] ||          >>> for n; k in enumerate(keys): ||          ...     print(\""Key %d: %s\"" % (n; k)) ||          ... ||          Key 0: a ||          Key 1: b ||          Key 2: c ||  ||    list ||        A Python container that can hold any number of objects or items. ||        The items do not have to be of the same type; and can even be ||        lists themselves:: ||  ||          >>> x = [2; 2.0; \""two\""; [2; 2.0]] ||  ||        The list `x` contains 4 items; each which can be accessed individually:: ||  ||          >>> x[2] # the string 'two' ||          'two' ||  ||          >>> x[3] # a list; containing an integer 2 and a float 2.0 ||          [2; 2.0] ||  ||        It is also possible to select more than one item at a time; ||        using *slicing*:: ||  ||          >>> x[0:2] # or; equivalently; x[:2] ||          [2; 2.0] ||  ||        In code; arrays are often conveniently expressed as nested lists:: ||  ||  ||          >>> np.array([[1; 2]; [3; 4]]) ||          array([[1; 2]; ||                 [3; 4]]) ||  ||        For more information; read the section on lists in the `Python ||        tutorial <https:\/\/docs.python.org\/tutorial\/>`_.  For a mapping ||        type (key-value); see *dictionary*. ||  ||    little-endian ||        When storing a multi-byte value in memory as a sequence of bytes; the ||        sequence addresses\/sends\/stores the least significant byte first (lowest ||        address) and the most significant byte last (highest address). Common in ||        x86 processors. ||  ||    mask ||        A boolean array; used to select only certain elements for an operation:: ||  ||          >>> x = np.arange(5) ||          >>> x ||          array([0; 1; 2; 3; 4]) ||  ||          >>> mask = (x > 2) ||          >>> mask ||          array([False; False; False; True;  True]) ||  ||          >>> x[mask] = -1 ||          >>> x ||          array([ 0;  1;  2;  -1; -1]) ||  ||    masked array ||        Array that suppressed values indicated by a mask:: ||  ||          >>> x = np.ma.masked_array([np.nan; 2; np.nan]; [True; False; True]) ||          >>> x ||          masked_array(data = [-- 2.0 --]; ||                       mask = [ True False  True]; ||                 fill_value = 1e+20) ||          <BLANKLINE> ||  ||          >>> x + [1; 2; 3] ||          masked_array(data = [-- 4.0 --]; ||                       mask = [ True False  True]; ||                 fill_value = 1e+20) ||          <BLANKLINE> ||  ||  ||        Masked arrays are often used when operating on arrays containing ||        missing or invalid entries. ||  ||    matrix ||        A 2-dimensional ndarray that preserves its two-dimensional nature ||        throughout operations.  It has certain special operations; such as ``*`` ||        (matrix multiplication) and ``**`` (matrix power); defined:: ||  ||          >>> x = np.mat([[1; 2]; [3; 4]]) ||          >>> x ||          matrix([[1; 2]; ||                  [3; 4]]) ||  ||          >>> x**2 ||          matrix([[ 7; 10]; ||                [15; 22]]) ||  ||    method ||        A function associated with an object.  For example; each ndarray has a ||        method called ``repeat``:: ||  ||          >>> x = np.array([1; 2; 3]) ||          >>> x.repeat(2) ||          array([1; 1; 2; 2; 3; 3]) ||  ||    ndarray ||        See *array*. ||  ||    record array ||        An :term:`ndarray` with :term:`structured data type` which has been ||        subclassed as ``np.recarray`` and whose dtype is of type ``np.record``; ||        making the fields of its data type to be accessible by attribute. ||  ||    reference ||        If ``a`` is a reference to ``b``; then ``(a is b) == True``.  Therefore; ||        ``a`` and ``b`` are different names for the same Python object. ||  ||    row-major ||        A way to represent items in a N-dimensional array in the 1-dimensional ||        computer memory. In row-major order; the rightmost index \""varies ||        the fastest\"": for example the array:: ||  ||             [[1; 2; 3]; ||              [4; 5; 6]] ||  ||        is represented in the row-major order as:: ||  ||            [1; 2; 3; 4; 5; 6] ||  ||        Row-major order is also known as the C order; as the C programming ||        language uses it. New NumPy arrays are by default in row-major order. ||  ||    self ||        Often seen in method signatures; ``self`` refers to the instance ||        of the associated class.  For example: ||  ||          >>> class Paintbrush(object): ||          ...     color = 'blue' ||          ... ||          ...     def paint(self): ||          ...         print(\""Painting the city %s!\"" % self.color) ||          ... ||          >>> p = Paintbrush() ||          >>> p.color = 'red' ||          >>> p.paint() # self refers to 'p' ||          Painting the city red! ||  ||    slice ||        Used to select only certain elements from a sequence:: ||  ||          >>> x = range(5) ||          >>> x ||          [0; 1; 2; 3; 4] ||  ||          >>> x[1:3] # slice from 1 to 3 (excluding 3 itself) ||          [1; 2] ||  ||          >>> x[1:5:2] # slice from 1 to 5; but skipping every second element ||          [1; 3] ||  ||          >>> x[::-1] # slice a sequence in reverse ||          [4; 3; 2; 1; 0] ||  ||        Arrays may have more than one dimension; each which can be sliced ||        individually:: ||  ||          >>> x = np.array([[1; 2]; [3; 4]]) ||          >>> x ||          array([[1; 2]; ||                 [3; 4]]) ||  ||          >>> x[:; 1] ||          array([2; 4]) ||  ||    structure ||        See :term:`structured data type` ||  ||    structured data type ||        A data type composed of other datatypes ||  ||    tuple ||        A sequence that may contain a variable number of types of any ||        kind.  A tuple is immutable; i.e.; once constructed it cannot be ||        changed.  Similar to a list; it can be indexed and sliced:: ||  ||          >>> x = (1; 'one'; [1; 2]) ||          >>> x ||          (1; 'one'; [1; 2]) ||  ||          >>> x[0] ||          1 ||  ||          >>> x[:2] ||          (1; 'one') ||  ||        A useful concept is \""tuple unpacking\""; which allows variables to ||        be assigned to the contents of a tuple:: ||  ||          >>> x; y = (1; 2) ||          >>> x; y = 1; 2 ||  ||        This is often used when a function returns multiple values: ||  ||          >>> def return_many(): ||          ...     return 1; 'alpha'; None ||  ||          >>> a; b; c = return_many() ||          >>> a; b; c ||          (1; 'alpha'; None) ||  ||          >>> a ||          1 ||          >>> b ||          'alpha' ||  ||    ufunc ||        Universal function.  A fast element-wise array operation.  Examples include ||        ``add``; ``sin`` and ``logical_or``. ||  ||    view ||        An array that does not own its data; but refers to another array's ||        data instead.  For example; we may create a view that only shows ||        every second element of another array:: ||  ||          >>> x = np.arange(5) ||          >>> x ||          array([0; 1; 2; 3; 4]) ||  ||          >>> y = x[::2] ||          >>> y ||          array([0; 2; 4]) ||  ||          >>> x[0] = 3 # changing x changes y as well; since y is a view on x ||          >>> y ||          array([3; 2; 4]) ||  ||    wrapper ||        Python is a high-level (highly abstracted; or English-like) language. ||        This abstraction comes at a price in execution speed; and sometimes ||        it becomes necessary to use lower level languages to do fast ||        computations.  A wrapper is code that provides a bridge between ||        high and the low level languages; allowing; e.g.; Python to execute ||        code written in C or Fortran. ||  ||        Examples include ctypes; SWIG and Cython (which wraps C and C++) ||        and f2py (which wraps Fortran). ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4254,ryfeus/gcf-packs,tensorflow2.0/source/numpy/doc/misc.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"\""\""\"" || ============= || Miscellaneous || ============= ||  || IEEE 754 Floating Point Special Values || -------------------------------------- ||  || Special values defined in numpy: nan; inf; ||  || NaNs can be used as a poor-man's mask (if you don't care what the || original value was) ||  || Note: cannot use equality to test NaNs. E.g.: :: ||  ||  >>> myarr = np.array([1.; 0.; np.nan; 3.]) ||  >>> np.nonzero(myarr == np.nan) ||  (array([]; dtype=int64);) ||  >>> np.nan == np.nan  # is always False! Use special numpy functions instead. ||  False ||  >>> myarr[myarr == np.nan] = 0. # doesn't work ||  >>> myarr ||  array([  1.;   0.;  NaN;   3.]) ||  >>> myarr[np.isnan(myarr)] = 0. # use this instead find ||  >>> myarr ||  array([ 1.;  0.;  0.;  3.]) ||  || Other related special value functions: :: ||  ||  isinf():    True if value is inf ||  isfinite(): True if not nan or inf ||  nan_to_num(): Map nan to 0; inf to max float; -inf to min float ||  || The following corresponds to the usual functions except that nans are excluded || from the results: :: ||  ||  nansum() ||  nanmax() ||  nanmin() ||  nanargmax() ||  nanargmin() ||  ||  >>> x = np.arange(10.) ||  >>> x[3] = np.nan ||  >>> x.sum() ||  nan ||  >>> np.nansum(x) ||  42.0 ||  || How numpy handles numerical exceptions || -------------------------------------- ||  || The default is to ``'warn'`` for ``invalid``; ``divide``; and ``overflow`` || and ``'ignore'`` for ``underflow``.  But this can be changed; and it can be || set individually for different kinds of exceptions. The different behaviors || are: ||  ||  - 'ignore' : Take no action when the exception occurs. ||  - 'warn'   : Print a `RuntimeWarning` (via the Python `warnings` module). ||  - 'raise'  : Raise a `FloatingPointError`. ||  - 'call'   : Call a function specified using the `seterrcall` function. ||  - 'print'  : Print a warning directly to ``stdout``. ||  - 'log'    : Record error in a Log object specified by `seterrcall`. ||  || These behaviors can be set for all kinds of errors or specific ones: ||  ||  - all       : apply to all numeric exceptions ||  - invalid   : when NaNs are generated ||  - divide    : divide by zero (for integers as well!) ||  - overflow  : floating point overflows ||  - underflow : floating point underflows ||  || Note that integer divide-by-zero is handled by the same machinery. || These behaviors are set on a per-thread basis. ||  || Examples || -------- ||  || :: ||  ||  >>> oldsettings = np.seterr(all='warn') ||  >>> np.zeros(5;dtype=np.float32)\/0. ||  invalid value encountered in divide ||  >>> j = np.seterr(under='ignore') ||  >>> np.array([1.e-100])**10 ||  >>> j = np.seterr(invalid='raise') ||  >>> np.sqrt(np.array([-1.])) ||  FloatingPointError: invalid value encountered in sqrt ||  >>> def errorhandler(errstr; errflag): ||  ...      print(\""saw stupid error!\"") ||  >>> np.seterrcall(errorhandler) ||  <function err_handler at 0x...> ||  >>> j = np.seterr(all='call') ||  >>> np.zeros(5; dtype=np.int32)\/0 ||  FloatingPointError: invalid value encountered in divide ||  saw stupid error! ||  >>> j = np.seterr(**oldsettings) # restore previous ||  ...                              # error-handling settings ||  || Interfacing to C || ---------------- || Only a survey of the choices. Little detail on how each works. ||  || 1) Bare metal; wrap your own C-code manually. ||  ||  - Plusses: ||  ||    - Efficient ||    - No dependencies on other tools ||  ||  - Minuses: ||  ||    - Lots of learning overhead: ||  ||      - need to learn basics of Python C API ||      - need to learn basics of numpy C API ||      - need to learn how to handle reference counting and love it. ||  ||    - Reference counting often difficult to get right. ||  ||      - getting it wrong leads to memory leaks; and worse; segfaults ||  ||    - API will change for Python 3.0! ||  || 2) Cython ||  ||  - Plusses: ||  ||    - avoid learning C API's ||    - no dealing with reference counting ||    - can code in pseudo python and generate C code ||    - can also interface to existing C code ||    - should shield you from changes to Python C api ||    - has become the de-facto standard within the scientific Python community ||    - fast indexing support for arrays ||  ||  - Minuses: ||  ||    - Can write code in non-standard form which may become obsolete ||    - Not as flexible as manual wrapping ||  || 3) ctypes ||  ||  - Plusses: ||  ||    - part of Python standard library ||    - good for interfacing to existing sharable libraries; particularly ||      Windows DLLs ||    - avoids API\/reference counting issues ||    - good numpy support: arrays have all these in their ctypes ||      attribute: :: ||  ||        a.ctypes.data              a.ctypes.get_strides ||        a.ctypes.data_as           a.ctypes.shape ||        a.ctypes.get_as_parameter  a.ctypes.shape_as ||        a.ctypes.get_data          a.ctypes.strides ||        a.ctypes.get_shape         a.ctypes.strides_as ||  ||  - Minuses: ||  ||    - can't use for writing code to be turned into C extensions; only a wrapper ||      tool. ||  || 4) SWIG (automatic wrapper generator) ||  ||  - Plusses: ||  ||    - around a long time ||    - multiple scripting language support ||    - C++ support ||    - Good for wrapping large (many functions) existing C libraries ||  ||  - Minuses: ||  ||    - generates lots of code between Python and the C code ||    - can cause performance problems that are nearly impossible to optimize ||      out ||    - interface files can be hard to write ||    - doesn't necessarily avoid reference counting issues or needing to know ||      API's ||  || 5) scipy.weave ||  ||  - Plusses: ||  ||    - can turn many numpy expressions into C code ||    - dynamic compiling and loading of generated C code ||    - can embed pure C code in Python module and have weave extract; generate ||      interfaces and compile; etc. ||  ||  - Minuses: ||  ||    - Future very uncertain: it's the only part of Scipy not ported to Python 3 ||      and is effectively deprecated in favor of Cython. ||  || 6) Psyco ||  ||  - Plusses: ||  ||    - Turns pure python into efficient machine code through jit-like ||      optimizations ||    - very fast when it optimizes well ||  ||  - Minuses: ||  ||    - Only on intel (windows?) ||    - Doesn't do much for numpy? ||  || Interfacing to Fortran: || ----------------------- || The clear choice to wrap Fortran code is || `f2py <https:\/\/docs.scipy.org\/doc\/numpy\/f2py\/>`_. ||  || Pyfort is an older alternative; but not supported any longer. || Fwrap is a newer project that looked promising but isn't being developed any || longer. ||  || Interfacing to C++: || ------------------- ||  1) Cython ||  2) CXX ||  3) Boost.python ||  4) SWIG ||  5) SIP (used mainly in PyQT) ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,No
4255,ryfeus/gcf-packs,tensorflow2.0/source/numpy/doc/structured_arrays.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"\""\""\"" || ================= || Structured Arrays || ================= ||  || Introduction || ============ ||  || Structured arrays are ndarrays whose datatype is a composition of simpler || datatypes organized as a sequence of named :term:`fields <field>`. For example; || :: ||  ||  >>> x = np.array([('Rex'; 9; 81.0); ('Fido'; 3; 27.0)]; ||  ...              dtype=[('name'; 'U10'); ('age'; 'i4'); ('weight'; 'f4')]) ||  >>> x ||  array([('Rex'; 9; 81.0); ('Fido'; 3; 27.0)]; ||        dtype=[('name'; 'S10'); ('age'; '<i4'); ('weight'; '<f4')]) ||  || Here ``x`` is a one-dimensional array of length two whose datatype is a || structure with three fields: 1. A string of length 10 or less named 'name'; 2. || a 32-bit integer named 'age'; and 3. a 32-bit float named 'weight'. ||  || If you index ``x`` at position 1 you get a structure:: ||  ||  >>> x[1] ||  ('Fido'; 3; 27.0) ||  || You can access and modify individual fields of a structured array by indexing || with the field name:: ||  ||  >>> x['age'] ||  array([9; 3]; dtype=int32) ||  >>> x['age'] = 5 ||  >>> x ||  array([('Rex'; 5; 81.0); ('Fido'; 5; 27.0)]; ||        dtype=[('name'; 'S10'); ('age'; '<i4'); ('weight'; '<f4')]) ||  || Structured datatypes are designed to be able to mimic 'structs' in the C || language; and share a similar memory layout. They are meant for interfacing with || C code and for low-level manipulation of structured buffers; for example for || interpreting binary blobs. For these purposes they support specialized features || such as subarrays; nested datatypes; and unions; and allow control over the || memory layout of the structure. ||  || Users looking to manipulate tabular data; such as stored in csv files; may find || other pydata projects more suitable; such as xarray; pandas; or DataArray. || These provide a high-level interface for tabular data analysis and are better || optimized for that use. For instance; the C-struct-like memory layout of || structured arrays in numpy can lead to poor cache behavior in comparison. ||  || .. _defining-structured-types: ||  || Structured Datatypes || ==================== ||  || A structured datatype can be thought of as a sequence of bytes of a certain || length (the structure's :term:`itemsize`) which is interpreted as a collection || of fields. Each field has a name; a datatype; and a byte offset within the || structure. The datatype of a field may be any numpy datatype including other || structured datatypes; and it may also be a :term:`sub-array` which behaves like || an ndarray of a specified shape. The offsets of the fields are arbitrary; and || fields may even overlap. These offsets are usually determined automatically by || numpy; but can also be specified. ||  || Structured Datatype Creation || ---------------------------- ||  || Structured datatypes may be created using the function :func:`numpy.dtype`. || There are 4 alternative forms of specification which vary in flexibility and || conciseness. These are further documented in the || :ref:`Data Type Objects <arrays.dtypes.constructing>` reference page; and in || summary they are: ||  || 1.   A list of tuples; one tuple per field ||  ||      Each tuple has the form ``(fieldname; datatype; shape)`` where shape is ||      optional. ``fieldname`` is a string (or tuple if titles are used; see ||      :ref:`Field Titles <titles>` below); ``datatype`` may be any object ||      convertible to a datatype; and ``shape`` is a tuple of integers specifying ||      subarray shape. ||  ||       >>> np.dtype([('x'; 'f4'); ('y'; np.float32); ('z'; 'f4'; (2;2))]) ||       dtype=[('x'; '<f4'); ('y'; '<f4'); ('z'; '<f4'; (2; 2))]) ||  ||      If ``fieldname`` is the empty string ``''``; the field will be given a ||      default name of the form ``f#``; where ``#`` is the integer index of the ||      field; counting from 0 from the left:: ||  ||       >>> np.dtype([('x'; 'f4');(''; 'i4');('z'; 'i8')]) ||       dtype([('x'; '<f4'); ('f1'; '<i4'); ('z'; '<i8')]) ||  ||      The byte offsets of the fields within the structure and the total ||      structure itemsize are determined automatically. ||  || 2.   A string of comma-separated dtype specifications ||  ||      In this shorthand notation any of the :ref:`string dtype specifications ||      <arrays.dtypes.constructing>` may be used in a string and separated by ||      commas. The itemsize and byte offsets of the fields are determined ||      automatically; and the field names are given the default names ``f0``; ||      ``f1``; etc. :: ||  ||       >>> np.dtype('i8;f4;S3') ||       dtype([('f0'; '<i8'); ('f1'; '<f4'); ('f2'; 'S3')]) ||       >>> np.dtype('3int8; float32; (2;3)float64') ||       dtype([('f0'; 'i1'; 3); ('f1'; '<f4'); ('f2'; '<f8'; (2; 3))]) ||  || 3.   A dictionary of field parameter arrays ||  ||      This is the most flexible form of specification since it allows control ||      over the byte-offsets of the fields and the itemsize of the structure. ||  ||      The dictionary has two required keys; 'names' and 'formats'; and four ||      optional keys; 'offsets'; 'itemsize'; 'aligned' and 'titles'. The values ||      for 'names' and 'formats' should respectively be a list of field names and ||      a list of dtype specifications; of the same length. The optional 'offsets' ||      value should be a list of integer byte-offsets; one for each field within ||      the structure. If 'offsets' is not given the offsets are determined ||      automatically. The optional 'itemsize' value should be an integer ||      describing the total size in bytes of the dtype; which must be large ||      enough to contain all the fields. ||      :: ||  ||       >>> np.dtype({'names': ['col1'; 'col2']; 'formats': ['i4';'f4']}) ||       dtype([('col1'; '<i4'); ('col2'; '<f4')]) ||       >>> np.dtype({'names': ['col1'; 'col2']; ||       ...           'formats': ['i4';'f4']; ||       ...           'offsets': [0; 4]; ||       ...           'itemsize': 12}) ||       dtype({'names':['col1';'col2']; 'formats':['<i4';'<f4']; 'offsets':[0;4]; 'itemsize':12}) ||  ||      Offsets may be chosen such that the fields overlap; though this will mean ||      that assigning to one field may clobber any overlapping field's data. As ||      an exception; fields of :class:`numpy.object` type cannot overlap with ||      other fields; because of the risk of clobbering the internal object ||      pointer and then dereferencing it. ||  ||      The optional 'aligned' value can be set to ``True`` to make the automatic ||      offset computation use aligned offsets (see :ref:`offsets-and-alignment`); ||      as if the 'align' keyword argument of :func:`numpy.dtype` had been set to ||      True. ||  ||      The optional 'titles' value should be a list of titles of the same length ||      as 'names'; see :ref:`Field Titles <titles>` below. ||  || 4.   A dictionary of field names ||  ||      The use of this form of specification is discouraged; but documented here ||      because older numpy code may use it. The keys of the dictionary are the ||      field names and the values are tuples specifying type and offset:: ||  ||       >>> np.dtype=({'col1': ('i1';0); 'col2': ('f4';1)}) ||       dtype([(('col1'); 'i1'); (('col2'); '>f4')]) ||  ||      This form is discouraged because Python dictionaries do not preserve order ||      in Python versions before Python 3.6; and the order of the fields in a ||      structured dtype has meaning. :ref:`Field Titles <titles>` may be ||      specified by using a 3-tuple; see below. ||  || Manipulating and Displaying Structured Datatypes || ------------------------------------------------ ||  || The list of field names of a structured datatype can be found in the ``names`` || attribute of the dtype object:: ||  ||  >>> d = np.dtype([('x'; 'i8'); ('y'; 'f4')]) ||  >>> d.names ||  ('x'; 'y') ||  || The field names may be modified by assigning to the ``names`` attribute using a || sequence of strings of the same length. ||  || The dtype object also has a dictionary-like attribute; ``fields``; whose keys || are the field names (and :ref:`Field Titles <titles>`; see below) and whose || values are tuples containing the dtype and byte offset of each field. :: ||  ||  >>> d.fields ||  mappingproxy({'x': (dtype('int64'); 0); 'y': (dtype('float32'); 8)}) ||  || Both the ``names`` and ``fields`` attributes will equal ``None`` for || unstructured arrays. The recommended way to test if a dtype is structured is || with `if dt.names is not None` rather than `if dt.names`; to account for dtypes || with 0 fields. ||  || The string representation of a structured datatype is shown in the \""list of || tuples\"" form if possible; otherwise numpy falls back to using the more general || dictionary form. ||  || .. _offsets-and-alignment: ||  || Automatic Byte Offsets and Alignment || ------------------------------------ ||  || Numpy uses one of two methods to automatically determine the field byte offsets || and the overall itemsize of a structured datatype; depending on whether || ``align=True`` was specified as a keyword argument to :func:`numpy.dtype`. ||  || By default (``align=False``); numpy will pack the fields together such that || each field starts at the byte offset the previous field ended; and the fields || are contiguous in memory. :: ||  ||  >>> def print_offsets(d): ||  ...     print(\""offsets:\""; [d.fields[name][1] for name in d.names]) ||  ...     print(\""itemsize:\""; d.itemsize) ||  >>> print_offsets(np.dtype('u1;u1;i4;u1;i8;u2')) ||  offsets: [0; 1; 2; 6; 7; 15] ||  itemsize: 17 ||  || If ``align=True`` is set; numpy will pad the structure in the same way many C || compilers would pad a C-struct. Aligned structures can give a performance || improvement in some cases; at the cost of increased datatype size. Padding || bytes are inserted between fields such that each field's byte offset will be a || multiple of that field's alignment; which is usually equal to the field's size || in bytes for simple datatypes; see :c:member:`PyArray_Descr.alignment`.  The || structure will also have trailing padding added so that its itemsize is a || multiple of the largest field's alignment. :: ||  ||  >>> print_offsets(np.dtype('u1;u1;i4;u1;i8;u2'; align=True)) ||  offsets: [0; 1; 4; 8; 16; 24] ||  itemsize: 32 ||  || Note that although almost all modern C compilers pad in this way by default; || padding in C structs is C-implementation-dependent so this memory layout is not || guaranteed to exactly match that of a corresponding struct in a C program. Some || work may be needed; either on the numpy side or the C side; to obtain exact || correspondence. ||  || If offsets were specified using the optional ``offsets`` key in the || dictionary-based dtype specification; setting ``align=True`` will check that || each field's offset is a multiple of its size and that the itemsize is a || multiple of the largest field size; and raise an exception if not. ||  || If the offsets of the fields and itemsize of a structured array satisfy the || alignment conditions; the array will have the ``ALIGNED`` :ref:`flag || <numpy.ndarray.flags>` set. ||  || A convenience function :func:`numpy.lib.recfunctions.repack_fields` converts an || aligned dtype or array to a packed one and vice versa. It takes either a dtype || or structured ndarray as an argument; and returns a copy with fields re-packed; || with or without padding bytes. ||  || .. _titles: ||  || Field Titles || ------------ ||  || In addition to field names; fields may also have an associated :term:`title`; || an alternate name; which is sometimes used as an additional description or || alias for the field. The title may be used to index an array; just like a || field name. ||  || To add titles when using the list-of-tuples form of dtype specification; the || field name may be specified as a tuple of two strings instead of a single || string; which will be the field's title and field name respectively. For || example:: ||  ||  >>> np.dtype([(('my title'; 'name'); 'f4')]) ||  || When using the first form of dictionary-based specification; the titles may be || supplied as an extra ``'titles'`` key as described above. When using the second || (discouraged) dictionary-based specification; the title can be supplied by || providing a 3-element tuple ``(datatype; offset; title)`` instead of the usual || 2-element tuple:: ||  ||  >>> np.dtype({'name': ('i4'; 0; 'my title')}) ||  || The ``dtype.fields`` dictionary will contain :term:`titles` as keys; if any || titles are used.  This means effectively that a field with a title will be || represented twice in the fields dictionary. The tuple values for these fields || will also have a third element; the field title. Because of this; and because || the ``names`` attribute preserves the field order while the ``fields`` || attribute may not; it is recommended to iterate through the fields of a dtype || using the ``names`` attribute of the dtype; which will not list titles; as || in:: ||  ||  >>> for name in d.names: ||  ...     print(d.fields[name][:2]) ||  || Union types || ----------- ||  || Structured datatypes are implemented in numpy to have base type || :class:`numpy.void` by default; but it is possible to interpret other numpy || types as structured types using the ``(base_dtype; dtype)`` form of dtype || specification described in || :ref:`Data Type Objects <arrays.dtypes.constructing>`.  Here; ``base_dtype`` is || the desired underlying dtype; and fields and flags will be copied from || ``dtype``. This dtype is similar to a 'union' in C. ||  || Indexing and Assignment to Structured arrays || ============================================ ||  || Assigning data to a Structured Array || ------------------------------------ ||  || There are a number of ways to assign values to a structured array: Using python || tuples; using scalar values; or using other structured arrays. ||  || Assignment from Python Native Types (Tuples) || ```````````````````````````````````````````` ||  || The simplest way to assign values to a structured array is using python tuples. || Each assigned value should be a tuple of length equal to the number of fields || in the array; and not a list or array as these will trigger numpy's || broadcasting rules. The tuple's elements are assigned to the successive fields || of the array; from left to right:: ||  ||  >>> x = np.array([(1;2;3);(4;5;6)]; dtype='i8;f4;f8') ||  >>> x[1] = (7;8;9) ||  >>> x ||  array([(1; 2.; 3.); (7; 8.; 9.)]; ||       dtype=[('f0'; '<i8'); ('f1'; '<f4'); ('f2'; '<f8')]) ||  || Assignment from Scalars || ``````````````````````` ||  || A scalar assigned to a structured element will be assigned to all fields. This || happens when a scalar is assigned to a structured array; or when an || unstructured array is assigned to a structured array:: ||  ||  >>> x = np.zeros(2; dtype='i8;f4;?;S1') ||  >>> x[:] = 3 ||  >>> x ||  array([(3; 3.0; True; b'3'); (3; 3.0; True; b'3')]; ||        dtype=[('f0'; '<i8'); ('f1'; '<f4'); ('f2'; '?'); ('f3'; 'S1')]) ||  >>> x[:] = np.arange(2) ||  >>> x ||  array([(0; 0.0; False; b'0'); (1; 1.0; True; b'1')]; ||        dtype=[('f0'; '<i8'); ('f1'; '<f4'); ('f2'; '?'); ('f3'; 'S1')]) ||  || Structured arrays can also be assigned to unstructured arrays; but only if the || structured datatype has just a single field:: ||  ||  >>> twofield = np.zeros(2; dtype=[('A'; 'i4'); ('B'; 'i4')]) ||  >>> onefield = np.zeros(2; dtype=[('A'; 'i4')]) ||  >>> nostruct = np.zeros(2; dtype='i4') ||  >>> nostruct[:] = twofield ||  ValueError: Can't cast from structure to non-structure; except if the structure only has a single field. ||  >>> nostruct[:] = onefield ||  >>> nostruct ||  array([0; 0]; dtype=int32) ||  || Assignment from other Structured Arrays || ``````````````````````````````````````` ||  || Assignment between two structured arrays occurs as if the source elements had || been converted to tuples and then assigned to the destination elements. That || is; the first field of the source array is assigned to the first field of the || destination array; and the second field likewise; and so on; regardless of || field names. Structured arrays with a different number of fields cannot be || assigned to each other. Bytes of the destination structure which are not || included in any of the fields are unaffected. :: ||  ||  >>> a = np.zeros(3; dtype=[('a'; 'i8'); ('b'; 'f4'); ('c'; 'S3')]) ||  >>> b = np.ones(3; dtype=[('x'; 'f4'); ('y'; 'S3'); ('z'; 'O')]) ||  >>> b[:] = a ||  >>> b ||  array([(0.0; b'0.0'; b''); (0.0; b'0.0'; b''); (0.0; b'0.0'; b'')]; ||        dtype=[('x'; '<f4'); ('y'; 'S3'); ('z'; 'O')]) ||  ||  || Assignment involving subarrays || `````````````````````````````` ||  || When assigning to fields which are subarrays; the assigned value will first be || broadcast to the shape of the subarray. ||  || Indexing Structured Arrays || -------------------------- ||  || Accessing Individual Fields || ``````````````````````````` ||  || Individual fields of a structured array may be accessed and modified by indexing || the array with the field name. :: ||  ||  >>> x = np.array([(1;2);(3;4)]; dtype=[('foo'; 'i8'); ('bar'; 'f4')]) ||  >>> x['foo'] ||  array([1; 3]) ||  >>> x['foo'] = 10 ||  >>> x ||  array([(10; 2.); (10; 4.)]; ||        dtype=[('foo'; '<i8'); ('bar'; '<f4')]) ||  || The resulting array is a view into the original array. It shares the same || memory locations and writing to the view will modify the original array. :: ||  ||  >>> y = x['bar'] ||  >>> y[:] = 10 ||  >>> x ||  array([(10; 5.); (10; 5.)]; ||        dtype=[('foo'; '<i8'); ('bar'; '<f4')]) ||  || This view has the same dtype and itemsize as the indexed field; so it is || typically a non-structured array; except in the case of nested structures. ||  ||  >>> y.dtype; y.shape; y.strides ||  (dtype('float32'); (2;); (12;)) ||  || If the accessed field is a subarray; the dimensions of the subarray || are appended to the shape of the result:: ||  ||    >>> x = np.zeros((2;2); dtype=[('a'; np.int32); ('b'; np.float64; (3;3))]) ||    >>> x['a'].shape ||    (2; 2) ||    >>> x['b'].shape ||    (2; 2; 3; 3) ||  || Accessing Multiple Fields || ``````````````````````````` ||  || One can index and assign to a structured array with a multi-field index; where || the index is a list of field names. ||  || .. warning:: ||     The behavior of multi-field indexes changed from Numpy 1.15 to Numpy 1.16. ||  || The result of indexing with a multi-field index is a view into the original || array; as follows:: ||  ||  >>> a = np.zeros(3; dtype=[('a'; 'i4'); ('b'; 'i4'); ('c'; 'f4')]) ||  >>> a[['a'; 'c']] ||  array([(0; 0.); (0; 0.); (0; 0.)]; ||       dtype={'names':['a';'c']; 'formats':['<i4';'<f4']; 'offsets':[0;8]; 'itemsize':12}) ||  || Assignment to the view modifies the original array. The view's fields will be || in the order they were indexed. Note that unlike for single-field indexing; the || view's dtype has the same itemsize as the original array; and has fields at the || same offsets as in the original array; and unindexed fields are merely missing. ||  || .. warning:: ||     In Numpy 1.15; indexing an array with a multi-field index returned a copy of ||     the result above; but with fields packed together in memory as if ||     passed through :func:`numpy.lib.recfunctions.repack_fields`. ||  ||     The new behavior as of Numpy 1.16 leads to extra \""padding\"" bytes at the ||     location of unindexed fields compared to 1.15. You will need to update any ||     code which depends on the data having a \""packed\"" layout. For instance code ||     such as:: ||  ||      >>> a = np.zeros(3; dtype=[('a'; 'i4'); ('b'; 'i4'); ('c'; 'f4')]) ||      >>> a[['a';'c']].view('i8')  # Fails in Numpy 1.16 ||      ValueError: When changing to a smaller dtype; its size must be a divisor of the size of original dtype ||  ||     will need to be changed. This code has raised a ``FutureWarning`` since ||     Numpy 1.12; and similar code has raised ``FutureWarning`` since 1.7. ||  ||     In 1.16 a number of functions have been introduced in the ||     :module:`numpy.lib.recfunctions` module to help users account for this ||     change. These are ||     :func:`numpy.lib.recfunctions.repack_fields`. ||     :func:`numpy.lib.recfunctions.structured_to_unstructured`; ||     :func:`numpy.lib.recfunctions.unstructured_to_structured`; ||     :func:`numpy.lib.recfunctions.apply_along_fields`; ||     :func:`numpy.lib.recfunctions.assign_fields_by_name`;  and ||     :func:`numpy.lib.recfunctions.require_fields`. ||  ||     The function :func:`numpy.lib.recfunctions.repack_fields` can always be ||     used to reproduce the old behavior; as it will return a packed copy of the ||     structured array. The code above; for example; can be replaced with: ||  ||      >>> repack_fields(a[['a';'c']]).view('i8')  # supported in 1.16 ||      array([0; 0; 0]) ||  ||     Furthermore; numpy now provides a new function ||     :func:`numpy.lib.recfunctions.structured_to_unstructured` which is a safer ||     and more efficient alternative for users who wish to convert structured ||     arrays to unstructured arrays; as the view above is often indeded to do. ||     This function allows safe conversion to an unstructured type taking into ||     account padding; often avoids a copy; and also casts the datatypes ||     as needed; unlike the view. Code such as: ||  ||      >>> a = np.zeros(3; dtype=[('x'; 'f4'); ('y'; 'f4'); ('z'; 'f4')]) ||      >>> a[['x'; 'z']].view('f4') ||  ||     can be made safer by replacing with: ||  ||      >>> structured_to_unstructured(a[['x'; 'z']]) ||      array([0; 0; 0]) ||  ||  || Assignment to an array with a multi-field index modifies the original array:: ||  ||  >>> a[['a'; 'c']] = (2; 3) ||  >>> a ||  array([(2; 0; 3.0); (2; 0; 3.0); (2; 0; 3.0)]; ||        dtype=[('a'; '<i8'); ('b'; '<i4'); ('c'; '<f8')]) ||  || This obeys the structured array assignment rules described above. For example; || this means that one can swap the values of two fields using appropriate || multi-field indexes:: ||  ||  >>> a[['a'; 'c']] = a[['c'; 'a']] ||  || Indexing with an Integer to get a Structured Scalar || ``````````````````````````````````````````````````` ||  || Indexing a single element of a structured array (with an integer index) returns || a structured scalar:: ||  ||  >>> x = np.array([(1; 2.; 3.)]; dtype='i;f;f') ||  >>> scalar = x[0] ||  >>> scalar ||  (1; 2.; 3.) ||  >>> type(scalar) ||  numpy.void ||  || Unlike other numpy scalars; structured scalars are mutable and act like views || into the original array; such that modifying the scalar will modify the || original array. Structured scalars also support access and assignment by field || name:: ||  ||  >>> x = np.array([(1;2);(3;4)]; dtype=[('foo'; 'i8'); ('bar'; 'f4')]) ||  >>> s = x[0] ||  >>> s['bar'] = 100 ||  >>> x ||  array([(1; 100.); (3; 4.)]; ||        dtype=[('foo'; '<i8'); ('bar'; '<f4')]) ||  || Similarly to tuples; structured scalars can also be indexed with an integer:: ||  ||  >>> scalar = np.array([(1; 2.; 3.)]; dtype='i;f;f')[0] ||  >>> scalar[0] ||  1 ||  >>> scalar[1] = 4 ||  || Thus; tuples might be thought of as the native Python equivalent to numpy's || structured types; much like native python integers are the equivalent to || numpy's integer types. Structured scalars may be converted to a tuple by || calling :func:`ndarray.item`:: ||  ||  >>> scalar.item(); type(scalar.item()) ||  ((1; 2.0; 3.0); tuple) ||  || Viewing Structured Arrays Containing Objects || -------------------------------------------- ||  || In order to prevent clobbering object pointers in fields of || :class:`numpy.object` type; numpy currently does not allow views of structured || arrays containing objects. ||  || Structure Comparison || -------------------- ||  || If the dtypes of two void structured arrays are equal; testing the equality of || the arrays will result in a boolean array with the dimensions of the original || arrays; with elements set to ``True`` where all fields of the corresponding || structures are equal. Structured dtypes are equal if the field names; || dtypes and titles are the same; ignoring endianness; and the fields are in || the same order:: ||  ||  >>> a = np.zeros(2; dtype=[('a'; 'i4'); ('b'; 'i4')]) ||  >>> b = np.ones(2; dtype=[('a'; 'i4'); ('b'; 'i4')]) ||  >>> a == b ||  array([False; False]) ||  || Currently; if the dtypes of two void structured arrays are not equivalent the || comparison fails; returning the scalar value ``False``. This behavior is || deprecated as of numpy 1.10 and will raise an error or perform elementwise || comparison in the future. ||  || The ``<`` and ``>`` operators always return ``False`` when comparing void || structured arrays; and arithmetic and bitwise operations are not supported. ||  || Record Arrays || ============= ||  || As an optional convenience numpy provides an ndarray subclass; || :class:`numpy.recarray`; and associated helper functions in the || :mod:`numpy.rec` submodule; that allows access to fields of structured arrays || by attribute instead of only by index. Record arrays also use a special || datatype; :class:`numpy.record`; that allows field access by attribute on the || structured scalars obtained from the array. ||  || The simplest way to create a record array is with :func:`numpy.rec.array`:: ||  ||  >>> recordarr = np.rec.array([(1;2.;'Hello');(2;3.;\""World\"")]; ||  ...                    dtype=[('foo'; 'i4');('bar'; 'f4'); ('baz'; 'S10')]) ||  >>> recordarr.bar ||  array([ 2.;  3.]; dtype=float32) ||  >>> recordarr[1:2] ||  rec.array([(2; 3.0; 'World')]; ||        dtype=[('foo'; '<i4'); ('bar'; '<f4'); ('baz'; 'S10')]) ||  >>> recordarr[1:2].foo ||  array([2]; dtype=int32) ||  >>> recordarr.foo[1:2] ||  array([2]; dtype=int32) ||  >>> recordarr[1].baz ||  'World' ||  || :func:`numpy.rec.array` can convert a wide variety of arguments into record || arrays; including structured arrays:: ||  ||  >>> arr = array([(1;2.;'Hello');(2;3.;\""World\"")]; ||  ...             dtype=[('foo'; 'i4'); ('bar'; 'f4'); ('baz'; 'S10')]) ||  >>> recordarr = np.rec.array(arr) ||  || The :mod:`numpy.rec` module provides a number of other convenience functions for || creating record arrays; see :ref:`record array creation routines || <routines.array-creation.rec>`. ||  || A record array representation of a structured array can be obtained using the || appropriate :ref:`view`:: ||  ||  >>> arr = np.array([(1;2.;'Hello');(2;3.;\""World\"")]; ||  ...                dtype=[('foo'; 'i4');('bar'; 'f4'); ('baz'; 'a10')]) ||  >>> recordarr = arr.view(dtype=dtype((np.record; arr.dtype)); ||  ...                      type=np.recarray) ||  || For convenience; viewing an ndarray as type :class:`np.recarray` will || automatically convert to :class:`np.record` datatype; so the dtype can be left || out of the view:: ||  ||  >>> recordarr = arr.view(np.recarray) ||  >>> recordarr.dtype ||  dtype((numpy.record; [('foo'; '<i4'); ('bar'; '<f4'); ('baz'; 'S10')])) ||  || To get back to a plain ndarray both the dtype and type must be reset. The || following view does so; taking into account the unusual case that the || recordarr was not a structured type:: ||  ||  >>> arr2 = recordarr.view(recordarr.dtype.fields or recordarr.dtype; np.ndarray) ||  || Record array fields accessed by index or by attribute are returned as a record || array if the field has a structured type but as a plain ndarray otherwise. :: ||  ||  >>> recordarr = np.rec.array([('Hello'; (1;2));(\""World\""; (3;4))]; ||  ...                 dtype=[('foo'; 'S6');('bar'; [('A'; int); ('B'; int)])]) ||  >>> type(recordarr.foo) ||  <type 'numpy.ndarray'> ||  >>> type(recordarr.bar) ||  <class 'numpy.core.records.recarray'> ||  || Note that if a field has the same name as an ndarray attribute; the ndarray || attribute takes precedence. Such fields will be inaccessible by attribute but || will still be accessible by index. ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,No
4256,ryfeus/gcf-packs,tensorflow2.0/source/numpy/doc/subclassing.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"\""\""\""============================= || Subclassing ndarray in python || ============================= ||  || Introduction || ------------ ||  || Subclassing ndarray is relatively simple; but it has some complications || compared to other Python objects.  On this page we explain the machinery || that allows you to subclass ndarray; and the implications for || implementing a subclass. ||  || ndarrays and object creation || ============================ ||  || Subclassing ndarray is complicated by the fact that new instances of || ndarray classes can come about in three different ways.  These are: ||  || #. Explicit constructor call - as in ``MySubClass(params)``.  This is ||    the usual route to Python instance creation. || #. View casting - casting an existing ndarray as a given subclass || #. New from template - creating a new instance from a template ||    instance. Examples include returning slices from a subclassed array; ||    creating return types from ufuncs; and copying arrays.  See ||    :ref:`new-from-template` for more details ||  || The last two are characteristics of ndarrays - in order to support || things like array slicing.  The complications of subclassing ndarray are || due to the mechanisms numpy has to support these latter two routes of || instance creation. ||  || .. _view-casting: ||  || View casting || ------------ ||  || *View casting* is the standard ndarray mechanism by which you take an || ndarray of any subclass; and return a view of the array as another || (specified) subclass: ||  || >>> import numpy as np || >>> # create a completely useless ndarray subclass || >>> class C(np.ndarray): pass || >>> # create a standard ndarray || >>> arr = np.zeros((3;)) || >>> # take a view of it; as our useless subclass || >>> c_arr = arr.view(C) || >>> type(c_arr) || <class 'C'> ||  || .. _new-from-template: ||  || Creating new from template || -------------------------- ||  || New instances of an ndarray subclass can also come about by a very || similar mechanism to :ref:`view-casting`; when numpy finds it needs to || create a new instance from a template instance.  The most obvious place || this has to happen is when you are taking slices of subclassed arrays. || For example: ||  || >>> v = c_arr[1:] || >>> type(v) # the view is of type 'C' || <class 'C'> || >>> v is c_arr # but it's a new instance || False ||  || The slice is a *view* onto the original ``c_arr`` data.  So; when we || take a view from the ndarray; we return a new ndarray; of the same || class; that points to the data in the original. ||  || There are other points in the use of ndarrays where we need such views; || such as copying arrays (``c_arr.copy()``); creating ufunc output arrays || (see also :ref:`array-wrap`); and reducing methods (like || ``c_arr.mean()``. ||  || Relationship of view casting and new-from-template || -------------------------------------------------- ||  || These paths both use the same machinery.  We make the distinction here; || because they result in different input to your methods.  Specifically; || :ref:`view-casting` means you have created a new instance of your array || type from any potential subclass of ndarray.  :ref:`new-from-template` || means you have created a new instance of your class from a pre-existing || instance; allowing you - for example - to copy across attributes that || are particular to your subclass. ||  || Implications for subclassing || ---------------------------- ||  || If we subclass ndarray; we need to deal not only with explicit || construction of our array type; but also :ref:`view-casting` or || :ref:`new-from-template`.  NumPy has the machinery to do this; and this || machinery that makes subclassing slightly non-standard. ||  || There are two aspects to the machinery that ndarray uses to support || views and new-from-template in subclasses. ||  || The first is the use of the ``ndarray.__new__`` method for the main work || of object initialization; rather then the more usual ``__init__`` || method.  The second is the use of the ``__array_finalize__`` method to || allow subclasses to clean up after the creation of views and new || instances from templates. ||  || A brief Python primer on ``__new__`` and ``__init__`` || ===================================================== ||  || ``__new__`` is a standard Python method; and; if present; is called || before ``__init__`` when we create a class instance. See the `python || __new__ documentation || <https:\/\/docs.python.org\/reference\/datamodel.html#object.__new__>`_ for more detail. ||  || For example; consider the following Python code: ||  || .. testcode:: ||  ||   class C(object): ||       def __new__(cls; *args): ||           print('Cls in __new__:'; cls) ||           print('Args in __new__:'; args) ||           return object.__new__(cls; *args) ||  ||       def __init__(self; *args): ||           print('type(self) in __init__:'; type(self)) ||           print('Args in __init__:'; args) ||  || meaning that we get: ||  || >>> c = C('hello') || Cls in __new__: <class 'C'> || Args in __new__: ('hello';) || type(self) in __init__: <class 'C'> || Args in __init__: ('hello';) ||  || When we call ``C('hello')``; the ``__new__`` method gets its own class || as first argument; and the passed argument; which is the string || ``'hello'``.  After python calls ``__new__``; it usually (see below) || calls our ``__init__`` method; with the output of ``__new__`` as the || first argument (now a class instance); and the passed arguments || following. ||  || As you can see; the object can be initialized in the ``__new__`` || method or the ``__init__`` method; or both; and in fact ndarray does || not have an ``__init__`` method; because all the initialization is || done in the ``__new__`` method. ||  || Why use ``__new__`` rather than just the usual ``__init__``?  Because || in some cases; as for ndarray; we want to be able to return an object || of some other class.  Consider the following: ||  || .. testcode:: ||  ||   class D(C): ||       def __new__(cls; *args): ||           print('D cls is:'; cls) ||           print('D args in __new__:'; args) ||           return C.__new__(C; *args) ||  ||       def __init__(self; *args): ||           # we never get here ||           print('In D __init__') ||  || meaning that: ||  || >>> obj = D('hello') || D cls is: <class 'D'> || D args in __new__: ('hello';) || Cls in __new__: <class 'C'> || Args in __new__: ('hello';) || >>> type(obj) || <class 'C'> ||  || The definition of ``C`` is the same as before; but for ``D``; the || ``__new__`` method returns an instance of class ``C`` rather than || ``D``.  Note that the ``__init__`` method of ``D`` does not get || called.  In general; when the ``__new__`` method returns an object of || class other than the class in which it is defined; the ``__init__`` || method of that class is not called. ||  || This is how subclasses of the ndarray class are able to return views || that preserve the class type.  When taking a view; the standard || ndarray machinery creates the new ndarray object with something || like:: ||  ||   obj = ndarray.__new__(subtype; shape; ... ||  || where ``subdtype`` is the subclass.  Thus the returned view is of the || same class as the subclass; rather than being of class ``ndarray``. ||  || That solves the problem of returning views of the same type; but now || we have a new problem.  The machinery of ndarray can set the class || this way; in its standard methods for taking views; but the ndarray || ``__new__`` method knows nothing of what we have done in our own || ``__new__`` method in order to set attributes; and so on.  (Aside - || why not call ``obj = subdtype.__new__(...`` then?  Because we may not || have a ``__new__`` method with the same call signature). ||  || The role of ``__array_finalize__`` || ================================== ||  || ``__array_finalize__`` is the mechanism that numpy provides to allow || subclasses to handle the various ways that new instances get created. ||  || Remember that subclass instances can come about in these three ways: ||  || #. explicit constructor call (``obj = MySubClass(params)``).  This will ||    call the usual sequence of ``MySubClass.__new__`` then (if it exists) ||    ``MySubClass.__init__``. || #. :ref:`view-casting` || #. :ref:`new-from-template` ||  || Our ``MySubClass.__new__`` method only gets called in the case of the || explicit constructor call; so we can't rely on ``MySubClass.__new__`` or || ``MySubClass.__init__`` to deal with the view casting and || new-from-template.  It turns out that ``MySubClass.__array_finalize__`` || *does* get called for all three methods of object creation; so this is || where our object creation housekeeping usually goes. ||  || * For the explicit constructor call; our subclass will need to create a ||   new ndarray instance of its own class.  In practice this means that ||   we; the authors of the code; will need to make a call to ||   ``ndarray.__new__(MySubClass;...)``; a class-hierarchy prepared call to ||   ``super(MySubClass; cls).__new__(cls; ...)``; or do view casting of an ||   existing array (see below) || * For view casting and new-from-template; the equivalent of ||   ``ndarray.__new__(MySubClass;...`` is called; at the C level. ||  || The arguments that ``__array_finalize__`` receives differ for the three || methods of instance creation above. ||  || The following code allows us to look at the call sequences and arguments: ||  || .. testcode:: ||  ||    import numpy as np ||  ||    class C(np.ndarray): ||        def __new__(cls; *args; **kwargs): ||            print('In __new__ with class %s' % cls) ||            return super(C; cls).__new__(cls; *args; **kwargs) ||  ||        def __init__(self; *args; **kwargs): ||            # in practice you probably will not need or want an __init__ ||            # method for your subclass ||            print('In __init__ with class %s' % self.__class__) ||  ||        def __array_finalize__(self; obj): ||            print('In array_finalize:') ||            print('   self type is %s' % type(self)) ||            print('   obj type is %s' % type(obj)) ||  ||  || Now: ||  || >>> # Explicit constructor || >>> c = C((10;)) || In __new__ with class <class 'C'> || In array_finalize: ||    self type is <class 'C'> ||    obj type is <type 'NoneType'> || In __init__ with class <class 'C'> || >>> # View casting || >>> a = np.arange(10) || >>> cast_a = a.view(C) || In array_finalize: ||    self type is <class 'C'> ||    obj type is <type 'numpy.ndarray'> || >>> # Slicing (example of new-from-template) || >>> cv = c[:1] || In array_finalize: ||    self type is <class 'C'> ||    obj type is <class 'C'> ||  || The signature of ``__array_finalize__`` is:: ||  ||     def __array_finalize__(self; obj): ||  || One sees that the ``super`` call; which goes to || ``ndarray.__new__``; passes ``__array_finalize__`` the new object; of our || own class (``self``) as well as the object from which the view has been || taken (``obj``).  As you can see from the output above; the ``self`` is || always a newly created instance of our subclass; and the type of ``obj`` || differs for the three instance creation methods: ||  || * When called from the explicit constructor; ``obj`` is ``None`` || * When called from view casting; ``obj`` can be an instance of any ||   subclass of ndarray; including our own. || * When called in new-from-template; ``obj`` is another instance of our ||   own subclass; that we might use to update the new ``self`` instance. ||  || Because ``__array_finalize__`` is the only method that always sees new || instances being created; it is the sensible place to fill in instance || defaults for new object attributes; among other tasks. ||  || This may be clearer with an example. ||  || Simple example - adding an extra attribute to ndarray || ----------------------------------------------------- ||  || .. testcode:: ||  ||   import numpy as np ||  ||   class InfoArray(np.ndarray): ||  ||       def __new__(subtype; shape; dtype=float; buffer=None; offset=0; ||                   strides=None; order=None; info=None): ||           # Create the ndarray instance of our type; given the usual ||           # ndarray input arguments.  This will call the standard ||           # ndarray constructor; but return an object of our type. ||           # It also triggers a call to InfoArray.__array_finalize__ ||           obj = super(InfoArray; subtype).__new__(subtype; shape; dtype; ||                                                   buffer; offset; strides; ||                                                   order) ||           # set the new 'info' attribute to the value passed ||           obj.info = info ||           # Finally; we must return the newly created object: ||           return obj ||  ||       def __array_finalize__(self; obj): ||           # ``self`` is a new object resulting from ||           # ndarray.__new__(InfoArray; ...); therefore it only has ||           # attributes that the ndarray.__new__ constructor gave it - ||           # i.e. those of a standard ndarray. ||           # ||           # We could have got to the ndarray.__new__ call in 3 ways: ||           # From an explicit constructor - e.g. InfoArray(): ||           #    obj is None ||           #    (we're in the middle of the InfoArray.__new__ ||           #    constructor; and self.info will be set when we return to ||           #    InfoArray.__new__) ||           if obj is None: return ||           # From view casting - e.g arr.view(InfoArray): ||           #    obj is arr ||           #    (type(obj) can be InfoArray) ||           # From new-from-template - e.g infoarr[:3] ||           #    type(obj) is InfoArray ||           # ||           # Note that it is here; rather than in the __new__ method; ||           # that we set the default value for 'info'; because this ||           # method sees all creation of default objects - with the ||           # InfoArray.__new__ constructor; but also with ||           # arr.view(InfoArray). ||           self.info = getattr(obj; 'info'; None) ||           # We do not need to return anything ||  ||  || Using the object looks like this: ||  ||   >>> obj = InfoArray(shape=(3;)) # explicit constructor ||   >>> type(obj) ||   <class 'InfoArray'> ||   >>> obj.info is None ||   True ||   >>> obj = InfoArray(shape=(3;); info='information') ||   >>> obj.info ||   'information' ||   >>> v = obj[1:] # new-from-template - here - slicing ||   >>> type(v) ||   <class 'InfoArray'> ||   >>> v.info ||   'information' ||   >>> arr = np.arange(10) ||   >>> cast_arr = arr.view(InfoArray) # view casting ||   >>> type(cast_arr) ||   <class 'InfoArray'> ||   >>> cast_arr.info is None ||   True ||  || This class isn't very useful; because it has the same constructor as the || bare ndarray object; including passing in buffers and shapes and so on. || We would probably prefer the constructor to be able to take an already || formed ndarray from the usual numpy calls to ``np.array`` and return an || object. ||  || Slightly more realistic example - attribute added to existing array || ------------------------------------------------------------------- ||  || Here is a class that takes a standard ndarray that already exists; casts || as our type; and adds an extra attribute. ||  || .. testcode:: ||  ||   import numpy as np ||  ||   class RealisticInfoArray(np.ndarray): ||  ||       def __new__(cls; input_array; info=None): ||           # Input array is an already formed ndarray instance ||           # We first cast to be our class type ||           obj = np.asarray(input_array).view(cls) ||           # add the new attribute to the created instance ||           obj.info = info ||           # Finally; we must return the newly created object: ||           return obj ||  ||       def __array_finalize__(self; obj): ||           # see InfoArray.__array_finalize__ for comments ||           if obj is None: return ||           self.info = getattr(obj; 'info'; None) ||  ||  || So: ||  ||   >>> arr = np.arange(5) ||   >>> obj = RealisticInfoArray(arr; info='information') ||   >>> type(obj) ||   <class 'RealisticInfoArray'> ||   >>> obj.info ||   'information' ||   >>> v = obj[1:] ||   >>> type(v) ||   <class 'RealisticInfoArray'> ||   >>> v.info ||   'information' ||  || .. _array-ufunc: ||  || ``__array_ufunc__`` for ufuncs || ------------------------------ ||  ||   .. versionadded:: 1.13 ||  || A subclass can override what happens when executing numpy ufuncs on it by || overriding the default ``ndarray.__array_ufunc__`` method. This method is || executed *instead* of the ufunc and should return either the result of the || operation; or :obj:`NotImplemented` if the operation requested is not || implemented. ||  || The signature of ``__array_ufunc__`` is:: ||  ||     def __array_ufunc__(ufunc; method; *inputs; **kwargs): ||  ||     - *ufunc* is the ufunc object that was called. ||     - *method* is a string indicating how the Ufunc was called; either ||       ``\""__call__\""`` to indicate it was called directly; or one of its ||       :ref:`methods<ufuncs.methods>`: ``\""reduce\""``; ``\""accumulate\""``; ||       ``\""reduceat\""``; ``\""outer\""``; or ``\""at\""``. ||     - *inputs* is a tuple of the input arguments to the ``ufunc`` ||     - *kwargs* contains any optional or keyword arguments passed to the ||       function. This includes any ``out`` arguments; which are always ||       contained in a tuple. ||  || A typical implementation would convert any inputs or outputs that are || instances of one's own class; pass everything on to a superclass using || ``super()``; and finally return the results after possible || back-conversion. An example; taken from the test case || ``test_ufunc_override_with_super`` in ``core\/tests\/test_umath.py``; is the || following. ||  || .. testcode:: ||  ||     input numpy as np ||  ||     class A(np.ndarray): ||         def __array_ufunc__(self; ufunc; method; *inputs; **kwargs): ||             args = [] ||             in_no = [] ||             for i; input_ in enumerate(inputs): ||                 if isinstance(input_; A): ||                     in_no.append(i) ||                     args.append(input_.view(np.ndarray)) ||                 else: ||                     args.append(input_) ||  ||             outputs = kwargs.pop('out'; None) ||             out_no = [] ||             if outputs: ||                 out_args = [] ||                 for j; output in enumerate(outputs): ||                     if isinstance(output; A): ||                         out_no.append(j) ||                         out_args.append(output.view(np.ndarray)) ||                     else: ||                         out_args.append(output) ||                 kwargs['out'] = tuple(out_args) ||             else: ||                 outputs = (None;) * ufunc.nout ||  ||             info = {} ||             if in_no: ||                 info['inputs'] = in_no ||             if out_no: ||                 info['outputs'] = out_no ||  ||             results = super(A; self).__array_ufunc__(ufunc; method; ||                                                      *args; **kwargs) ||             if results is NotImplemented: ||                 return NotImplemented ||  ||             if method == 'at': ||                 if isinstance(inputs[0]; A): ||                     inputs[0].info = info ||                 return ||  ||             if ufunc.nout == 1: ||                 results = (results;) ||  ||             results = tuple((np.asarray(result).view(A) ||                              if output is None else output) ||                             for result; output in zip(results; outputs)) ||             if results and isinstance(results[0]; A): ||                 results[0].info = info ||  ||             return results[0] if len(results) == 1 else results ||  || So; this class does not actually do anything interesting: it just || converts any instances of its own to regular ndarray (otherwise; we'd || get infinite recursion!); and adds an ``info`` dictionary that tells || which inputs and outputs it converted. Hence; e.g.; ||  || >>> a = np.arange(5.).view(A) || >>> b = np.sin(a) || >>> b.info || {'inputs': [0]} || >>> b = np.sin(np.arange(5.); out=(a;)) || >>> b.info || {'outputs': [0]} || >>> a = np.arange(5.).view(A) || >>> b = np.ones(1).view(A) || >>> c = a + b || >>> c.info || {'inputs': [0; 1]} || >>> a += b || >>> a.info || {'inputs': [0; 1]; 'outputs': [0]} ||  || Note that another approach would be to to use ``getattr(ufunc; || methods)(*inputs; **kwargs)`` instead of the ``super`` call. For this example; || the result would be identical; but there is a difference if another operand || also defines ``__array_ufunc__``. E.g.; lets assume that we evalulate || ``np.add(a; b)``; where ``b`` is an instance of another class ``B`` that has || an override.  If you use ``super`` as in the example; || ``ndarray.__array_ufunc__`` will notice that ``b`` has an override; which || means it cannot evaluate the result itself. Thus; it will return || `NotImplemented` and so will our class ``A``. Then; control will be passed || over to ``b``; which either knows how to deal with us and produces a result; || or does not and returns `NotImplemented`; raising a ``TypeError``. ||  || If instead; we replace our ``super`` call with ``getattr(ufunc; method)``; we || effectively do ``np.add(a.view(np.ndarray); b)``. Again; ``B.__array_ufunc__`` || will be called; but now it sees an ``ndarray`` as the other argument. Likely; || it will know how to handle this; and return a new instance of the ``B`` class || to us. Our example class is not set up to handle this; but it might well be || the best approach if; e.g.; one were to re-implement ``MaskedArray`` using || ``__array_ufunc__``. ||  || As a final note: if the ``super`` route is suited to a given class; an || advantage of using it is that it helps in constructing class hierarchies. || E.g.; suppose that our other class ``B`` also used the ``super`` in its || ``__array_ufunc__`` implementation; and we created a class ``C`` that depended || on both; i.e.; ``class C(A; B)`` (with; for simplicity; not another || ``__array_ufunc__`` override). Then any ufunc on an instance of ``C`` would || pass on to ``A.__array_ufunc__``; the ``super`` call in ``A`` would go to || ``B.__array_ufunc__``; and the ``super`` call in ``B`` would go to || ``ndarray.__array_ufunc__``; thus allowing ``A`` and ``B`` to collaborate. ||  || .. _array-wrap: ||  || ``__array_wrap__`` for ufuncs and other functions || ------------------------------------------------- ||  || Prior to numpy 1.13; the behaviour of ufuncs could only be tuned using || ``__array_wrap__`` and ``__array_prepare__``. These two allowed one to || change the output type of a ufunc; but; in contrast to || ``__array_ufunc__``; did not allow one to make any changes to the inputs. || It is hoped to eventually deprecate these; but ``__array_wrap__`` is also || used by other numpy functions and methods; such as ``squeeze``; so at the || present time is still needed for full functionality. ||  || Conceptually; ``__array_wrap__`` \""wraps up the action\"" in the sense of || allowing a subclass to set the type of the return value and update || attributes and metadata.  Let's show how this works with an example.  First || we return to the simpler example subclass; but with a different name and || some print statements: ||  || .. testcode:: ||  ||   import numpy as np ||  ||   class MySubClass(np.ndarray): ||  ||       def __new__(cls; input_array; info=None): ||           obj = np.asarray(input_array).view(cls) ||           obj.info = info ||           return obj ||  ||       def __array_finalize__(self; obj): ||           print('In __array_finalize__:') ||           print('   self is %s' % repr(self)) ||           print('   obj is %s' % repr(obj)) ||           if obj is None: return ||           self.info = getattr(obj; 'info'; None) ||  ||       def __array_wrap__(self; out_arr; context=None): ||           print('In __array_wrap__:') ||           print('   self is %s' % repr(self)) ||           print('   arr is %s' % repr(out_arr)) ||           # then just call the parent ||           return super(MySubClass; self).__array_wrap__(self; out_arr; context) ||  || We run a ufunc on an instance of our new array: ||  || >>> obj = MySubClass(np.arange(5); info='spam') || In __array_finalize__: ||    self is MySubClass([0; 1; 2; 3; 4]) ||    obj is array([0; 1; 2; 3; 4]) || >>> arr2 = np.arange(5)+1 || >>> ret = np.add(arr2; obj) || In __array_wrap__: ||    self is MySubClass([0; 1; 2; 3; 4]) ||    arr is array([1; 3; 5; 7; 9]) || In __array_finalize__: ||    self is MySubClass([1; 3; 5; 7; 9]) ||    obj is MySubClass([0; 1; 2; 3; 4]) || >>> ret || MySubClass([1; 3; 5; 7; 9]) || >>> ret.info || 'spam' ||  || Note that the ufunc (``np.add``) has called the ``__array_wrap__`` method || with arguments ``self`` as ``obj``; and ``out_arr`` as the (ndarray) result || of the addition.  In turn; the default ``__array_wrap__`` || (``ndarray.__array_wrap__``) has cast the result to class ``MySubClass``; || and called ``__array_finalize__`` - hence the copying of the ``info`` || attribute.  This has all happened at the C level. ||  || But; we could do anything we wanted: ||  || .. testcode:: ||  ||   class SillySubClass(np.ndarray): ||  ||       def __array_wrap__(self; arr; context=None): ||           return 'I lost your data' ||  || >>> arr1 = np.arange(5) || >>> obj = arr1.view(SillySubClass) || >>> arr2 = np.arange(5) || >>> ret = np.multiply(obj; arr2) || >>> ret || 'I lost your data' ||  || So; by defining a specific ``__array_wrap__`` method for our subclass; || we can tweak the output from ufuncs. The ``__array_wrap__`` method || requires ``self``; then an argument - which is the result of the ufunc - || and an optional parameter *context*. This parameter is returned by || ufuncs as a 3-element tuple: (name of the ufunc; arguments of the ufunc; || domain of the ufunc); but is not set by other numpy functions. Though; || as seen above; it is possible to do otherwise; ``__array_wrap__`` should || return an instance of its containing class.  See the masked array || subclass for an implementation. ||  || In addition to ``__array_wrap__``; which is called on the way out of the || ufunc; there is also an ``__array_prepare__`` method which is called on || the way into the ufunc; after the output arrays are created but before any || computation has been performed. The default implementation does nothing || but pass through the array. ``__array_prepare__`` should not attempt to || access the array data or resize the array; it is intended for setting the || output array type; updating attributes and metadata; and performing any || checks based on the input that may be desired before computation begins. || Like ``__array_wrap__``; ``__array_prepare__`` must return an ndarray or || subclass thereof or raise an error. ||  || Extra gotchas - custom ``__del__`` methods and ndarray.base || ----------------------------------------------------------- ||  || One of the problems that ndarray solves is keeping track of memory || ownership of ndarrays and their views.  Consider the case where we have || created an ndarray; ``arr`` and have taken a slice with ``v = arr[1:]``. || The two objects are looking at the same memory.  NumPy keeps track of || where the data came from for a particular array or view; with the || ``base`` attribute: ||  || >>> # A normal ndarray; that owns its own data || >>> arr = np.zeros((4;)) || >>> # In this case; base is None || >>> arr.base is None || True || >>> # We take a view || >>> v1 = arr[1:] || >>> # base now points to the array that it derived from || >>> v1.base is arr || True || >>> # Take a view of a view || >>> v2 = v1[1:] || >>> # base points to the view it derived from || >>> v2.base is v1 || True ||  || In general; if the array owns its own memory; as for ``arr`` in this || case; then ``arr.base`` will be None - there are some exceptions to this || - see the numpy book for more details. ||  || The ``base`` attribute is useful in being able to tell whether we have || a view or the original array.  This in turn can be useful if we need || to know whether or not to do some specific cleanup when the subclassed || array is deleted.  For example; we may only want to do the cleanup if || the original array is deleted; but not the views.  For an example of || how this can work; have a look at the ``memmap`` class in || ``numpy.core``. ||  || Subclassing and Downstream Compatibility || ---------------------------------------- ||  || When sub-classing ``ndarray`` or creating duck-types that mimic the ``ndarray`` || interface; it is your responsibility to decide how aligned your APIs will be || with those of numpy. For convenience; many numpy functions that have a corresponding || ``ndarray`` method (e.g.; ``sum``; ``mean``; ``take``; ``reshape``) work by checking || if the first argument to a function has a method of the same name. If it exists; the || method is called instead of coercing the arguments to a numpy array. ||  || For example; if you want your sub-class or duck-type to be compatible with || numpy's ``sum`` function; the method signature for this object's ``sum`` method || should be the following: ||  || .. testcode:: ||  ||     def sum(self; axis=None; dtype=None; out=None; keepdims=False): ||     ... ||  || This is the exact same method signature for ``np.sum``; so now if a user calls || ``np.sum`` on this object; numpy will call the object's own ``sum`` method and || pass in these arguments enumerated above in the signature; and no errors will || be raised because the signatures are completely compatible with each other. ||  || If; however; you decide to deviate from this signature and do something like this: ||  || .. testcode:: ||  ||    def sum(self; axis=None; dtype=None): ||    ... ||  || This object is no longer compatible with ``np.sum`` because if you call ``np.sum``; || it will pass in unexpected arguments ``out`` and ``keepdims``; causing a TypeError || to be raised. ||  || If you wish to maintain compatibility with numpy and its subsequent versions (which || might add new keyword arguments) but do not want to surface all of numpy's arguments; || your function's signature should accept ``**kwargs``. For example: ||  || .. testcode:: ||  ||    def sum(self; axis=None; dtype=None; **unused_kwargs): ||    ... ||  || This object is now compatible with ``np.sum`` again because any extraneous arguments || (i.e. keywords that are not ``axis`` or ``dtype``) will be hidden away in the || ``**unused_kwargs`` parameter. ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4257,ryfeus/gcf-packs,tensorflow2.0/source/numpy/f2py/crackfortran.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,XXX: non-zero reset values need testing,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4258,ryfeus/gcf-packs,tensorflow2.0/source/numpy/f2py/crackfortran.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,TODO: test .eq.; .neq.; etc replacements.,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4259,ryfeus/gcf-packs,tensorflow2.0/source/numpy/lib/info.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"\""\""\"" || Basic functions used by several sub-packages and || useful to have in the main name-space. ||  || Type Handling || ------------- || ================ =================== || iscomplexobj     Test for complex object; scalar result || isrealobj        Test for real object; scalar result || iscomplex        Test for complex elements; array result || isreal           Test for real elements; array result || imag             Imaginary part || real             Real part || real_if_close    Turns complex number with tiny imaginary part to real || isneginf         Tests for negative infinity; array result || isposinf         Tests for positive infinity; array result || isnan            Tests for nans; array result || isinf            Tests for infinity; array result || isfinite         Tests for finite numbers; array result || isscalar         True if argument is a scalar || nan_to_num       Replaces NaN's with 0 and infinities with large numbers || cast             Dictionary of functions to force cast to each type || common_type      Determine the minimum common type code for a group ||                  of arrays || mintypecode      Return minimal allowed common typecode. || ================ =================== ||  || Index Tricks || ------------ || ================ =================== || mgrid            Method which allows easy construction of N-d ||                  'mesh-grids' || ``r_``           Append and construct arrays: turns slice objects into ||                  ranges and concatenates them; for 2d arrays appends rows. || index_exp        Konrad Hinsen's index_expression class instance which ||                  can be useful for building complicated slicing syntax. || ================ =================== ||  || Useful Functions || ---------------- || ================ =================== || select           Extension of where to multiple conditions and choices || extract          Extract 1d array from flattened array according to mask || insert           Insert 1d array of values into Nd array according to mask || linspace         Evenly spaced samples in linear space || logspace         Evenly spaced samples in logarithmic space || fix              Round x to nearest integer towards zero || mod              Modulo mod(x;y) = x % y except keeps sign of y || amax             Array maximum along axis || amin             Array minimum along axis || ptp              Array max-min along axis || cumsum           Cumulative sum along axis || prod             Product of elements along axis || cumprod          Cumluative product along axis || diff             Discrete differences along axis || angle            Returns angle of complex argument || unwrap           Unwrap phase along given axis (1-d algorithm) || sort_complex     Sort a complex-array (based on real; then imaginary) || trim_zeros       Trim the leading and trailing zeros from 1D array. || vectorize        A class that wraps a Python function taking scalar ||                  arguments into a generalized function which can handle ||                  arrays of arguments using the broadcast rules of ||                  numerix Python. || ================ =================== ||  || Shape Manipulation || ------------------ || ================ =================== || squeeze          Return a with length-one dimensions removed. || atleast_1d       Force arrays to be >= 1D || atleast_2d       Force arrays to be >= 2D || atleast_3d       Force arrays to be >= 3D || vstack           Stack arrays vertically (row on row) || hstack           Stack arrays horizontally (column on column) || column_stack     Stack 1D arrays as columns into 2D array || dstack           Stack arrays depthwise (along third dimension) || stack            Stack arrays along a new axis || split            Divide array into a list of sub-arrays || hsplit           Split into columns || vsplit           Split into rows || dsplit           Split along third dimension || ================ =================== ||  || Matrix (2D Array) Manipulations || ------------------------------- || ================ =================== || fliplr           2D array with columns flipped || flipud           2D array with rows flipped || rot90            Rotate a 2D array a multiple of 90 degrees || eye              Return a 2D array with ones down a given diagonal || diag             Construct a 2D array from a vector; or return a given ||                  diagonal from a 2D array. || mat              Construct a Matrix || bmat             Build a Matrix from blocks || ================ =================== ||  || Polynomials || ----------- || ================ =================== || poly1d           A one-dimensional polynomial class || poly             Return polynomial coefficients from roots || roots            Find roots of polynomial given coefficients || polyint          Integrate polynomial || polyder          Differentiate polynomial || polyadd          Add polynomials || polysub          Subtract polynomials || polymul          Multiply polynomials || polydiv          Divide polynomials || polyval          Evaluate polynomial at given argument || ================ =================== ||  || Iterators || --------- || ================ =================== || Arrayterator     A buffered iterator for big arrays. || ================ =================== ||  || Import Tricks || ------------- || ================ =================== || ppimport         Postpone module import until trying to use it || ppimport_attr    Postpone module import until trying to use its attribute || ppresolve        Import postponed module and return it. || ================ =================== ||  || Machine Arithmetics || ------------------- || ================ =================== || machar_single    Single precision floating point arithmetic parameters || machar_double    Double precision floating point arithmetic parameters || ================ =================== ||  || Threading Tricks || ---------------- || ================ =================== || ParallelExec     Execute commands in parallel thread. || ================ =================== ||  || Array Set Operations || ----------------------- || Set operations for numeric arrays based on sort() function. ||  || ================ =================== || unique           Unique elements of an array. || isin             Test whether each element of an ND array is present  ||                  anywhere within a second array. || ediff1d          Array difference (auxiliary function). || intersect1d      Intersection of 1D arrays with unique elements. || setxor1d         Set exclusive-or of 1D arrays with unique elements. || in1d             Test whether elements in a 1D array are also present in ||                  another array. || union1d          Union of 1D arrays with unique elements. || setdiff1d        Set difference of 1D arrays with unique elements. || ================ =================== ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,No
4260,ryfeus/gcf-packs,tensorflow2.0/source/pasta/base/scope_test.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,TODO: Test references to C.aaa; C.bbb once supported,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,No
4261,ryfeus/gcf-packs,tensorflow2.0/source/pkg_resources/_vendor/packaging/requirements.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,TODO: Can we test whether something is contained within a requirement?,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4262,ryfeus/gcf-packs,tensorflow2.0/source/setuptools/_vendor/packaging/requirements.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,TODO: Can we test whether something is contained within a requirement?,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4263,ryfeus/gcf-packs,tensorflow2.0/source/setuptools/command/easy_install.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,XXX should this check for latest version?,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4264,ryfeus/gcf-packs,tensorflow2.0/source/tensorboard/_vendor/html5lib/html5parser.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,XXX Need tests that trigger the following,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4265,ryfeus/gcf-packs,tensorflow2.0/source/tensorboard/_vendor/html5lib/treebuilders/__init__.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"\""\""\""A collection of modules for building different kinds of tree from || HTML documents. ||  || To create a treebuilder for a new type of tree; you need to do || implement several things: ||  || 1) A set of classes for various types of elements: Document; Doctype; || Comment; Element. These must implement the interface of || _base.treebuilders.Node (although comment nodes have a different || signature for their constructor; see treebuilders.etree.Comment) || Textual content may also be implemented as another node type; or not; as || your tree implementation requires. ||  || 2) A treebuilder object (called TreeBuilder by convention) that || inherits from treebuilders._base.TreeBuilder. This has 4 required attributes: || documentClass - the class to use for the bottommost node of a document || elementClass - the class to use for HTML Elements || commentClass - the class to use for comments || doctypeClass - the class to use for doctypes || It also has one required method: || getDocument - Returns the root node of the complete document tree ||  || 3) If you wish to run the unit tests; you must also create a || testSerializer method on your treebuilder which accepts a node and || returns a string containing Node and its children serialized according || to the format used in the unittests || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,No
4266,ryfeus/gcf-packs,tensorflow2.0/source/tensorflow/python/ops/linalg/linear_operator_test_util.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,TODO(langmore) Test tf.float16 once tf.matrix_solve works in 16bit.,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,No
4267,ryfeus/gcf-packs,tensorflow2.0/source/tensorflow/python/tools/strip_unused.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,"r\""\""\""Removes unneeded nodes from a GraphDef file. ||  || This script is designed to help streamline models; by taking the input and || output nodes that will be used by an application and figuring out the smallest || set of operations that are required to run for those arguments. The resulting || minimal graph is then saved out. ||  || The advantages of running this script are: ||  - You may be able to shrink the file size. ||  - Operations that are unsupported on your platform but still present can be ||    safely removed. || The resulting graph may not be as flexible as the original though; since any || input nodes that weren't explicitly mentioned may not be accessible any more. ||  || An example of command-line usage is: || bazel build tensorflow\/python\/tools:strip_unused && \\ || bazel-bin\/tensorflow\/python\/tools\/strip_unused \\ || --input_graph=some_graph_def.pb \\ || --output_graph=\/tmp\/stripped_graph.pb \\ || --input_node_names=input0 || --output_node_names=softmax ||  || You can also look at strip_unused_test.py for an example of how to use it. ||  || \""\""\""",https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
4268,keithjjones/malgazer,library/files.py,c4f135a1ca6ee8639a2f7b3aefc4c4f324cc75e2,TODO: Above may be section.SizeOfRawData - test this.,https://github.com/keithjjones/malgazer/commit/c4f135a1ca6ee8639a2f7b3aefc4c4f324cc75e2,Yes
4269,keithjjones/malgazer,library/files.py,a2195888c71455f07290978f9bb5abe35209256d,TODO: Above may be section.SizeOfRawData - test this.,https://github.com/keithjjones/malgazer/commit/a2195888c71455f07290978f9bb5abe35209256d,Yes
4270,keithjjones/malgazer,library/files.py,2758264b2d1412554691dd8f9c7f9e6a7428e7a8,TODO: Above may be section.Misc_VirtualSize - test this.,https://github.com/keithjjones/malgazer/commit/2758264b2d1412554691dd8f9c7f9e6a7428e7a8,Yes
4271,funkey/gunpowder,gunpowder/nodes/intensity_augment.py,7e74c1d95f0c7f44c87cb098ba60d1cf649cf7eb,TODO: write a test for this node,https://github.com/funkey/gunpowder/commit/7e74c1d95f0c7f44c87cb098ba60d1cf649cf7eb,No
4272,funkey/gunpowder,gunpowder/nodes/intensity_augment.py,343358efac96d3db6bc8246ca010faadc1c49ae6,TODO: write a test for this node,https://github.com/funkey/gunpowder/commit/343358efac96d3db6bc8246ca010faadc1c49ae6,No
4273,jefkine/zeta-learn,ztlearn/utils/data_utils.py,b70f3ec614f831b250af418e4aa82036c39bd568,@@TODO: split the sample_folds and label_folds into sizes K-1 and 1 for test and training sets,https://github.com/jefkine/zeta-learn/commit/b70f3ec614f831b250af418e4aa82036c39bd568,No
4274,nielstron/quantulum3,quantulum3/parser.py,d26b046d7ff355cc483e7b6c764ef9655a78bcf4,TODO make sure that this is tested at some point,https://github.com/nielstron/quantulum3/commit/d26b046d7ff355cc483e7b6c764ef9655a78bcf4,No
4275,nielstron/quantulum3,quantulum3/parser.py,ca2bca27c9fce0270e104f21e683d23b8766b281,TODO make sure that this is tested at some point,https://github.com/nielstron/quantulum3/commit/ca2bca27c9fce0270e104f21e683d23b8766b281,No
4276,SekouD/mlconjug,setup.py,76495ce4b6d7f99d4bd131b1cf77384d751439d8,TODO: put package test requirements here,https://github.com/SekouD/mlconjug/commit/76495ce4b6d7f99d4bd131b1cf77384d751439d8,No
4277,chrislit/abydos,abydos/tests/test_clustering.py,8d270c21424352b490e12be798bbcc50dca52eb2,TODO: add non-trivial tests,https://github.com/chrislit/abydos/commit/8d270c21424352b490e12be798bbcc50dca52eb2,No
4278,chrislit/abydos,abydos/tests/test_distance.py,58124166d0296db8e97d377fc5aeb9750fdd6b5b,TODO: Add bias test(s) and unequal alpha & beta tests,https://github.com/chrislit/abydos/commit/58124166d0296db8e97d377fc5aeb9750fdd6b5b,Yes
4279,chrislit/abydos,abydos/tests/test_distance.py,58124166d0296db8e97d377fc5aeb9750fdd6b5b,TODO: find non-trivial strcmp95 tests or manufacture some,https://github.com/chrislit/abydos/commit/58124166d0296db8e97d377fc5aeb9750fdd6b5b,Yes
4280,chrislit/abydos,abydos/tests/test_distance.py,02325f737a34587aa03617689cc0509227ecef37,TODO: Add bias test(s) and unequal alpha & beta tests,https://github.com/chrislit/abydos/commit/02325f737a34587aa03617689cc0509227ecef37,Yes
4281,chrislit/abydos,abydos/tests/test_distance.py,02325f737a34587aa03617689cc0509227ecef37,TODO: find non-trivial strcmp95 tests or manufacture some,https://github.com/chrislit/abydos/commit/02325f737a34587aa03617689cc0509227ecef37,Yes
4282,chrislit/abydos,abydos/phones.py,6da092970e952341c60ab024a4e35c36a820f239,TODO: finish implementation\/testing\/tuning,https://github.com/chrislit/abydos/commit/6da092970e952341c60ab024a4e35c36a820f239,No
4283,chrislit/abydos,abydos/phones/_phones.py,0186998d684753e070edf1fdc124aaa56133db27,TODO: finish implementation\/testing\/tuning,https://github.com/chrislit/abydos/commit/0186998d684753e070edf1fdc124aaa56133db27,No
4284,explosion/spaCy,spacy/tests/conftest.py,26849874adcad723e7ed7883948df34098426086,TODO: re-implement generic tokenizer tests,https://github.com/explosion/spaCy/commit/26849874adcad723e7ed7883948df34098426086,Yes
4285,explosion/spaCy,spacy/tests/conftest.py,2e84ec151320866efaa13b4f48351865cf481330,TODO: re-implement generic tokenizer tests,https://github.com/explosion/spaCy/commit/2e84ec151320866efaa13b4f48351865cf481330,Yes
4286,explosion/spaCy,spacy/tests/conftest.py,b41d64825af40c0bff5e9daff14d94f4d4618ec9,TODO: re-implement generic tokenizer tests,https://github.com/explosion/spaCy/commit/b41d64825af40c0bff5e9daff14d94f4d4618ec9,Yes
4287,explosion/spaCy,spacy/tests/conftest.py,1448ad100cfa3642904ca5e23426f37fca13905e,TODO: re-implement generic tokenizer tests,https://github.com/explosion/spaCy/commit/1448ad100cfa3642904ca5e23426f37fca13905e,Yes
4288,explosion/spaCy,spacy/tests/conftest.py,f1c3108d52e3306a8f2c91b1f71d9d69b88ac672,TODO: re-implement generic tokenizer tests,https://github.com/explosion/spaCy/commit/f1c3108d52e3306a8f2c91b1f71d9d69b88ac672,Yes
4289,explosion/spaCy,examples/training/pretrain_kb.py,0ba1b5eebcf8d7d8a5c1e2859948622481df70d7,"\""\""\""Example of defining and (pre)training spaCy's knowledge base; || which is needed to implement entity linking functionality. ||  || For more details; see the documentation: || * Knowledge base: https:\/\/spacy.io\/api\/kb || * Entity Linking: https:\/\/spacy.io\/usage\/linguistic-features#entity-linking ||  || Compatible with: spaCy vX.X || Last tested with: vX.X || \""\""\""",https://github.com/explosion/spaCy/commit/0ba1b5eebcf8d7d8a5c1e2859948622481df70d7,No
4290,explosion/spaCy,spacy/tests/lang/hy/test_tokenizer.py,beef184e53f5fed4721a69190958a6b0b4cf6a89,TODO add test cases with valid punctuation signs.,https://github.com/explosion/spaCy/commit/beef184e53f5fed4721a69190958a6b0b4cf6a89,Yes
4291,explosion/spaCy,spacy/cli/pretrain.py,4ed6278663c9482e14b549b2079f02cc186bc078,Load in pretrained weights - TODO test,https://github.com/explosion/spaCy/commit/4ed6278663c9482e14b549b2079f02cc186bc078,No
4292,explosion/spaCy,examples/training/rehearsal.py,8c29268749fc7ffc47ed662d5fb65dc8c57157f9,TODO: further fix & test this script for v.3 ? (read_gold_data is never called),https://github.com/explosion/spaCy/commit/8c29268749fc7ffc47ed662d5fb65dc8c57157f9,Yes
4293,explosion/spaCy,examples/training/rehearsal.py,fcbf899b086c785261abe9ddb776af9615fd1f58,TODO: further fix & test this script for v.3 ? (read_gold_data is never called),https://github.com/explosion/spaCy/commit/fcbf899b086c785261abe9ddb776af9615fd1f58,Yes
4294,explosion/spaCy,spacy/tests/test_language.py,43b960c01b0c64e56859ad5eb304a5422af46516,TODO: add more tests,https://github.com/explosion/spaCy/commit/43b960c01b0c64e56859ad5eb304a5422af46516,No
4295,explosion/spaCy,spacy/cli/project/pull.py,17a6b0a1731321380914d3638e7e3bc25fd23a28,TODO: We don't have tests for this :(. It would take a bit of mockery to,https://github.com/explosion/spaCy/commit/17a6b0a1731321380914d3638e7e3bc25fd23a28,Yes
4296,explosion/spaCy,spacy/tests/regression/test_issue5918.py,549758f67dea544ec64271fe88513dbc4117fed8,TODO: test for logging here,https://github.com/explosion/spaCy/commit/549758f67dea544ec64271fe88513dbc4117fed8,No
4297,explosion/spaCy,spacy/tests/regression/test_issue5501-6000.py,b1d568a4dffca6828824953d0cfa0e56ae3cfbfc,TODO: test for logging here,https://github.com/explosion/spaCy/commit/b1d568a4dffca6828824953d0cfa0e56ae3cfbfc,No
4298,scikit-learn/scikit-learn,scikits/learn/svm/tests/test_classification.py,3aa08282f37a3c6b78d5e788860281993e072457,XXX this test fails,https://github.com/scikit-learn/scikit-learn/commit/3aa08282f37a3c6b78d5e788860281993e072457,Yes
4299,scikit-learn/scikit-learn,scikits/learn/svm/tests/test_classification.py,3aa08282f37a3c6b78d5e788860281993e072457,XXX these tests fail,https://github.com/scikit-learn/scikit-learn/commit/3aa08282f37a3c6b78d5e788860281993e072457,Yes
4300,scikit-learn/scikit-learn,scikits/learn/tests/test_svm.py,2f4844ca54934b842fd76a5fe38b2c4de090b666,"\""\""\"" || Testing for Support Vector Machine ||  || TODO: remove hardcoding numerical results when possible || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/2f4844ca54934b842fd76a5fe38b2c4de090b666,Yes
4301,scikit-learn/scikit-learn,scikits/learn/tests/test_svm.py,1d3754b5a9e9c354b258dac0e94fdfde5f47f3c1,"\""\""\"" || Testing for Support Vector Machine ||  || TODO: remove hardcoding numerical results when possible || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/1d3754b5a9e9c354b258dac0e94fdfde5f47f3c1,Yes
4302,scikit-learn/scikit-learn,scikits/learn/glm.py,27d3900dceac69273f9208e983752a5338574a58,# TODO: this should have a tests.,https://github.com/scikit-learn/scikit-learn/commit/27d3900dceac69273f9208e983752a5338574a58,Yes
4303,scikit-learn/scikit-learn,scikits/learn/features/text.py,3e7af84136ffb9bb04468439f028c2e795e2b7a3,TODO: write unittests instead!,https://github.com/scikit-learn/scikit-learn/commit/3e7af84136ffb9bb04468439f028c2e795e2b7a3,Yes
4304,scikit-learn/scikit-learn,examples/mlcomp_document_classification.py,434eed65b4b2eb5f0f8814467576cd808742ccfd,"\""\""\"" || ================================ || Classification of text documents || ================================ ||  || This is an example showing how the scikit-learn can be used to classify || documents by topics using a bag-of-words approach. ||  || The dataset used in this example is the 20 newsgroups dataset and should be || downloaded from the http:\/\/mlcomp.org (free registration required): ||  ||   http:\/\/mlcomp.org\/datasets\/379 ||  || Once downloaded unzip the arhive somewhere on your filesystem. For instance in:: ||  ||   % mkdir -p ~\/data\/mlcomp ||   % cd  ~\/data\/mlcomp ||   % unzip \/path\/to\/dataset-379-20news-18828_XXXXX.zip ||  || You should get a folder ``~\/data\/mlcomp\/379`` with a file named ``metadata`` and || subfolders ``raw``; ``train`` and ``test`` holding the text documents organized by || newsgroups. ||  || Then set the ``MLCOMP_DATASETS_HOME`` environment variable pointing to || the root folder holding the uncompressed archive:: ||  ||   % export MLCOMP_DATASETS_HOME=\""~\/data\/mlcomp\"" ||  || Then you are ready to run this example using your favorite python shell:: ||  ||   % ipython examples\/mlcomp_document_classification.py ||  || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/434eed65b4b2eb5f0f8814467576cd808742ccfd,Yes
4305,scikit-learn/scikit-learn,scikits/learn/externals/joblib/test/test_memory.py,a3bd41971720ae466b4e1e99f8b1484501c8954e,FIXME: Need to test that memmapping does not force recomputing.,https://github.com/scikit-learn/scikit-learn/commit/a3bd41971720ae466b4e1e99f8b1484501c8954e,No
4306,scikit-learn/scikit-learn,examples/mlcomp_sparse_document_classification.py,8c61c972e2278f9a4f6eb919521e198bb6e20834,"\""\""\"" || ====================================================== || Classification of text documents using sparse features || ====================================================== ||  || This is an example showing how the scikit-learn can be used to classify || documents by topics using a bag-of-words approach. This example uses || a scipy.sparse matrix to store the features instead of standard numpy arrays. ||  || The dataset used in this example is the 20 newsgroups dataset and should be || downloaded from the http:\/\/mlcomp.org (free registration required): ||  ||   http:\/\/mlcomp.org\/datasets\/379 ||  || Once downloaded unzip the arhive somewhere on your filesystem. For instance in:: ||  ||   % mkdir -p ~\/data\/mlcomp ||   % cd  ~\/data\/mlcomp ||   % unzip \/path\/to\/dataset-379-20news-18828_XXXXX.zip ||  || You should get a folder ``~\/data\/mlcomp\/379`` with a file named ``metadata`` and || subfolders ``raw``; ``train`` and ``test`` holding the text documents organized by || newsgroups. ||  || Then set the ``MLCOMP_DATASETS_HOME`` environment variable pointing to || the root folder holding the uncompressed archive:: ||  ||   % export MLCOMP_DATASETS_HOME=\""~\/data\/mlcomp\"" ||  || Then you are ready to run this example using your favorite python shell:: ||  ||   % ipython examples\/mlcomp_sparse_document_classification.py ||  || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/8c61c972e2278f9a4f6eb919521e198bb6e20834,Yes
4307,scikit-learn/scikit-learn,scikits/learn/glm/base.py,df8c593ce694869e11fd459fc310d0ce0a55788e,# TODO: this should have a tests.,https://github.com/scikit-learn/scikit-learn/commit/df8c593ce694869e11fd459fc310d0ce0a55788e,Yes
4308,scikit-learn/scikit-learn,examples/sgd/mlcomp_sparse_document_classification_sgd.py,658002360d5bc3394ab199665498c8c262916e4f,"\""\""\"" || ====================================================== || Classification of text documents using sparse features || ====================================================== ||  || This is an example showing how the scikit-learn can be used to classify || documents by topics using a bag-of-words approach. This example uses || a scipy.sparse matrix to store the features instead of standard numpy arrays. ||  || The dataset used in this example is the 20 newsgroups dataset and should be || downloaded from the http:\/\/mlcomp.org (free registration required): ||  ||   http:\/\/mlcomp.org\/datasets\/379 ||  || Once downloaded unzip the arhive somewhere on your filesystem. For instance in:: ||  ||   % mkdir -p ~\/data\/mlcomp ||   % cd  ~\/data\/mlcomp ||   % unzip \/path\/to\/dataset-379-20news-18828_XXXXX.zip ||  || You should get a folder ``~\/data\/mlcomp\/379`` with a file named ``metadata`` and || subfolders ``raw``; ``train`` and ``test`` holding the text documents organized by || newsgroups. ||  || Then set the ``MLCOMP_DATASETS_HOME`` environment variable pointing to || the root folder holding the uncompressed archive:: ||  ||   % export MLCOMP_DATASETS_HOME=\""~\/data\/mlcomp\"" ||  || Then you are ready to run this example using your favorite python shell:: ||  ||   % ipython examples\/mlcomp_sparse_document_classification.py ||  || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/658002360d5bc3394ab199665498c8c262916e4f,Yes
4309,scikit-learn/scikit-learn,examples/gmm/gmm_classifier.py,678ae5e69fc11fdc3901477e149025bfb9161a5e,"\""\""\"" || ================== || GMM classification || ================== ||  || Demonstration of Gaussian Mixture Models for classification. ||  || Plots predicted labels on both training and held out test data using a || variety of GMM classifiers on the iris dataset. ||  || Compares GMMs with spherical; diagonal; full; and tied covariance || matrices in increasing order of performance.  Although one would || expect full covariance to perform best in general; it is prone to || overfitting on small datasets and does not generalize well to held out || test data. || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/678ae5e69fc11fdc3901477e149025bfb9161a5e,No
4310,scikit-learn/scikit-learn,skeletons/exercise_01_language_train_model.py,d859f402eb8b1eb0a03b961280e50b50144e15e1,TODO: define variables 'filenames_train' and 'filenames_test',https://github.com/scikit-learn/scikit-learn/commit/d859f402eb8b1eb0a03b961280e50b50144e15e1,Yes
4311,scikit-learn/scikit-learn,skeletons/exercise_01_language_train_model.py,d859f402eb8b1eb0a03b961280e50b50144e15e1,TODO: define variables 'y_train' and 'y_test',https://github.com/scikit-learn/scikit-learn/commit/d859f402eb8b1eb0a03b961280e50b50144e15e1,No
4312,scikit-learn/scikit-learn,skeletons/exercise_01_language_train_model.py,d859f402eb8b1eb0a03b961280e50b50144e15e1,TODO: define a variable named 'X_test',https://github.com/scikit-learn/scikit-learn/commit/d859f402eb8b1eb0a03b961280e50b50144e15e1,Yes
4313,scikit-learn/scikit-learn,skeletons/exercise_04_face_recognition.py,40beda3eb74bb5a4a89b2304d160c2785c4595dc,TODO: define variables X_train; X_test; y_train; y_test by splitting the data,https://github.com/scikit-learn/scikit-learn/commit/40beda3eb74bb5a4a89b2304d160c2785c4595dc,Yes
4314,scikit-learn/scikit-learn,scikits/learn/datasets/mldata.py,744f25ade0af609a36b5e230e24458a0a83d3903,TODO: test: it download the first time; it loads it the second,https://github.com/scikit-learn/scikit-learn/commit/744f25ade0af609a36b5e230e24458a0a83d3903,Yes
4315,scikit-learn/scikit-learn,scikits/learn/datasets/tests/test_mldata.py,946ff5158e45b96fc978c1ad3db339ed15e5d41d,"\""\""\""Test functionality of mldata fetching utilities.\""\""\""",https://github.com/scikit-learn/scikit-learn/commit/946ff5158e45b96fc978c1ad3db339ed15e5d41d,No
4316,scikit-learn/scikit-learn,scikits/learn/tests/test_naive_bayes.py,ea11518e0d60c5b25bf7bac793fd696ae7849da2,FIXME: write a test to show this.,https://github.com/scikit-learn/scikit-learn/commit/ea11518e0d60c5b25bf7bac793fd696ae7849da2,Yes
4317,scikit-learn/scikit-learn,scikits/learn/tests/test_naive_bayes.py,841a295ffc876604d849123c6c2f5a1682a89138,FIXME: write a test to show this.,https://github.com/scikit-learn/scikit-learn/commit/841a295ffc876604d849123c6c2f5a1682a89138,Yes
4318,scikit-learn/scikit-learn,scikits/learn/manifold/tests/test_locally_linear.py,24ab71e11182e81ed13be5c201ddcfd3a1f50f47,FIXME: ARPACK fails this test ...,https://github.com/scikit-learn/scikit-learn/commit/24ab71e11182e81ed13be5c201ddcfd3a1f50f47,No
4319,scikit-learn/scikit-learn,scikits/learn/hmm.py,5bcbb3c203cf07db01d8d9a8c6c7f30182c5d79e,XXX Replacing this with logaddexp.reduce; as in mixture.py; causes a test to,https://github.com/scikit-learn/scikit-learn/commit/5bcbb3c203cf07db01d8d9a8c6c7f30182c5d79e,Yes
4320,scikit-learn/scikit-learn,sklearn/tests/test_label_propagation.py,ff4a3f318105c73798386522bf2d702505bbc296,TODO: test LabelSpreading as well,https://github.com/scikit-learn/scikit-learn/commit/ff4a3f318105c73798386522bf2d702505bbc296,Yes
4321,scikit-learn/scikit-learn,sklearn/tests/test_hmm.py,81c222f26522b1b2af5014e07e76b0b6cebba50f,XXX: This test is bugged and creates weird errors -- skipped,https://github.com/scikit-learn/scikit-learn/commit/81c222f26522b1b2af5014e07e76b0b6cebba50f,No
4322,scikit-learn/scikit-learn,sklearn/tests/test_hmm.py,81c222f26522b1b2af5014e07e76b0b6cebba50f,XXX: this test appears to check that training log likelihood should,https://github.com/scikit-learn/scikit-learn/commit/81c222f26522b1b2af5014e07e76b0b6cebba50f,No
4323,scikit-learn/scikit-learn,sklearn/tests/test_common.py,4264acdbd3ce14edd3811a5211fe34c2e92aba52,TODO also test these!,https://github.com/scikit-learn/scikit-learn/commit/4264acdbd3ce14edd3811a5211fe34c2e92aba52,Yes
4324,scikit-learn/scikit-learn,sklearn/tests/test_common.py,6aa167c5c83948539773295cbc1749e65a0e9dcf,TODO: test with intercept,https://github.com/scikit-learn/scikit-learn/commit/6aa167c5c83948539773295cbc1749e65a0e9dcf,Yes
4325,scikit-learn/scikit-learn,sklearn/tests/test_common.py,6aa167c5c83948539773295cbc1749e65a0e9dcf,TODO: test with multiple responses,https://github.com/scikit-learn/scikit-learn/commit/6aa167c5c83948539773295cbc1749e65a0e9dcf,Yes
4326,scikit-learn/scikit-learn,sklearn/tests/test_common.py,bd94b49a2d75efed045a3ca2c68c0474e0350caf,TODO also test these!,https://github.com/scikit-learn/scikit-learn/commit/bd94b49a2d75efed045a3ca2c68c0474e0350caf,Yes
4327,scikit-learn/scikit-learn,sklearn/base.py,f18295c8aeea9361d56f200de2586083a6f8ef7f,XXX: should we rather test if instance of estimator?,https://github.com/scikit-learn/scikit-learn/commit/f18295c8aeea9361d56f200de2586083a6f8ef7f,No
4328,scikit-learn/scikit-learn,sklearn/cluster/k_means_.py,917f309559d1b441dc28a5aca1cda9f2681a9da0,XXX This skips _check_test_data; which may change the dtype;,https://github.com/scikit-learn/scikit-learn/commit/917f309559d1b441dc28a5aca1cda9f2681a9da0,No
4329,scikit-learn/scikit-learn,sklearn/tests/test_hmm.py,bf87bf5893c18b1e7e4966b40043104f392608c1,XXX: This test is bugged and creates weird errors -- skipped,https://github.com/scikit-learn/scikit-learn/commit/bf87bf5893c18b1e7e4966b40043104f392608c1,No
4330,scikit-learn/scikit-learn,examples/ensemble/plot_adaboost_multiclass.py,e93bd6e8898d092b7d747fab3fb1032030c6d76d,"\""\""\"" || ===================================== || Multi-Class AdaBoosted Decision Trees || ===================================== ||  || This example reproduces Figure 1 of Zhu et al [1] and shows how boosting can || improve prediction accuracy on a multi-class problem. The classification || dataset is constructed by taking a ten-dimensional standard normal distribution || and defining three classes separated by nested concentric ten-dimensional || spheres such that roughly equal numbers of samples are in each class (quantiles || of the :math:`\\Chi^2` distribution). ||  || The performance of the SAMME and SAMME.R [1] algorithms are compared. || The error of each algorithm on the test set after each boosting iteration is || shown on the left; the classification error on the test set of each tree is || shown in the middle; and the boost weight of each tree is shown on the right. ||  || .. [1] J. Zhu; H. Zou; S. Rosset; T. Hastie; \""Multi-class AdaBoost\""; 2009. ||  || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/e93bd6e8898d092b7d747fab3fb1032030c6d76d,Yes
4331,scikit-learn/scikit-learn,sklearn/tests/test_common.py,5e3230cc507df59307f81a629e9a6f13d4ee49e8,TODO also test these!,https://github.com/scikit-learn/scikit-learn/commit/5e3230cc507df59307f81a629e9a6f13d4ee49e8,Yes
4332,scikit-learn/scikit-learn,sklearn/tests/test_common.py,60374eeb3baefdea513b49610a73555e6af2b162,TODO: test with intercept,https://github.com/scikit-learn/scikit-learn/commit/60374eeb3baefdea513b49610a73555e6af2b162,Yes
4333,scikit-learn/scikit-learn,sklearn/tests/test_common.py,60374eeb3baefdea513b49610a73555e6af2b162,TODO: test with multiple responses,https://github.com/scikit-learn/scikit-learn/commit/60374eeb3baefdea513b49610a73555e6af2b162,Yes
4334,scikit-learn/scikit-learn,sklearn/tests/test_common.py,2ba21ecebef1260528b030a2843d8e33a98f19a5,TODO also test these!,https://github.com/scikit-learn/scikit-learn/commit/2ba21ecebef1260528b030a2843d8e33a98f19a5,Yes
4335,scikit-learn/scikit-learn,sklearn/tests/test_common.py,ec0fc91d97bb97d5135c591e54beeb63f2cd7ec3,TODO also test these!,https://github.com/scikit-learn/scikit-learn/commit/ec0fc91d97bb97d5135c591e54beeb63f2cd7ec3,Yes
4336,scikit-learn/scikit-learn,sklearn/tests/test_common.py,ec0fc91d97bb97d5135c591e54beeb63f2cd7ec3,TODO: test with intercept,https://github.com/scikit-learn/scikit-learn/commit/ec0fc91d97bb97d5135c591e54beeb63f2cd7ec3,Yes
4337,scikit-learn/scikit-learn,sklearn/tests/test_common.py,ec0fc91d97bb97d5135c591e54beeb63f2cd7ec3,TODO: test with multiple responses,https://github.com/scikit-learn/scikit-learn/commit/ec0fc91d97bb97d5135c591e54beeb63f2cd7ec3,Yes
4338,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_sgd.py,01801886b23579b87345b2f6fae94958c79e1407,XXX the sparse test gets a different X2 (?),https://github.com/scikit-learn/scikit-learn/commit/01801886b23579b87345b2f6fae94958c79e1407,Yes
4339,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_sgd.py,01801886b23579b87345b2f6fae94958c79e1407,XXX not true in sparse test case (why?),https://github.com/scikit-learn/scikit-learn/commit/01801886b23579b87345b2f6fae94958c79e1407,No
4340,scikit-learn/scikit-learn,sklearn/tests/test_common.py,fe0335748252cb75a6b1eb2f7d08843edb28216f,TODO also test these!,https://github.com/scikit-learn/scikit-learn/commit/fe0335748252cb75a6b1eb2f7d08843edb28216f,Yes
4341,scikit-learn/scikit-learn,sklearn/tests/test_common.py,fe0335748252cb75a6b1eb2f7d08843edb28216f,TODO: test with intercept,https://github.com/scikit-learn/scikit-learn/commit/fe0335748252cb75a6b1eb2f7d08843edb28216f,Yes
4342,scikit-learn/scikit-learn,sklearn/tests/test_common.py,fe0335748252cb75a6b1eb2f7d08843edb28216f,TODO: test with multiple responses,https://github.com/scikit-learn/scikit-learn/commit/fe0335748252cb75a6b1eb2f7d08843edb28216f,Yes
4343,scikit-learn/scikit-learn,examples/randomized_search.py,5d7aaaec50391506a9b1ffd2d7e26b9a4d96015f,"\""\""\"" || ========================================================================= || Comparing randomized search and grid search for hyperparameter estimation || ========================================================================= ||  || Compare randomized search and grid search for optimizing hyperparameters of a || random forest. || All parameters that influence the learning are searched simultaneously || (except for the number of estimators; which poses a time \/ quality tradeoff). ||  || The randomized search and the grid search explore exactly the same space of || parameters. The result in parameter settings is quite similar; while the run || time for randomized search is drastically lower. ||  || The performance is slightly worse for the randomized search; though this || is most likely a noise effect and would not carry over to a held-out test set. ||  || Note that in practice; one would not search over this many different parameters || simultaneously using grid search; but pick only the ones deemed most important. || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/5d7aaaec50391506a9b1ffd2d7e26b9a4d96015f,Yes
4344,scikit-learn/scikit-learn,sklearn/grid_search.py,5d7aaaec50391506a9b1ffd2d7e26b9a4d96015f,TODO: shall we also store the test_fold_sizes?,https://github.com/scikit-learn/scikit-learn/commit/5d7aaaec50391506a9b1ffd2d7e26b9a4d96015f,Yes
4345,scikit-learn/scikit-learn,sklearn/decomposition/tests/test_truncated_svd.py,cacb14ef0a5605e10e4c476061774c706aa1ad63,"equal\"" in all positions. XXX Test means or sums instead?",https://github.com/scikit-learn/scikit-learn/commit/cacb14ef0a5605e10e4c476061774c706aa1ad63,Yes
4346,scikit-learn/scikit-learn,sklearn/neighbors/kde.py,df98f79ba24506513f4d10324f8f6d170bb58415,TODO: implement a brute force version for testing purposes,https://github.com/scikit-learn/scikit-learn/commit/df98f79ba24506513f4d10324f8f6d170bb58415,Yes
4347,scikit-learn/scikit-learn,sklearn/utils/tests/test_linear_assignment.py,1627a9231187877394bcc28743263f96c45516f4,XXX we should be testing the public API here,https://github.com/scikit-learn/scikit-learn/commit/1627a9231187877394bcc28743263f96c45516f4,No
4348,scikit-learn/scikit-learn,sklearn/decomposition/tests/test_pca.py,bf1cf77e4ddfa781fea5da1f63612961b6487c81,XXX : Don't test as homoscedastic=False is buggy,https://github.com/scikit-learn/scikit-learn/commit/bf1cf77e4ddfa781fea5da1f63612961b6487c81,Yes
4349,scikit-learn/scikit-learn,sklearn/feature_selection/univariate_selection.py,9e10ec017b15f7549ba462dcd5c2bf089bc318e4,XXX could use corr \/= row_norms(X.T) here; but the test doesn't pass,https://github.com/scikit-learn/scikit-learn/commit/9e10ec017b15f7549ba462dcd5c2bf089bc318e4,Yes
4350,scikit-learn/scikit-learn,sklearn/manifold/tests/test_t_sne.py,58717d34f323650f7383588b2939eb944003fb92,TODO tests with verbose = 1 or 2,https://github.com/scikit-learn/scikit-learn/commit/58717d34f323650f7383588b2939eb944003fb92,Yes
4351,scikit-learn/scikit-learn,doc/sphinxext/gen_rst.py,6f6a537d5f832b4d06a690cf1dad8f131235e2a2,HACK: Stop nosetests running setup() above,https://github.com/scikit-learn/scikit-learn/commit/6f6a537d5f832b4d06a690cf1dad8f131235e2a2,Yes
4352,scikit-learn/scikit-learn,doc/sphinxext/gen_rst.py,aaefdbd3def81a0318b07990dadc907bf63123ce,HACK: Stop nosetests running setup() above,https://github.com/scikit-learn/scikit-learn/commit/aaefdbd3def81a0318b07990dadc907bf63123ce,Yes
4353,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_omp.py,1f32603b860c4c0230ee50f7858196e8ec37319f,FIXME: This test is unstable on Travis; see issue #3190 for more detail.,https://github.com/scikit-learn/scikit-learn/commit/1f32603b860c4c0230ee50f7858196e8ec37319f,No
4354,scikit-learn/scikit-learn,sklearn/tests/test_common.py,25e11813b7a19cd336cc6697ac94106920531446,FIXME: This test is unstable on Travis; see issue #3190.,https://github.com/scikit-learn/scikit-learn/commit/25e11813b7a19cd336cc6697ac94106920531446,No
4355,scikit-learn/scikit-learn,sklearn/feature_selection/tests/test_rfe.py,9248948344a878edbcafce6640202e420799e241,Test fix on grid_scores,https://github.com/scikit-learn/scikit-learn/commit/9248948344a878edbcafce6640202e420799e241,No
4356,scikit-learn/scikit-learn,examples/text/mlcomp_sparse_document_classification.py,5b247f90e4b518ef7ce470bafa17f281de3a1c01,"\""\""\"" || ======================================================== || Classification of text documents: using a MLComp dataset || ======================================================== ||  || This is an example showing how the scikit-learn can be used to classify || documents by topics using a bag-of-words approach. This example uses || a scipy.sparse matrix to store the features instead of standard numpy arrays. ||  || The dataset used in this example is the 20 newsgroups dataset and should be || downloaded from the http:\/\/mlcomp.org (free registration required): ||  ||   http:\/\/mlcomp.org\/datasets\/379 ||  || Once downloaded unzip the archive somewhere on your filesystem. || For instance in:: ||  ||   % mkdir -p ~\/data\/mlcomp ||   % cd  ~\/data\/mlcomp ||   % unzip \/path\/to\/dataset-379-20news-18828_XXXXX.zip ||  || You should get a folder ``~\/data\/mlcomp\/379`` with a file named ``metadata`` || and subfolders ``raw``; ``train`` and ``test`` holding the text documents || organized by newsgroups. ||  || Then set the ``MLCOMP_DATASETS_HOME`` environment variable pointing to || the root folder holding the uncompressed archive:: ||  ||   % export MLCOMP_DATASETS_HOME=\""~\/data\/mlcomp\"" ||  || Then you are ready to run this example using your favorite python shell:: ||  ||   % ipython examples\/mlcomp_sparse_document_classification.py ||  || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/5b247f90e4b518ef7ce470bafa17f281de3a1c01,No
4357,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_omp.py,ce29c70cae50d46b851effa40deb5e7e44b3976a,FIXME: This test is unstable on Travis; see issue #3190 for more detail.,https://github.com/scikit-learn/scikit-learn/commit/ce29c70cae50d46b851effa40deb5e7e44b3976a,No
4358,scikit-learn/scikit-learn,doc/sphinxext/gen_rst.py,4b6768f368b14566fe8b6e5da469593f07ba199e,HACK: Stop nosetests running setup() above,https://github.com/scikit-learn/scikit-learn/commit/4b6768f368b14566fe8b6e5da469593f07ba199e,Yes
4359,scikit-learn/scikit-learn,sklearn/metrics/tests/test_common.py,6d881d3fb1e69938db4d184f2e79978cb12ca6a0,TODO Curve are currently not coverd by invariance test,https://github.com/scikit-learn/scikit-learn/commit/6d881d3fb1e69938db4d184f2e79978cb12ca6a0,No
4360,scikit-learn/scikit-learn,sklearn/metrics/tests/test_common.py,6d881d3fb1e69938db4d184f2e79978cb12ca6a0,When you add a new metric or functionality; check if a general test,https://github.com/scikit-learn/scikit-learn/commit/6d881d3fb1e69938db4d184f2e79978cb12ca6a0,Yes
4361,scikit-learn/scikit-learn,sklearn/tests/test_dummy.py,bdd5cb162e9e2a37b84b2f06fb31fed1c825a1a4,XXX a test of the cahnge to the fit function only,https://github.com/scikit-learn/scikit-learn/commit/bdd5cb162e9e2a37b84b2f06fb31fed1c825a1a4,Yes
4362,scikit-learn/scikit-learn,sklearn/metrics/tests/test_common.py,081a5546988e4514afecf00250901f66e29b7543,These are needed to test averaging,https://github.com/scikit-learn/scikit-learn/commit/081a5546988e4514afecf00250901f66e29b7543,Yes
4363,scikit-learn/scikit-learn,sklearn/tests/test_naive_bayes.py,c81c679033fd767001306d04ceefc39feeb0cada,FIXME Remove this test once the more general partial_fit tests are merged,https://github.com/scikit-learn/scikit-learn/commit/c81c679033fd767001306d04ceefc39feeb0cada,Yes
4364,scikit-learn/scikit-learn,benchmarks/bench_mnist.py,fa0ceee4dc943fee1f7550282363968069c40d09,"\""\""\"" || ======================= || MNIST dataset benchmark || ======================= ||  || Benchmark on the MNIST dataset.  The dataset comprises 70;000 samples || and 784 features. Here; we consider the task of predicting || 10 classes -  digits from 0 to 9 from their raw images. By contrast to the || covertype dataset; the feature space is homogenous. ||  ||     [..] ||     Classification performance: ||     =========================== ||     Classifier       train-time test-time error-rate ||     -------------------------------------------- ||     Nystroem-SVM     118.4512s   0.8624s     0.0231 ||     ExtraTrees        47.5039s   0.5075s     0.0288 ||     RandomForest      46.2317s   0.4342s     0.0304 ||     SampledRBF-SVM   137.0131s   0.8688s     0.0488 ||     CART              21.0593s   0.0134s     0.1214 ||  ||  ||  || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/fa0ceee4dc943fee1f7550282363968069c40d09,Yes
4365,scikit-learn/scikit-learn,sklearn/preprocessing/tests/test_data.py,22f718dcceab10e36583779a365fe8a109cdbaa7,TODO: for some reason assert_raise doesn't test this correctly,https://github.com/scikit-learn/scikit-learn/commit/22f718dcceab10e36583779a365fe8a109cdbaa7,No
4366,scikit-learn/scikit-learn,sklearn/utils/tests/test_validation.py,69827c4b58c3778f99e9bb8909a3971c2964d209,XXX: We should have a test with a string; but what is correct behaviour?,https://github.com/scikit-learn/scikit-learn/commit/69827c4b58c3778f99e9bb8909a3971c2964d209,Yes
4367,scikit-learn/scikit-learn,sklearn/utils/estimator_checks.py,8820701fddc1150f362fba4b37cb6da3b3cbbaf0,FIXME: This test is unstable on Travis; see issue #3190.,https://github.com/scikit-learn/scikit-learn/commit/8820701fddc1150f362fba4b37cb6da3b3cbbaf0,No
4368,scikit-learn/scikit-learn,sklearn/utils/estimator_checks.py,ed5519fe8adf70437433a5b17487cc0c1decfe38,FIXME: This test is unstable on Travis; see issue #3190.,https://github.com/scikit-learn/scikit-learn/commit/ed5519fe8adf70437433a5b17487cc0c1decfe38,No
4369,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_ridge.py,cd2ee7e454f9bedceb04ba8613a4e52d1d9a76bf,TODO: for this test to be robust; we should use a dataset instead,https://github.com/scikit-learn/scikit-learn/commit/cd2ee7e454f9bedceb04ba8613a4e52d1d9a76bf,No
4370,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_ridge.py,cd2ee7e454f9bedceb04ba8613a4e52d1d9a76bf,TODO: test also n_samples > n_features,https://github.com/scikit-learn/scikit-learn/commit/cd2ee7e454f9bedceb04ba8613a4e52d1d9a76bf,No
4371,scikit-learn/scikit-learn,sklearn/cross_validation.py,34c85ed995907f018279decb36be41fe2d29fba6,todo this won't work correctly when test_size > train_size,https://github.com/scikit-learn/scikit-learn/commit/34c85ed995907f018279decb36be41fe2d29fba6,Yes
4372,scikit-learn/scikit-learn,sklearn/utils/estimator_checks.py,669a4ea853114dbba3621cb96c72b35560471646,TODO: test with intercept,https://github.com/scikit-learn/scikit-learn/commit/669a4ea853114dbba3621cb96c72b35560471646,Yes
4373,scikit-learn/scikit-learn,sklearn/utils/estimator_checks.py,669a4ea853114dbba3621cb96c72b35560471646,TODO: test with multiple responses,https://github.com/scikit-learn/scikit-learn/commit/669a4ea853114dbba3621cb96c72b35560471646,Yes
4374,scikit-learn/scikit-learn,sklearn/preprocessing/tests/test_data.py,4ff47d9eface2aab6772d0c8269a02d88d35fd4d,TODO: for some reason assert_raise doesn't test this correctly,https://github.com/scikit-learn/scikit-learn/commit/4ff47d9eface2aab6772d0c8269a02d88d35fd4d,No
4375,scikit-learn/scikit-learn,sklearn/neighbors/tests/test_neighbors.py,ab8f155641af856d974bbe4c0b631d290323b485,TODO: also test radius_neighbors; but requires different assertion,https://github.com/scikit-learn/scikit-learn/commit/ab8f155641af856d974bbe4c0b631d290323b485,Yes
4376,scikit-learn/scikit-learn,sklearn/ensemble/tests/test_forest.py,5a0db1717a66ce39bae9b7c6f27e2e6e3f6c7647,XXX: Remove this test in 0.19 after transform support to estimators,https://github.com/scikit-learn/scikit-learn/commit/5a0db1717a66ce39bae9b7c6f27e2e6e3f6c7647,Yes
4377,scikit-learn/scikit-learn,sklearn/ensemble/tests/test_gradient_boosting.py,5a0db1717a66ce39bae9b7c6f27e2e6e3f6c7647,XXX: Remove this test in 0.19 after transform support to estimators,https://github.com/scikit-learn/scikit-learn/commit/5a0db1717a66ce39bae9b7c6f27e2e6e3f6c7647,Yes
4378,scikit-learn/scikit-learn,sklearn/utils/estimator_checks.py,92068ef7de92752ab14a1e601e021cda4a51149d,TODO: Test with Multitask later,https://github.com/scikit-learn/scikit-learn/commit/92068ef7de92752ab14a1e601e021cda4a51149d,Yes
4379,scikit-learn/scikit-learn,benchmarks/bench_plot_randomized_svd.py,b18f2951e4be3ce293a9d0bb61d4ca3be19496ca,"\""\""\"" || Benchmarks on the power iterations phase in randomized SVD. ||  || We test on various synthetic and real datasets the effect of increasing || the number of power iterations in terms of quality of approximation || and running time. A number greater than 0 should help with noisy matrices; || which are characterized by a slow spectral decay. ||  || We test several policy for normalizing the power iterations. Normalization || is crucial to avoid numerical issues. ||  || The quality of the approximation is measured by the spectral norm discrepancy || between the original input matrix and the reconstructed one (by multiplying || the randomized_svd's outputs). The spectral norm is always equivalent to the || largest singular value of a matrix. (3) justifies this choice. However; one can || notice in these experiments that Frobenius and spectral norms behave || very similarly in a qualitative sense. Therefore; we suggest to run these || benchmarks with `enable_spectral_norm = False`; as Frobenius' is MUCH faster to || compute. ||  || The benchmarks follow. ||  || (a) plot: time vs norm; varying number of power iterations ||     data: many datasets ||     goal: compare normalization policies and study how the number of power ||     iterations affect time and norm ||  || (b) plot: n_iter vs norm; varying rank of data and number of components for ||     randomized_SVD ||     data: low-rank matrices on which we control the rank ||     goal: study whether the rank of the matrix and the number of components ||     extracted by randomized SVD affect \""the optimal\"" number of power iterations ||  || (c) plot: time vs norm; varing datasets ||     data: many datasets ||     goal: compare default configurations ||  || We compare the following algorithms: || -   randomized_svd(...; power_iteration_normalizer='none') || -   randomized_svd(...; power_iteration_normalizer='LU') || -   randomized_svd(...; power_iteration_normalizer='QR') || -   randomized_svd(...; power_iteration_normalizer='auto') || -   fbpca.pca() from https:\/\/github.com\/facebook\/fbpca (if installed) ||  || Conclusion || ---------- || - n_iter=2 appears to be a good default value || - power_iteration_normalizer='none' is OK if n_iter is small; otherwise LU ||   gives similar errors to QR but is cheaper. That's what 'auto' implements. ||  || References || ---------- || (1) Finding structure with randomness: Stochastic algorithms for constructing ||     approximate matrix decompositions ||     Halko; et al.; 2009 http:\/\/arxiv.org\/abs\/arXiv:0909.4061 ||  || (2) A randomized algorithm for the decomposition of matrices ||     Per-Gunnar Martinsson; Vladimir Rokhlin and Mark Tygert ||  || (3) An implementation of a randomized algorithm for principal component ||     analysis ||     A. Szlam et al. 2014 || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/b18f2951e4be3ce293a9d0bb61d4ca3be19496ca,Yes
4380,scikit-learn/scikit-learn,sklearn/model_selection/_search.py,3f8743f47b61a269e8bfff2322cb544170976574,TODO: shall we also store the test_fold_sizes?,https://github.com/scikit-learn/scikit-learn/commit/3f8743f47b61a269e8bfff2322cb544170976574,Yes
4381,scikit-learn/scikit-learn,doc/sphinxext/sphinx_gallery/gen_gallery.py,ca4e8023e4244e4638210b7ee6a2ead70430ad76,HACK: Stop nosetests running setup() above,https://github.com/scikit-learn/scikit-learn/commit/ca4e8023e4244e4638210b7ee6a2ead70430ad76,Yes
4382,scikit-learn/scikit-learn,doc/sphinxext/sphinx_gallery/gen_rst.py,1054e072da3235d6befcbd0d3cd2625f1b4e6473,XXX This check can break during testing e.g. if you uncomment the,https://github.com/scikit-learn/scikit-learn/commit/1054e072da3235d6befcbd0d3cd2625f1b4e6473,Yes
4383,scikit-learn/scikit-learn,doc/sphinxext/sphinx_gallery/gen_gallery.py,7a23b2493fbe094db2528114d75edc2e9739da17,TODO: Test this behavior.,https://github.com/scikit-learn/scikit-learn/commit/7a23b2493fbe094db2528114d75edc2e9739da17,Yes
4384,scikit-learn/scikit-learn,doc/sphinxext/sphinx_gallery/gen_gallery.py,71408e09e4bf01a72567f9167a1e33fb9620d786,TODO: Test this behavior.,https://github.com/scikit-learn/scikit-learn/commit/71408e09e4bf01a72567f9167a1e33fb9620d786,Yes
4385,scikit-learn/scikit-learn,sklearn/utils/_unittest_backport.py,2b9405f977b175a96d0bef6d68e1a74d6b90e60f,"\""\""\"" || This is a backport of assertRaises() and assertRaisesRegex from Python 3.5.4 ||  || The original copyright message is as follows ||  || Python unit testing framework; based on Erich Gamma's JUnit and Kent Beck's || Smalltalk testing framework (used with permission). ||  || This module contains the core framework classes that form the basis of || specific test cases and suites (TestCase; TestSuite etc.); and also a || text-based utility class for running the tests and reporting the results ||  (TextTestRunner). ||  || Simple usage: ||  ||     import unittest ||  ||     class IntegerArithmeticTestCase(unittest.TestCase): ||         def testAdd(self):  # test method names begin with 'test' ||             self.assertEqual((1 + 2); 3) ||             self.assertEqual(0 + 1; 1) ||         def testMultiply(self): ||             self.assertEqual((0 * 10); 0) ||             self.assertEqual((5 * 8); 40) ||  ||     if __name__ == '__main__': ||         unittest.main() ||  || Further information is available in the bundled documentation; and from ||  ||   http:\/\/docs.python.org\/library\/unittest.html ||  || Copyright (c) 1999-2003 Steve Purcell || Copyright (c) 2003-2010 Python Software Foundation || This module is free software; and you may redistribute it and\/or modify || it under the same terms as Python itself; so long as this copyright message || and disclaimer are retained in their original form. ||  || IN NO EVENT SHALL THE AUTHOR BE LIABLE TO ANY PARTY FOR DIRECT; INDIRECT; || SPECIAL; INCIDENTAL; OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OF || THIS CODE; EVEN IF THE AUTHOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH || DAMAGE. ||  || THE AUTHOR SPECIFICALLY DISCLAIMS ANY WARRANTIES; INCLUDING; BUT NOT || LIMITED TO; THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A || PARTICULAR PURPOSE.  THE CODE PROVIDED HEREUNDER IS ON AN \""AS IS\"" BASIS; || AND THERE IS NO OBLIGATION WHATSOEVER TO PROVIDE MAINTENANCE; || SUPPORT; UPDATES; ENHANCEMENTS; OR MODIFICATIONS. || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/2b9405f977b175a96d0bef6d68e1a74d6b90e60f,Yes
4386,scikit-learn/scikit-learn,sklearn/manifold/tests/test_t_sne.py,4aaf45baf1cddbd368ab39801f4de40a6caaeece,FIXME: Remove this test in v0.23,https://github.com/scikit-learn/scikit-learn/commit/4aaf45baf1cddbd368ab39801f4de40a6caaeece,No
4387,scikit-learn/scikit-learn,sklearn/model_selection/tests/test_search.py,8e2c2aa35d234ba5d14f6c2492e2e5ca57de8af6,TODO Remove test in 0.20,https://github.com/scikit-learn/scikit-learn/commit/8e2c2aa35d234ba5d14f6c2492e2e5ca57de8af6,Yes
4388,scikit-learn/scikit-learn,sklearn/datasets/tests/test_openml.py,ab82f5739f98a7ea18e8f8220506638a240ebce4,XXX Test is intended to verify\/ensure correct decoding behavior,https://github.com/scikit-learn/scikit-learn/commit/ab82f5739f98a7ea18e8f8220506638a240ebce4,No
4389,scikit-learn/scikit-learn,sklearn/datasets/tests/test_openml.py,ab82f5739f98a7ea18e8f8220506638a240ebce4,XXX: Test per column; as this makes it easier to avoid problems with,https://github.com/scikit-learn/scikit-learn/commit/ab82f5739f98a7ea18e8f8220506638a240ebce4,No
4390,scikit-learn/scikit-learn,sklearn/cluster/tests/test_bicluster.py,e0e738760625129be71839fac2dad9325fb90225,XXX test always skipped,https://github.com/scikit-learn/scikit-learn/commit/e0e738760625129be71839fac2dad9325fb90225,Yes
4391,scikit-learn/scikit-learn,sklearn/decomposition/tests/test_sparse_pca.py,e0e738760625129be71839fac2dad9325fb90225,XXX: test always skipped,https://github.com/scikit-learn/scikit-learn/commit/e0e738760625129be71839fac2dad9325fb90225,No
4392,scikit-learn/scikit-learn,sklearn/utils/tests/test_sparsefuncs.py,afe0a9b70d5ac53836b38b9af3a95659307574fc,XXX: test fails on Appveyor (python2.7 32bit),https://github.com/scikit-learn/scikit-learn/commit/afe0a9b70d5ac53836b38b9af3a95659307574fc,Yes
4393,scikit-learn/scikit-learn,sklearn/cluster/tests/test_bicluster.py,362cb3bcabad2ec5d9d0c29ea1ec69558a64d76b,XXX test always skipped,https://github.com/scikit-learn/scikit-learn/commit/362cb3bcabad2ec5d9d0c29ea1ec69558a64d76b,Yes
4394,scikit-learn/scikit-learn,sklearn/decomposition/tests/test_sparse_pca.py,fa98a72dcca91920e5e807fdc1ce5b54486ad652,XXX: test always skipped,https://github.com/scikit-learn/scikit-learn/commit/fa98a72dcca91920e5e807fdc1ce5b54486ad652,No
4395,scikit-learn/scikit-learn,sklearn/tests/test_common.py,ab2f539a32b8099a941cefc598c9625e830ecfe4,FIXME _skip_test should be used here (if we could),https://github.com/scikit-learn/scikit-learn/commit/ab2f539a32b8099a941cefc598c9625e830ecfe4,No
4396,scikit-learn/scikit-learn,sklearn/utils/tests/test_linear_assignment.py,cfaf352caf0f5c80094b7389d5ca69b50050b472,TODO #0.23: Remove this test module as the methods being tested,https://github.com/scikit-learn/scikit-learn/commit/cfaf352caf0f5c80094b7389d5ca69b50050b472,No
4397,scikit-learn/scikit-learn,sklearn/cluster/tests/test_bicluster.py,19c068a2ec95b18bea41e717406d333ff2b6e94c,XXX test always skipped,https://github.com/scikit-learn/scikit-learn/commit/19c068a2ec95b18bea41e717406d333ff2b6e94c,Yes
4398,scikit-learn/scikit-learn,sklearn/model_selection/tests/test_search.py,c2e742c48c3a8dd1b963276a7a48102595447e79,FIXME remove test_fit_grid_point as the function will be removed on 0.25,https://github.com/scikit-learn/scikit-learn/commit/c2e742c48c3a8dd1b963276a7a48102595447e79,Yes
4399,scikit-learn/scikit-learn,sklearn/model_selection/tests/test_search.py,c2e742c48c3a8dd1b963276a7a48102595447e79,FIXME remove test_fit_grid_point_deprecated as,https://github.com/scikit-learn/scikit-learn/commit/c2e742c48c3a8dd1b963276a7a48102595447e79,No
4400,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_sgd.py,9b39c4c4d20eef7a2b0b8420945f09d3731e1b67,XXX untested as of v0.22,https://github.com/scikit-learn/scikit-learn/commit/9b39c4c4d20eef7a2b0b8420945f09d3731e1b67,No
4401,scikit-learn/scikit-learn,sklearn/neighbors/tests/test_kde.py,9b39c4c4d20eef7a2b0b8420945f09d3731e1b67,XXX Duplicated in test_neighbors_tree; test_kde,https://github.com/scikit-learn/scikit-learn/commit/9b39c4c4d20eef7a2b0b8420945f09d3731e1b67,Yes
4402,scikit-learn/scikit-learn,sklearn/tests/test_common.py,fc0041546a6e186bbacdf96e200fc863d620c44d,TODO: remove test in 0.24,https://github.com/scikit-learn/scikit-learn/commit/fc0041546a6e186bbacdf96e200fc863d620c44d,Yes
4403,scikit-learn/scikit-learn,sklearn/utils/tests/test_estimator_checks.py,fc0041546a6e186bbacdf96e200fc863d620c44d,TODO: remove whole test in 0.24 since passes classes to check_estimator(),https://github.com/scikit-learn/scikit-learn/commit/fc0041546a6e186bbacdf96e200fc863d620c44d,Yes
4404,scikit-learn/scikit-learn,sklearn/ensemble/tests/test_gradient_boosting.py,5a33360ebb84c5f89035417c5aee042dab235127,FIXME: We temporarily bypass this test. This is due to the fact,https://github.com/scikit-learn/scikit-learn/commit/5a33360ebb84c5f89035417c5aee042dab235127,Yes
4405,scikit-learn/scikit-learn,sklearn/utils/tests/test_estimator_checks.py,c29092d6994a43ea19a82e91c0331e8b7d6e7d36,TODO: remove whole test in 0.24 since passes classes to check_estimator(),https://github.com/scikit-learn/scikit-learn/commit/c29092d6994a43ea19a82e91c0331e8b7d6e7d36,Yes
4406,scikit-learn/scikit-learn,sklearn/manifold/tests/test_t_sne.py,89e49b6641871d7106d5e196be69707d81c8d30d,FIXME remove test when square_distances=True becomes the default in 0.26,https://github.com/scikit-learn/scikit-learn/commit/89e49b6641871d7106d5e196be69707d81c8d30d,Yes
4407,scikit-learn/scikit-learn,sklearn/cluster/tests/test_affinity_propagation.py,3cb3d4109e7acc497ad1e306013547e5f72ee5f4,Test to fix incorrect clusters due to dtype change,https://github.com/scikit-learn/scikit-learn/commit/3cb3d4109e7acc497ad1e306013547e5f72ee5f4,Yes
4408,scikit-learn/scikit-learn,sklearn/utils/tests/test_estimator_checks.py,86101d7ac8a42c96b205658a23db728e811a93ee,TODO: remove whole test in 0.24 since passes classes to check_estimator(),https://github.com/scikit-learn/scikit-learn/commit/86101d7ac8a42c96b205658a23db728e811a93ee,Yes
4409,scikit-learn/scikit-learn,sklearn/ensemble/tests/test_common.py,8ba49f628092fe3fb1deea101910006aa6c76d49,FIXME: we should move this test in `estimator_checks` once we are able,https://github.com/scikit-learn/scikit-learn/commit/8ba49f628092fe3fb1deea101910006aa6c76d49,Yes
4410,scikit-learn/scikit-learn,sklearn/tests/test_multiclass.py,8ba49f628092fe3fb1deea101910006aa6c76d49,FIXME: we should move this test in `estimator_checks` once we are able,https://github.com/scikit-learn/scikit-learn/commit/8ba49f628092fe3fb1deea101910006aa6c76d49,Yes
4411,scikit-learn/scikit-learn,sklearn/tests/test_multioutput.py,8ba49f628092fe3fb1deea101910006aa6c76d49,FIXME: we should move this test in `estimator_checks` once we are able,https://github.com/scikit-learn/scikit-learn/commit/8ba49f628092fe3fb1deea101910006aa6c76d49,Yes
4412,scikit-learn/scikit-learn,sklearn/cross_decomposition/tests/test_pls.py,8061aace11e36c9f5384e0bde723e0a98b81a1bd,TODO: remove attributes and test in 0.26,https://github.com/scikit-learn/scikit-learn/commit/8061aace11e36c9f5384e0bde723e0a98b81a1bd,Yes
4413,scikit-learn/scikit-learn,sklearn/cross_decomposition/tests/test_pls.py,0e6d415c517aa65528cfa0c189745de50e4c6565,TODO: Remove test in 0.26,https://github.com/scikit-learn/scikit-learn/commit/0e6d415c517aa65528cfa0c189745de50e4c6565,No
4414,scikit-learn/scikit-learn,sklearn/tests/test_multiclass.py,8ce329174e31b1eecd5f92ca81273c233b35de1c,TODO: Remove this test in version 0.26 when,https://github.com/scikit-learn/scikit-learn/commit/8ce329174e31b1eecd5f92ca81273c233b35de1c,No
4415,scikit-learn/scikit-learn,sklearn/utils/tests/test_validation.py,b197cc04e55fc2890a0199f46040d3d7cb3c4a12,TODO: remove test in 0.26 once this behavior is deprecated,https://github.com/scikit-learn/scikit-learn/commit/b197cc04e55fc2890a0199f46040d3d7cb3c4a12,No
4416,scikit-learn/scikit-learn,sklearn/model_selection/tests/test_search.py,255718b4ad9a3490bc99c992d467f85737bd1291,FIXME: Replace this test with a full `check_estimator` once we have API only,https://github.com/scikit-learn/scikit-learn/commit/255718b4ad9a3490bc99c992d467f85737bd1291,Yes
4417,scikit-learn/scikit-learn,sklearn/tests/test_pipeline.py,255718b4ad9a3490bc99c992d467f85737bd1291,FIXME: Replace this test with a full `check_estimator` once we have API only,https://github.com/scikit-learn/scikit-learn/commit/255718b4ad9a3490bc99c992d467f85737bd1291,Yes
4418,scikit-learn/scikit-learn,sklearn/utils/tests/test_estimator_checks.py,255718b4ad9a3490bc99c992d467f85737bd1291,FIXME: this test should be uncommented when the checks will be granular,https://github.com/scikit-learn/scikit-learn/commit/255718b4ad9a3490bc99c992d467f85737bd1291,Yes
4419,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_base.py,306826f7b6bf5fd61af74062c0ba8f0f21aa3dae,FIXME update test in 1.2 for new versions,https://github.com/scikit-learn/scikit-learn/commit/306826f7b6bf5fd61af74062c0ba8f0f21aa3dae,Yes
4420,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_base.py,306826f7b6bf5fd61af74062c0ba8f0f21aa3dae,FIXME remove test in 1.4,https://github.com/scikit-learn/scikit-learn/commit/306826f7b6bf5fd61af74062c0ba8f0f21aa3dae,No
4421,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_coordinate_descent.py,fcf4740b4538657997b0f4b8015728d64e2d563e,XXX: this test does not pass for weaker regularization (lower values of,https://github.com/scikit-learn/scikit-learn/commit/fcf4740b4538657997b0f4b8015728d64e2d563e,No
4422,lai-bluejay/diego,diego/classifier.py,33ea32953c6219d750942a0ebae7a81ec7247b62,Map test values to actual values - TODO: copy to all kinds of,https://github.com/lai-bluejay/diego/commit/33ea32953c6219d750942a0ebae7a81ec7247b62,Yes
4423,jmaggio14/imagepypelines,imagepypelines_gui/MainWindow.py,068c3cb50ebac06bd1884386b7a7a37759583b2d,ND FIXME: Hack for testing,https://github.com/jmaggio14/imagepypelines/commit/068c3cb50ebac06bd1884386b7a7a37759583b2d,Yes
4424,plstcharles/thelper,thelper/data/geo/utils.py,3760256a3bbff7a65eb8dbc11dc325bfc35cb722,add dirty hack for geojsons used in testbed15-d104,https://github.com/plstcharles/thelper/commit/3760256a3bbff7a65eb8dbc11dc325bfc35cb722,Yes
4425,plstcharles/thelper,thelper/data/geo/utils.py,9fce29d5f7a4512d6d69f245e0c44850a708e37d,add dirty hack for geojsons used in testbed15-d104,https://github.com/plstcharles/thelper/commit/9fce29d5f7a4512d6d69f245e0c44850a708e37d,Yes
4426,plstcharles/thelper,thelper/data/geo/bigearthnet.py,478fb0dc2bbd0f0654ebf5709160db5d65529084,@@@@ TODO: CONVERT TO PROPER TEST,https://github.com/plstcharles/thelper/commit/478fb0dc2bbd0f0654ebf5709160db5d65529084,No
4427,plstcharles/thelper,thelper/data/geo/agrivis.py,aa1d56a767165251dd55953d268411bcc58c62ed,@@@@ TODO: CONVERT TO PROPER TEST,https://github.com/plstcharles/thelper/commit/aa1d56a767165251dd55953d268411bcc58c62ed,No
4428,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_trp_cage/current_work/src/ANN_simulation_trp_cage.py,17e30302667386004e7574c6f8b99d0ffc87580a,TODO: to be tested,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/17e30302667386004e7574c6f8b99d0ffc87580a,No
4429,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/ANN_simulation_trp_cage.py,0f7dc1afffb031619f0c6f3ed6355a5648e48f8f,FIXME: how to write unit test for this function?,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/0f7dc1afffb031619f0c6f3ed6355a5648e48f8f,No
4430,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/ANN_simulation_trp_cage.py,0f7dc1afffb031619f0c6f3ed6355a5648e48f8f,TODO: to be tested,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/0f7dc1afffb031619f0c6f3ed6355a5648e48f8f,No
4431,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/molecule_spec_sutils.py,39215c64324741916cee01bc723799e7b31881c0,FIXME: how to write unit test for this function?,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/39215c64324741916cee01bc723799e7b31881c0,No
4432,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/molecule_spec_sutils.py,39215c64324741916cee01bc723799e7b31881c0,TODO: to be tested,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/39215c64324741916cee01bc723799e7b31881c0,No
4433,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/tests/ANN_simulation_test.py,e6324a33e3e02c08c5377ba887d405806633db38,TODO: add testing for values; currently only tests basic functionality,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/e6324a33e3e02c08c5377ba887d405806633db38,Yes
4434,weiHelloWorld/accelerated_sampling_with_autoencoder,MD_simulation_on_alanine_dipeptide/current_work/src/helper_func.py,64fe53efeebecae66e5784f0bf6306fed1619382,TODO: test if this function is correct,https://github.com/weiHelloWorld/accelerated_sampling_with_autoencoder/commit/64fe53efeebecae66e5784f0bf6306fed1619382,No
4435,DreamingRaven/nemesyst,ravenRecSyst.py,f9c2ba78e6210d7a4b2bcec131732504d3f61c46,test #TODO: implement complementary testing to training set selection,https://github.com/DreamingRaven/nemesyst/commit/f9c2ba78e6210d7a4b2bcec131732504d3f61c46,Yes
4436,DreamingRaven/nemesyst,src/neuralNetwork.py,7cfeec9d89ade8568534daf3f64adef5ed6c3015,TODO seems like this is also called in normal testing but the message suggest recursiveness may want to reword,https://github.com/DreamingRaven/nemesyst/commit/7cfeec9d89ade8568534daf3f64adef5ed6c3015,No
4437,manuwhs/Trapyng,MarketModels/CAPM/CAPM_IFE.py,df8c673a0b2963d29f4402c83912b5c8320475db,Then; using also the last year ( test); recalculate the portfolio needed,https://github.com/manuwhs/Trapyng/commit/df8c673a0b2963d29f4402c83912b5c8320475db,Yes
4438,manuwhs/Trapyng,Examples/5.1 AllenNLP/2.main_example_name_country.py,51cd064a4727e108c81935f0ed69956e4113853d,"\""\""\"" ||   Thoughts on DataReader: ||            To allow lazy mode we should add the lazy flag in the initialization and the _read() function should not return a list of instances but ||            rather yield them within a loop. ||            When calling read() of the DataReader; if it is not lazy mode it will read the Iterator (Yielder) until it has all samples and will join them into a list and return them. ||            If we have lazy mode then we only call read() once as well; and then every time we iterate over the return element; it will fetch the data from _read() and return the instance. ||            instances = reader.read('my_instances.txt') ||            for epoch in range(10): ||                for instance in instances: ||                    process(instance) ||  ||             Then each epoch's for instance in instances results in a new call to MyDatasetReader._read(); and your instances will be read from disk 10 times. ||  ||         AllenNLP uses a DataIterator abstraction to iterate over datasets using configurable batching; shuffling; and so on. ||         (the padding is taken care of by the Batching; but we could order the sequences in our dataset do have batches with same lenghts; this is what iterators can do). ||  ||         The included dataset readers all work with lazy datasets; but they have extra options that you might want to specify in that case. ||  ||        How do you know when the yield iterator is over ? because it returns None ? for the non lazy loading it is fine; we have full control. ||  ||         _max_instances_in_memory _instances_per_epoch ||  ||          BucketIterator; which can order your instances by e.g. sentence length. ||  ||       --- ||       In the seq2seq entropy loss we used the mask as the vector of weights for the elements in the chains. ||  ||       -- ||       For using the Iterator; once the iterator object is initialized; we just call __call__ with the iterable from the data reader and it will ||       output another iterable with the batches; already converted to tensors of indexes using the vocabulary. ||       We could also use the function: get_num_batches(self; instances: Iterable[Instance]) but how is this going to work for lazy mode ? do we have to load everything once to test it ?  ||       Ok; for lazy mode it wount work unless we specify the number of samples in each epoch. for non lazy it will but it also will ahve do reload everything; so not worth it. ||           Shold we check at each iteration if the batch result is None and the call the iterator again ?  ||  || \""\""\""",https://github.com/manuwhs/Trapyng/commit/51cd064a4727e108c81935f0ed69956e4113853d,No
4439,manuwhs/Trapyng,Examples/5.2 HBM/pymc3-master/pymc3/tests/conftest.py,6b25ff82f383b3dac092795657538789ae7160c8,TODO: use this instead of SeededTest,https://github.com/manuwhs/Trapyng/commit/6b25ff82f383b3dac092795657538789ae7160c8,Yes
4440,manuwhs/Trapyng,Examples/5.2 HBM/pymc3-master/pymc3/variational/opvi.py,6b25ff82f383b3dac092795657538789ae7160c8,"R\""\""\"" || Variational inference is a great approach for doing really complex; || often intractable Bayesian inference in approximate form. Common methods || (e.g. ADVI) lack from complexity so that approximate posterior does not || reveal the true nature of underlying problem. In some applications it can || yield unreliable decisions. ||  || Recently on NIPS 2017 `OPVI  <https:\/\/arxiv.org\/abs\/1610.09033\/>`_ framework || was presented. It generalizes variational inference so that the problem is || build with blocks. The first and essential block is Model itself. Second is || Approximation; in some cases :math:`log Q(D)` is not really needed. Necessity || depends on the third and fourth part of that black box; Operator and || Test Function respectively. ||  || Operator is like an approach we use; it constructs loss from given Model; || Approximation and Test Function. The last one is not needed if we minimize || KL Divergence from Q to posterior. As a drawback we need to compute :math:`loq Q(D)`. || Sometimes approximation family is intractable and :math:`loq Q(D)` is not available; || here comes LS(Langevin Stein) Operator with a set of test functions. ||  || Test Function has more unintuitive meaning. It is usually used with LS operator || and represents all we want from our approximate distribution. For any given vector || based function of :math:`z` LS operator yields zero mean function under posterior. || :math:`loq Q(D)` is no more needed. That opens a door to rich approximation || families as neural networks. ||  || References || ---------- || -   Rajesh Ranganath; Jaan Altosaar; Dustin Tran; David M. Blei ||     Operator Variational Inference ||     https:\/\/arxiv.org\/abs\/1610.09033 (2016) || \""\""\""",https://github.com/manuwhs/Trapyng/commit/6b25ff82f383b3dac092795657538789ae7160c8,Yes
4441,proycon/foliapy,tests/folia.py,e3dfbf6718a851d41bd06ac65489acf22a95fe28,TODO: THIS TEST FAILS; SHOULD BE FIXED; TEST MAY CHANGE,https://github.com/proycon/foliapy/commit/e3dfbf6718a851d41bd06ac65489acf22a95fe28,Yes
4442,proycon/foliapy,tests/folia.py,e3dfbf6718a851d41bd06ac65489acf22a95fe28,TODO: THIS TEST FAILS NOW; BUT SHOULDN'T,https://github.com/proycon/foliapy/commit/e3dfbf6718a851d41bd06ac65489acf22a95fe28,Yes
4443,proycon/foliapy,folia/tests/maintest.py,20fbe24298415c1bcea6b752c733f4874f85b907,xxx -> replace with a number at some point when there are more new tests,https://github.com/proycon/foliapy/commit/20fbe24298415c1bcea6b752c733f4874f85b907,No
4444,proycon/foliapy,folia/tests/maintest.py,66c8cf2e416e2a49dcd2d7d2d625e60808ad348f,xxx -> replace with a number at some point when there are more new tests,https://github.com/proycon/foliapy/commit/66c8cf2e416e2a49dcd2d7d2d625e60808ad348f,No
4445,iamDecode/sklearn-pmml-model,tests/linear_model/test_base.py,d16bcfb0661ce5ccac055e31305a8a098d8bff32,TODO: invalidity tests required for 100% coverage,https://github.com/iamDecode/sklearn-pmml-model/commit/d16bcfb0661ce5ccac055e31305a8a098d8bff32,Yes
4446,thu-ml/zhusuan,tests/test_layers.py,b4da9b7c008f2bbd68398686aca1dbce708a1cc9,TODO: test values,https://github.com/thu-ml/zhusuan/commit/b4da9b7c008f2bbd68398686aca1dbce708a1cc9,Yes
4447,thu-ml/zhusuan,tests/test_evaluation.py,12cafb24fd3be0b34772e0a930691226b1127323,TODO: is_loglikelihood test,https://github.com/thu-ml/zhusuan/commit/12cafb24fd3be0b34772e0a930691226b1127323,Yes
4448,thu-ml/zhusuan,tests/test_variational.py,12cafb24fd3be0b34772e0a930691226b1127323,TODO: advi test,https://github.com/thu-ml/zhusuan/commit/12cafb24fd3be0b34772e0a930691226b1127323,Yes
4449,thu-ml/zhusuan,tests/test_layers.py,b03c5e49862902deda03c6403a7683368b2a51d2,TODO: test values,https://github.com/thu-ml/zhusuan/commit/b03c5e49862902deda03c6403a7683368b2a51d2,Yes
4450,thu-ml/zhusuan,tests/test_layers.py,6db1ce0d902e0d5ca50eaadb8d1a502c42b82848,TODO: test values,https://github.com/thu-ml/zhusuan/commit/6db1ce0d902e0d5ca50eaadb8d1a502c42b82848,Yes
4451,thu-ml/zhusuan,tests/model/test_base.py,2a84fba6df541f3bb80556403acf48d73c1b4319,TODO: add control flow test for StochasticGraph.get_output,https://github.com/thu-ml/zhusuan/commit/2a84fba6df541f3bb80556403acf48d73c1b4319,Yes
4452,thu-ml/zhusuan,tests/distributions/test_univariate.py,776b06718a639c957d0138b65d4caa64005c7416,TODO: test sample value,https://github.com/thu-ml/zhusuan/commit/776b06718a639c957d0138b65d4caa64005c7416,Yes
4453,thu-ml/zhusuan,tests/distributions/test_univariate.py,776b06718a639c957d0138b65d4caa64005c7416,TODO: test sample type,https://github.com/thu-ml/zhusuan/commit/776b06718a639c957d0138b65d4caa64005c7416,No
4454,thu-ml/zhusuan,tests/model/test_base.py,776b06718a639c957d0138b65d4caa64005c7416,TODO: test all operators,https://github.com/thu-ml/zhusuan/commit/776b06718a639c957d0138b65d4caa64005c7416,Yes
4455,thu-ml/zhusuan,examples/dlgm_nf.py,8c15e8057ef454e76aadbbf9263e0953e507d451,TODO: add tests for repeated calls of flows,https://github.com/thu-ml/zhusuan/commit/8c15e8057ef454e76aadbbf9263e0953e507d451,Yes
4456,thu-ml/zhusuan,tests/variational/test_monte_carlo.py,60cc4ec7399c2b8869db96c34c6be03a09db4598,TODO: test k=1 equal to elbo,https://github.com/thu-ml/zhusuan/commit/60cc4ec7399c2b8869db96c34c6be03a09db4598,No
4457,thu-ml/zhusuan,tests/variational/test_monte_carlo.py,60cc4ec7399c2b8869db96c34c6be03a09db4598,TODO: test with k increase; increase,https://github.com/thu-ml/zhusuan/commit/60cc4ec7399c2b8869db96c34c6be03a09db4598,Yes
4458,thu-ml/zhusuan,tests/variational/test_monte_carlo.py,60cc4ec7399c2b8869db96c34c6be03a09db4598,TODO: test when k=1; gradients equal to elbo grads,https://github.com/thu-ml/zhusuan/commit/60cc4ec7399c2b8869db96c34c6be03a09db4598,No
4459,thu-ml/zhusuan,tests/variational/test_monte_carlo.py,60cc4ec7399c2b8869db96c34c6be03a09db4598,TODO: test grads with variance reduction equal to grads without it.,https://github.com/thu-ml/zhusuan/commit/60cc4ec7399c2b8869db96c34c6be03a09db4598,No
4460,thu-ml/zhusuan,examples/normalizing_flows/vae_nf.py,794c8fc3458437a3e8a15203ebb1c385d5746c7c,TODO: add tests for repeated calls of flows,https://github.com/thu-ml/zhusuan/commit/794c8fc3458437a3e8a15203ebb1c385d5746c7c,Yes
4461,pytorch/text,test/data/test_builtin_datasets.py,844f4038b4ab86835322863e3df1266ec8bc7ebb,smoke test to ensure ag_news dataset works properly,https://github.com/pytorch/text/commit/844f4038b4ab86835322863e3df1266ec8bc7ebb,Yes
4462,pytorch/text,torchtext/data/utils.py,038515ccfb6df094e22b9f14fcf9d8c391bc4eaa,TODO: Write more tests!,https://github.com/pytorch/text/commit/038515ccfb6df094e22b9f14fcf9d8c391bc4eaa,Yes
4463,pytorch/text,test/experimental/test_vectors.py,3f433ba1610eadf1688b69cbc65964de45bf2a11,TODO: reenable test once the GloVe dataset url starts working,https://github.com/pytorch/text/commit/3f433ba1610eadf1688b69cbc65964de45bf2a11,Yes
4464,pytorch/text,test/experimental/test_vectors.py,b6af5927b9d2318f0474adfa237f81fd9aa251d8,TODO: reenable test once the GloVe dataset url starts working,https://github.com/pytorch/text/commit/b6af5927b9d2318f0474adfa237f81fd9aa251d8,Yes
4465,biolab/orange3,Orange/data/variable.py,1c65f8552ef4a590cffa50cc43e5777708cc2509,"\""\""\"" || ======================== || Variables (``variable``) || ======================== ||  || Data instances in Orange can contain several types of variables: || :ref:`discrete <discrete>`; :ref:`continuous <continuous>`; || :ref:`strings <string>`; and :ref:`Python <Python>` and types derived from it. || The latter represent arbitrary Python objects. || The names; types; values (where applicable); functions for computing the || variable value from values of other variables; and other properties of the || variables are stored in descriptor classes defined in this module. ||  || Variable descriptors || -------------------- ||  || Variable descriptors can be constructed either directly; using  || constructors and passing attributes as parameters; or by a  || factory function :func:`Orange.data.variable.make`; which either  || retrieves an existing descriptor or constructs a new one. ||  || .. class:: Variable ||  ||     An abstract base class for variable descriptors. ||  ||     .. attribute:: name ||  ||         The name of the variable. Variable names do not need to be unique since two ||         variables are considered the same only if they have the same descriptor ||         (e.g. even multiple variables in the same table can have the same name). ||         This should; however; be avoided since it may result in unpredictable ||         behavior. ||      ||     .. attribute:: var_type ||         ||         Variable type; it can be Orange.data.Type.Discrete; ||         Orange.data.Type.Continuous; Orange.data.Type.String or ||         Orange.data.Type.Other.   ||  ||     .. attribute:: get_value_from ||  ||         A function (an instance of :obj:`Orange.classification.Classifier`) which computes ||         a value of the variable from values of one or more other variables. This ||         is used; for instance; in discretization where the variables describing ||         the discretized variable are computed from the original variable.  ||  ||     .. attribute:: ordered ||      ||         A flag telling whether the values of a discrete variable are ordered. At ||         the moment; no built-in method treats ordinal variables differently than ||         nominal ones. ||      ||     .. attribute:: distributed ||      ||         A flag telling whether the values of the variables are distributions. ||         As for the flag ordered; no methods treat such variables in any special ||         manner. ||      ||     .. attribute:: random_generator ||      ||         A local random number generator used by method ||         :obj:`Variable.random_value`. ||      ||     .. attribute:: default_meta_id ||      ||         A proposed (but not guaranteed) meta id to be used for that variable. ||         This is used; for instance; by the data loader for tab-delimited file ||         format instead of assigning an arbitrary new value; or by ||         :obj:`Orange.data.new_meta_id` if the variable is passed as an argument.  ||          ||     .. attribute:: attributes ||          ||         A dictionary which allows the user to store additional information ||         about the variable. All values should be strings. See the section  ||         about :ref:`storing additional information <attributes>`. ||  ||     .. method:: __call__(obj) ||      ||            Convert a string; number; or other suitable object into a variable ||            value. ||             ||            :param obj: An object to be converted into a variable value ||            :type o: any suitable ||            :rtype: :class:`Orange.data.Value` ||         ||     .. method:: randomvalue() ||  ||            Return a random value for the variable. ||         ||            :rtype: :class:`Orange.data.Value` ||         ||     .. method:: compute_value(inst) ||  ||            Compute the value of the variable given the instance by calling ||            obj:`~Variable.get_value_from` through a mechanism that prevents deadlocks by ||            circular calls. ||  ||            :rtype: :class:`Orange.data.Value` ||  || .. _discrete: || .. class:: Discrete ||  ||     Bases: :class:`Variable` ||     ||     Descriptor for discrete variables. ||      ||     .. attribute:: values ||      ||         A list with symbolic names for variables' values. Values are stored as ||         indices referring to this list. Therefore; modifying this list  ||         instantly changes the (symbolic) names of values as they are printed out or ||         referred to by user. ||      ||         .. note:: ||          ||             The size of the list is also used to indicate the number of ||             possible values for this variable. Changing the size - especially ||             shrinking the list - can have disastrous effects and is therefore not ||             really recommended. Also; do not add values to the list by ||             calling its append or extend method: call the :obj:`add_value` ||             method instead. ||  ||             It is also assumed that this attribute is always defined (but can ||             be empty); so never set it to None. ||      ||     .. attribute:: base_value ||  ||             Stores the base value for the variable as an index in `values`. ||             This can be; for instance; a \""normal\"" value; such as \""no ||             complications\"" as opposed to abnormal \""low blood pressure\"". The ||             base value is used by certain statistics; continuization etc. ||             potentially; learning algorithms. The default is -1 which means that ||             there is no base value. ||      ||     .. method:: add_value ||      ||             Add a value to values. Always call this function instead of ||             appending to values. ||  || .. _continuous: || .. class:: Continuous ||  ||     Bases: :class:`Variable` ||  ||     Descriptor for continuous variables. ||      ||     .. attribute:: number_of_decimals ||      ||         The number of decimals used when the value is printed out; converted to ||         a string or saved to a file. ||      ||     .. attribute:: scientific_format ||      ||         If ``True``; the value is printed in scientific format whenever it ||         would have more than 5 digits. In this case; :obj:`number_of_decimals` is ||         ignored. ||  ||     .. attribute:: adjust_decimals ||      ||         Tells Orange to monitor the number of decimals when the value is ||         converted from a string (when the values are read from a file or ||         converted by; e.g. ``inst[0]=\""3.14\""``):  ||         0: the number of decimals is not adjusted automatically; ||         1: the number of decimals is (and has already) been adjusted; ||         2: automatic adjustment is enabled; but no values have been converted yet. ||  ||         By default; adjustment of the number of decimals goes as follows: ||      ||         If the variable was constructed when data was read from a file; it will  ||         be printed with the same number of decimals as the largest number of  ||         decimals encountered in the file. If scientific notation occurs in the  ||         file; :obj:`scientific_format` will be set to ``True`` and scientific format  ||         will be used for values too large or too small.  ||      ||         If the variable is created in a script; it will have; by default; three ||         decimal places. This can be changed either by setting the value ||         from a string (e.g. ``inst[0]=\""3.14\""``; but not ``inst[0]=3.14``) or by ||         manually setting the :obj:`number_of_decimals`. ||  ||     .. attribute:: start_value; end_value; step_value ||      ||         The range used for :obj:`randomvalue`. ||  || .. _String: || .. class:: String ||  ||     Bases: :class:`Variable` ||  ||     Descriptor for variables that contain strings. No method can use them for  ||     learning; some will complain and others will silently ignore them when they  ||     encounter them. They can be; however; useful for meta-attributes; if  ||     instances in a dataset have unique IDs; the most efficient way to store them  ||     is to read them as meta-attributes. In general; never use discrete  ||     attributes with many (say; more than 50) values. Such attributes are  ||     probably not of any use for learning and should be stored as string ||     attributes. ||  ||     When converting strings into values and back; empty strings are treated  ||     differently than usual. For other types; an empty string can be used to ||     denote undefined values; while :obj:`String` will take empty strings ||     as empty strings -- except when loading or saving into file. ||     Empty strings in files are interpreted as undefined; to specify an empty ||     string; enclose the string in double quotes; these are removed when the ||     string is loaded. ||  || .. _Python: || .. class:: Python ||  ||     Bases: :class:`Variable` ||  ||     Base class for descriptors defined in Python. It is fully functional ||     and can be used as a descriptor for attributes that contain arbitrary Python ||     values. Since this is an advanced topic; PythonVariables are described on a  ||     separate page. !!TODO!! ||      ||      || Variables computed from other variables || --------------------------------------- ||  || Values of variables are often computed from other variables; such as in || discretization. The mechanism described below usually functions behind the scenes; || so understanding it is required only for implementing specific transformations. ||  || Monk 1 is a well-known dataset with target concept ``y := a==b or e==1``. || It can help the learning algorithm if the four-valued attribute ``e`` is || replaced with a binary attribute having values `\""1\""` and `\""not 1\""`. The || new variable will be computed from the old one on the fly.  ||  || .. literalinclude:: code\/variable-get_value_from.py ||     :lines: 7-17 ||      || The new variable is named ``e2``; we define it with a descriptor of type  || :obj:`Discrete`; with appropriate name and values ``\""not 1\""`` and ``1`` (we  || chose this order so that the ``not 1``'s index is ``0``; which can be; if  || needed; interpreted as ``False``). Finally; we tell e2 to use  || ``checkE`` to compute its value when needed; by assigning ``checkE`` to  || ``e2.get_value_from``.  ||  || ``checkE`` is a function that is passed an instance and another argument we  || do not care about here. If the instance's ``e`` equals ``1``; the function  || returns value ``1``; otherwise it returns ``not 1``. Both are returned as  || values; not plain strings. ||  || In most circumstances the value of ``e2`` can be computed on the fly - we can  || pretend that the variable exists in the data; although it does not (but  || can be computed from it). For instance; we can compute the information gain of || variable ``e2`` or its distribution without actually constructing data containing || the new variable. ||  || .. literalinclude:: code\/variable-get_value_from.py ||     :lines: 19-22 ||  || There are methods which cannot compute values on the fly because it would be || too complex or time consuming. In such cases; the data need to be converted || to a new :obj:`Orange.data.Table`:: ||  ||     new_domain = Orange.data.Domain([data.domain[\""a\""]; data.domain[\""b\""]; e2; data.domain.class_var]) ||     new_data = Orange.data.Table(new_domain; data)  ||  || Automatic computation is useful when the data is split into training and  || testing examples. Training instances can be modified by adding; removing  || and transforming variables (in a typical setup; continuous variables  || are discretized prior to learning; therefore the original variables are  || replaced by new ones). Test instances; on the other hand; are left as they  || are. When they are classified; the classifier automatically converts the  || testing instances into the new domain; which includes recomputation of  || transformed variables.  ||  || .. literalinclude:: code\/variable-get_value_from.py ||     :lines: 24- ||  || .. _attributes: ||  || Storing additional variables || ----------------------------- ||  || All variables have a field :obj:`~Variable.attributes`; a dictionary || which can contain strings. Although the current implementation allows all || types of value we strongly advise to use only strings. An example: ||  || .. literalinclude:: code\/attributes.py ||  || These attributes can only be saved to a .tab file. They are listed in the || third line in <name>=<value> format; after other attribute specifications || (such as \""meta\"" or \""class\""); and are separated by spaces.  ||  || .. _variable_descriptor_reuse: ||  || Reuse of descriptors || -------------------- ||  || There are situations when variable descriptors need to be reused. Typically; the  || user loads some training examples; trains a classifier; and then loads a separate || test set. For the classifier to recognize the variables in the second data set; || the descriptors; not just the names; need to be the same.  ||  || When constructing new descriptors for data read from a file or during unpickling; || Orange checks whether an appropriate descriptor (with the same name and; in case || of discrete variables; also values) already exists and reuses it. When new || descriptors are constructed by explicitly calling the above constructors; this || always creates new descriptors and thus new variables; although a variable with || the same name may already exist. ||  || The search for an existing variable is based on four attributes: the variable's name; || type; ordered values; and unordered values. As for the latter two; the values can  || be explicitly ordered by the user; e.g. in the second line of the tab-delimited  || file. For instance; sizes can be ordered as small; medium; or big. ||  || The search for existing variables can end with one of the following statuses. ||  || .. data:: Orange.data.variable.MakeStatus.NotFound (4) ||  ||     The variable with that name and type does not exist.  ||  || .. data:: Orange.data.variable.MakeStatus.Incompatible (3) ||  ||     There are variables with matching name and type; but their ||     values are incompatible with the prescribed ordered values. For example; ||     if the existing variable already has values [\""a\""; \""b\""] and the new one ||     wants [\""b\""; \""a\""]; the old variable cannot be reused. The existing list can; ||     however be appended with the new values; so searching for [\""a\""; \""b\""; \""c\""] would ||     succeed. Likewise a search for [\""a\""] would be successful; since the extra existing value ||     does not matter. The formal rule is thus that the values are compatible iff ``existing_values[:len(ordered_values)] == ordered_values[:len(existing_values)]``. ||  || .. data:: Orange.data.variable.MakeStatus.NoRecognizedValues (2) ||  ||     There is a matching variable; yet it has none of the values that the new ||     variable will have (this is obviously possible only if the new variable has ||     no prescribed ordered values). For instance; we search for a variable ||     \""sex\"" with values \""male\"" and \""female\""; while there is a variable of the same  ||     name with values \""M\"" and \""F\"" (or; well; \""no\"" and \""yes\"" :). Reuse of this  ||     variable is possible; though this should probably be a new variable since it  ||     obviously comes from a different data set. If we do decide to reuse the variable; the  ||     old variable will get some unneeded new values and the new one will inherit  ||     some from the old. ||  || .. data:: Orange.data.variable.MakeStatus.MissingValues (1) ||  ||     There is a matching variable with some of the values that the new one  ||     requires; but some values are missing. This situation is neither uncommon  ||     nor suspicious: in case of separate training and testing data sets there may ||     be values which occur in one set but not in the other. ||  || .. data:: Orange.data.variable.MakeStatus.OK (0) ||  ||     There is a perfect match which contains all the prescribed values in the ||     correct order. The existing variable may have some extra values; though. ||  || Continuous variables can obviously have only two statuses;  || :obj:`~Orange.data.variable.MakeStatus.NotFound` or :obj:`~Orange.data.variable.MakeStatus.OK`. ||  || When loading the data using :obj:`Orange.data.Table`; Orange takes the safest  || approach and; by default; reuses everything that is compatible up to  || and including :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`. Unintended reuse would be obvious from the || variable having too many values; which the user can notice and fix. More on that  || in the page on `loading data`. !!TODO!! ||  || There are two functions for reusing the variables instead of creating new ones. ||  || .. function:: Orange.data.variable.make(name; type; ordered_values; unordered_values[; create_new_on]) ||  ||     Find and return an existing variable or create a new one if none of the existing ||     variables matches the given name; type and values. ||      ||     The optional `create_new_on` specifies the status at which a new variable is ||     created. The status must be at most :obj:`~Orange.data.variable.MakeStatus.Incompatible` since incompatible (or ||     non-existing) variables cannot be reused. If it is set lower; for instance  ||     to :obj:`~Orange.data.variable.MakeStatus.MissingValues`; a new variable is created even if there exists ||     a variable which is only missing the same values. If set to :obj:`~Orange.data.variable.MakeStatus.OK`; the function ||     always creates a new variable. ||      ||     The function returns a tuple containing a variable descriptor and the ||     status of the best matching variable. So; if ``create_new_on`` is set to ||     :obj:`~Orange.data.variable.MakeStatus.MissingValues`; and there exists a variable whose status is; say; ||     :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`; a variable would be created; while the second  ||     element of the tuple would contain :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`. If; on the other ||     hand; there exists a variable which is perfectly OK; its descriptor is  ||     returned and the returned status is :obj:`~Orange.data.variable.MakeStatus.OK`. The function returns no  ||     indicator whether the returned variable is reused or not. This can be; ||     however; read from the status code: if it is smaller than the specified ||     ``create_new_on``; the variable is reused; otherwise a new descriptor has been constructed. ||  ||     The exception to the rule is when ``create_new_on`` is OK. In this case; the  ||     function does not search through the existing variables and cannot know the  ||     status; so the returned status in this case is always :obj:`~Orange.data.variable.MakeStatus.OK`. ||  ||     :param name: Variable name ||     :param type: Variable type ||     :type type: Orange.data.variable.Type ||     :param ordered_values: a list of ordered values ||     :param unordered_values: a list of values; for which the order does not ||         matter ||     :param create_new_on: gives the condition for constructing a new variable instead ||         of using the new one ||      ||     :return_type: a tuple (:class:`Orange.data.variable.Variable`; int) ||      || .. function:: Orange.data.variable.retrieve(name; type; ordered_values; onordered_values[; create_new_on]) ||  ||     Find and return an existing variable; or :obj:`None` if no match is found. ||      ||     :param name: variable name. ||     :param type: variable type. ||     :type type: Orange.data.variable.Type ||     :param ordered_values: a list of ordered values ||     :param unordered_values: a list of values; for which the order does not ||         matter ||     :param create_new_on: gives the condition for constructing a new variable instead ||         of using the new one ||  ||     :return_type: :class:`Orange.data.variable.Variable` ||      || These following examples (from :download:`variable-reuse.py <code\/variable-reuse.py>`) give the shown results if || executed only once (in a Python session) and in this order. ||  || :func:`Orange.data.variable.make` can be used for the construction of new variables. :: ||      ||     >>> v1; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""a\""; \""b\""]) ||     >>> print s; v1.values ||     4 <a; b> ||  || No surprises here: a new variable is created and the status is :obj:`~Orange.data.variable.MakeStatus.NotFound`. :: ||  ||     >>> v2; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""a\""]; [\""c\""]) ||     >>> print s; v2 is v1; v1.values ||     1 True <a; b; c> ||  || The status is 1 (:obj:`~Orange.data.variable.MakeStatus.MissingValues`); yet the variable is reused (``v2 is v1``). || ``v1`` gets a new value; ``\""c\""``; which was given as an unordered value. It does || not matter that the new variable does not need the value ``b``. :: ||  ||     >>> v3; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""a\""; \""b\""; \""c\""; \""d\""]) ||     >>> print s; v3 is v1; v1.values ||     1 True <a; b; c; d> ||  || This is like before; except that the new value; ``d`` is not among the || ordered values. :: ||  ||     >>> v4; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; [\""b\""]) ||     >>> print s; v4 is v1; v1.values; v4.values ||     3; False; <b>; <a; b; c; d> ||  || The new variable needs to have ``b`` as the first value; so it is incompatible  || with the existing variables. The status is thus 3 (:obj:`~Orange.data.variable.MakeStatus.Incompatible`); the two  || variables are not equal and have different lists of values. :: ||  ||     >>> v5; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; None; [\""c\""; \""a\""]) ||     >>> print s; v5 is v1; v1.values; v5.values ||     0 True <a; b; c; d> <a; b; c; d> ||  || The new variable has values ``c`` and ``a``; but the order is not important;  || so the existing attribute is :obj:`~Orange.data.variable.MakeStatus.OK`. :: ||  ||     >>> v6; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; None; [\""e\""]) \""a\""]) ||     >>> print s; v6 is v1; v1.values; v6.values ||     2 True <a; b; c; d; e> <a; b; c; d; e> ||  || The new variable has different values than the existing variable (status is 2; || :obj:`~Orange.data.variable.MakeStatus.NoRecognizedValues`); but the existing one is nonetheless reused. Note that we || gave ``e`` in the list of unordered values. If it was among the ordered; the || reuse would fail. :: ||  ||     >>> v7; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; None; ||             [\""f\""]; Orange.data.variable.MakeStatus.NoRecognizedValues))) ||     >>> print s; v7 is v1; v1.values; v7.values ||     2 False <a; b; c; d; e> <f> ||  || This is the same as before; except that we prohibited reuse when there are no || recognized values. Hence a new variable is created; though the returned status is  || the same as before:: ||  ||     >>> v8; s = Orange.data.variable.make(\""a\""; Orange.data.Type.Discrete; ||             [\""a\""; \""b\""; \""c\""; \""d\""; \""e\""]; None; Orange.data.variable.MakeStatus.OK) ||     >>> print s; v8 is v1; v1.values; v8.values ||     0 False <a; b; c; d; e> <a; b; c; d; e> ||  || Finally; this is a perfect match; but any reuse is prohibited; so a new  || variable is created. ||  || \""\""\""",https://github.com/biolab/orange3/commit/1c65f8552ef4a590cffa50cc43e5777708cc2509,Yes
4466,biolab/orange3,Orange/misc/__init__.py,2fece659e06d63d8c7e2af365e1af79b96b6a287,"\""\""\"" || .. index:: misc ||  || .. index: CostMatrix ||  || ----------------------- || CostMatrix || ----------------------- ||  || CostMatrix is an object that stores costs of (mis)classifications. Costs can be either negative or positive. ||  || .. class:: CostMatrix ||  ||     .. attribute:: class_var  ||          ||         The (class) attribute to which the matrix applies. This can ||         also be None. ||          ||     .. attribute:: dimension (read only) ||      ||         Matrix dimension; ie. number of classes. ||          ||     .. method:: CostMatrix(dimension[; default cost]) ||      ||         Constructs a matrix of the given size and initializes it with ||         the default cost (1; if not given). All elements of the matrix ||         are assigned the given cost; except for the diagonal that have ||         the default cost of 0.  (Diagonal elements represent correct ||         classifications and these usually have no price; you can; ||         however; change this.) ||          ||         .. literalinclude:: code\/CostMatrix.py ||             :lines: 1-8 ||          ||         This initializes the matrix and print it out: ||          ||         .. literalinclude:: code\/CostMatrix.res ||             :lines: 1-3 ||      ||     .. method:: CostMatrix(class descriptor[; default cost]) ||      ||         Similar as above; except that classVar is also set to the given descriptor. ||         The number of values of the given attribute (which must be discrete) is used ||         for dimension. ||          ||         .. literalinclude:: code\/CostMatrix.py ||             :lines: 10-11 ||              ||         This constructs a matrix similar to the one above (the class attribute in iris ||         domain is three-valued) except that the matrix contains 2s instead of 1s. ||          ||     .. method:: CostMatrix([attribute descriptor; ]matrix) ||      ||         Initializes the matrix with the elements given as a sequence of sequences (you ||         can mix lists and tuples if you find it funny). Each subsequence represents a row. ||          ||         .. literalinclude:: code\/CostMatrix.py ||             :lines: 13 ||  ||         If you print this matrix out; will it look like this: ||          ||         .. literalinclude:: code\/CostMatrix.res ||             :lines: 5-7 ||              ||     .. method:: setcost(predicted; correct; cost) ||      ||         Set the misclassification cost. The matrix above could be ||         constructed by first initializing it with 2s and then changing ||         the prices for virginica's into 1s. ||          ||         .. literalinclude:: code\/CostMatrix.py ||             :lines: 15-17 ||              ||     .. method:: getcost(predicted; correct) ||      ||         Returns the cost of prediction. Values must be integer ||         indices; if class_var is set; you can also use symbolic values ||         (strings). Note that there's no way to change the size of the ||         matrix. Size is set at construction and does not change.  For ||         the final example; we shall compute the profits of knowing ||         attribute values in the dataset lenses with the same ||         cost-matrix as printed above. ||          ||         .. literalinclude:: code\/CostMatrix.py ||             :lines: 19-23 ||              ||         As the script shows; you don't have to (and usually won't) call the constructor ||         explicitly. Instead; you will set the corresponding field (in our case meas.cost) ||         to a matrix and let Orange convert it to CostMatrix automatically. Funny as it ||         might look; but since Orange uses constructor to perform such conversion; even ||         the above statement is correct (although the cost matrix is rather dull; ||         with 0s on the diagonal and 1s around):             ||              ||         .. literalinclude:: code\/CostMatrix.py ||             :lines: 25 ||                  || .. index: SymMatrix ||  || ----------------------- || SymMatrix || ----------------------- ||  || :obj:`SymMatrix` implements symmetric matrices of size fixed at  || construction time (and stored in :obj:`SymMatrix.dim`). ||  || .. class:: SymMatrix ||  ||     .. attribute:: dim || \t ||         Matrix dimension. ||              ||     .. attribute:: matrix_type  ||  ||         Can be ``SymMatrix.Lower`` (0); ``SymMatrix.Upper`` (1);  ||         ``SymMatrix.Symmetric`` (2; default); ``SymMatrix.LowerFilled`` (3) or ||         ``SymMatrix.Upper_Filled`` (4).  ||  ||         If the matrix type is ``Lower`` or ``Upper``; indexing  ||         above or below the diagonal; respectively; will fail.  ||         With ``LowerFilled`` and ``Upper_Filled``; ||         the elements upper or lower; respectively; still  ||         exist and are set to zero; but they cannot be modified. The  ||         default matrix type is ``Symmetric``; but can be changed  ||         at any time. ||  ||         If matrix type is ``Upper``; it is printed as: ||  ||         >>> import Orange ||         >>> m = Orange.misc.SymMatrix( ||         ...     [[1];  ||         ...      [2; 4];  ||         ...      [3; 6; 9];  ||         ...      [4; 8; 12; 16]]) ||         >>> m.matrix_type = m.Upper ||         >>> print m ||         (( 1.000;  2.000;  3.000;  4.000); ||          (         4.000;  6.000;  8.000); ||          (                 9.000; 12.000); ||          (                        16.000)) ||  ||         Changing the type to ``LowerFilled`` changes the printout to ||  ||         >>> m.matrix_type = m.LowerFilled ||         >>> print m ||         (( 1.000;  0.000;  0.000;  0.000); ||          ( 2.000;  4.000;  0.000;  0.000); ||          ( 3.000;  6.000;  9.000;  0.000); ||          ( 4.000;  8.000; 12.000; 16.000)) || \t ||     .. method:: __init__(dim[; value]) ||  ||         Construct a symmetric matrix of the given dimension. ||  ||         :param dim: matrix dimension ||         :type dim: int ||  ||         :param value: default value (0 by default) ||         :type value: double ||          ||          ||     .. method:: __init__(data) ||  ||         Construct a new symmetric matrix containing the given data.  ||         These can be given as Python list containing lists or tuples. ||          ||         The following example fills a matrix created above with ||         data in a list:: ||  ||             import Orange ||             m = [[]; ||                  [ 3]; ||                  [ 2; 4]; ||                  [17; 5; 4]; ||                  [ 2; 8; 3; 8]; ||                  [ 7; 5; 10; 11; 2]; ||                  [ 8; 4; 1; 5; 11; 13]; ||                  [ 4; 7; 12; 8; 10; 1; 5]; ||                  [13; 9; 14; 15; 7; 8; 4; 6]; ||                  [12; 10; 11; 15; 2; 5; 7; 3; 1]] ||                      ||             matrix = Orange.data.SymMatrix(m) ||  ||         SymMatrix also stores diagonal elements. They are set ||         to zero; if they are not specified. The missing elements ||         (shorter lists) are set to zero as well. If a list ||         spreads over the diagonal; the constructor checks ||         for asymmetries. For instance; the matrix ||  ||         :: ||  ||             m = [[]; ||                  [ 3;  0; f]; ||                  [ 2;  4]] ||      ||         is only OK if f equals 2. Finally; no row can be longer  ||         than matrix size.   ||  ||     .. method:: get_values() ||      ||         Return all matrix values in a Python list. ||  ||     .. method:: get_KNN(i; k) ||      ||         Return k columns with the lowest value in the i-th row.  ||          ||         :param i: i-th row ||         :type i: int ||          ||         :param k: number of neighbors ||         :type k: int ||          ||     .. method:: avg_linkage(clusters) ||      ||         Return a symmetric matrix with average distances between given clusters.   ||        ||         :param clusters: list of clusters ||         :type clusters: list of lists ||          ||     .. method:: invert(type) ||      ||         Invert values in the symmetric matrix. ||          ||         :param type: 0 (-X); 1 (1 - X); 2 (max - X); 3 (1 \/ X) ||         :type type: int ||  ||     .. method:: normalize(type) ||      ||         Normalize values in the symmetric matrix. ||          ||         :param type: 0 (normalize to [0; 1] interval); 1 (Sigmoid) ||         :type type: int ||          ||          ||  || Indexing || .......... ||  || For symmetric matrices the order of indices is not important:  || if ``m`` is a SymMatrix; then ``m[2; 4]`` addresses the same element as ``m[4; 2]``. ||  || .. ||     .. literalinclude:: code\/symmatrix.py ||         :lines: 1-6 ||  || >>> import Orange || >>> m = Orange.misc.SymMatrix(4) || >>> for i in range(4): || ...    for j in range(i+1): || ...        m[i; j] = (i+1)*(j+1) ||  ||  || Although only the lower left half of the matrix was set explicitely;  || the whole matrix is constructed. ||  || >>> print m || (( 1.000;  2.000;  3.000;  4.000); ||  ( 2.000;  4.000;  6.000;  8.000); ||  ( 3.000;  6.000;  9.000; 12.000); ||  ( 4.000;  8.000; 12.000; 16.000)) ||   || Entire rows are indexed with a single index. They can be iterated || over in a for loop or sliced (with; for example; ``m[:3]``): ||  || >>> print m[1] || (2.0; 4.0; 6.0; 8.0) || >>> m.matrix_type = m.Lower || >>> for row in m: || ...     print row || (1.0;) || (2.0; 4.0) || (3.0; 6.0; 9.0) || (4.0; 8.0; 12.0; 16.0) ||  || .. index: Random number generator ||  || ----------------------- || Random number generator || ----------------------- ||  || :obj:`Random` uses the  || `Mersenne twister <http:\/\/en.wikipedia.org\/wiki\/Mersenne_twister>`_ algorithm || to generate random numbers. ||  || :: ||  ||     >>> import Orange ||     >>> rg = Orange.misc.Random(42) ||     >>> rg(10) ||     4 ||     >>> rg(10) ||     7 ||     >>> rg.uses  # We called rg two times. ||     2 ||     >>> rg.reset() ||     >>> rg(10) ||     4 ||     >>> rg(10) ||     7 ||     >>> rg.uses ||     2 ||  ||  || .. class:: Random(seed) ||  ||     :param initseed: Seed used for initializing the random generator. ||     :type initseed: int ||  ||     .. method:: __call__(n) ||  ||         Return a random integer R such that 0 <= R < n. ||  ||         :type n: int ||  ||     .. method:: reset([seed]) ||  ||         Reinitialize the random generator with `initseed`. If `initseed` ||         is not given use the existing value of attribute `initseed`. ||  ||     .. attribute:: uses ||          ||         The number of times the generator was called after ||         initialization\/reset. ||      ||     .. attribute:: initseed ||  ||         Random seed. ||  || Two examples or random number generator uses found in the documentation || are :obj:`Orange.evaluation.testing` and :obj:`Orange.data.Table`. ||  ||  || ---------------- || Other submodules || ---------------- ||  || .. automodule:: Orange.misc.selection ||  || \""\""\""",https://github.com/biolab/orange3/commit/2fece659e06d63d8c7e2af365e1af79b96b6a287,No
4467,biolab/orange3,Orange/__init__.py,5cf6b70c5115524f48b8f3eb9a2ce2632ebe4933,"\""\""\"" || _import(\""data.sample\"") || _import(\""data.outliers\"") || _import(\""data.preprocess\"") || _import(\""data.preprocess.scaling\"") || _import(\""data.utils\"") || _import(\""data.discretization\"") || _import(\""data.continuization\"") || _import(\""data.filter\"") || _import(\""data.imputation\"") ||  || _import(\""feature\"") || _import(\""feature.construction\"") || _import(\""feature.construction.functionDecomposition\"") || _import(\""feature.construction.univariate\"") || _import(\""feature.discretization\"") || _import(\""feature.imputation\"") || _import(\""feature.scoring\"") || _import(\""feature.selection\"") ||  || _import(\""network\"") ||  || _import(\""stat\"") ||  || _import(\""statistics\"") || _import(\""statistics.estimate\"") || _import(\""statistics.contingency\"") || _import(\""statistics.distribution\"") || _import(\""statistics.basic\"") || _import(\""statistics.evd\"") ||  || _import(\""classification\"") || _import(\""classification.tree\"") ||  || _import(\""classification.rules\"") ||  || _import(\""classification.lookup\"") || _import(\""classification.bayes\"") || _import(\""classification.svm\"") || _import(\""classification.logreg\"") || _import(\""classification.knn\"") || _import(\""classification.majority\"") ||  || _import(\""tuning\"") ||  || _import(\""projection\"") || _import(\""projection.linear\"") || _import(\""projection.mds\"") || _import(\""projection.som\"") ||  || _import(\""ensemble\"") || _import(\""ensemble.bagging\"") || _import(\""ensemble.boosting\"") || _import(\""ensemble.forest\"") || _import(\""ensemble.stacking\"") ||  || _import(\""regression\"") || _import(\""regression.base\"") || _import(\""regression.earth\"") || _import(\""regression.lasso\"") || _import(\""regression.linear\"") || _import(\""regression.mean\"") || _import(\""regression.pls\"") || _import(\""regression.tree\"") ||  || _import(\""multitarget\"") || _import(\""multitarget.tree\"") ||  || _import(\""multilabel\"") || _import(\""multilabel.multibase\"") || _import(\""multilabel.br\"") || _import(\""multilabel.lp\"") || _import(\""multilabel.mlknn\"") || _import(\""multilabel.brknn\"") || _import(\""multilabel.mulan\"") ||  || _import(\""associate\"") ||  || _import(\""distance\"") ||  || _import(\""wrappers\"") ||  || _import(\""featureConstruction\"") || _import(\""featureConstruction.univariate\"") || _import(\""featureConstruction.functionDecomposition\"") ||  || _import(\""evaluation\"") || _import(\""evaluation.scoring\"") || _import(\""evaluation.testing\"") ||  || _import(\""clustering\"") || _import(\""clustering.kmeans\"") || _import(\""clustering.hierarchical\"") || _import(\""clustering.consensus\"") ||  || _import(\""misc\"") ||  || _import(\""utils\"") #TODO hide utils from the user || _import(\""utils.environ\"") || _import(\""utils.counters\"") || _import(\""utils.addons\"") || _import(\""utils.render\"") || _import(\""utils.serverfiles\"") ||  || _import_addons() || \""\""\""",https://github.com/biolab/orange3/commit/5cf6b70c5115524f48b8f3eb9a2ce2632ebe4933,Yes
4468,biolab/orange3,Orange/tests/test_table.py,61e4eddd037f51cc5a39484387b8981d862b33d3,TODO reenable tests with metas when they are implemented,https://github.com/biolab/orange3/commit/61e4eddd037f51cc5a39484387b8981d862b33d3,Yes
4469,biolab/orange3,Orange/tests/test_table.py,61e4eddd037f51cc5a39484387b8981d862b33d3,TODO Test conjunctions and disjunctions of conditions,https://github.com/biolab/orange3/commit/61e4eddd037f51cc5a39484387b8981d862b33d3,Yes
4470,biolab/orange3,Orange/data/table.py,64cb7c5764587596e7821b53964e06480e403003,"TODO: unit tests do not pass because sort does not sort by \""header row\""; but rather sorts each row separately",https://github.com/biolab/orange3/commit/64cb7c5764587596e7821b53964e06480e403003,No
4471,biolab/orange3,Orange/canvas/scheme/widgetsscheme.py,ff062da91ca42f483a3a7c739835de3825c7898d,TODO: Test if async processing works; then remove this,https://github.com/biolab/orange3/commit/ff062da91ca42f483a3a7c739835de3825c7898d,No
4472,biolab/orange3,Orange/widgets/visualize/owscatterplot.py,f1e43c50c47784192d7c7f515bf4e15807bf2def,#TODO tukaj mas testni graf!,https://github.com/biolab/orange3/commit/f1e43c50c47784192d7c7f515bf4e15807bf2def,Yes
4473,biolab/orange3,Orange/widgets/evaluate/owtestlearners.py,2da78f3224624173933b47eaa5634a36d00bbae6,TODO: Test each learner individually,https://github.com/biolab/orange3/commit/2da78f3224624173933b47eaa5634a36d00bbae6,Yes
4474,biolab/orange3,Orange/tests/test_svm.py,6993ef8bcf62f1e9c33a1a7b25b835e6406b0725,TODO: improve the test - what does it check?,https://github.com/biolab/orange3/commit/6993ef8bcf62f1e9c33a1a7b25b835e6406b0725,No
4475,biolab/orange3,Orange/widgets/tests/base.py,69ceb538919215ebc369a737bbb8215f6a7750ff,only needed in TestOWMDS,https://github.com/biolab/orange3/commit/69ceb538919215ebc369a737bbb8215f6a7750ff,No
4476,biolab/orange3,Orange/widgets/model/tests/test_owsgd.py,78f51648eecdbe91df6a6c9b80c46228f9abeaf4,TODO Fix the base tests to support parameter delegation with,https://github.com/biolab/orange3/commit/78f51648eecdbe91df6a6c9b80c46228f9abeaf4,Yes
4477,biolab/orange3,Orange/widgets/model/tests/test_owadaboost.py,f74547687080f419319fec87d1f9b931fae6f71a,TODO Due to the way params are tested on the learner and the fact,https://github.com/biolab/orange3/commit/f74547687080f419319fec87d1f9b931fae6f71a,Yes
4478,biolab/orange3,Orange/widgets/visualize/ownomogram.py,a622f7aeffd2cbeed6f43b531fd7bd65d62e2002,XXX: This prevents some tests failing,https://github.com/biolab/orange3/commit/a622f7aeffd2cbeed6f43b531fd7bd65d62e2002,Yes
4479,biolab/orange3,Orange/widgets/visualize/tests/test_owprojectionwidget.py,453222b587dd5dcdb75386c7d528b6f52b8c32f0,Test that get_columns modify a copy of the data and not the data,https://github.com/biolab/orange3/commit/453222b587dd5dcdb75386c7d528b6f52b8c32f0,Yes
4480,biolab/orange3,Orange/tests/test_naive_bayes.py,9f3ce8ff7592992dd92b813d2d23cc7885232f6e,Test prediction by directly calling predict. This is needed to test,https://github.com/biolab/orange3/commit/9f3ce8ff7592992dd92b813d2d23cc7885232f6e,No
4481,biolab/orange3,Orange/tests/test_naive_bayes.py,5e0fa06f195aab47aef1a8877636557faf83d7f2,Test prediction by directly calling predict. This is needed to test,https://github.com/biolab/orange3/commit/5e0fa06f195aab47aef1a8877636557faf83d7f2,No
4482,biolab/orange3,Orange/widgets/data/owsave.py,98a7415b612fd6a014b64a119e21a2062b3fa74a,TODO: This is not tested!!!,https://github.com/biolab/orange3/commit/98a7415b612fd6a014b64a119e21a2062b3fa74a,Yes
4483,biolab/orange3,Orange/widgets/visualize/tests/test_owlineplot.py,03f2412cdbfc0ef4ed4beac0e134e3c63896d2f5,- remove test_selection_line workaround for 0.11.0,https://github.com/biolab/orange3/commit/03f2412cdbfc0ef4ed4beac0e134e3c63896d2f5,No
4484,pytorch/ignite,tests/ignite/trainer/test_trainer.py,78beb636670bf810134bc43feece960bdc6dffa7,TODO add test to assure history is written to from trainer,https://github.com/pytorch/ignite/commit/78beb636670bf810134bc43feece960bdc6dffa7,Yes
4485,skorch-dev/skorch,examples/word_language_model/train.py,47227980c2b0e3c3c8dc4ae0dfc0e9e1fc457344,FIXME: iterator_test does not use corpus.valid as dataset,https://github.com/skorch-dev/skorch/commit/47227980c2b0e3c3c8dc4ae0dfc0e9e1fc457344,No
4486,skorch-dev/skorch,inferno/tests/test_dataset.py,b54b1ef852fef919c5bf83f2e534bb74605b8612,TODO: test regression with dummy y as well,https://github.com/skorch-dev/skorch/commit/b54b1ef852fef919c5bf83f2e534bb74605b8612,No
4487,skorch-dev/skorch,inferno/tests/test_dataset.py,a1f30103525c50445567565d8ff0e6f730b3a0c0,TODO: test regression with dummy y as well,https://github.com/skorch-dev/skorch/commit/a1f30103525c50445567565d8ff0e6f730b3a0c0,No
4488,skorch-dev/skorch,skorch/tests/callbacks/test_training.py,e38d5d92c6ec69efbe03034fe659c88d2815e7cc,TODO: remove this test when the target argument is removed,https://github.com/skorch-dev/skorch/commit/e38d5d92c6ec69efbe03034fe659c88d2815e7cc,No
4489,skorch-dev/skorch,examples/benchmarks/freezing.py,350dfc4003609d2d034f327debd831c819fdeb9e,"\""\""\""Benchmark to test runtime and memory performance of || different freezing approaches. ||  || Test A is done by setting `requires_grad=False` manually || while filtering these parameters from the optimizer using || ``skorch.helper.filtered_optimizer``. ||  || Test B uses the ``Freezer`` via ``ParamMapper`` without || explicitly removing the parameters from the optimizer. ||  || In theory there should be no difference in memory || consumption and runtime. || \""\""\""",https://github.com/skorch-dev/skorch/commit/350dfc4003609d2d034f327debd831c819fdeb9e,Yes
4490,RasaHQ/rasa_core,docs/conf.py,999a1bbb3d964bb592226501e4f410a387d21a28,patch agent handle to fix indefinite running during doctests,https://github.com/RasaHQ/rasa_core/commit/999a1bbb3d964bb592226501e4f410a387d21a28,No
4491,RasaHQ/rasa_core,tests/test_featurizer.py,97980c7277924880e75def490624aa813efc1886,TODO featurizers changed quite a lot this is more like for testing FeaturizeMechanism,https://github.com/RasaHQ/rasa_core/commit/97980c7277924880e75def490624aa813efc1886,Yes
4492,RasaHQ/rasa_core,tests/test_policies.py,97980c7277924880e75def490624aa813efc1886,TODO new tests due to new system,https://github.com/RasaHQ/rasa_core/commit/97980c7277924880e75def490624aa813efc1886,Yes
4493,RasaHQ/rasa_core,rasa_core/domain.py,bbf0accc7c02f0368fe0d4293c0c02e2dfe5e3e3,TODO then create states only for latest massage,https://github.com/RasaHQ/rasa_core/commit/bbf0accc7c02f0368fe0d4293c0c02e2dfe5e3e3,No
4494,RasaHQ/rasa_core,rasa_core/training/generator.py,2d010d9004e7eaeb0f1dd1ff72154de54e88c2f6,TODO remove permutation; just for test,https://github.com/RasaHQ/rasa_core/commit/2d010d9004e7eaeb0f1dd1ff72154de54e88c2f6,No
4495,RasaHQ/rasa_core,rasa_core/training/generator.py,9bc791425029e389e0957f05179c4d552bb1561a,TODO remove permutation; just for testing,https://github.com/RasaHQ/rasa_core/commit/9bc791425029e389e0957f05179c4d552bb1561a,No
4496,RasaHQ/rasa_core,rasa_core/training/generator.py,fc5b3a49f8c638989db3be44f56df5af1f418e67,TODO remove permutation; just for testing,https://github.com/RasaHQ/rasa_core/commit/fc5b3a49f8c638989db3be44f56df5af1f418e67,No
4497,RasaHQ/rasa_core,tests/test_channels.py,8433baa606856fd7e70731a2ca297afb8733e916,this is needed so that the tests included as code examples look better,https://github.com/RasaHQ/rasa_core/commit/8433baa606856fd7e70731a2ca297afb8733e916,Yes
4498,RasaHQ/rasa_core,tests/test_channels.py,e14a3cae885b8e30a466b2413538a180fc3b82f5,this is needed so that the tests included as code examples look better,https://github.com/RasaHQ/rasa_core/commit/e14a3cae885b8e30a466b2413538a180fc3b82f5,Yes
4499,RasaHQ/rasa_core,tests/test_channels.py,31feb99dc26036e254a3f519622b922d65ed5dfc,this is needed so that the tests included as code examples look better,https://github.com/RasaHQ/rasa_core/commit/31feb99dc26036e254a3f519622b922d65ed5dfc,Yes
4500,RasaHQ/rasa_core,rasa_core/channels/console.py,390792a0008237995a47da2f7b1cfb430d08788b,this builtin is needed so we can overwrite in test,https://github.com/RasaHQ/rasa_core/commit/390792a0008237995a47da2f7b1cfb430d08788b,Yes
4501,RasaHQ/rasa_core,rasa_core/training/dsl.py,8acca07316fb2fe032c6a031dc7f825a13462d50,TODO async make sure this is tested,https://github.com/RasaHQ/rasa_core/commit/8acca07316fb2fe032c6a031dc7f825a13462d50,No
4502,RasaHQ/rasa_core,rasa_core/training/dsl.py,a75820dd72bf9bcc24df894ec87c45a559f0138f,TODO async make sure this is tested,https://github.com/RasaHQ/rasa_core/commit/a75820dd72bf9bcc24df894ec87c45a559f0138f,No
4503,RasaHQ/rasa_core,rasa_core/training/dsl.py,7aad97549452d8c092c39779d113f5665e29eb16,TODO async make sure this is tested,https://github.com/RasaHQ/rasa_core/commit/7aad97549452d8c092c39779d113f5665e29eb16,No
4504,dmlc/dgl,tests/mxnet/test_basics.py,5567f4a4d91687ca6dd869ff01ab8615b4c1d65f,TODO we need to enable the test later.,https://github.com/dmlc/dgl/commit/5567f4a4d91687ca6dd869ff01ab8615b4c1d65f,Yes
4505,dmlc/dgl,tests/mxnet/test_specialization.py,440aecee8f114625a56be81cd4efec86e51d164b,TODO for some reason; this test doesn't pass in MXNet.,https://github.com/dmlc/dgl/commit/440aecee8f114625a56be81cd4efec86e51d164b,Yes
4506,dmlc/dgl,tests/compute/test_heterograph.py,44089c8b4d4db4ca71e816e0de50dca972dbabdb,TODO: rewrite this test case to accept different graphs so we,https://github.com/dmlc/dgl/commit/44089c8b4d4db4ca71e816e0de50dca972dbabdb,Yes
4507,dmlc/dgl,tests/pytorch/test_nn.py,44089c8b4d4db4ca71e816e0de50dca972dbabdb,TODO: add test for blocks,https://github.com/dmlc/dgl/commit/44089c8b4d4db4ca71e816e0de50dca972dbabdb,Yes
4508,dmlc/dgl,tests/compute/test_shared_mem.py,40950629b2b8f639d46a7efea0f40557108fbdcf,TODO: Test calling shared_memory with Blocks (a subclass of HeteroGraph),https://github.com/dmlc/dgl/commit/40950629b2b8f639d46a7efea0f40557108fbdcf,Yes
4509,dmlc/dgl,tests/distributed/test_mp_dataloader.py,4f499c7ffb5f6746c4ccd87b93b0ef09c32cf424,this is needed since there's two test here in one process,https://github.com/dmlc/dgl/commit/4f499c7ffb5f6746c4ccd87b93b0ef09c32cf424,No
4510,dmlc/dgl,tests/distributed/test_mp_dataloader.py,ee30b2aadd49af86696c4bab8b7b15153a55c9c7,this is needed since there's two test here in one process,https://github.com/dmlc/dgl/commit/ee30b2aadd49af86696c4bab8b7b15153a55c9c7,No
4511,dmlc/dgl,tests/distributed/test_mp_dataloader.py,1ad46fd0157639f8d8a8e641f6b4b17e29890b38,this is needed since there's two test here in one process,https://github.com/dmlc/dgl/commit/1ad46fd0157639f8d8a8e641f6b4b17e29890b38,No
4512,dmlc/gluon-cv,scripts/detection/train_ssd.py,5767d1d1dee332faef3057a8416aa48179006edb,"\""\""\"" || Train SSD on Pascal VOC dataset || =============================== ||  || This article walk you through the components GluonVision provided to you || that are very useful to start an object detection project. By going || through this tutorial; we show how stacking the existing modules can || produce a SOTA Single Shot Multibox Detection [1] model. ||  || **Feel free to skip this tutorial because the training script is || self-complete and only requires a single command line to launch.** ||  || Dataset || ------- ||  || We hope you already read this `article <http:\/\/gluon-vision.mxnet.io.s3-website-us-west-2.amazonaws.com\/examples\/datasets\/setup_pascal_voc.html>`__ so Pascal VOC dataset is || well sitting on your disk. If so we are ready to load some training and || validation images. ||  || .. code:: python ||  ||     from gluonvision.data import VOCDetection ||     # typically we use 2007+2012 trainval splits as training data ||     train_dataset = VOCDetection(splits=[(2007; 'trainval'); (2012; 'trainval')]) ||     # use 2007 test as validation ||     val_dataset = VOCDetection(splits=[(2007; 'test')]) ||  ||     print('Training images:'; len(train_dataset)) ||     print('Validation images:'; len(val_dataset)) ||  ||  || .. parsed-literal:: ||  ||     Training images: 16551 ||     Validation images: 4952 ||  ||  || Data Transform || -------------- ||  || We can read a image and label pair from training dataset: ||  || .. code:: python ||  ||     train_image; train_label = train_dataset[0] ||     bboxes = train_label[:; :4] ||     cids = train_label[:; 4:5] ||     print('image:'; train_image.shape) ||     print('bboxes:'; bboxes.shape; 'class ids:'; cids.shape) ||  ||  || .. parsed-literal:: ||  ||     image: (375; 500; 3) ||     bboxes: (5; 4) class ids: (5; 1) ||  ||  || We could illustrate the image; together with the bounding box labels. ||  || .. code:: python ||  ||     from matplotlib import pyplot as plt ||     %matplotlib inline ||     from gluonvision.utils import viz ||  ||     ax = viz.plot_bbox(train_image.asnumpy(); bboxes; labels=cids; class_names=train_dataset.classes) ||     plt.show() ||  ||  ||  || .. image:: https:\/\/github.com\/zhreshold\/gluonvision-tutorials\/blob\/master\/detection\/ssd_train_voc\/output_6_0.png?raw=true ||  ||  || At this point; validation images are quite similar. ||  || .. code:: python ||  ||     val_image; val_label = val_dataset[0] ||     bboxes = val_label[:; :4] ||     cids = val_label[:; 4:5] ||     ax = viz.plot_bbox(val_image.asnumpy(); bboxes; labels=cids; class_names=train_dataset.classes) ||     plt.show() ||  ||  ||  || .. image:: https:\/\/github.com\/zhreshold\/gluonvision-tutorials\/blob\/master\/detection\/ssd_train_voc\/output_8_0.png?raw=true ||  ||  || For SSD networks; it is critical to apply data augmentation (see || explanations in paper [1]). We provide tons of image and bounding box || transform functions to supply that. It is very convenient to use as || well. ||  || .. code:: python ||  ||     from gluonvision.data.transforms import presets ||     from gluonvision import utils ||     from mxnet import nd ||  ||     width; height = 512; 512  # suppose we use 512 as base training size ||     train_transform = presets.ssd.SSDDefaultTrainTransform(width; height) ||     val_transform = presets.ssd.SSDDefaultValTransform(width; height) ||  ||     utils.random.seed(233)  # fix seed in this tutorial ||  ||     # apply transforms to train image ||     train_image2; train_label2 = train_transform(train_image; train_label) ||     print('tensor shape:'; train_image2.shape) ||  ||  || .. parsed-literal:: ||  ||     tensor shape: (3; 512; 512) ||  ||  || Images directly from tensor is distorted because they no longer sit in || (0; 255) range. Let's convert it back so we can see it clearly. ||  || .. code:: python ||  ||     train_image2 = train_image2.transpose((1; 2; 0)) * nd.array((0.229; 0.224; 0.225)) + nd.array((0.485; 0.456; 0.406)) ||     train_image2 = (train_image2 * 255).clip(0; 255) ||     ax = viz.plot_bbox(train_image2.asnumpy(); train_label2[:; :4]; ||                        labels=train_label2[:; 4:5]; class_names=train_dataset.classes) ||     plt.show() ||  ||     # apply transforms to validation image ||     val_image2; val_label2 = val_transform(val_image; val_label) ||     val_image2 = val_image2.transpose((1; 2; 0)) * nd.array((0.229; 0.224; 0.225)) + nd.array((0.485; 0.456; 0.406)) ||     val_image2 = (val_image2 * 255).clip(0; 255) ||     ax = viz.plot_bbox(val_image2.clip(0; 255).asnumpy(); val_label2[:; :4]; ||                        labels=val_label2[:; 4:5]; class_names=train_dataset.classes) ||     plt.show() ||  ||  ||  || .. image:: https:\/\/github.com\/zhreshold\/gluonvision-tutorials\/blob\/master\/detection\/ssd_train_voc\/output_12_0.png?raw=true ||  ||  ||  || .. image:: https:\/\/github.com\/zhreshold\/gluonvision-tutorials\/blob\/master\/detection\/ssd_train_voc\/output_12_1.png?raw=true ||  ||  || Transforms used in training include random expanding; random cropping; || color distortion; random flipping; etc. In comparison; validation || transforms are conservative; where only resizing and color normalization || is used. ||  || DataLoader || ---------- ||  || We want iterate through the entire dataset many times during training. || Keep in mind that raw images have to be transformed into tensors(mxnet || use BCHW format) before they are fed into neural networks. Besides; to || be able to run in mini-batches; images must be resized to same shape. ||  || A handy DataLoader would be very convenient for us to apply different || transforms and aggregate data into mini-batches. ||  || Because number of objects varys a lot in different images; we have || fluctuating label sizes. As a result; we need to pad those labels to the || same size. In response; we have DetectionDataLoader ready for you which || handles it automatically. ||  || .. code:: python ||  ||     from gluonvision.data import DetectionDataLoader ||  ||     batch_size = 4  # for tutorial; we use smaller batch-size ||     num_workers = 4  # multi processing worker to accelerate data processing ||  ||     train_loader = DetectionDataLoader(train_dataset.transform(train_transform); batch_size; shuffle=True; ||                                        last_batch='rollover'; num_workers=num_workers) ||     val_loader = DetectionDataLoader(val_dataset.transform(val_transform); batch_size; shuffle=False; ||                                      last_batch='keep'; num_workers=num_workers) ||  ||     for ib; batch in enumerate(train_loader): ||         if ib > 5: ||             break ||         print('data:'; batch[0].shape; 'label:'; batch[1].shape) ||  ||  || .. parsed-literal:: ||  ||     data: (4; 3; 512; 512) label: (4; 1; 6) ||     data: (4; 3; 512; 512) label: (4; 1; 6) ||     data: (4; 3; 512; 512) label: (4; 1; 6) ||     data: (4; 3; 512; 512) label: (4; 6; 6) ||     data: (4; 3; 512; 512) label: (4; 2; 6) ||     data: (4; 3; 512; 512) label: (4; 5; 6) ||  ||  || SSD network || ----------- ||  || SSD network is a composite Gluon HybridBlock(which means it can be || exported to symbol to run in C++; Scala and other language bindings; but || we will cover it future tutorials). In terms of structure; SSD networks || are composed of feature extraction base network; anchor generators; || class predictors and bounding box offsets predictors. If you have read || our introductory || `tutorial <http:\/\/gluon.mxnet.io\/chapter08_computer-vision\/object-detection.html>`__ || of SSD; you may have better idea how it works. You can also refer to || original paper and entry level tutorials for idea that support SSD. ||  || GluonVision has a model zoo which has a lot of built-in SSD networks. || Therefore you can simply load them from model\\_zoo module like this: ||  || .. code:: python ||  ||     from gluonvision import model_zoo ||     net = model_zoo.get_model('ssd_300_vgg16_atrous_voc'; pretrained_base=False) ||     print(net) ||  ||  || .. parsed-literal:: ||  ||     SSD( ||       (class_predictors): HybridSequential( ||         (0): ConvPredictor( ||           (predictor): Conv2D(None -> 84; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (1): ConvPredictor( ||           (predictor): Conv2D(None -> 126; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (2): ConvPredictor( ||           (predictor): Conv2D(None -> 126; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (3): ConvPredictor( ||           (predictor): Conv2D(None -> 126; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (4): ConvPredictor( ||           (predictor): Conv2D(None -> 84; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (5): ConvPredictor( ||           (predictor): Conv2D(None -> 84; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||       ) ||       (box_predictors): HybridSequential( ||         (0): ConvPredictor( ||           (predictor): Conv2D(None -> 16; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (1): ConvPredictor( ||           (predictor): Conv2D(None -> 24; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (2): ConvPredictor( ||           (predictor): Conv2D(None -> 24; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (3): ConvPredictor( ||           (predictor): Conv2D(None -> 24; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (4): ConvPredictor( ||           (predictor): Conv2D(None -> 16; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||         (5): ConvPredictor( ||           (predictor): Conv2D(None -> 16; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||         ) ||       ) ||       (cls_decoder): MultiPerClassDecoder( ||  ||       ) ||       (features): VGGAtrousExtractor( ||         (norm4): Normalize( ||  ||         ) ||         (extras): HybridSequential( ||           (0): HybridSequential( ||             (0): Conv2D(None -> 256; kernel_size=(1; 1); stride=(1; 1)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 512; kernel_size=(3; 3); stride=(2; 2); padding=(1; 1)) ||             (3): Activation(relu) ||           ) ||           (1): HybridSequential( ||             (0): Conv2D(None -> 128; kernel_size=(1; 1); stride=(1; 1)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 256; kernel_size=(3; 3); stride=(2; 2); padding=(1; 1)) ||             (3): Activation(relu) ||           ) ||           (2): HybridSequential( ||             (0): Conv2D(None -> 128; kernel_size=(1; 1); stride=(1; 1)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 256; kernel_size=(3; 3); stride=(1; 1)) ||             (3): Activation(relu) ||           ) ||           (3): HybridSequential( ||             (0): Conv2D(None -> 128; kernel_size=(1; 1); stride=(1; 1)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 256; kernel_size=(3; 3); stride=(1; 1)) ||             (3): Activation(relu) ||           ) ||         ) ||         (stages): HybridSequential( ||           (0): HybridSequential( ||             (0): Conv2D(None -> 64; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 64; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (3): Activation(relu) ||           ) ||           (1): HybridSequential( ||             (0): Conv2D(None -> 128; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 128; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (3): Activation(relu) ||           ) ||           (2): HybridSequential( ||             (0): Conv2D(None -> 256; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 256; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (3): Activation(relu) ||             (4): Conv2D(None -> 256; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (5): Activation(relu) ||           ) ||           (3): HybridSequential( ||             (0): Conv2D(None -> 512; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 512; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (3): Activation(relu) ||             (4): Conv2D(None -> 512; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (5): Activation(relu) ||           ) ||           (4): HybridSequential( ||             (0): Conv2D(None -> 512; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 512; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (3): Activation(relu) ||             (4): Conv2D(None -> 512; kernel_size=(3; 3); stride=(1; 1); padding=(1; 1)) ||             (5): Activation(relu) ||           ) ||           (5): HybridSequential( ||             (0): Conv2D(None -> 1024; kernel_size=(3; 3); stride=(1; 1); padding=(6; 6); dilation=(6; 6)) ||             (1): Activation(relu) ||             (2): Conv2D(None -> 1024; kernel_size=(1; 1); stride=(1; 1)) ||             (3): Activation(relu) ||           ) ||         ) ||       ) ||       (anchor_generators): HybridSequential( ||         (0): SSDAnchorGenerator( ||  ||         ) ||         (1): SSDAnchorGenerator( ||  ||         ) ||         (2): SSDAnchorGenerator( ||  ||         ) ||         (3): SSDAnchorGenerator( ||  ||         ) ||         (4): SSDAnchorGenerator( ||  ||         ) ||         (5): SSDAnchorGenerator( ||  ||         ) ||       ) ||       (bbox_decoder): NormalizedBoxCenterDecoder( ||  ||       ) ||     ) ||  ||  || SSD network is a HybridBlock as mentioned before. So you can call it || with an input as simple as: ||  || .. code:: python ||  ||     import mxnet as mx ||     x = mx.nd.zeros(shape=(1; 3; 300; 300)) ||     net.initialize() ||     cids; scores; bboxes = net(x) ||  || where ``cids`` is the class labels; ``scores`` are confidences of each || predictions; ``bboxes`` are corresponding bounding boxes' absolute || coordinates. ||  || Training targets || ---------------- ||  || Unlike a single ``SoftmaxCrossEntropyLoss`` used in image || classification; the losses used in SSD is more complicated. Don't worry || though; because we have these modules available out of box. ||  || Checkout the ``target_generator`` in SSD networks. ||  || .. code:: python ||  ||     print(net.target_generator) ||  ||  || .. parsed-literal:: ||  ||     SSDTargetGenerator( ||       (_sampler): OHEMSampler( ||  ||       ) ||       (_center_to_corner): BBoxCenterToCorner( ||  ||       ) ||       (_box_encoder): NormalizedBoxCenterEncoder( ||         (corner_to_center): BBoxCornerToCenter( ||  ||         ) ||       ) ||       (_cls_encoder): MultiClassEncoder( ||  ||       ) ||       (_matcher): CompositeMatcher( ||  ||       ) ||     ) ||  ||  || You can see there are bounding boxes encoder which transfers raw || coordinates to bbox prediction targets; a class encoder which generates || class labels for each anchor box. Matcher and samplers included are used || to apply various advanced strategies described in paper. ||  || References || ---------- ||  || [1] Wei Liu; Dragomir Anguelov; Dumitru Erhan; Christian Szegedy; Scott || Reed; Cheng-Yang Fu; Alexander C. Berg. SSD: Single Shot MultiBox || Detector. ECCV 2016. ||  || Dive deep into training script || ------------------------------ ||  || We include an training script for reproducing SOTA models. ||  || Example: ||  || :: ||  ||     python train_ssd.py --network vgg16_atrous --data-shape 300 --batch-size 32 --gpus 0;1;2;3 -j 32 --log-interval 20 ||  || \""\""\""",https://github.com/dmlc/gluon-cv/commit/5767d1d1dee332faef3057a8416aa48179006edb,No
4513,dmlc/gluon-cv,scripts/classification/cifar/demo.py,e7389b15ea6509268a5c54d239992df43603d50b,"\""\""\""Training Your First Classification Model on CIFAR10 || =================================================== ||  || ```CIFAR10`` <https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html>`__ is a || labeled dataset of tiny (32x32) images; collected by Alex Krizhevsky; || Vinod Nair; and Geoffrey Hinton. It is widely used as a benchmark in || conputer vision research. ||  || In this tutorial; we will demonstrate how to use ``Gluon`` to train a || model from scratch and reproduce the performance from papers. || Specifically; we offer a script to prepare the ``CIFAR10`` dataset and || train a ``ResNet`` model at || `scripts\/classification\/cifar\/train.py <https:\/\/github.com\/dmlc\/gluon-vision\/blob\/master\/scripts\/classification\/cifar\/train.py>`__. ||  || In the following content; we will demonstrate how to ||  || -  How well can our model predict || -  train a model || -  plot the training history ||  || Demo and Benchmark || ------------------ ||  || Before busying with training and parameter tuning; you may want to get || an idea of what the result may look like. ||  || Here we provide you a script; || ```demo.py`` <https:\/\/github.com\/hetong007\/gluon-vision\/blob\/master\/scripts\/classification\/cifar\/demo.py>`__; || to load a pre-trained model from us and predict on any image on your || disk. ||  || This is an airplane: ||  || |image0| ||  || We can make prediction by ||  || :: ||  ||     python demo.py --model cifar_resnet110_v1 --input-pic bird.jpg ||  || And the model thinks that ||  || :: ||  ||     The input picture is classified to be [airplane]; with probability 0.517. ||  || Feel free to feed in your own image to see how well it does the job. || Keep in mind; that ``CIFAR10`` is relatively small and has only 10 || classes. Models trained on ``CIFAR10`` only recognize objects from those || 10 classes; therefore it may surprise you: ||  || :: ||  ||     python demo.py --model cifar_resnet110_v1 --input-pic surprise.jpg ||  || The result is: ||  || :: ||  ||     bird! ||  || To experience a more real world friendly demo; please checkout models || trained on `ImageNet <>`__. ||  || Train Your First Model || ---------------------- ||  || In the demo; we have used a pretrained model. So how did we train it? ||  || We trained the models with || ```train.py`` <https:\/\/github.com\/hetong007\/gluon-vision\/blob\/master\/scripts\/classification\/cifar\/train.py>`__. || It takes a lot of parameters to control the model training process. To || start; you can try the following command: ||  || :: ||  ||     python train.py --num-epochs 240 --mode hybrid --num-gpus 2 -j 32 --batch-size 64\\ ||         --wd 0.0001 --lr 0.1 --lr-decay 0.1 --lr-decay-epoch 80;160 --model cifar_resnet20_v2 ||  || This command trains a ``ResNet20_V2`` model for 240 epochs on two GPUs. || The batch size for each GPU is 64; thus the total batch size is 128. We || decay the learning rate by a factor of 10 at the 80-th and 160-th epoch. || The script prints information for each epoch so that we can have a sense || of the progress and watchout for any unexpected issues. ||  || :: ||  ||     INFO:root:[Epoch 0] train=0.229176 val=0.422300 loss=101760.375931 time: 21.937484 ||     INFO:root:[Epoch 1] train=0.218320 val=0.524200 loss=93098.808853 time: 21.637539 ||     INFO:root:[Epoch 2] train=0.211201 val=0.559400 loss=89708.618820 time: 21.596765 ||     INFO:root:[Epoch 3] train=0.205876 val=0.609300 loss=87576.185600 time: 20.972680 ||     INFO:root:[Epoch 4] train=0.202815 val=0.614800 loss=85852.031380 time: 21.104631 ||     INFO:root:[Epoch 5] train=0.200063 val=0.659800 loss=83747.976288 time: 21.788607 ||     ... ||  || The dataset and the model are relatively small; thus it won\u2019t take you || too long to train the model. Of course it depends on how powerful your || machine is; the result on our side is: ||  || -  20 seconds with two V100 GPUs per epoch; and 32 CPU threads. || -  100 seconds with a GTX1060 GPU per epoch; and 4 CPU threads. ||  || With limited computational power; it is good in practice to firstly test || a few epochs to ensure everything works; then leave it running for a || night; and wake up to see the result :) ||  || After the training; the accuracy is expect to be around 91%. To get a || better accuracy; we can train a ``ResNet110_V2`` model instead by || ``--model cifar_resnet110_v2``; at the cost of around 4 times of the || training time. With ``ResNet110_V2``; we expect the accuracy to be || around 94%. ||  || Don\u2019t Overfit || ------------- ||  || The training of a deep learning model is usually a trial-and-error || process. A good way to review the result is to have a plot: ||  || This is a plot generated from the following command: ||  || :: ||  || We see that the issue could be not enough epochs. We then change to ||  || :: ||  || and observe that ||  || Model Zoo || --------- ||  || We train various models and store them on cloud as a \u201Czoo of the || models\u201D. Users can pick the model with regards to the accuracy and model || complexity. ||  || Here\u2019s what we have for ``CIFAR10`` so far: ||  || +---------------------------+----------+ || | Model                     | Accuracy | || +===========================+==========+ || | ``CIFAR_ResNet20_v1``     | 0.9160   | || +---------------------------+----------+ || | ``CIFAR_ResNet56_v1``     | 0.9387   | || +---------------------------+----------+ || | ``CIFAR_ResNet110_v1``    | 0.9471   | || +---------------------------+----------+ || | ``CIFAR_ResNet20_v2``     | 0.9130   | || +---------------------------+----------+ || | ``CIFAR_ResNet56_v2``     | 0.9413   | || +---------------------------+----------+ || | ``CIFAR_ResNet110_v2``    | 0.9464   | || +---------------------------+----------+ || | ``CIFAR_WideResNet16_10`` | 0.9614   | || +---------------------------+----------+ || | ``CIFAR_WideResNet28_10`` | 0.9667   | || +---------------------------+----------+ || | ``CIFAR_WideResNet40_8``  | 0.9673   | || +---------------------------+----------+ ||  || Most of them are more accurate than the claims in the original papers. || The reason is that we incorporate a technique called || ```Mix-Up`` <https:\/\/arxiv.org\/abs\/1710.09412>`__ to improve the || performance without changing the network structure. ||  || Specifically; we train the ``cifar_resnet`` models with: ||  || :: ||  ||     python train_mixup.py --num-epochs 450 --mode hybrid --num-gpus 2 -j 32\\ ||         --batch-size 64 --wd 0.0001 --lr 0.1 --lr-decay 0.1 --lr-decay-epoch 150;250\\ ||         --model cifar_resnet20_v1 ||  || and the ``cifar_wideresnet`` models with: ||  || :: ||  ||     python train_mixup.py --num-epochs 500 --mode hybrid --num-gpus 2 -j 32\\ ||         --batch-size 64 --wd 0.0001 --lr 0.1 --lr-decay 0.1 --lr-decay-epoch 100;200;300\\ ||         --model cifar_wideresnet16_10 ||  || Next Step || --------- ||  || Congratulations! You\u2019ve just finished reading our first tutorial. We || have a lot more others to help you learn more and get familiar with || ``gluonvision``. ||  || If you would like to dig deeper in the topic of ``CIFAR10`` training; || feel free to read `the next tutorial on ``CIFAR10`` <>`__. ||  || Or; if you would like to try a more powerful demo; i.e. models trained || on ImageNet; please read `xxx <>`__. ||  || .. |image0| image:: plane-draw.jpeg ||  || \""\""\""",https://github.com/dmlc/gluon-cv/commit/e7389b15ea6509268a5c54d239992df43603d50b,Yes
4514,dmlc/gluon-cv,scripts/classification/cifar/train.py,e7389b15ea6509268a5c54d239992df43603d50b,"\""\""\""Dive Deep Into CIFAR10 || ====================== ||  || Hope you enjoyed playing with our demo script. One question; as may || naturally arise in your mind: how exactly did we train the model? ||  || In this tutorial; we will focus on answering this question. || Specifically; the following discusses ||  || -  Model Structure || -  Data Augmentation and Data Loader || -  Optimizer; Loss and Metric || -  Validation || -  Training || -  Model save and load ||  || Prerequisites || ------------- ||  || We assume readers have a basic understanding of ``Gluon``. If you would || like to know more about it; we suggest `Crash || Course <http:\/\/gluon-crash-course.mxnet.io\/index.html>`__ as a good || place to start. ||  || As we all know; deep learning training is way faster on GPU than on CPU. || In our demo; we only used CPU by default since that script only performs || minimum conputation. However; since we are about to train a model; it is || strongly recommended to have a platform with GPU(s). ||  || Training usually takes several hours. While you are reading the || tutorial; it is a good idea to start the training script with ||  || :: ||  ||     python train.py --num-epochs 240 --mode hybrid --num-gpus 2 -j 32 --batch-size 64\\ ||         --wd 0.0001 --lr 0.1 --lr-decay 0.1 --lr-decay-epoch 80;160 --model cifar_resnet20_v1 ||  || and remember to replace ``--num-gpus`` to the number of GPUs you have; || and ``-j`` to a number not larger than your CPU threads. ||  || Let's load the necessary libraries first. ||  || .. code:: ipython2 ||  ||     from __future__ import division ||  ||     import matplotlib ||     matplotlib.use('Agg') ||  ||     import argparse; time; logging; random; math ||  ||     import numpy as np ||     import mxnet as mx ||  ||     from mxnet import gluon; nd ||     from mxnet import autograd as ag ||     from mxnet.gluon import nn ||     from mxnet.gluon.data.vision import transforms ||  ||     from gluonvision.model_zoo import get_model ||     from gluonvision.utils import makedirs; TrainingHistory ||  || Network Structure || ----------------- ||  || There are numerous structures for convolutional neural networks. The || structure mainly affects ||  || -  The upperbound of the accuracy. || -  The cost of resources; in terms of training time and memory. ||  || Here we pick a simple yet good structure; ``cifar_resnet20_v1``; for the || tutorial. ||  || .. code:: ipython2 ||  ||     # GPUs to use ||     ctx = [mx.gpu(0); mx.gpu(1)] ||  ||     # Get the model CIFAR_ResNet20_v1; with 10 output classes ||     net = get_model('cifar_resnet20_v1'; classes=10) ||     net.initialize(mx.init.Xavier(); ctx = ctx) ||  || Data Augmentation and Data Loader || --------------------------------- ||  || Data augmentation is a common technique used in model training. It is || proposed base on this assumption: given an object; photos with different || composition; lighting condition; or different color may still be || classified as the same object. ||  || Here are photos taken by different people; at different time. We can all || tell that they are the photo for the same thing. ||  || .. figure:: ||    :alt: ||  || We would like to teach the model to learn about it; by playing \""tricks\"" || on the input picture. The trick is to transform the picture with || resizing; cropping and flipping before sending to the model. ||  || In ``Gluon``; we can compose our transform function as following: ||  || .. code:: ipython2 ||  ||     transform_train = transforms.Compose([ ||         # Resize the short edge of the input to 32 pixels ||         transforms.Resize(32); ||         # Randomly crop an area; and then resize it to be 32x32 ||         transforms.RandomResizedCrop(32); ||         # Randomly flip the picture horizontally ||         transforms.RandomFlipLeftRight(); ||         # Randomly manipulate the brightness; contrast and saturation of the picture ||         transforms.RandomColorJitter(brightness=0.1; contrast=0.1; saturation=0.1); ||         # Randomly adding noise to the picture ||         transforms.RandomLighting(0.1); ||         # Transpose the data from Height*Width*Channel to Channel*Height*Width ||         # and map values from [0; 255] to [0;1] ||         transforms.ToTensor(); ||         # Normalize the image ||         transforms.Normalize([0.4914; 0.4822; 0.4465]; [0.2023; 0.1994; 0.2010]) ||     ]) ||  || You may notice that most of the operations are at random. This largely || increase the number of pictures the model can see during the training || process. The more data we have; the better our model can generalize on || unseen pictures. ||  || On the other hand; when making prediction; we would like to remove all || random operations because we want a deterministic result. The transform || function for prediction is: ||  || .. code:: ipython2 ||  ||     transform_test = transforms.Compose([ ||         transforms.Resize(32); ||         transforms.ToTensor(); ||         transforms.Normalize([0.4914; 0.4822; 0.4465]; [0.2023; 0.1994; 0.2010]) ||     ]) ||  || Notice that it is important to keep the normalization step; since the || model only works well on input with the same distribution. ||  || With the transform functions; we can define data loaders for our || training and validation datasets. ||  || .. code:: ipython2 ||  ||     # Batch Size for Each GPU ||     per_device_batch_size = 64 ||     # Number of data loader workers ||     num_workers = 32 ||     # Calculate effective total batch size ||     batch_size = per_device_batch_size * 2 ||  ||     # Set train=True for training data ||     # Set shuffle=True to shuffle the training data ||     train_data = gluon.data.DataLoader( ||         gluon.data.vision.CIFAR10(train=True).transform_first(transform_train); ||         batch_size=batch_size; shuffle=True; last_batch='discard'; num_workers=num_workers) ||  ||     # Set train=False for validation data ||     val_data = gluon.data.DataLoader( ||         gluon.data.vision.CIFAR10(train=False).transform_first(transform_test); ||         batch_size=batch_size; shuffle=False; num_workers=num_workers) ||  ||  || Now the data is ready. Let's move on to the optimizer. ||  || Optimizer; Loss and Metric || -------------------------- ||  || Optimizer is what improves the model during training. We use the popular || Nesterov accelerated gradient descent algorithm. ||  || .. code:: ipython2 ||  ||     # Learning rate decay factor ||     lr_decay = 0.1 ||     # Epochs where learning rate decays ||     lr_decay_epoch = [80; 160; np.inf] ||  ||     # Nesterov accelerated gradient descent ||     optimizer = 'nag' ||     # Set parameters ||     optimizer_params = {'learning_rate': 0.1; 'wd': 0.0001; 'momentum': 0.9} ||  ||     # Define our trainer for net ||     trainer = gluon.Trainer(net.collect_params(); optimizer; optimizer_params) ||  || In the above code; ``lr_decay`` and ``lr_decay_epoch`` are not directly || used in ``trainer``. One important idea in model training is to decrease || the learning rate in a later stage. This means the model takes larger || steps at the beginning; and smaller steps after a while. ||  || Our plan is to have the learning rate as 0.1 at the beginning; then || divide it by 10 at the 80-th epoch; then again at the 160-th epoch. || Later we'll show how to implement it. ||  || In order to let the optimizer work; we need a loss function. In plain || words; the loss function measures how good our model performs; and pass || the \""difference\"" to the model. For the Nesterov algorithm we are using; || the difference is the gradient of the loss function. With the || difference; the optimizer knows towards which direction to improve the || model parameters. ||  || For classification tasks; we usually use softmax cross entropy as the || loss function. ||  || .. code:: ipython2 ||  ||     loss_fn = gluon.loss.SoftmaxCrossEntropyLoss() ||  || Metric is somehow similar to loss function; but they are essentially || different. ||  || -  Metric is how we evaluate the model performance. It is related to the ||    specific task; but independent from the model training process. || -  For classification; we usually only use one loss function to train ||    our model; but we can have multiplt metrics to evaluate the ||    performance. || -  Loss function can be used as a metric; but sometimes it is hard to ||    interpretate its value. For instance; the concept \""accuracy\"" is ||    easier to understand than \""softmax cross entropy\"" ||  || For simplicity; we use accuracy as the metric to monitor our training || process. Besides; we record the metric values; and will print it in the || end of the training. ||  || .. code:: ipython2 ||  ||     train_metric = mx.metric.Accuracy() ||     train_history = TrainingHistory(['training-error'; 'validation-error']) ||  || Validation || ---------- ||  || The existance of the validation dataset provides us a way to monitor the || training process. We have the labels on validation data; but just don't || use it to train. Therefore we can predict on the validation with the || model; and evaluate the performance at anytime. ||  || .. code:: ipython2 ||  ||     def test(ctx; val_data): ||         metric = mx.metric.Accuracy() ||         for i; batch in enumerate(val_data): ||             data = gluon.utils.split_and_load(batch[0]; ctx_list=ctx; batch_axis=0) ||             label = gluon.utils.split_and_load(batch[1]; ctx_list=ctx; batch_axis=0) ||             outputs = [net(X) for X in data] ||             metric.update(label; outputs) ||         return metric.get() ||  || In order to evaluate the performance; we need a metric. Then we loop || through the validation data and predict with our model. We'll plug it || into the end of each training epoch to show the improvement. ||  || Training || -------- ||  || After all these preparation; we can finally start our training process! || Following is the script. ||  || Notice: in order to speed up the training process; we only train the || model for 5 epochs. ||  || .. code:: ipython2 ||  ||     epochs = 5 ||     lr_decay_count = 0 ||  ||     for epoch in range(epochs): ||         tic = time.time() ||         train_metric.reset() ||         train_loss = 0 ||  ||         # Learning rate decay ||         if epoch == lr_decay_epoch[lr_decay_count]: ||             trainer.set_learning_rate(trainer.learning_rate*lr_decay) ||             lr_decay_count += 1 ||  ||         # Loop through each batch of training data ||         for i; batch in enumerate(train_data): ||             # Extract data and label ||             data = gluon.utils.split_and_load(batch[0]; ctx_list=ctx; batch_axis=0) ||             label = gluon.utils.split_and_load(batch[1]; ctx_list=ctx; batch_axis=0) ||  ||             # AutoGrad ||             with ag.record(): ||                 output = [net(X) for X in data] ||                 loss = [loss_fn(yhat; y) for yhat; y in zip(output; label)] ||  ||             # Backpropagation ||             for l in loss: ||                 l.backward() ||  ||             # Optimize ||             trainer.step(batch_size) ||  ||             # Update metrics ||             train_loss += sum([l.sum().asscalar() for l in loss]) ||             train_metric.update(label; output) ||  ||         name; acc = train_metric.get() ||         # Evaluate on Validation data ||         name; val_acc = test(ctx; val_data) ||  ||         # Update history and print metrics ||         train_history.update({'training-error': 1-acc; 'validation-error': 1-val_acc}) ||         print('[Epoch %d] train=%f val=%f loss=%f time: %f' % ||             (epoch; acc; val_acc; train_loss; time.time()-tic)) ||  ||  || .. parsed-literal:: ||  ||     [Epoch 0] train=0.362941 val=0.426600 loss=86895.413788 time: 25.992320 ||     [Epoch 1] train=0.496014 val=0.584200 loss=69834.481152 time: 25.585870 ||     [Epoch 2] train=0.562099 val=0.646200 loss=61467.327183 time: 25.127841 ||     [Epoch 3] train=0.599479 val=0.658300 loss=56717.487179 time: 25.180641 ||     [Epoch 4] train=0.624119 val=0.670500 loss=52825.070946 time: 25.824175 ||  ||  || We can plot the metric scores with: ||  || .. code:: ipython2 ||  ||     train_history.plot(items=['training-error'; 'validation-error']) ||  || This is just a plot for 5 epochs. Instead; if you change to || ``epochs=240``; the plot may look like: ||  || Model Save and Load || ------------------- ||  || Since the model is here; we may want to save it for later use; for || example; to predict the class from an arbitrary picture. ||  || It's simple! We can do it by: ||  || .. code:: ipython2 ||  ||     net.save_params('dive_deep_cifar10_resnet20_v2.params') ||  || Next time if you need to use it; just ||  || .. code:: ipython2 ||  ||     net.load_params('dive_deep_cifar10_resnet20_v2.params'; ctx=ctx) ||  || Next Step || --------- ||  || This is the end of our adventure with ``CIFAR10``; but there are still a || lot more we can do! ||  || Following is a script extending our tutorial with command line arguments. || Please train a model with your own configurations. ||  || If you would like to know how to train a model on a much larger dataset || than ``CIFAR10``; e.g. ImageNet; please read `xxx <>`__. ||  || Or; if you want like to know what can be done with the model you just || trained; please read `finetune <>`__. ||  || \""\""\""",https://github.com/dmlc/gluon-cv/commit/e7389b15ea6509268a5c54d239992df43603d50b,No
4515,dmlc/gluon-cv,scripts/classification/imagenet/demo.py,e7389b15ea6509268a5c54d239992df43603d50b,"\""\""\""Training Your First Classification Model on ImageNet || =================================================== ||  || ```ImageNet`` <http:\/\/www.image-net.org\/>`__ is a || large labeled dataset of real-world images. It is the most || well-known dataset for computer vision tasks. ||  || In this tutorial; we will demonstrate how to use ``Gluon`` to train a || model from scratch and reproduce the performance from papers. || Specifically; we offer a script to prepare the ``CIFAR10`` dataset and || train a ``ResNet`` model at || `scripts\/classification\/cifar\/train.py <https:\/\/github.com\/dmlc\/gluon-vision\/blob\/master\/scripts\/classification\/cifar\/train.py>`__. ||  || In the following content; we will demonstrate how to ||  || -  How well can our model predict || -  train a model || -  plot the training history ||  || Demo and Benchmark || ------------------ ||  || Before busying with training and parameter tuning; you may want to get || an idea of what the result may look like. ||  || Here we provide you a script; || ```demo.py`` <https:\/\/github.com\/hetong007\/gluon-vision\/blob\/master\/scripts\/classification\/cifar\/demo.py>`__; || to load a pre-trained model from us and predict on any image on your || disk. ||  || This is an airplane: ||  || |image0| ||  || We can make prediction by ||  || :: ||  ||     python demo.py --model cifar_resnet110_v1 --input-pic bird.jpg ||  || And the model thinks that ||  || :: ||  ||     The input picture is classified to be [airplane]; with probability 0.517. ||  || Feel free to feed in your own image to see how well it does the job. || Keep in mind; that ``CIFAR10`` is relatively small and has only 10 || classes. Models trained on ``CIFAR10`` only recognize objects from those || 10 classes; therefore it may surprise you: ||  || :: ||  ||     python demo.py --model cifar_resnet110_v1 --input-pic surprise.jpg ||  || The result is: ||  || :: ||  ||     bird! ||  || To experience a more real world friendly demo; please checkout models || trained on `ImageNet <>`__. ||  || Train Your First Model || ---------------------- ||  || In the demo; we have used a pretrained model. So how did we train it? ||  || We trained the models with || ```train.py`` <https:\/\/github.com\/hetong007\/gluon-vision\/blob\/master\/scripts\/classification\/cifar\/train.py>`__. || It takes a lot of parameters to control the model training process. To || start; you can try the following command: ||  || :: ||  ||     python train.py --num-epochs 240 --mode hybrid --num-gpus 2 -j 32 --batch-size 64\\ ||         --wd 0.0001 --lr 0.1 --lr-decay 0.1 --lr-decay-epoch 80;160 --model cifar_resnet20_v2 ||  || This command trains a ``ResNet20_V2`` model for 240 epochs on two GPUs. || The batch size for each GPU is 64; thus the total batch size is 128. We || decay the learning rate by a factor of 10 at the 80-th and 160-th epoch. || The script prints information for each epoch so that we can have a sense || of the progress and watchout for any unexpected issues. ||  || :: ||  ||     INFO:root:[Epoch 0] train=0.229176 val=0.422300 loss=101760.375931 time: 21.937484 ||     INFO:root:[Epoch 1] train=0.218320 val=0.524200 loss=93098.808853 time: 21.637539 ||     INFO:root:[Epoch 2] train=0.211201 val=0.559400 loss=89708.618820 time: 21.596765 ||     INFO:root:[Epoch 3] train=0.205876 val=0.609300 loss=87576.185600 time: 20.972680 ||     INFO:root:[Epoch 4] train=0.202815 val=0.614800 loss=85852.031380 time: 21.104631 ||     INFO:root:[Epoch 5] train=0.200063 val=0.659800 loss=83747.976288 time: 21.788607 ||     ... ||  || The dataset and the model are relatively small; thus it won\u2019t take you || too long to train the model. Of course it depends on how powerful your || machine is; the result on our side is: ||  || -  20 seconds with two V100 GPUs per epoch; and 32 CPU threads. || -  100 seconds with a GTX1060 GPU per epoch; and 4 CPU threads. ||  || With limited computational power; it is good in practice to firstly test || a few epochs to ensure everything works; then leave it running for a || night; and wake up to see the result :) ||  || After the training; the accuracy is expect to be around 91%. To get a || better accuracy; we can train a ``ResNet110_V2`` model instead by || ``--model cifar_resnet110_v2``; at the cost of around 4 times of the || training time. With ``ResNet110_V2``; we expect the accuracy to be || around 94%. ||  || Don\u2019t Overfit || ------------- ||  || The training of a deep learning model is usually a trial-and-error || process. A good way to review the result is to have a plot: ||  || This is a plot generated from the following command: ||  || :: ||  || We see that the issue could be not enough epochs. We then change to ||  || :: ||  || and observe that ||  || Model Zoo || --------- ||  || We train various models and store them on cloud as a \u201Czoo of the || models\u201D. Users can pick the model with regards to the accuracy and model || complexity. ||  || Here\u2019s what we have for ``CIFAR10`` so far: ||  || +---------------------------+----------+ || | Model                     | Accuracy | || +===========================+==========+ || | ``CIFAR_ResNet20_v1``     | 0.9160   | || +---------------------------+----------+ || | ``CIFAR_ResNet56_v1``     | 0.9387   | || +---------------------------+----------+ || | ``CIFAR_ResNet110_v1``    | 0.9471   | || +---------------------------+----------+ || | ``CIFAR_ResNet20_v2``     | 0.9130   | || +---------------------------+----------+ || | ``CIFAR_ResNet56_v2``     | 0.9413   | || +---------------------------+----------+ || | ``CIFAR_ResNet110_v2``    | 0.9464   | || +---------------------------+----------+ || | ``CIFAR_WideResNet16_10`` | 0.9614   | || +---------------------------+----------+ || | ``CIFAR_WideResNet28_10`` | 0.9667   | || +---------------------------+----------+ || | ``CIFAR_WideResNet40_8``  | 0.9673   | || +---------------------------+----------+ ||  || Most of them are more accurate than the claims in the original papers. || The reason is that we incorporate a technique called || ```Mix-Up`` <https:\/\/arxiv.org\/abs\/1710.09412>`__ to improve the || performance without changing the network structure. ||  || Specifically; we train the ``cifar_resnet`` models with: ||  || :: ||  ||     python train_mixup.py --num-epochs 450 --mode hybrid --num-gpus 2 -j 32\\ ||         --batch-size 64 --wd 0.0001 --lr 0.1 --lr-decay 0.1 --lr-decay-epoch 150;250\\ ||         --model cifar_resnet20_v1 ||  || and the ``cifar_wideresnet`` models with: ||  || :: ||  ||     python train_mixup.py --num-epochs 500 --mode hybrid --num-gpus 2 -j 32\\ ||         --batch-size 64 --wd 0.0001 --lr 0.1 --lr-decay 0.1 --lr-decay-epoch 100;200;300\\ ||         --model cifar_wideresnet16_10 ||  || Next Step || --------- ||  || Congratulations! You\u2019ve just finished reading our first tutorial. We || have a lot more others to help you learn more and get familiar with || ``gluonvision``. ||  || If you would like to dig deeper in the topic of ``CIFAR10`` training; || feel free to read `the next tutorial on ``CIFAR10`` <>`__. ||  || Or; if you would like to try a more powerful demo; i.e. models trained || on ImageNet; please read `xxx <>`__. ||  || .. |image0| image:: mt_baker.jpeg ||  || \""\""\""",https://github.com/dmlc/gluon-cv/commit/e7389b15ea6509268a5c54d239992df43603d50b,No
4516,dmlc/gluon-cv,scripts/segmentation/train.py,f1fd98531a1706cf1b6afa0872f5990b937087de,"\""\""\""Train FCN on Pascal VOC Dataset || =============================== ||  || This is a semantic segmentation tutorial using Gluon Vison; a step-by-step example. || The readers should have basic knowledge of deep learning and should be familiar with Gluon API. || New users may first go through Gluon tutorials || `Deep Learning - The Straight Dope <http:\/\/gluon.mxnet.io\/>`_. ||  || Fully Convolutional Network || --------------------------- ||  || .. image:: https:\/\/cdn-images-1.medium.com\/max\/800\/1*wRkj6lsQ5ckExB5BoYkrZg.png ||     :width: 70% ||     :align: center ||  || (figure redit to `Long et al. <https:\/\/arxiv.org\/pdf\/1411.4038.pdf>`_ ) ||  || State-of-the-art approaches of semantic segmentation are typically based on || Fully Convolutional Network (FCN) [Long15]_ . || The key idea of a fully convolutional network is that it is \""fully convolutional\""; || which means it does have any fully connected layers. Therefore; the network can || accept arbitrary input size and make dense per-pixel predictions. || Base\/Encoder network is typically pre-trained on ImageNet; because the features || learned from diverse set of images contain rich contextual information; which || can be beneficial for semantic segmentation. ||  ||  || Model Dilation || ~~~~~~~~~~~~~~ ||  || The adaption of base network pre-trained on ImageNet leads to loss spatial resolution; || because these networks are originally designed for classification task. || Following recent works in semantic segmentation; we apply dilation strategy to the || stage 3 and stage 4 of the pre-trained networks; which produces stride of 8 || featuremaps (models are provided in :class:`gluonvision.model_zoo.Dilated_ResNetV2`). || Visualization of dilated\/atrous convoution: ||  || .. image:: https:\/\/raw.githubusercontent.com\/vdumoulin\/conv_arithmetic\/master\/gif\/dilation.gif ||     :width: 40% ||     :align: center ||  || (figure credit to `conv_arithmetic <https:\/\/github.com\/vdumoulin\/conv_arithmetic>`_ ) ||  || For example; loading a dilated ResNet50 is simply:: ||  ||     pretrained_net = gluonvision.model_zoo.dilated_resnet50(pretrained=True) ||  || For convenience; we provide a base model for semantic segmentation; which automatically || load the pre-trained dilated ResNet :class:`gluonvision.model_zoo.SegBaseModel`; which can || be easily inherited and used. ||  || FCN Block || ~~~~~~~~~ ||  || We build a fully convolutional \""head\"" on top of the basenetwork (FCN model is provided || in :class:`gluonvision.model_zoo.FCN`):: ||  ||     class _FCNHead(HybridBlock): ||         def __init__(self; nclass; norm_layer): ||             super(_FCNHead; self).__init__() ||             with self.name_scope(): ||                 self.block = nn.HybridSequential(prefix='') ||                 self.block.add(norm_layer(in_channels=2048)) ||                 self.block.add(nn.Activation('relu')) ||                 self.block.add(nn.Conv2D(in_channels=2048; channels=512; ||                                          kernel_size=3; padding=1)) ||                 self.block.add(norm_layer(in_channels=512)) ||                 self.block.add(nn.Activation('relu')) ||                 self.block.add(nn.Dropout(0.1)) ||                 self.block.add(nn.Conv2D(in_channels=512; channels=nclass; ||                                          kernel_size=1)) ||  ||         def hybrid_forward(self; F; x): ||             return self.block(x) ||  ||     class FCN(SegBaseModel): ||         def __init__(self; nclass; backbone='resnet50'; norm_layer=nn.BatchNorm): ||             super(FCN; self).__init__(backbone; norm_layer) ||             self._prefix = '' ||             with self.name_scope(): ||                 self.head = _FCNHead(nclass; norm_layer=norm_layer) ||             self.head.initialize(init=init.Xavier()) ||  ||         def forward(self; x): ||             _; _; H; W = x.shape ||             x = self.pretrained(x) ||             x = self.head(x) ||             x = F.contrib.BilinearResize2D(x; height=H; width=W) ||             return x ||  || Dataset and Data Augmentation || ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ||  || We provide semantic segmentation datasets in :class:`gluonvision.data`. || For example; we can easily get the Pascal VOC 2012 dataset:: ||  ||     train_set = gluonvision.data.VOCSegmentationDataset(root) ||  || We follow the standard data augmentation routine to transform the input image || and the ground truth label map synchronously. (Note that \""nearest\"" || mode upsample are applied to the label maps to avoid messing up the boundaries.) || We first randomly scale the input image from 0.5 to 2.0 times; then rotate || the image from -10 to 10 degrees; and crop the image with padding if needed. ||  || Benchmarks and Training || ~~~~~~~~~~~~~~~~~~~~~~~ ||  || - Training command example:: ||  ||     # First training on augmented set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_aug --model fcn --backbone resnet50 --lr 0.001 --checkname mycheckpoint ||     # Finetuning on original set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_voc --model fcn --backbone resnet50 --lr 0.0001 --checkname mycheckpoint --resume runs\/pascal_aug\/fcn\/mycheckpoint\/checkpoint.params ||  ||   For more training commands; please see the ``Commands`` in the pre-trained Table_. ||  || - Detail training options:: ||      ||     -h; --help            show this help message and exit ||     --model MODEL         model name (default: fcn) ||     --backbone BACKBONE   backbone name (default: resnet50) ||     --dataset DATASET     dataset name (default: pascal) ||     --nclass NCLASS       nclass for pre-trained model (default: None) ||     --workers N           dataloader threads ||     --data-folder         training dataset folder (default: $(HOME)\/data\/) ||     --epochs N            number of epochs to train (default: 50) ||     --start_epoch N       start epochs (default:0) ||     --batch-size N        input batch size for training (default: 16) ||     --test-batch-size N   input batch size for testing (default: 32) ||     --lr LR               learning rate (default: 1e-3) ||     --momentum M          momentum (default: 0.9) ||     --weight-decay M      w-decay (default: 1e-4) ||     --kvstore KVSTORE     kvstore to use for trainer\/module. ||     --no-cuda             disables CUDA training ||     --ngpus NGPUS         number of GPUs (default: 4) ||     --seed S              random seed (default: 1) ||     --resume RESUME       put the path to resuming file if needed ||     --checkname           set the checkpoint name ||     --eval                evaluating mIoU ||     --test                test a set of images and save the prediction ||     --syncbn              using Synchronized Cross-GPU BatchNorm ||  ||  || - Table of pre-trained models and its performance (models :math:`^\\ast` denotes pre-trained on COCO): ||  || .. role:: raw-html(raw) ||    :format: html ||  || .. _Table: ||  ||     +------------------------+------------+-----------+-----------+-----------+-----------+----------------------------------------------------------------------------------------------+ ||     | Method                 | Backbone   | Dataset   | Note      | pixAcc    | mIoU      | Training Scripts                                                                             | ||     +========================+============+===========+===========+===========+===========+==============================================================================================+ ||     | FCN                    | ResNet50   | PASCAL12  | stride 8  | N\/A       | 70.9_     | :raw-html:`<a href=\""javascript:toggleblock('cmd_fcn_50')\"" class=\""toggleblock\"">cmd<\/a>`       | ||     +------------------------+------------+-----------+-----------+-----------+-----------+----------------------------------------------------------------------------------------------+ ||     | FCN                    | ResNet101  | PASCAL12  | stride 8  | N\/A       |           | :raw-html:`<a href=\""javascript:toggleblock('cmd_fcn_101')\"" class=\""toggleblock\"">cmd<\/a>`      | ||     +------------------------+------------+-----------+-----------+-----------+-----------+----------------------------------------------------------------------------------------------+ ||  ||     .. _70.9:  http:\/\/host.robots.ox.ac.uk:8080\/anonymous\/FR9APO.html ||  || .. raw:: html ||  ||     <code xml:space=\""preserve\"" id=\""cmd_fcn_50\"" style=\""display: none; text-align: left; white-space: pre-wrap\""> ||     # First training on augmented set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_aug --model fcn --backbone resnet50 --lr 0.001 --syncbn --checkname mycheckpoint ||     # Finetuning on original set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_voc --model fcn --backbone resnet50 --lr 0.0001 --syncbn --checkname mycheckpoint --resume runs\/pascal_aug\/fcn\/mycheckpoint\/checkpoint.params ||     <\/code> ||  ||     <code xml:space=\""preserve\"" id=\""cmd_fcn_101\"" style=\""display: none; text-align: left; white-space: pre-wrap\""> ||     # First training on augmented set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_aug --model fcn --backbone resnet101 --lr 0.001 --syncbn --checkname mycheckpoint ||     # Finetuning on original set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_voc --model fcn --backbone resnet101 --lr 0.0001 --syncbn --checkname mycheckpoint --resume runs\/pascal_aug\/fcn\/mycheckpoint\/checkpoint.params ||     <\/code> ||  ||     <code xml:space=\""preserve\"" id=\""cmd_psp_50\"" style=\""display: none; text-align: left; white-space: pre-wrap\""> ||     # First training on augmented set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_aug --model pspnet --backbone resnet50 --lr 0.001 --syncbn --checkname mycheckpoint ||     # Finetuning on original set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_voc --model pspnet --backbone resnet50 --lr 0.0001 --syncbn --checkname mycheckpoint --resume runs\/pascal_aug\/fcn\/mycheckpoint\/checkpoint.params ||     <\/code> ||  ||     <code xml:space=\""preserve\"" id=\""cmd_psp_101\"" style=\""display: none; text-align: left; white-space: pre-wrap\""> ||     # First training on augmented set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_aug --model pspnet --backbone resnet101 --lr 0.001 --syncbn --checkname mycheckpoint ||     # Finetuning on original set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_voc --model pspnet --backbone resnet101 --lr 0.0001 --syncbn --checkname mycheckpoint --resume runs\/pascal_aug\/fcn\/mycheckpoint\/checkpoint.params ||     <\/code> ||  ||     <code xml:space=\""preserve\"" id=\""cmd_psp_101_coco\"" style=\""display: none; text-align: left; white-space: pre-wrap\""> ||     # Pre-training on COCO dataset ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset mscoco --model pspnet --backbone resnet101 --lr 0.01 --syncbn --checkname mycheckpoint ||     # Training on augmented set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_aug --model pspnet --backbone resnet101 --lr 0.001 --syncbn --checkname mycheckpoint ||     # Finetuning on original set ||     CUDA_VISIBLE_DEVICES=0;1;2;3 python main.py --dataset pascal_voc --model pspnet --backbone resnet101 --lr 0.0001 --syncbn --checkname mycheckpoint --resume runs\/pascal_aug\/fcn\/mycheckpoint\/checkpoint.params ||     <\/code> ||  || References || ---------- ||  || .. [Long15] Long; Jonathan; Evan Shelhamer; and Trevor Darrell. \\ ||     \""Fully convolutional networks for semantic segmentation.\"" \\ ||     Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. ||  || Dive Deep into the code || ----------------------- ||  || \""\""\""",https://github.com/dmlc/gluon-cv/commit/f1fd98531a1706cf1b6afa0872f5990b937087de,Yes
4517,dmlc/gluon-cv,scripts/datasets/ade20k.py,97bb60b6f4e5e66fa4fdc257bc724f2b29cbfd54,"\""\""\""Prepare ADE20K datasets. || ======================== ||  || This script download and prepare the `ADE20K || <http:\/\/sceneparsing.csail.mit.edu\/>`_ dataset for scene parsing.  It contains || more than 20 thousands scene-centric images annotated with 150 object || categories. ||  || .. image:: http:\/\/groups.csail.mit.edu\/vision\/datasets\/ADE20K\/assets\/images\/examples.png ||    :width: 600 px ||  || Prepare the dataset || ------------------- ||  || The easiest way is simply running this script; which will automatically download || and extract the data into ``~\/.mxnet\/datasets\/ade``. ||  || .. code-block:: bash ||  ||    python scripts\/datasets\/ade20k.py ||  || .. note:: ||  ||    You need 2.3 GB disk space to download and extract this dataset. SSD is ||    preferred over HDD because of its better performance. ||  || .. note:: ||  ||    The total time to prepare the dataset depends on your Internet speed and disk ||    performance. For example; it may take 15 min on AWS EC2 with EBS. ||  ||  || If you have already downloaded the following required files; whose URLs can be || obtained from the source codes at the end of this tutorial; ||  || ===========================  ====== || Filename                     Size || ===========================  ====== || ADEChallengeData2016.zip     923 MB || release_test.zip             202 MB || ===========================  ====== ||  || then you can specify the folder name through ``--download-dir`` to avoid || download them again. For example ||  || .. code-block:: python ||  ||    python scripts\/datasets\/ade20k.py --download-dir ~\/ade_downloads ||  || How to load the dataset || ----------------------- ||  || TODO. ||  || Dive deep into source code || -------------------------- ||  || The implementation of ade20k.py is straightforward. It simply downloads and || extract the data. || \""\""\""",https://github.com/dmlc/gluon-cv/commit/97bb60b6f4e5e66fa4fdc257bc724f2b29cbfd54,Yes
4518,dmlc/gluon-cv,docs/tutorials/classification/dive_deep_imagenet.py,037379107e3b7960a07987e8519d955d1f1fa494,"\""\""\""Train Your Own Model on ImageNet || ====================== ||  || ``ImageNet`` is the most well-known dataset for image classification task. || Since it's been published; there has been plenty of works pushing the performance || of classification accuracy. ||  || Although there are a lot of available models; it is still a non-trivial job to || train a well-performing model on ``ImageNet`` from scratch. In this tutorial; we will || try to pull necessary code together to walk you through the process of || training a model on ``ImageNet``. ||  || .. note:: ||  ||     :download:`Download Python Script train_imagenet.py<..\/..\/..\/scripts\/classification\/imagenet\/train_imagenet.py>` ||  ||     Before dive into the details of the training; one can take a look at the complete ||     training script. The commands to reproduce papers are given in our model zoo. ||  ||     Since the training is extremely resource consuming; we don't actually ||     execute code blocks in this tutorial. ||  || Prerequisites || ------------- ||  || **Knowledge** ||  || We assume readers have a basic understanding of ``Gluon``. If you would || like to know more about it; we suggest the `Gluon Crash || Course <http:\/\/gluon-crash-course.mxnet.io\/index.html>`__ as a good || place to start. ||  || Also; it is suggested that readers have read our tutorials on || `CIFAR10 Training <dive_deep_cifar10.html>`_ || and `ImageNet Demo <demo_imagenet.html>`_ || to gain some experience of model training. ||  || **Data Preparation** ||  || Unlike ``CIFAR10``; we need to prepare the data manually. || If you haven't done so; please go through our tutorial on || `Prepare ImageNet Data <..\/examples_datasets\/imagenet.html>`_ and || prepare the data. ||  || **Hardware** ||  || Training a deep learning model on a dataset of over 100GB is resource demanding. || Two main bottlenecks are training computation and data IO. ||  || For training computation; it is required to have a GPU; preferably a high-end || GPU. Training on CPU is almost mission impossible; and training with a low-end GPU || may still largely limit your performance. It is the best to have multiple || strong GPUs to work together. ||  || For data IO; we recommend a strong CPU and a SSD disk. Data loading can largely benefit || from multiple CPU threads; and a fast SSD disk. Note that in total the compressed || and extracted ``ImageNet`` data could occupy around 300GB disk space; thus a SSD with || at least 300GB is necessary. ||  || Network structure || ----------------- ||  || If everything's prepared; Let's get started! ||  || First; load the necessary libraries into python. ||  || .. code-block:: python ||  ||     import argparse; time; ||  ||     import numpy as np ||     import mxnet as mx ||  ||     from mxnet import gluon; nd ||     from mxnet import autograd as ag ||     from mxnet.gluon import nn ||     from mxnet.gluon.data.vision import transforms ||  ||     from gluonvision.model_zoo import get_model ||     from gluonvision.utils import makedirs; TrainingHistory ||  || ``ResNet50_v2`` is a network with balanced prediction accuracy and computational cost. || Here we use this model to demonstrate the training process. ||  || .. code-block:: python ||  ||     # number of GPUs to use ||     num_gpus = 4 ||     ctx = [mx.gpu(i) for i in range(num_gpus)] ||  ||     # Get the model ResNet50_v2; with 10 output classes ||     net = get_model('ResNet50_v2'; classes=1000) ||     net.initialize(mx.init.Xavier(magnitude=2); ctx = ctx) ||  ||  || Note that the ResNet model we use here for ``ImageNet`` is different in structure from || the one we used to train ``CIFAR10``. Please refer to the original paper or || our implementations for details. ||  || Data Augmentation and Data Loader || --------------------------------- ||  || Data augmentation is essential for a good result. It is similar to what we have || in the ``CIFAR10`` training tutorial; just different in the parameters. ||  || We compose our transform functions as following: ||  || .. code-block:: python ||  ||     normalize = transforms.Normalize([0.485; 0.456; 0.406]; [0.229; 0.224; 0.225]) ||     jitter_param = 0.4 ||     lighting_param = 0.1 ||  ||     transform_train = transforms.Compose([ ||         transforms.Resize(480); ||         transforms.RandomResizedCrop(224); ||         transforms.RandomFlipLeftRight(); ||         transforms.RandomColorJitter(brightness=jitter_param; contrast=jitter_param; ||                                      saturation=jitter_param); ||         transforms.RandomLighting(lighting_param); ||         transforms.ToTensor(); ||         normalize ||     ]) ||  ||  || Since ``ImageNet`` contains images with much higher resolution and quality than || ``CIFAR10``; thus we can crop a larger image (224x224) as the input to the model. ||  || For prediction; we still need a deterministic result. The transform function is: ||  || .. code-block:: python ||  ||     transform_test = transforms.Compose([ ||         transforms.Resize(256); ||         transforms.CenterCrop(224); ||         transforms.ToTensor(); ||         normalize ||     ]) ||  ||  || Notice that it is important to keep the normalization consistent; since the || model only works well on input with the same distribution. ||  || With the transform functions; we can define data loaders for our || training and validation datasets. ||  || .. code-block:: python ||  ||     # Batch Size for Each GPU ||     per_device_batch_size = 64 ||     # Number of data loader workers ||     num_workers = 32 ||     # Calculate effective total batch size ||     batch_size = per_device_batch_size * num_gpus ||  ||     data_path = '~\/.mxnet\/datasets\/imagenet' ||  ||     # Set train=True for training data ||     # Set shuffle=True to shuffle the training data ||     train_data = gluon.data.DataLoader( ||         imagenet.classification.ImageNet(data_path; train=True).transform_first(transform_train); ||         batch_size=batch_size; shuffle=True; last_batch='discard'; num_workers=num_workers) ||  ||     # Set train=False for validation data ||     val_data = gluon.data.DataLoader( ||         imagenet.classification.ImageNet(data_path; train=False).transform_first(transform_test); ||         batch_size=batch_size; shuffle=False; num_workers=num_workers) ||  || Note that we set ``per_device_batch_size=64``; which may not suit GPUs with || Memory smaller than 12GB. Please tune the value according to your specific configuration. ||  || The path ``'~\/.mxnet\/datasets\/imagenet'`` is the default path if you || prepare the data `with our script <..\/examples_datasets\/imagenet.html>`_. ||  || Optimizer; Loss and Metric || -------------------------- ||  || Optimizer is what improves the model during training. We use the popular || Nesterov accelerated gradient descent algorithm. ||  || .. code-block:: python ||  ||     # Learning rate decay factor ||     lr_decay = 0.1 ||     # Epochs where learning rate decays ||     lr_decay_epoch = [30; 60; 90; np.inf] ||  ||     # Nesterov accelerated gradient descent ||     optimizer = 'nag' ||     # Set parameters ||     optimizer_params = {'learning_rate': 0.1; 'wd': 0.0001; 'momentum': 0.9} ||  ||     # Define our trainer for net ||     trainer = gluon.Trainer(net.collect_params(); optimizer; optimizer_params) ||  ||  || For classification tasks; we usually use softmax cross entropy as the || loss function. ||  || .. code-block:: python ||  ||     loss_fn = gluon.loss.SoftmaxCrossEntropyLoss() ||  ||  || With 1000 classes the model may not always rate the correct answer with the highest || rank. Besides the top-1 accuracy; we also consider top-5 accuracy as a measurement || of how well a model can predict. ||  || At the end of every epoch; we record and print the metric scores. ||  || .. code-block:: python ||  ||     acc_top1 = mx.metric.Accuracy() ||     acc_top5 = mx.metric.TopKAccuracy(5) ||     train_history = TrainingHistory(['training-top1-err'; 'training-top5-err'; ||                                      'validation-top1-err'; 'validation-top5-err']) ||  ||  || Validation || ---------- ||  || At the end of every training epoch; we evaluate it on the validation data set; || and report the top-1 and top-5 error rate. ||  || .. code-block:: python ||  ||     def test(ctx; val_data): ||         acc_top1_val = mx.metric.Accuracy() ||         acc_top5_val = mx.metric.TopKAccuracy(5) ||         for i; batch in enumerate(val_data): ||             data = gluon.utils.split_and_load(batch[0]; ctx_list=ctx; batch_axis=0) ||             label = gluon.utils.split_and_load(batch[1]; ctx_list=ctx; batch_axis=0) ||             outputs = [net(X) for X in data] ||             acc_top1_val.update(label; outputs) ||             acc_top5_val.update(label; outputs) ||  ||         _; top1 = acc_top1_val.get() ||         _; top5 = acc_top5_val.get() ||         return (1 - top1; 1 - top5) ||  || Training || -------- ||  || After all these preparation; we can finally start our training process! || Following is the main training loop ||  || .. code-block:: python ||  ||     epochs = 120 ||     lr_decay_count = 0 ||     log_interval = 50 ||     num_batch = len(train_data) ||  ||     for epoch in range(epochs): ||         tic = time.time() ||         btic = time.time() ||         acc_top1.reset() ||         acc_top5.reset() ||         train_loss = 0 ||  ||         if lr_decay_period == 0 and epoch == lr_decay_epoch[lr_decay_count]: ||             trainer.set_learning_rate(trainer.learning_rate*lr_decay) ||             lr_decay_count += 1 ||  ||         for i; batch in enumerate(train_data): ||             data = gluon.utils.split_and_load(batch[0]; ctx_list=ctx; batch_axis=0) ||             label = gluon.utils.split_and_load(batch[1]; ctx_list=ctx; batch_axis=0) ||             with ag.record(): ||                 outputs = [net(X) for X in data] ||                 loss = [L(yhat; y) for yhat; y in zip(outputs; label)] ||             for l in loss: ||                 l.backward() ||             trainer.step(batch_size) ||             acc_top1.update(label; outputs) ||             acc_top5.update(label; outputs) ||             train_loss += sum([l.sum().asscalar() for l in loss]) ||             if log_interval and not log_interval: ||                 _; top1 = acc_top1.get() ||                 _; top5 = acc_top5.get() ||                 err_top1; err_top5 = (1-top1; 1-top5) ||                 print('Epoch[%d] Batch [%d]\\tSpeed: %f samples\/sec\\ttop1-err=%f\\ttop5-err=%f'%( ||                              epoch; i; batch_size*opt.log_interval\/(time.time()-btic); err_top1; err_top5)) ||                 btic = time.time() ||  ||         _; top1 = acc_top1.get() ||         _; top5 = acc_top5.get() ||         err_top1; err_top5 = (1-top1; 1-top5) ||         train_loss \/= num_batch * batch_size ||  ||         err_top1_val; err_top5_val = test(ctx; val_data) ||         train_history.update([err_top1; err_top5; err_top1_val; err_top5_val]) ||  ||         print('[Epoch %d] training: err-top1=%f err-top5=%f loss=%f'%(epoch; err_top1; err_top5; train_loss)) ||         print('[Epoch %d] time cost: %f'%(epoch; time.time()-tic)) ||         print('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch; err_top1_val; err_top5_val)) ||  ||  || We can plot the top-1 error rates with: ||  || .. code-block:: python ||  ||     train_history.plot(['training-top1-err'; 'validation-top1-err']) ||  || If you train the model with ``epochs=120``; the plot may look like: ||  || |image-imagenet-curve| ||  || Next Step || --------- ||  || This is our script to help you to train a model on ``ImageNet``. ||  || If you want like to know what can be done with the model you just || trained; please read the tutorial about `Transfer learning <transfer_learning_minc.html>`__. ||  || Besides classification; deep learning models nowadays can do other exciting jobs || like `object detection <..\/examples_detection\/index.html>`_; || `semantic segmentation <..\/examples_segmentation\/index.html>`_. Check out their tutorials! ||  || .. |image-imagenet-curve| image:: https:\/\/raw.githubusercontent.com\/dmlc\/web-data\/master\/gluonvision\/classification\/imagenet_resnet50_v2.png || \""\""\""",https://github.com/dmlc/gluon-cv/commit/037379107e3b7960a07987e8519d955d1f1fa494,No
4519,dmlc/gluon-cv,docs/tutorials/detection/demo_webcam.py,7a4863aef069f8c3d5b3d146df987d0fe673e2d7,"\""\""\""9. Run an object detection model on your webcam || ================================================== ||  || This article will shows how to play with pre-trained object detection models by running || them directly on your webcam video stream. ||  || .. note:: ||  ||     - This tutorial has only been tested in a MacOS environment ||     - Python packages required: cv2; matplotlib ||     - You need a webcam :) ||     - Python compatible with matplotlib rendering; installed as a framework in MacOS see guide `here <https:\/\/matplotlib.org\/faq\/osx_framework.html>`__ ||  ||  || Loading the model and webcam || ---------------------------- || Finished preparation? Let's get started! || First; import the necessary libraries into python. ||  || .. code-block:: python ||  ||     import time ||  ||     import cv2 ||     import gluoncv as gcv ||     import matplotlib.pyplot as plt ||     import mxnet as mx ||  ||  || In this tutorial we use ``ssd_512_mobilenet1.0_voc``; a snappy network with good accuracy that should be || well above 1 frame per second on most laptops. Feel free to try a different model from || the `Gluon Model Zoo <..\/..\/model_zoo\/detection.html>`__ ! ||  || .. code-block:: python ||  ||     # Load the model ||     net = gcv.model_zoo.get_model('ssd_512_mobilenet1.0_voc'; pretrained=True) ||  ||  || We create the webcam handler in opencv to be able to acquire the frames: ||  || .. code-block:: python ||  ||     # Load the webcam handler ||     cap = cv2.VideoCapture(0) ||     time.sleep(1) ### letting the camera autofocus ||  ||  || Detection loop || -------------- ||  || The detection loop consists of four phases: ||  || * loading the webcam frame ||  || * pre-processing the image ||  || * running the image through the network ||  || * updating the output with the resulting predictions ||  ||  || .. code-block:: python ||  ||     axes = None ||     NUM_FRAMES = 200 # you can change this ||     for i in range(NUM_FRAMES): ||         # Load frame from the camera ||         ret; frame = cap.read() ||  ||         # Image pre-processing ||         frame = mx.nd.array(cv2.cvtColor(frame; cv2.COLOR_BGR2RGB)).astype('uint8') ||         rgb_nd; frame = gcv.data.transforms.presets.ssd.transform_test(frame; short=512; max_size=700) ||  ||         # Run frame through network ||         class_IDs; scores; bounding_boxes = net(rgb_nd) ||  ||         # Display the result ||         plt.cla() ||         axes = gcv.utils.viz.plot_bbox(frame; bounding_boxes[0]; scores[0]; class_IDs[0]; class_names=net.classes; ax=axes) ||         plt.draw() ||         plt.pause(0.001) ||  ||  || We release the webcam before exiting the script ||  || .. code-block:: python ||  ||     cap.release() ||  || Results || --------- ||  || Download the script to run the demo: ||  || :download:`Download demo_webcam_run.py<..\/..\/..\/scripts\/detection\/demo_webcam_run.py>` ||  ||  || Run the script using `pythonw` on MacOS: ||  || .. code-block:: bash ||  ||     pythonw demo_webcam_run.py --num-frames 200 ||  ||  || .. note:: ||  ||     On MacOS; to enable matplotlib rendering you need python installed as a framework; ||     see guide `here <https:\/\/matplotlib.org\/faq\/osx_framework.html>`__ ||  ||  || If all goes well you should be able to detect objects from the available || classes of the VOC dataset. That includes persons; chairs and TV Screens! ||  || .. image:: https:\/\/media.giphy.com\/media\/9JvoKeUeCt4bdRf3Cv\/giphy.gif ||  ||  || \""\""\""",https://github.com/dmlc/gluon-cv/commit/7a4863aef069f8c3d5b3d146df987d0fe673e2d7,No
4520,snipsco/snips-nlu,snips_nlu/intent_parser/probabilistic_intent_parser.py,9bac26ac7424ed066d4923066e472511bf0bad8b,Only needed to improve testability,https://github.com/snipsco/snips-nlu/commit/9bac26ac7424ed066d4923066e472511bf0bad8b,Yes
4521,snipsco/snips-nlu,snips_nlu/intent_parser/probabilistic_intent_parser.py,e8ab2c6a2e467bfc91ae774b3290e2e0a724d8e1,Only needed to improve testability,https://github.com/snipsco/snips-nlu/commit/e8ab2c6a2e467bfc91ae774b3290e2e0a724d8e1,Yes
4522,snipsco/snips-nlu,snips_nlu/tests/test_dataset.py,4f53f76929756eff69a3c33f1207726cfc96c128,TODO: This test is temporary; and must be removed once the backward,https://github.com/snipsco/snips-nlu/commit/4f53f76929756eff69a3c33f1207726cfc96c128,Yes
4523,snipsco/snips-nlu,snips_nlu/tests/test_dataset.py,8bb4db23851421a241170d765da24be41771c051,TODO: This test is temporary; and must be removed once the backward,https://github.com/snipsco/snips-nlu/commit/8bb4db23851421a241170d765da24be41771c051,Yes
4524,iterative/dvc,tests/test_data_cloud.py,a05eddd746317f3274763a8eb3f30c2fecca0754,FIXME enable this test for windows,https://github.com/iterative/dvc/commit/a05eddd746317f3274763a8eb3f30c2fecca0754,Yes
4525,iterative/dvc,dvc/remote/ssh/__init__.py,110da0c366b133a9981e89f221fe6dcd1ca166f2,XXX: We are testing if file exists rather than if file is a link,https://github.com/iterative/dvc/commit/110da0c366b133a9981e89f221fe6dcd1ca166f2,No
4526,iterative/dvc,dvc/remote/base.py,40c61b3aaa5a29fa69842fa3a8fe2e73734a4cae,XXX: We are testing if file exists rather than if file is a link,https://github.com/iterative/dvc/commit/40c61b3aaa5a29fa69842fa3a8fe2e73734a4cae,No
4527,iterative/dvc,tests/func/test_api.py,fcebad821e9102eefa0e4ab27c1fd7510a3fbd5f,FIXME: this test doesn't use scm ;D,https://github.com/iterative/dvc/commit/fcebad821e9102eefa0e4ab27c1fd7510a3fbd5f,Yes
4528,iterative/dvc,tests/dir_helpers.py,659eb04705b2788693c8723f144e461b9b13a281,"\""\""\"" || The goal of this module is making dvc functional tests setup a breeze. This || includes a temporary dir; initializing git and dvc repos and bootstraping some || file structure. ||  || The cornerstone of these fixtures is `tmp_dir`; which creates a temporary dir || and changes path to it; it might be combined with `scm` and `dvc` to initialize || empty git and dvc repos. `tmp_dir` returns a Path instance; which should save || you from using `open()`; `os` and `os.path` utils many times: ||  ||     (tmp_dir \/ \""some_file\"").write_text(\""some text\"") ||     # ... ||     assert \""some text\"" == (tmp_dir \/ \""some_file\"").read_text() ||     assert (tmp_dir \/ \""some_file\"").exists() ||  || Additionally it provides `.gen()`; `.scm_gen()` and `.dvc_gen()` methoda to || bootstrap a required file structure in a single call: ||  ||     # Generate a dir with files ||     tmp_dir.gen({\""dir\"": {\""file\"": \""file text\""; \""second_file\"": \""...\""}}) ||  ||     # Generate a single file; dirs will be created along the way ||     tmp_dir.gen(\""dir\/file\""; \""file text\"") ||  ||     # Generate + git add ||     tmp_dir.scm_gen({\""file1\"": \""...\""; ...}) ||  ||     # Generate + git add + git commit ||     tmp_dir.scm_gen({\""file1\"": \""...\""; ...}; commit=\""add files\"") ||  ||     # Generate + dvc add ||     tmp_dir.dvc_gen({\""file1\"": \""...\""; ...}) ||  ||     # Generate + dvc add + git commit -am \""...\"" ||     # This commits stages to git not the generated files. ||     tmp_dir.dvc_gen({\""file1\"": \""...\""; ...}; commit=\""add files\"") ||  || Making it easier to bootstrap things has a supergoal of incentivizing a move || from global repo template to creating everything inplace; which: ||  ||     - makes all path references local to test; enhancing readability ||     - allows using telling filenames; e.g. \""git_tracked_file\"" instead of \""foo\"" ||     - does not create whatever is not needed || \""\""\""",https://github.com/iterative/dvc/commit/659eb04705b2788693c8723f144e461b9b13a281,Yes
4529,iterative/dvc,tests/dir_helpers.py,0cd6106189e373b54efa2d8f5b917f230dfabda1,"\""\""\"" || The goal of this module is making dvc functional tests setup a breeze. This || includes a temporary dir; initializing git and dvc repos and bootstraping some || file structure. ||  || The cornerstone of these fixtures is `tmp_dir`; which creates a temporary dir || and changes path to it; it might be combined with `scm` and `dvc` to initialize || empty git and dvc repos. `tmp_dir` returns a Path instance; which should save || you from using `open()`; `os` and `os.path` utils many times: ||  ||     (tmp_dir \/ \""some_file\"").write_text(\""some text\"") ||     # ... ||     assert \""some text\"" == (tmp_dir \/ \""some_file\"").read_text() ||     assert (tmp_dir \/ \""some_file\"").exists() ||  || Additionally it provides `.gen()`; `.scm_gen()` and `.dvc_gen()` methods to || bootstrap a required file structure in a single call: ||  ||     # Generate a dir with files ||     tmp_dir.gen({\""dir\"": {\""file\"": \""file text\""; \""second_file\"": \""...\""}}) ||  ||     # Generate a single file; dirs will be created along the way ||     tmp_dir.gen(\""dir\/file\""; \""file text\"") ||  ||     # Generate + git add ||     tmp_dir.scm_gen({\""file1\"": \""...\""; ...}) ||  ||     # Generate + git add + git commit ||     tmp_dir.scm_gen({\""file1\"": \""...\""; ...}; commit=\""add files\"") ||  ||     # Generate + dvc add ||     tmp_dir.dvc_gen({\""file1\"": \""...\""; ...}) ||  ||     # Generate + dvc add + git commit -am \""...\"" ||     # This commits stages to git not the generated files. ||     tmp_dir.dvc_gen({\""file1\"": \""...\""; ...}; commit=\""add files\"") ||  || Making it easier to bootstrap things has a supergoal of incentivizing a move || from global repo template to creating everything inplace; which: ||  ||     - makes all path references local to test; enhancing readability ||     - allows using telling filenames; e.g. \""git_tracked_file\"" instead of \""foo\"" ||     - does not create whatever is not needed || \""\""\""",https://github.com/iterative/dvc/commit/0cd6106189e373b54efa2d8f5b917f230dfabda1,Yes
4530,iterative/dvc,tests/dir_helpers.py,237704ef2bdf45f4c5f32ef8d423728c20d8a12a,"\""\""\"" || The goal of this module is making dvc functional tests setup a breeze. This || includes a temporary dir; initializing git and dvc repos and bootstrapping some || file structure. ||  || The cornerstone of these fixtures is `tmp_dir`; which creates a temporary dir || and changes path to it; it might be combined with `scm` and `dvc` to initialize || empty git and dvc repos. `tmp_dir` returns a Path instance; which should save || you from using `open()`; `os` and `os.path` utils many times: ||  ||     (tmp_dir \/ \""some_file\"").write_text(\""some text\"") ||     # ... ||     assert \""some text\"" == (tmp_dir \/ \""some_file\"").read_text() ||     assert (tmp_dir \/ \""some_file\"").exists() ||  || Additionally it provides `.gen()`; `.scm_gen()` and `.dvc_gen()` methods to || bootstrap a required file structure in a single call: ||  ||     # Generate a dir with files ||     tmp_dir.gen({\""dir\"": {\""file\"": \""file text\""; \""second_file\"": \""...\""}}) ||  ||     # Generate a single file; dirs will be created along the way ||     tmp_dir.gen(\""dir\/file\""; \""file text\"") ||  ||     # Generate + git add ||     tmp_dir.scm_gen({\""file1\"": \""...\""; ...}) ||  ||     # Generate + git add + git commit ||     tmp_dir.scm_gen({\""file1\"": \""...\""; ...}; commit=\""add files\"") ||  ||     # Generate + dvc add ||     tmp_dir.dvc_gen({\""file1\"": \""...\""; ...}) ||  ||     # Generate + dvc add + git commit -am \""...\"" ||     # This commits stages to git not the generated files. ||     tmp_dir.dvc_gen({\""file1\"": \""...\""; ...}; commit=\""add files\"") ||  || Making it easier to bootstrap things has a supergoal of incentivizing a move || from global repo template to creating everything inplace; which: ||  ||     - makes all path references local to test; enhancing readability ||     - allows using telling filenames; e.g. \""git_tracked_file\"" instead of \""foo\"" ||     - does not create unnecessary files || \""\""\""",https://github.com/iterative/dvc/commit/237704ef2bdf45f4c5f32ef8d423728c20d8a12a,No
4531,iterative/dvc,tests/func/test_api.py,b194bfa1ffd51ccac662b9b549ace34fdf0df691,FIXME: this test doesn't use scm ;D,https://github.com/iterative/dvc/commit/b194bfa1ffd51ccac662b9b549ace34fdf0df691,Yes
4532,iterative/dvc,tests/func/test_repro.py,128fa7f887923d792d872ac2be349627f1babbdc,TODO: Test ^ for multistage,https://github.com/iterative/dvc/commit/128fa7f887923d792d872ac2be349627f1babbdc,Yes
4533,iterative/dvc,tests/func/test_repro_multistage.py,128fa7f887923d792d872ac2be349627f1babbdc,TODO: TestReproWorkingDirectoryAsOutput,https://github.com/iterative/dvc/commit/128fa7f887923d792d872ac2be349627f1babbdc,No
4534,iterative/dvc,setup.py,9ca3c05f6dc8d53d11af80c1f2b965e93fb28cdf,FIXME: pylint complaining for pytest.mark.* on v6.0,https://github.com/iterative/dvc/commit/9ca3c05f6dc8d53d11af80c1f2b965e93fb28cdf,No
4535,iterative/dvc,tests/__init__.py,fc6999ad17dc219d1f0db6da6d1650b3c2f918a0,FIXME: Search and replace these from the tests if pyarrow wheel is available,https://github.com/iterative/dvc/commit/fc6999ad17dc219d1f0db6da6d1650b3c2f918a0,Yes
4536,microsoft/nni,src/sdk/pynni/nni/nas/tensorflow/mutables.py,76c819c00acb2952436da611c574030aa0beedd4,FIXME: not tested,https://github.com/microsoft/nni/commit/76c819c00acb2952436da611c574030aa0beedd4,Yes
4537,microsoft/nni,src/sdk/pynni/nni/compression/torch/pruning/amc/channel_pruning_env.py,e9f3cddf95e58ca17641db793c420a1b6e5424c0,TODO: to be tested! Share index for residual connection,https://github.com/microsoft/nni/commit/e9f3cddf95e58ca17641db793c420a1b6e5424c0,Yes
4538,microsoft/nni,nni/retiarii/codegen/tensorflow.py,002af91f0bedc3100638c8b9568ba7f22a8ccea1,"\""\""\"" || FIXME || This file is inherited from last version. ||  || I expect it can work with a few modifications to incorporate with the latest API; but it hasn't || been tested and I'm not sure. || \""\""\""",https://github.com/microsoft/nni/commit/002af91f0bedc3100638c8b9568ba7f22a8ccea1,Yes
4539,microsoft/nni,test/ut/retiarii/test_convert.py,0f88b86b4ec6760bf8107fc8bdd2f6219953d6cd,For deterministic testing TODO: change by fixing seed in checkTrace?,https://github.com/microsoft/nni/commit/0f88b86b4ec6760bf8107fc8bdd2f6219953d6cd,Yes
4540,OpenMined/PySyft,test/torch_test.py,58ce89a5cebe01469f7f1581fd202721a7925416,TODO: To pass this test; y and x should be connected. What happens right now this that x.abs_(),https://github.com/OpenMined/PySyft/commit/58ce89a5cebe01469f7f1581fd202721a7925416,Yes
4541,OpenMined/PySyft,syft/differential_privacy/pate.py,d0a712ac4398eb29552ae0bdb4f722c6140003de,Store unused part of test set for use as a test set after student training,https://github.com/OpenMined/PySyft/commit/d0a712ac4398eb29552ae0bdb4f722c6140003de,No
4542,OpenMined/PySyft,test/torch/tensors/test_chinese_remainder.py,929ec3d415e106087e72ad9f88e16359cd0c3c90,TODO add tests for values > Q\/2 and also wrapping,https://github.com/OpenMined/PySyft/commit/929ec3d415e106087e72ad9f88e16359cd0c3c90,Yes
4543,OpenMined/PySyft,test/workers/test_websocket_worker.py,14c6591f583d664398fd2486f7a98348a751abae,TODO: check why unit test sometimes fails when WebsocketServerWorker is started from the unit test. Fails when run after test_federated_client.py,https://github.com/OpenMined/PySyft/commit/14c6591f583d664398fd2486f7a98348a751abae,Yes
4544,OpenMined/PySyft,test/message/test_plan.py,8211c8382dfefacae0b878da951012bac853f3af,TODO: this test is not working properly with remote workers.,https://github.com/OpenMined/PySyft/commit/8211c8382dfefacae0b878da951012bac853f3af,Yes
4545,OpenMined/PySyft,test/notebooks/test_notebooks.py,b899b2adda4fe7f0e01c19c44f845d871c16155d,new note: now that travis has been replaced with github actions; someone should test this to see if this is still needed or removed.,https://github.com/OpenMined/PySyft/commit/b899b2adda4fe7f0e01c19c44f845d871c16155d,Yes
4546,OpenMined/PySyft,test/execution/test_state.py,20670fe2c481d42f279fdaeb3fa81647b3eb1c14,TODO: this test is not working properly with remote workers.,https://github.com/OpenMined/PySyft/commit/20670fe2c481d42f279fdaeb3fa81647b3eb1c14,Yes
4547,OpenMined/PySyft,test/serde/serde_helpers.py,e44610bcea374571fbac244134e61a58fcf15808,TODO: Add proper testing for paillier tensor,https://github.com/OpenMined/PySyft/commit/e44610bcea374571fbac244134e61a58fcf15808,Yes
4548,OpenMined/PySyft,syft/messaging/message.py,f63bd7148bc4c3bb885a1049e2dc519f1abe9538,TODO: when testing is fixed; uncomment this to enable worker command message support.,https://github.com/OpenMined/PySyft/commit/f63bd7148bc4c3bb885a1049e2dc519f1abe9538,No
4549,OpenMined/PySyft,tests/syft/worker/virtual/virtual_client_test.py,fd4644785826c16ca7ad951a3dc1af99dd91693a,TODO (gmuraru) Add tests after we finish the VirtualClient,https://github.com/OpenMined/PySyft/commit/fd4644785826c16ca7ad951a3dc1af99dd91693a,No
4550,OpenMined/PySyft,tests/syft/worker/virtual/virtual_worker_test.py,fd4644785826c16ca7ad951a3dc1af99dd91693a,TODO (gmuraru) Add tests after we finish the VirtualWorker,https://github.com/OpenMined/PySyft/commit/fd4644785826c16ca7ad951a3dc1af99dd91693a,No
4551,OpenMined/PySyft,src/syft/core/worker/__init__.py,c76363f5011f7796d8b3490b229ad0d6a67d70d3,"\""\""\"" || Welcome to the :py:mod:`syft.core.worker` module! This is a good place to begin your education of || what Syft is and how it works. ||  || At it's core; Syft is a set of libraries which allows you to perform data processing on data you || cannot see. We have two core \""personas\"" within the Syft ecosystem; the \""data owner\"" and the \""data scientist\"" || (who is sometimes referred to as the \""model owner\""). The data owner has data to protect; and the data scientist || wants to answer a question using data owned by one or more data owners. ||  || Note that a data owner could be a consumer who has data on their phone; a hospital with medical records; or || even a Raspberry PI floating in the middle of the Pacific Ocean! It just represents a collection of data || within a Data Owner's domain of ownership. As such; there are three core abstractions you should know about: ||  || * :py:mod:`syft.core.worker.domain.Domain` - this API is the interface to a collection of datasets owned by a single \\ || entity. || * :py:mod:`syft.core.worker.worker.Worker` - this API is the interface to a remote machine within a data owner's \\ || domain. || * :py:mod:`syft.core.worker.client.Client` - This API is the interface a data scientist uses to interact with a worker \\ || within a domain ||  || So; a domain would be something like \""Big Fancy Hospital\"" and a worker would be a single machine within that hospital || which a Data Scientist can use to process some data. A domain will have many workers; each of which could be serving || a different Data Scientist. Alternatively; one Data Scientist could be using multiple workers (such as if they have || very computationally expensive programs to run and they want to run them in parallel). ||  || However; a Data Scientist wouldn't interact with these remote Domain and Worker APIs directly. Instead; they use their || :py:mod:`syft.core.worker.client.Client` api which has lots of convenience functions for interacting with remote || machines held within remote domains. ||  || The :py:mod:`syft.core.worker.client.Client` API will send :py:mod:`syft.core.message.message.Message` objects to the || :py:mod:`syft.core.worker.domain.Domain` API which will handle some messages itself and; if appropriate; forward other || messages to the appropriate :py:mod:`syft.core.worker.worker.Worker` for execution on real objects. ||  || However; you will find that Domain; Worker; and Client all have missing functionality; namely how to send a message! || This might seem like a critical oversight; but it's actually an important abstraction. These classes are merely || interfaces for the API language that all Domain; Worker; and Client objects should use. This language is universal || regardless of whether two phones; two hospitals; or two satellites are talking to each other! However; the exact || transport protocol for _how_ messages are sent we leave up to the _specific instantiation_ of a worker. ||  || The primary instantiation of Domain\/Worker\/Client that we use for development; testing; and learning is the || VirtualDomain\/VirtualWorker\/VirtualClient instantiation. More on that in a moment... ||  || If you're learning Syft; the next step you should take is to jump into the :py:mod:`syft.core.worker.client.Client` || class... ||  ||  || \""\""\""",https://github.com/OpenMined/PySyft/commit/c76363f5011f7796d8b3490b229ad0d6a67d70d3,No
4552,OpenMined/PySyft,src/syft/core/worker/__init__.py,77c09a85b96e995d599cefc289aa04c8935a39ba,"\""\""\"" || Welcome to the :py:mod:`syft.core.worker` module! This is a good place to begin your education of || what Syft is and how it works. ||  || At it's core; Syft is a set of libraries which allows you to perform data processing on data you || cannot see. We have two core \""personas\"" within the Syft ecosystem; the \""data owner\"" and the \""data scientist\"" || (who is sometimes referred to as the \""model owner\""). The data owner has data to protect; and the data scientist || wants to answer a question using data owned by one or more data owners. ||  || Note that a data owner could be a consumer who has data on their phone; a hospital with medical records; or || even a Raspberry PI floating in the middle of the Pacific Ocean! It just represents a collection of data || within a Data Owner's domain of ownership. As such; there are three core abstractions you should know about: ||  || * :py:mod:`syft.core.worker.domain.Domain` - this API is the interface to a collection of datasets owned by a single \\ || entity. || * :py:mod:`syft.core.worker.worker.Worker` - this API is the interface to a remote machine within a data owner's \\ || domain. || * :py:mod:`syft.core.worker.client.Client` - This API is the interface a data scientist uses to interact with a worker \\ || within a domain ||  || So; a domain would be something like \""Big Fancy Hospital\"" and a worker would be a single machine within that hospital || which a Data Scientist can use to process some data. A domain will have many workers; each of which could be serving || a different Data Scientist. Alternatively; one Data Scientist could be using multiple workers (such as if they have || very computationally expensive programs to run and they want to run them in parallel). ||  || However; a Data Scientist wouldn't interact with these remote Domain and Worker APIs directly. Instead; they use their || :py:mod:`syft.core.worker.client.Client` api which has lots of convenience functions for interacting with remote || machines held within remote domains. ||  || The :py:mod:`syft.core.worker.client.Client` API will send :py:mod:`syft.core.message.message.Message` objects to the || :py:mod:`syft.core.worker.domain.Domain` API which will handle some messages itself and; if appropriate; forward other || messages to the appropriate :py:mod:`syft.core.worker.worker.Worker` for execution on real objects. ||  || However; you will find that Domain; Worker; and Client all have missing functionality; namely how to send a message! || This might seem like a critical oversight; but it's actually an important abstraction. These classes are merely || interfaces for the API language that all Domain; Worker; and Client objects should use. This language is universal || regardless of whether two phones; two hospitals; or two satellites are talking to each other! However; the exact || transport protocol for _how_ messages are sent we leave up to the _specific instance of a worker. ||  || The primary instance of Domain\/Worker\/Client that we use for development; testing; and learning is the || VirtualDomain\/VirtualWorker\/VirtualClient instance. More on that in a moment... ||  || If you're learning Syft; the next step you should take is to jump into the :py:mod:`syft.core.worker.domain.Domain` || class... ||  ||  || \""\""\""",https://github.com/OpenMined/PySyft/commit/77c09a85b96e995d599cefc289aa04c8935a39ba,Yes
4553,OpenMined/PySyft,src/syft/core/node/common/action/run_class_method_action.py,46989791954f09511ae6cfb833c99bbc44032dff,# TODO: Add tests; this could happen if the isprimitive fails due to an,https://github.com/OpenMined/PySyft/commit/46989791954f09511ae6cfb833c99bbc44032dff,Yes
4554,OpenMined/PySyft,src/syft/lib/torch/allowlist.py,5a8c69273c7b08ada41ee0dd440cc4eb1e11a1a0,TODO here for testing purpose,https://github.com/OpenMined/PySyft/commit/5a8c69273c7b08ada41ee0dd440cc4eb1e11a1a0,Yes
4555,OpenMined/PySyft,tests/syft/core/node/common/action/function_or_constructor_action_test.py,e9903b81627dca360250c86223cfbab7933ab6a1,TODO test execution,https://github.com/OpenMined/PySyft/commit/e9903b81627dca360250c86223cfbab7933ab6a1,Yes
4556,OpenMined/PySyft,tests/syft/core/node/common/action/function_or_constructor_action_test.py,e9903b81627dca360250c86223cfbab7933ab6a1,TODO test permissions,https://github.com/OpenMined/PySyft/commit/e9903b81627dca360250c86223cfbab7933ab6a1,No
4557,OpenMined/PySyft,src/syft/core/common/module.py,5c66e3d20241bdcb2d885c683dc721ca884eaf37,TODO: make work nested with a count and add tests,https://github.com/OpenMined/PySyft/commit/5c66e3d20241bdcb2d885c683dc721ca884eaf37,No
4558,OpenMined/PySyft,tests/syft/grid/connections/webrtc_test.py,8a1ba1118dc7e11ecd675bca4ec58662235cf49d,"FIXME: Nahua is not happy with this test because it \""indirectly\"" triggered exception",https://github.com/OpenMined/PySyft/commit/8a1ba1118dc7e11ecd675bca4ec58662235cf49d,Yes
4559,OpenMined/PySyft,src/syft/core/node/common/action/run_class_method_action.py,b7aa3526779ee6bd335d0af0bfc9bdffe61c5e18,# TODO: Add tests; this could happen if the isprimitive fails due to an,https://github.com/OpenMined/PySyft/commit/b7aa3526779ee6bd335d0af0bfc9bdffe61c5e18,Yes
4560,chainer/chainer,chainer/links/normalization/batch_normalization.py,d518feb1ee563c9ec5dc22d6fb5a1481fe3561b1,(TODO: mkusumoto) Test finetuning in recomputation.,https://github.com/chainer/chainer/commit/d518feb1ee563c9ec5dc22d6fb5a1481fe3561b1,Yes
4561,chainer/chainer,tests/onnx_chainer_tests/functions_tests/test_arrays.py,8382d89d82cfdb2ce4957ea8e16163528b578459,FIXME(syoyo): Currently the test will fail due to the different,https://github.com/chainer/chainer/commit/8382d89d82cfdb2ce4957ea8e16163528b578459,No
4562,allenai/allennlp,allennlp/service/models/simple_tagger.py,6fada919dbf086079ed4d9bd9df77886b9848fca,this is a bad hack to get the same data as the test case,https://github.com/allenai/allennlp/commit/6fada919dbf086079ed4d9bd9df77886b9848fca,Yes
4563,allenai/allennlp,tests/service/server_flask_test.py,6fada919dbf086079ed4d9bd9df77886b9848fca,TODO(joelgrus): write a better test,https://github.com/allenai/allennlp/commit/6fada919dbf086079ed4d9bd9df77886b9848fca,No
4564,allenai/allennlp,tests/modules/seq2seq_encoders/pytorch_seq2seq_wrapper_test.py,5acb5a785b9ed60743e6f687a96bd92dd4e88578,To test the stateful functionality we need to call the encoder multiple times.,https://github.com/allenai/allennlp/commit/5acb5a785b9ed60743e6f687a96bd92dd4e88578,Yes
4565,allenai/allennlp,allennlp/tests/models/sniff_test.py,681a9cfee26b6a0bccb4d4671e64e5fb9f84475e,TODO(Mark) - Fix this test once a new dependency parser is ready.,https://github.com/allenai/allennlp/commit/681a9cfee26b6a0bccb4d4671e64e5fb9f84475e,No
4566,allenai/allennlp,scripts/check_requirements_and_setup.py,64f253faa5aeb42edff1cb367b8e1fbb96da2931,Parse packages only needed for testing.,https://github.com/allenai/allennlp/commit/64f253faa5aeb42edff1cb367b8e1fbb96da2931,Yes
4567,allenai/allennlp,allennlp/tests/training/multi_task_trainer_test.py,e08ade81ff6684869883ce3da66941b9d3a37503,"\""\""\"" || This isn't testing a real class; it's a proof-of-concept || for how multi-task training could work. This is certainly || not the only way to do multi-task training using AllenNLP. ||  || Note that you could almost fit this whole setup into || the \""SingleTaskTrainer\"" paradigm; if you just wrote like a || ``MinglingDatasetReader`` that wrapped multiple dataset readers. || The main problem is that the ``SingleTaskTrainer`` expects || a single ``train_path``. (Even that you could fudge by passing || in a Dict[str; str] serialized as JSON; but that's really hacky.) || \""\""\""",https://github.com/allenai/allennlp/commit/e08ade81ff6684869883ce3da66941b9d3a37503,Yes
4568,allenai/allennlp,tests/common/registrable_test.py,97db5387e846350285c25bf7e39be7b64102baa7,TODO(mattg): maybe move all of these into tests for the base class?,https://github.com/allenai/allennlp/commit/97db5387e846350285c25bf7e39be7b64102baa7,No
4569,allenai/allennlp,allennlp/common/testing/model_test_case.py,5b57be292c7d193419d7c8436441aff907068d5e,TODO: can this be done using pytest.skipif?,https://github.com/allenai/allennlp/commit/5b57be292c7d193419d7c8436441aff907068d5e,Yes
4570,clips/pattern,test/test_web.py,7bba151c8739ca2ccaabdb3b4e012d1314d68288,"XXX should test <td colspan=\""x\""> more thoroughly.",https://github.com/clips/pattern/commit/7bba151c8739ca2ccaabdb3b4e012d1314d68288,No
4571,clips/pattern,pattern/server/cherrypy/test/benchmark.py,5e62f02ecc9dea0add2f81d041df889c67e49ec8,"\""\""\""CherryPy Benchmark Tool ||  ||     Usage: ||         benchmark.py --null --notests --help --cpmodpy --modpython --ab=path --apache=path ||  ||     --null:        use a null Request object (to bench the HTTP server only) ||     --notests:     start the server but do not run the tests; this allows ||                    you to check the tested pages with a browser ||     --help:        show this help message ||     --cpmodpy:     run tests via apache on 54583 (with the builtin _cpmodpy) ||     --modpython:   run tests via apache on 54583 (with modpython_gateway) ||     --ab=path:     Use the ab script\/executable at 'path' (see below) ||     --apache=path: Use the apache script\/exe at 'path' (see below) ||  ||     To run the benchmarks; the Apache Benchmark tool \""ab\"" must either be on ||     your system path; or specified via the --ab=path option. ||  ||     To run the modpython tests; the \""apache\"" executable or script must be ||     on your system path; or provided via the --apache=path option. On some ||     platforms; \""apache\"" may be called \""apachectl\"" or \""apache2ctl\""--create ||     a symlink to them if needed. || \""\""\""",https://github.com/clips/pattern/commit/5e62f02ecc9dea0add2f81d041df889c67e49ec8,Yes
4572,clips/pattern,pattern/server/cherrypy/test/modfastcgi.py,5e62f02ecc9dea0add2f81d041df889c67e49ec8,"\""\""\""Wrapper for mod_fastcgi; for use as a CherryPy HTTP server when testing. ||  || To autostart fastcgi; the \""apache\"" executable or script must be || on your system path; or you must override the global APACHE_PATH. || On some platforms; \""apache\"" may be called \""apachectl\""; \""apache2ctl\""; || or \""httpd\""--create a symlink to them if needed. ||  || You'll also need the WSGIServer from flup.servers. || See http:\/\/projects.amor.org\/misc\/wiki\/ModPythonGateway ||  ||  || KNOWN BUGS || ========== ||  || 1. Apache processes Range headers automatically; CherryPy's truncated ||     output is then truncated again by Apache. See test_core.testRanges. ||     This was worked around in http:\/\/www.cherrypy.org\/changeset\/1319. || 2. Apache does not allow custom HTTP methods like CONNECT as per the spec. ||     See test_core.testHTTPMethods. || 3. Max request header and body settings do not work with Apache. || 4. Apache replaces status \""reason phrases\"" automatically. For example; ||     CherryPy may set \""304 Not modified\"" but Apache will write out ||     \""304 Not Modified\"" (capital \""M\""). || 5. Apache does not allow custom error codes as per the spec. || 6. Apache (or perhaps modpython; or modpython_gateway) unquotes %xx in the ||     Request-URI too early. || 7. mod_python will not read request bodies which use the \""chunked\"" ||     transfer-coding (it passes REQUEST_CHUNKED_ERROR to ap_setup_client_block ||     instead of REQUEST_CHUNKED_DECHUNK; see Apache2's http_protocol.c and ||     mod_python's requestobject.c). || 8. Apache will output a \""Content-Length: 0\"" response header even if there's ||     no response entity body. This isn't really a bug; it just differs from ||     the CherryPy default. || \""\""\""",https://github.com/clips/pattern/commit/5e62f02ecc9dea0add2f81d041df889c67e49ec8,Yes
4573,clips/pattern,pattern/server/cherrypy/test/modfcgid.py,5e62f02ecc9dea0add2f81d041df889c67e49ec8,"\""\""\""Wrapper for mod_fcgid; for use as a CherryPy HTTP server when testing. ||  || To autostart fcgid; the \""apache\"" executable or script must be || on your system path; or you must override the global APACHE_PATH. || On some platforms; \""apache\"" may be called \""apachectl\""; \""apache2ctl\""; || or \""httpd\""--create a symlink to them if needed. ||  || You'll also need the WSGIServer from flup.servers. || See http:\/\/projects.amor.org\/misc\/wiki\/ModPythonGateway ||  ||  || KNOWN BUGS || ========== ||  || 1. Apache processes Range headers automatically; CherryPy's truncated ||     output is then truncated again by Apache. See test_core.testRanges. ||     This was worked around in http:\/\/www.cherrypy.org\/changeset\/1319. || 2. Apache does not allow custom HTTP methods like CONNECT as per the spec. ||     See test_core.testHTTPMethods. || 3. Max request header and body settings do not work with Apache. || 4. Apache replaces status \""reason phrases\"" automatically. For example; ||     CherryPy may set \""304 Not modified\"" but Apache will write out ||     \""304 Not Modified\"" (capital \""M\""). || 5. Apache does not allow custom error codes as per the spec. || 6. Apache (or perhaps modpython; or modpython_gateway) unquotes %xx in the ||     Request-URI too early. || 7. mod_python will not read request bodies which use the \""chunked\"" ||     transfer-coding (it passes REQUEST_CHUNKED_ERROR to ap_setup_client_block ||     instead of REQUEST_CHUNKED_DECHUNK; see Apache2's http_protocol.c and ||     mod_python's requestobject.c). || 8. Apache will output a \""Content-Length: 0\"" response header even if there's ||     no response entity body. This isn't really a bug; it just differs from ||     the CherryPy default. || \""\""\""",https://github.com/clips/pattern/commit/5e62f02ecc9dea0add2f81d041df889c67e49ec8,Yes
4574,clips/pattern,pattern/server/cherrypy/test/modpy.py,5e62f02ecc9dea0add2f81d041df889c67e49ec8,"\""\""\""Wrapper for mod_python; for use as a CherryPy HTTP server when testing. ||  || To autostart modpython; the \""apache\"" executable or script must be || on your system path; or you must override the global APACHE_PATH. || On some platforms; \""apache\"" may be called \""apachectl\"" or \""apache2ctl\""-- || create a symlink to them if needed. ||  || If you wish to test the WSGI interface instead of our _cpmodpy interface; || you also need the 'modpython_gateway' module at: || http:\/\/projects.amor.org\/misc\/wiki\/ModPythonGateway ||  ||  || KNOWN BUGS || ========== ||  || 1. Apache processes Range headers automatically; CherryPy's truncated ||     output is then truncated again by Apache. See test_core.testRanges. ||     This was worked around in http:\/\/www.cherrypy.org\/changeset\/1319. || 2. Apache does not allow custom HTTP methods like CONNECT as per the spec. ||     See test_core.testHTTPMethods. || 3. Max request header and body settings do not work with Apache. || 4. Apache replaces status \""reason phrases\"" automatically. For example; ||     CherryPy may set \""304 Not modified\"" but Apache will write out ||     \""304 Not Modified\"" (capital \""M\""). || 5. Apache does not allow custom error codes as per the spec. || 6. Apache (or perhaps modpython; or modpython_gateway) unquotes %xx in the ||     Request-URI too early. || 7. mod_python will not read request bodies which use the \""chunked\"" ||     transfer-coding (it passes REQUEST_CHUNKED_ERROR to ap_setup_client_block ||     instead of REQUEST_CHUNKED_DECHUNK; see Apache2's http_protocol.c and ||     mod_python's requestobject.c). || 8. Apache will output a \""Content-Length: 0\"" response header even if there's ||     no response entity body. This isn't really a bug; it just differs from ||     the CherryPy default. || \""\""\""",https://github.com/clips/pattern/commit/5e62f02ecc9dea0add2f81d041df889c67e49ec8,Yes
4575,clips/pattern,pattern/server/cherrypy/test/modwsgi.py,5e62f02ecc9dea0add2f81d041df889c67e49ec8,"\""\""\""Wrapper for mod_wsgi; for use as a CherryPy HTTP server. ||  || To autostart modwsgi; the \""apache\"" executable or script must be || on your system path; or you must override the global APACHE_PATH. || On some platforms; \""apache\"" may be called \""apachectl\"" or \""apache2ctl\""-- || create a symlink to them if needed. ||  ||  || KNOWN BUGS || ========== ||  || ##1. Apache processes Range headers automatically; CherryPy's truncated || ##    output is then truncated again by Apache. See test_core.testRanges. || ##    This was worked around in http:\/\/www.cherrypy.org\/changeset\/1319. || 2. Apache does not allow custom HTTP methods like CONNECT as per the spec. ||     See test_core.testHTTPMethods. || 3. Max request header and body settings do not work with Apache. || ##4. Apache replaces status \""reason phrases\"" automatically. For example; || ##    CherryPy may set \""304 Not modified\"" but Apache will write out || ##    \""304 Not Modified\"" (capital \""M\""). || ##5. Apache does not allow custom error codes as per the spec. || ##6. Apache (or perhaps modpython; or modpython_gateway) unquotes %xx in the || ##    Request-URI too early. || 7. mod_wsgi will not read request bodies which use the \""chunked\"" ||     transfer-coding (it passes REQUEST_CHUNKED_ERROR to ap_setup_client_block ||     instead of REQUEST_CHUNKED_DECHUNK; see Apache2's http_protocol.c and ||     mod_python's requestobject.c). || 8. When responding with 204 No Content; mod_wsgi adds a Content-Length ||     header for you. || 9. When an error is raised; mod_wsgi has no facility for printing a ||     traceback as the response content (it's sent to the Apache log instead). || 10. Startup and shutdown of Apache when running mod_wsgi seems slow. || \""\""\""",https://github.com/clips/pattern/commit/5e62f02ecc9dea0add2f81d041df889c67e49ec8,Yes
4576,ray-project/ray,test/runtest.py,2e23ec89853e688b61c5dbc156d85ea1b642b65a,"self.assertEqual(p.wait(); 0; \""argument was not received by the test program\"") # todo: reactivate",https://github.com/ray-project/ray/commit/2e23ec89853e688b61c5dbc156d85ea1b642b65a,No
4577,ray-project/ray,test/runtest.py,bcc59e898d6236565086a8e81ed8428fbeda7232,self.numpyTypeTest('int16') # TODO(pcm): implement this,https://github.com/ray-project/ray/commit/bcc59e898d6236565086a8e81ed8428fbeda7232,Yes
4578,ray-project/ray,test/runtest.py,bcc59e898d6236565086a8e81ed8428fbeda7232,self.numpyTypeTest('int32') # TODO(pcm): implement this,https://github.com/ray-project/ray/commit/bcc59e898d6236565086a8e81ed8428fbeda7232,No
4579,ray-project/ray,test/runtest.py,c27e6c076cf3fc8e5defc376d2be0ed732082139,This is a hack to make the test run.,https://github.com/ray-project/ray/commit/c27e6c076cf3fc8e5defc376d2be0ed732082139,Yes
4580,ray-project/ray,test/runtest.py,362ffa1f3ce7be66d2977a9e8bc70ee95171247c,This is a hack to make the test run.,https://github.com/ray-project/ray/commit/362ffa1f3ce7be66d2977a9e8bc70ee95171247c,Yes
4581,ray-project/ray,python/ray/dataframe/test/test_dataframe.py,2b747ba46cee4445c8ff1640a2a5d7c88e2d1723,"\""\""\"" || TODO: Use this when Arrow issue resolves: || (https:\/\/issues.apache.org\/jira\/browse\/ARROW-2122) || @pytest.fixture || def test_fillna_datetime_columns(num_partitions=2): ||     # GH 7095 ||     df = pd.DataFrame({'A': [-1; -2; np.nan]; ||                        'B': date_range('20130101'; periods=3); ||                        'C': ['foo'; 'bar'; None]; ||                        'D': ['foo2'; 'bar2'; None]}; ||                       index=date_range('20130110'; periods=3)) ||     ray_df = rdf.from_pandas(df; num_partitions) ||     assert ray_df_equals_pandas( ||         ray_df.fillna('?'); ||         df.fillna('?') ||     ) ||  ||     df = pd.DataFrame({'A': [-1; -2; np.nan]; ||                        'B': [pd.Timestamp('2013-01-01'); ||                              pd.Timestamp('2013-01-02'); pd.NaT]; ||                        'C': ['foo'; 'bar'; None]; ||                        'D': ['foo2'; 'bar2'; None]}; ||                       index=date_range('20130110'; periods=3)) ||     ray_df = rdf.from_pandas(df; num_partitions) ||     assert ray_df_equals_pandas( ||         ray_df.fillna('?'); ||         df.fillna('?') ||     ) || \""\""\""",https://github.com/ray-project/ray/commit/2b747ba46cee4445c8ff1640a2a5d7c88e2d1723,Yes
4582,ray-project/ray,python/ray/dataframe/dataframe.py,558942648447743616b3bf669438893b8aaebaaf,"TODO: write explicit tests for \""short and wide\""",https://github.com/ray-project/ray/commit/558942648447743616b3bf669438893b8aaebaaf,No
4583,ray-project/ray,python/ray/tests/test_logical_graph.py,89ce4c56aadf1676f3a3dcbb3056a6326cb6ef49,TODO (john): Add simple wordcount test,https://github.com/ray-project/ray/commit/89ce4c56aadf1676f3a3dcbb3056a6326cb6ef49,Yes
4584,ray-project/ray,python/ray/tune/tests/test_ray_trial_executor.py,6630a35353c6bb722633180e728dd7ff3cd9d4d0,Needed for flaky tests,https://github.com/ray-project/ray/commit/6630a35353c6bb722633180e728dd7ff3cd9d4d0,No
4585,ray-project/ray,python/ray/tune/suggest/bayesopt.py,51dae23d5c8e81449bf381427fcb246c821a0196,Python 3 only -- needed for lint test.,https://github.com/ray-project/ray/commit/51dae23d5c8e81449bf381427fcb246c821a0196,Yes
4586,ray-project/ray,python/ray/tune/tests/test_cluster.py,1558307ac4e66d77d2ee92514b45311046fa93c6,TODO(ujvl): Fix test.,https://github.com/ray-project/ray/commit/1558307ac4e66d77d2ee92514b45311046fa93c6,Yes
4587,ray-project/ray,python/ray/cloudpickle/cloudpickle.py,f6883bf725caca57bcebcc94aeb93f6a94542984,XXX: can this ever happen in Python 3? If so add a test.,https://github.com/ray-project/ray/commit/f6883bf725caca57bcebcc94aeb93f6a94542984,Yes
4588,ray-project/ray,python/ray/cloudpickle/cloudpickle.py,fb1c1e2d27348b139bf1b686a1e81b94049d5190,XXX: can this ever happen in Python 3? If so add a test.,https://github.com/ray-project/ray/commit/fb1c1e2d27348b139bf1b686a1e81b94049d5190,Yes
4589,ray-project/ray,python/ray/tune/suggest/dragonfly.py,89ec4adb726aafd70026ac3ffdb21cadcde36278,Python 3 only -- needed for lint test.,https://github.com/ray-project/ray/commit/89ec4adb726aafd70026ac3ffdb21cadcde36278,Yes
4590,ray-project/ray,rllib/models/tests/test_distributions.py,e9ee5c4e5f27c5fa547f1c5c504b9efebef21026,Fix std inputs (shouldn't be too large for this test).,https://github.com/ray-project/ray/commit/e9ee5c4e5f27c5fa547f1c5c504b9efebef21026,Yes
4591,ray-project/ray,python/ray/cloudpickle/cloudpickle.py,ebea5c41110303523045f46a0f08bd03101b51d5,XXX: can this ever happen in Python 3? If so add a test.,https://github.com/ray-project/ray/commit/ebea5c41110303523045f46a0f08bd03101b51d5,Yes
4592,ray-project/ray,rllib/agents/dyna/dyna.py,14405b90d5457863d71168c613b4961d34f19cc5,TODO: (sven) Use random for testing purposes for now.,https://github.com/ray-project/ray/commit/14405b90d5457863d71168c613b4961d34f19cc5,No
4593,ray-project/ray,rllib/utils/exploration/curiosity.py,1826b29757dfb7b6ce0ddd6f6478d960dfe7efdc,TODO: (tanay) how to test if action space is discrete,https://github.com/ray-project/ray/commit/1826b29757dfb7b6ce0ddd6f6478d960dfe7efdc,Yes
4594,ray-project/ray,python/ray/tune/tests/ext_pytorch.py,efa1d51aeadeb23491d0159c588c31fd85e85eae,"\""\""\"" || Hyperparameter tuning with Ray Tune || =================================== ||  || Hyperparameter tuning can make the difference between an average model and a highly || accurate one. Often simple things like choosing a different learning rate or changing || a network layer size can have a dramatic impact on your model performance. ||  || Fortunately; there are tools that help with finding the best combination of parameters. || `Ray Tune <https:\/\/docs.ray.io\/en\/latest\/tune.html>`_ is an industry standard tool for || distributed hyperparameter tuning. Ray Tune includes the latest hyperparameter search || algorithms; integrates with TensorBoard and other analysis libraries; and natively || supports distributed training through `Ray's distributed machine learning engine || <https:\/\/ray.io\/>`_. ||  || In this tutorial; we will show you how to integrate Ray Tune into your PyTorch || training workflow. We will extend `this tutorial from the PyTorch documentation || <https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html>`_ for training || a CIFAR10 image classifier. ||  || As you will see; we only need to add some slight modifications. In particular; we || need to ||  || 1. wrap data loading and training in functions; || 2. make some network parameters configurable; || 3. add checkpointing (optional); || 4. and define the search space for the model tuning ||  || | ||  || To run this tutorial; please make sure the following packages are || installed: ||  || -  ``ray[tune]``: Distributed hyperparameter tuning library || -  ``torchvision``: For the data transformers ||  || Setup \/ Imports || --------------- || Let's start with the imports: || \""\""\""",https://github.com/ray-project/ray/commit/efa1d51aeadeb23491d0159c588c31fd85e85eae,Yes
4595,ray-project/ray,python/ray/tune/tests/ext_pytorch.py,efa1d51aeadeb23491d0159c588c31fd85e85eae,with which we iterate through the training and test sets are configurable as well.,https://github.com/ray-project/ray/commit/efa1d51aeadeb23491d0159c588c31fd85e85eae,Yes
4596,ray-project/ray,python/ray/cloudpickle/cloudpickle.py,ddd62a177fb2cb82a3dc267705fa01ed6e7decc6,XXX: can this ever happen in Python 3? If so add a test.,https://github.com/ray-project/ray/commit/ddd62a177fb2cb82a3dc267705fa01ed6e7decc6,Yes
4597,ray-project/ray,python/ray/services.py,5cbc411e387469dbafd9a8046337ad87ae31bf26,See `cluster_mode_test.cc` for why this workaround is currently needed,https://github.com/ray-project/ray/commit/5cbc411e387469dbafd9a8046337ad87ae31bf26,Yes
4598,ray-project/ray,python/ray/services.py,390107b6cbe1550defdcd86d4dbf945cadd43372,See `cluster_mode_test.cc` for why this workaround is currently needed,https://github.com/ray-project/ray/commit/390107b6cbe1550defdcd86d4dbf945cadd43372,Yes
4599,ray-project/ray,python/ray/_private/services.py,609c1b8acdb9696989757620b5efa34bc958cff8,See `cluster_mode_test.cc` for why this workaround is currently needed,https://github.com/ray-project/ray/commit/609c1b8acdb9696989757620b5efa34bc958cff8,Yes
4600,ray-project/ray,python/ray/tune/suggest/hebo.py,58d7398246726a6e1752b9ad0486355efa839448,Python 3 only -- needed for lint test.,https://github.com/ray-project/ray/commit/58d7398246726a6e1752b9ad0486355efa839448,Yes
4601,ray-project/ray,python/ray/tests/test_scheduling.py,5c6c9d5b91d37162f159bb3a3df1aa6864f56e2e,Needed because the above test sets this to False.,https://github.com/ray-project/ray/commit/5c6c9d5b91d37162f159bb3a3df1aa6864f56e2e,Yes
4602,ray-project/ray,rllib/examples/env/tests/test_coin_game_non_vectorized_env.py,9a7fbd3cdf73bb760ab631be05c0de80f1e32c3c,TODO add tests for grid_size != 3,https://github.com/ray-project/ray/commit/9a7fbd3cdf73bb760ab631be05c0de80f1e32c3c,No
4603,ray-project/ray,rllib/examples/env/tests/test_coin_game_vectorized_env.py,9a7fbd3cdf73bb760ab631be05c0de80f1e32c3c,TODO add tests for grid_size != 3,https://github.com/ray-project/ray/commit/9a7fbd3cdf73bb760ab631be05c0de80f1e32c3c,No
4604,OpenMined/PySyft,tests/syft/core/pointer/pointer_test.py,f03078ef8acc3f23cbf106a0814e794787865246,TODO: Fix this test,https://github.com/OpenMined/PySyft/commit/f03078ef8acc3f23cbf106a0814e794787865246,Yes
4605,OpenMined/PySyft,tests/syft/core/pointer/pointer_test.py,5b3d3c58e40eac23f096604ead9a472745f8988e,TODO: Fix this test,https://github.com/OpenMined/PySyft/commit/5b3d3c58e40eac23f096604ead9a472745f8988e,Yes
4606,OpenMined/PySyft,tests/syft/core/pointer/pointer_test.py,02c2fc357a72bf6238546d4e8e233ef281b83786,TODO: Fix this test,https://github.com/OpenMined/PySyft/commit/02c2fc357a72bf6238546d4e8e233ef281b83786,Yes
4607,OpenMined/PySyft,tests/syft/core/pointer/pointer_test.py,4795f2d8dca7ce8b59f4b0b364d472047ea8e3f4,TODO: Fix this test,https://github.com/OpenMined/PySyft/commit/4795f2d8dca7ce8b59f4b0b364d472047ea8e3f4,Yes
4608,OpenMined/PySyft,tests/syft/lib/python/dict/dict_test.py,14360cd7773b7eb40b77f1cd39fb386e3118c19e,test fix for seg fault reported in bpo-27945 part 3.,https://github.com/OpenMined/PySyft/commit/14360cd7773b7eb40b77f1cd39fb386e3118c19e,No
4609,OpenMined/PySyft,tests/syft/lib/python/dict/dict_test.py,14360cd7773b7eb40b77f1cd39fb386e3118c19e,test fix for seg fault reported in bpo-38588 part 1.,https://github.com/OpenMined/PySyft/commit/14360cd7773b7eb40b77f1cd39fb386e3118c19e,No
4610,OpenMined/PySyft,tests/syft/lib/python/dict/dict_test.py,04bd42c7ae71576f5896a9808346aa5fb533346d,test fix for seg fault reported in bpo-27945 part 3.,https://github.com/OpenMined/PySyft/commit/04bd42c7ae71576f5896a9808346aa5fb533346d,No
4611,OpenMined/PySyft,tests/syft/lib/python/dict/dict_test.py,04bd42c7ae71576f5896a9808346aa5fb533346d,test fix for seg fault reported in bpo-38588 part 1.,https://github.com/OpenMined/PySyft/commit/04bd42c7ae71576f5896a9808346aa5fb533346d,No
4612,OpenMined/PySyft,tests/syft/lib/python/dict/dict_test.py,4de851e5f21bbebd109cec91a03bb386b7a5e85c,test fix for seg fault reported in bpo-27945 part 3.,https://github.com/OpenMined/PySyft/commit/4de851e5f21bbebd109cec91a03bb386b7a5e85c,No
4613,OpenMined/PySyft,tests/syft/lib/python/dict/dict_test.py,4de851e5f21bbebd109cec91a03bb386b7a5e85c,test fix for seg fault reported in bpo-38588 part 1.,https://github.com/OpenMined/PySyft/commit/4de851e5f21bbebd109cec91a03bb386b7a5e85c,No
4614,OpenMined/PySyft,tests/syft/lib/python/dict/dict_test.py,5dd8e23930f4f080ccb6e4ae60ea7ddc485b06be,test fix for seg fault reported in bpo-27945 part 3.,https://github.com/OpenMined/PySyft/commit/5dd8e23930f4f080ccb6e4ae60ea7ddc485b06be,No
4615,OpenMined/PySyft,tests/syft/lib/python/dict/dict_test.py,5dd8e23930f4f080ccb6e4ae60ea7ddc485b06be,test fix for seg fault reported in bpo-38588 part 1.,https://github.com/OpenMined/PySyft/commit/5dd8e23930f4f080ccb6e4ae60ea7ddc485b06be,No
4616,NervanaSystems/neon,neon/params/tests/test_val_init.py,3f90773a6694aaf472b1ad84e02dece87cf1df48,TODO: test distribution of vals,https://github.com/NervanaSystems/neon/commit/3f90773a6694aaf472b1ad84e02dece87cf1df48,Yes
4617,NervanaSystems/neon,neon/backends/tests/test_autodiff.py,3497dcc8bb6200f2ce073b61bd6e5ad599f23141,TODO: stricter test to fix numerical issues,https://github.com/NervanaSystems/neon/commit/3497dcc8bb6200f2ce073b61bd6e5ad599f23141,Yes
4618,NervanaSystems/neon,neon/data/loader/test/compare.py,68b16057440aee8b5e1085f07229a832452a8e64,"\""\""\"" || Verify that different ways of loading datasets lead to the same result. ||  || This test utility accepts the same command line parameters as neon. It || downloads the CIFAR-10 dataset and saves it as individual JPEG files. It then || proceeds to fit and evaluate a model using two different ways of loading the || data. Macrobatches are written to disk as needed. ||  || \""\""\""",https://github.com/NervanaSystems/neon/commit/68b16057440aee8b5e1085f07229a832452a8e64,Yes
4619,NervanaSystems/neon,neon/data/loader/test/compare.py,bfb79b0231e6ab6c84516d0e33bb13f1b5c35de7,TODO: test macrobatch reading.,https://github.com/NervanaSystems/neon/commit/bfb79b0231e6ab6c84516d0e33bb13f1b5c35de7,Yes
4620,NervanaSystems/neon,tests/test_mergebroadcast_layer.py,f0b8daf4795c6c00c3778db8c933e21441ceada6,TODO switch to pytest.skip() once verified to be expected failure,https://github.com/NervanaSystems/neon/commit/f0b8daf4795c6c00c3778db8c933e21441ceada6,Yes
4621,NervanaSystems/neon,tests/test_backend_autodiff.py,fea87bc2574a3e11c2ef119d435dea951e9fb0ab,TODO: stricter test to fix numerical issues,https://github.com/NervanaSystems/neon/commit/fea87bc2574a3e11c2ef119d435dea951e9fb0ab,Yes
4622,cleverhans-lab/cleverhans,tests_tf/test_attacks.py,bf04c116c9e73cceacbaaf8e4c09a877e63c1fb3,nosetests apparently runs all functions in a class; not just,https://github.com/cleverhans-lab/cleverhans/commit/bf04c116c9e73cceacbaaf8e4c09a877e63c1fb3,Yes
4623,cleverhans-lab/cleverhans,cleverhans/augmentation.py,ff6a13ebe767a70bfd1cbe79ddd7fb1ae74ee9dc,"\""\""\"" || Dataset augmentation functionality ||  || NOTE: This module is much more free to change than many other modules || in CleverHans. CleverHans is very conservative about changes to any || code that affects the output of benchmark tests (attacks; evaluation || methods; etc.). This module provides *dataset augmentation* code for || building models to be benchmarked; not *benchmarks;* and || thus is free to change rapidly to provide better speed; accuracy; || etc. || \""\""\""",https://github.com/cleverhans-lab/cleverhans/commit/ff6a13ebe767a70bfd1cbe79ddd7fb1ae74ee9dc,Yes
4624,cleverhans-lab/cleverhans,cleverhans_tutorials/mnist_tutorial_keras.py,b3322827f0097738582007d10498ccec4780be4f,TODO: Add tests,https://github.com/cleverhans-lab/cleverhans/commit/b3322827f0097738582007d10498ccec4780be4f,No
4625,cleverhans-lab/cleverhans,tests_pytorch/test_attacks.py,c89025c05adee5cacfe379747ecaf4111a6622a0,NOTE this has been tested with the optimize_linear function in test_utils; so duplicate tests are not needed here.,https://github.com/cleverhans-lab/cleverhans/commit/c89025c05adee5cacfe379747ecaf4111a6622a0,Yes
4626,cleverhans-lab/cleverhans,tests_pytorch/test_attacks.py,8cc2345931fd07b32b7e4bb7e8f88b62d2224117,test_utils; so duplicate tests are not needed here.,https://github.com/cleverhans-lab/cleverhans/commit/8cc2345931fd07b32b7e4bb7e8f88b62d2224117,Yes
4627,cleverhans-lab/cleverhans,tests_pytorch/test_attacks.py,8cc2345931fd07b32b7e4bb7e8f88b62d2224117,TODO not sure what the original test does in tests_tf\/test_attacks,https://github.com/cleverhans-lab/cleverhans/commit/8cc2345931fd07b32b7e4bb7e8f88b62d2224117,No
4628,cleverhans-lab/cleverhans,cleverhans/future/torch/tests/test_utils.py,f97a4fd9cd309ca90a1a013a75ef4dbf016c41ab,TODO uncomment the actual test below after we have implemented the L1 attack,https://github.com/cleverhans-lab/cleverhans/commit/f97a4fd9cd309ca90a1a013a75ef4dbf016c41ab,Yes
4629,cleverhans-lab/cleverhans,tests_pytorch/test_attacks.py,1817d397b5480323a257bdab4115b4450a056c73,test_utils; so duplicate tests are not needed here.,https://github.com/cleverhans-lab/cleverhans/commit/1817d397b5480323a257bdab4115b4450a056c73,Yes
4630,cleverhans-lab/cleverhans,tests_pytorch/test_attacks.py,1817d397b5480323a257bdab4115b4450a056c73,TODO not sure what the original test does in tests_tf\/test_attacks,https://github.com/cleverhans-lab/cleverhans/commit/1817d397b5480323a257bdab4115b4450a056c73,No
4631,cleverhans-lab/cleverhans,cleverhans/future/torch/tests/test_utils.py,5a206122a80fc24054608ab4f5110531d4e528e3,TODO uncomment the actual test below after we have implemented the L1 attack,https://github.com/cleverhans-lab/cleverhans/commit/5a206122a80fc24054608ab4f5110531d4e528e3,Yes
4632,cleverhans-lab/cleverhans,cleverhans_v3.1.0/cleverhans/augmentation.py,1616e8d895da97d31e8ce6350be75a61007238bf,"\""\""\"" || Dataset augmentation functionality ||  || NOTE: This module is much more free to change than many other modules || in CleverHans. CleverHans is very conservative about changes to any || code that affects the output of benchmark tests (attacks; evaluation || methods; etc.). This module provides *dataset augmentation* code for || building models to be benchmarked; not *benchmarks;* and || thus is free to change rapidly to provide better speed; accuracy; || etc. || \""\""\""",https://github.com/cleverhans-lab/cleverhans/commit/1616e8d895da97d31e8ce6350be75a61007238bf,Yes
4633,cleverhans-lab/cleverhans,cleverhans_v3.1.0/scripts/make_confidence_report_spsa.py,1616e8d895da97d31e8ce6350be75a61007238bf,"\""\""\"" || make_confidence_report.py || Usage: ||   python make_confidence_report_spsa.py model.joblib ||  ||   where model.joblib is a file created by cleverhans.serial.save containing ||   a picklable cleverhans.model.Model instance. ||  || This script will run the model on a variety of types of data and save a || ConfidenceReport to model_report.joblib. || The report can be later loaded by another script using cleverhans.serial.load. || This script puts the following entries in the report: ||   clean : Clean data ||   mc: MaxConfidence SPSA adversarial examples ||  || This script works by running a single MaxConfidence attack on each example. || ( https:\/\/openreview.net\/forum?id=H1g0piA9tQ ) || The MaxConfidence attack uses the SPSA optimizer. || This is not intended to be a generic strong attack; rather it is intended || to be a test for gradient masking. || \""\""\""",https://github.com/cleverhans-lab/cleverhans/commit/1616e8d895da97d31e8ce6350be75a61007238bf,No
4634,chainer/chainer,chainer/initializations.py,206fa3cfc62acaa634dd791c9910e575d38baa63,check needed for bilinear tests to pass,https://github.com/chainer/chainer/commit/206fa3cfc62acaa634dd791c9910e575d38baa63,No
4635,chainer/chainer,chainer/initializers/__init__.py,e7fca819b7d420bf97cf681c7f2ed05b29d9f325,check needed for bilinear tests to pass,https://github.com/chainer/chainer/commit/e7fca819b7d420bf97cf681c7f2ed05b29d9f325,No
4636,chainer/chainer,tests/chainer_tests/links_tests/loss_tests/test_negative_sampling.py,ec3b1ea4b6bf8f45ea8dea2a9559b6262678abb7,Fix this test. The code seems to fix the samples in order to avoid,https://github.com/chainer/chainer/commit/ec3b1ea4b6bf8f45ea8dea2a9559b6262678abb7,Yes
4637,chainer/chainer,chainer/graph_optimimzations/static_graph.py,4b9cbf779b6918b8f5710aeb7965ede6e9eb1864,todo: add test that use the same random seed with two models: a static chain,https://github.com/chainer/chainer/commit/4b9cbf779b6918b8f5710aeb7965ede6e9eb1864,Yes
4638,chainer/chainer,tests/chainer_tests/optimizers_tests/test_optimizers_by_linear_model.py,fe436e0ec837af5cdbdfd399bb5c2c66d005e18c,TODO(niboshi): This is temporary workaround for skipping test not working,https://github.com/chainer/chainer/commit/fe436e0ec837af5cdbdfd399bb5c2c66d005e18c,Yes
4639,chainer/chainer,chainer/graph_optimizations/static_graph.py,3b8b5a03e41b1620d30488a467b0156ee06b2f91,todo: move code below into Chainer tests after initial debug.,https://github.com/chainer/chainer/commit/3b8b5a03e41b1620d30488a467b0156ee06b2f91,Yes
4640,chainer/chainer,tests/python_tests/testing_tests/test_helper.py,679a7e5bb8e322e0e9eced73937f639b8b167c9b,TODO(niboshi): Currently this test passes. Implement stride check and uncomment xfail.,https://github.com/chainer/chainer/commit/679a7e5bb8e322e0e9eced73937f639b8b167c9b,Yes
4641,chainer/chainer,tests/python_tests/testing_tests/test_helper.py,a33fb2a5c74f32bf18799cb970549facaa6abea4,TODO(niboshi): Currently this test fails. Fix it.,https://github.com/chainer/chainer/commit/a33fb2a5c74f32bf18799cb970549facaa6abea4,No
4642,chainer/chainer,chainer/graph_optimizations/static_graph.py,b446c363ae033dba840d62c959d9e27862adcf6b,todo: add test case for an example where the following,https://github.com/chainer/chainer/commit/b446c363ae033dba840d62c959d9e27862adcf6b,No
4643,chainer/chainer,chainerx/python/chainerx/__init__.py,37aae99064f0fbaf5794a51bd2413ec3ff07b5eb,`testing` needs to be imported before `_core`; because importing `_core` would populate sys.modules['chainerx.testing'].,https://github.com/chainer/chainer/commit/37aae99064f0fbaf5794a51bd2413ec3ff07b5eb,No
4644,chainer/chainer,tests/chainer_tests/links_tests/loss_tests/test_negative_sampling.py,8eacd20e35f31f124da36759953a21b29871e72f,Fix this test. The code seems to fix the samples in order to avoid,https://github.com/chainer/chainer/commit/8eacd20e35f31f124da36759953a21b29871e72f,Yes
4645,chainer/chainer,tests/chainer_tests/links_tests/loss_tests/test_negative_sampling.py,d16abee4b4272ccfdbaa7b846cd39d365f8c42be,Fix this test. The code seems to fix the samples in order to avoid,https://github.com/chainer/chainer/commit/d16abee4b4272ccfdbaa7b846cd39d365f8c42be,Yes
4646,chainer/chainer,tests/chainer_tests/links_tests/loss_tests/test_negative_sampling.py,401040aa701eaf2466a3d5f2bd4ed8236710759f,Fix this test. The code seems to fix the samples in order to avoid,https://github.com/chainer/chainer/commit/401040aa701eaf2466a3d5f2bd4ed8236710759f,Yes
4647,chainer/chainer,tests/chainer_tests/links_tests/loss_tests/test_negative_sampling.py,302686e849f68c2106398fabb54c7217b93fb7e1,Fix this test. The code seems to fix the samples in order to avoid,https://github.com/chainer/chainer/commit/302686e849f68c2106398fabb54c7217b93fb7e1,Yes
4648,mozilla/bugbug,run.py,a2533876feb2dc5065959b6917af93465d5eb3cf,TODO: Perform lemmatization and tokenization via Spacy instead of scikit-learn,https://github.com/mozilla/bugbug/commit/a2533876feb2dc5065959b6917af93465d5eb3cf,Yes
4649,lovit/soynlp,soynlp/lemmatizer/_predicate.py,f50ea51407e9996e54d935b79eb0ae9ba6dcba91,TODO: evaluation lemma of (stem; eomi),https://github.com/lovit/soynlp/commit/f50ea51407e9996e54d935b79eb0ae9ba6dcba91,Yes
4650,lovit/soynlp,soynlp/predicator/_eomi.py,24ff114ddb663daf5fad2d110c2ef6e7a243b586,TODO with lemma,https://github.com/lovit/soynlp/commit/24ff114ddb663daf5fad2d110c2ef6e7a243b586,No
4651,alvations/pywsd,lesk.py,970ed2b288d3d82f880ab8d4ae63c233211d8b3d,"#TODO: various stem / lemmatizers.
from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
def stem(word, option=""wnlemma"")
  if option == ""wnlemma"":
    return wnl.lemmatize(word)
  if option == ""porter"":
    return porter.stem(word)",https://github.com/alvations/pywsd/commit/970ed2b288d3d82f880ab8d4ae63c233211d8b3d,Yes
4652,alvations/pywsd,pywsd/utils.py,802c094aa3b0b97f9fa8d32d8f9379d5cb0329e3,"#TODO: various stem / lemmatizers.
from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
def stem(word, option=""wnlemma"")
  if option == ""wnlemma"":
    return wnl.lemmatize(word)
  if option == ""porter"":
    return porter.stem(word)",https://github.com/alvations/pywsd/commit/802c094aa3b0b97f9fa8d32d8f9379d5cb0329e3,Yes
4653,nert-nlp/streusle,sst2conllulex.py,b94ef47c8e33046a54de9e3a4981df9da8c7c7ac,"TODO: if data[\""lemmas\""]: assert data[\""lemmas\""][offset0] == lemma",https://github.com/nert-nlp/streusle/commit/b94ef47c8e33046a54de9e3a4981df9da8c7c7ac,Yes
4654,quadflor/Quadflor,Code/tests/test_integration.py,ac8b358a9a99f5eb91ce61b6085a90897d9dec8b,TODO: consider: this is removed by lemmatization.,https://github.com/quadflor/Quadflor/commit/ac8b358a9a99f5eb91ce61b6085a90897d9dec8b,Yes
4655,summanlp/textrank,textrank/textrank_word.py,154d7a7f63025c573fef9cd875e05b7f260f8233,"TODO: Aca se puede cambiar; para que la ventana se \""calcule\"" respecto de las palabras ya lemmatizadas",https://github.com/summanlp/textrank/commit/154d7a7f63025c573fef9cd875e05b7f260f8233,Yes
4656,cltk/cltk,cltk/corpus/readers.py,320e810184204d9171e683241adea4c5c1c73a04,TODO and add:  ['latin_text_perseus'; 'latin_treebank_perseus'; 'latin_text_latin_library'; 'phi5'; 'phi7'; 'latin_proper_names_cltk'; 'latin_models_cltk'; 'latin_pos_lemmata_cltk'; 'latin_treebank_index_thomisticus'; 'latin_lexica_perseus'; 'latin_training_set_sentence_cltk'; 'latin_word2vec_cltk'; 'latin_text_antique_digiliblt'; 'latin_text_corpus_grammaticorum_latinorum'; 'latin_text_poeti_ditalia'],https://github.com/cltk/cltk/commit/320e810184204d9171e683241adea4c5c1c73a04,Yes
4657,cltk/cltk,src/cltkv1/wordnet/processes.py,ac4a80caa7b438460381a22975cf9c96059b42f4,TODO: map CLTK lemmas to WN lemmas,https://github.com/cltk/cltk/commit/ac4a80caa7b438460381a22975cf9c96059b42f4,Yes
4658,Rostlab/nalaf,nalaf/download_data.py,13bf86d5973aa24fc75f93b73571308700bc94c3,TODO download non-packaged [biolemmatizer-core-1.2-jar-with-dependencies.jar](https:\/\/github.com\/Rostlab\/nalaf\/blob\/develop\/nalaf\/data\/biolemmatizer-core-1.2-jar-with-dependencies.jar),https://github.com/Rostlab/nalaf/commit/13bf86d5973aa24fc75f93b73571308700bc94c3,Yes
4659,explosion/spaCy,spacy/language.py,e5d9eaf79c5c935b4553c5ede31de383571fe0dc,TODO: Will be replaced when the lemmatizer becomes a pipeline component,https://github.com/explosion/spaCy/commit/e5d9eaf79c5c935b4553c5ede31de383571fe0dc,Yes
4660,andreasvc/disco-dop,fragmentseeker.py,14137f4a90b12b0b86a21c2fc92db3f007b658c9,FIXME: detect corpus reading errors here (e.g. wrong encoding),https://github.com/andreasvc/disco-dop/commit/14137f4a90b12b0b86a21c2fc92db3f007b658c9,Yes
4661,Erotemic/netharn,netharn/initializers/functional.py,8f77a3a1b79c76ffae839b671acdcfc2479c1a8d,TODO: token encoding scheme where subdirectories,https://github.com/Erotemic/netharn/commit/8f77a3a1b79c76ffae839b671acdcfc2479c1a8d,No
4662,Erotemic/netharn,netharn/initializers/_nx_extensions.py,62c5dd9e126e65d81ea4c5c624aecd3c2276e1ed,TODO: token encoding scheme where subdirectories,https://github.com/Erotemic/netharn/commit/62c5dd9e126e65d81ea4c5c624aecd3c2276e1ed,No
4663,Erotemic/netharn,netharn/initializers/balanced_sequence.py,474678e423d84db6b1684c22c3054aaa0f58ab5e,TODO: token encoding scheme where subdirectories,https://github.com/Erotemic/netharn/commit/474678e423d84db6b1684c22c3054aaa0f58ab5e,No
4664,Erotemic/netharn,netharn/initializers/bseq2.py,474678e423d84db6b1684c22c3054aaa0f58ab5e,TODO: token encoding scheme where subdirectories,https://github.com/Erotemic/netharn/commit/474678e423d84db6b1684c22c3054aaa0f58ab5e,No
4665,bmcfee/pumpp,pumpp/task/base.py,2a382f3ba9059baf3472073559e2f81ef88d79e4,FIXME: support sparse encoding,https://github.com/bmcfee/pumpp/commit/2a382f3ba9059baf3472073559e2f81ef88d79e4,Yes
4666,KMouratidis/EDA_miner,EDA_miner/visualization/maps.py,d0505ee0ec2d00f12832966f9b313f83aa4a9a58,"\""\""\"" || This module handles map plotting. Currently only 3 types of map types are \\ || supported (aggregated choropleth; geo-scatterplot; and lines on map). ||  || Global Variables: ||     - Sidebar: To be used for creating side-menus. ||  || Functions: ||     - Map_Options: Generate the layout of the dashboard. ||     - country2code: It takes a string and tries to convert it to a country \\ ||                     code by trying out various encodings. ||  || Dash callbacks: ||     - render_variable_choices_maps: Create a menu of dcc components for \\ ||                                     the user to choose plotting options. ||     - show_hide_aggregator_dropdown: Disable some dropdowns. Some maps do \\ ||                                      not handle all the fields. ||     - plot_map: Plot the map according to user choices. ||  || TODO: ||     Implement https:\/\/plot.ly\/python\/choropleth-maps\/#choropleth-inset-map || TODO: ||     Add text\/annotations to the various maps. || \""\""\""",https://github.com/KMouratidis/EDA_miner/commit/d0505ee0ec2d00f12832966f9b313f83aa4a9a58,No
4667,sbl-sdsc/mmtf-pyspark,mmtfPyspark/utils/mmtfCodec.py,a473c696f723ac1d8338b42ad09f02ace5bcad85,TODO use np.unique for run-length encoding?,https://github.com/sbl-sdsc/mmtf-pyspark/commit/a473c696f723ac1d8338b42ad09f02ace5bcad85,Yes
4668,IQTLabs/poseidon,plugins/deep_algos/deep_classifier/train_deep_classifier.py,028972ed51b6112f7457a476f2fd36ca9e78a02e,TODO: add character level encoding,https://github.com/IQTLabs/poseidon/commit/028972ed51b6112f7457a476f2fd36ca9e78a02e,Yes
4669,IQTLabs/poseidon,plugins/deep_algos/deep_classifier/train_deep_classifier.py,fe08166910320f0fdb4f9e35e2d48d416e1e1158,TODO: add character-level encoding,https://github.com/IQTLabs/poseidon/commit/fe08166910320f0fdb4f9e35e2d48d416e1e1158,Yes
4670,BindsNET/bindsnet,myprogram/MNIST_RSTDP.py,32d6b401ebc5e1c91cfebdee0af2da3ca6a47c07,TODO First spike encoding.,https://github.com/BindsNET/bindsnet/commit/32d6b401ebc5e1c91cfebdee0af2da3ca6a47c07,Yes
4671,proycon/clam,jobservice/jobservice.py,12cbc6c6ef8f8e5fe9d7fbc360548b8b71dfddde,f = open(JobService.Project.path(project) + '.upload';'w') #TODO: check for problems with character encoding?,https://github.com/proycon/clam/commit/12cbc6c6ef8f8e5fe9d7fbc360548b8b71dfddde,Yes
4672,proycon/clam,jobservice/jobservice.py,4a7b4005fc927d7226512fbb9964f6c4e889b617,TODO: catch encoding errors,https://github.com/proycon/clam/commit/4a7b4005fc927d7226512fbb9964f6c4e889b617,Yes
4673,proycon/clam,clamservice.py,fd79416b841d37182863406144588470be471a77,TODO: check for problems with character encoding?,https://github.com/proycon/clam/commit/fd79416b841d37182863406144588470be471a77,Yes
4674,proycon/clam,clamservice.py,f619bb09219e0c9d9b1834651e43beb4b2778383,"o += \"" format=\\\""\""+inputformat.__class__.__name__+\""\\\"" formatlabel=\\\""\""+inputformat.name+\""\\\"" encoding=\\\""\""+inputformat.encoding+\""\\\""\""; #TODO: output nice format labels?",https://github.com/proycon/clam/commit/f619bb09219e0c9d9b1834651e43beb4b2778383,Yes
4675,pln-fing-udelar/humor,extraction/py2_download_output_redownload.py,fa494e4afc51cdb2df2380bb8f96cec9efa3ea9e,TODO: only re-download the ones with encoding issues. Don't touch the rest.,https://github.com/pln-fing-udelar/humor/commit/fa494e4afc51cdb2df2380bb8f96cec9efa3ea9e,Yes
4676,hachmannlab/chemml,chemml/chem/local_features.py,864cc4a1e813f53d2bb97cfc3637a0caa1f50d13,TODO: Arguments for sparse vector encoding,https://github.com/hachmannlab/chemml/commit/864cc4a1e813f53d2bb97cfc3637a0caa1f50d13,Yes
4677,IBM/mi-prometheus,models/stacked_attention_vqa/model.py,b6c1b4ecea1205f963f43e8738b7d7402199b88a,TODO: use `image_encoding_channels` when calling class ImageEncoding,https://github.com/IBM/mi-prometheus/commit/b6c1b4ecea1205f963f43e8738b7d7402199b88a,Yes
4678,IBM/mi-prometheus,models/stacked_attention_vqa/model.py,e6f0ea9f920076886990f7b3bd35b6b3f47a5b20,TODO: use `image_encoding_channels` when calling class ImageEncoding,https://github.com/IBM/mi-prometheus/commit/e6f0ea9f920076886990f7b3bd35b6b3f47a5b20,Yes
4679,OpenNMT/OpenNMT-py,onmt/Models.py,26421ce20c6b626ceacafbb3282cad1d5dce04ca,todo: re-add positional encodings,https://github.com/OpenNMT/OpenNMT-py/commit/26421ce20c6b626ceacafbb3282cad1d5dce04ca,Yes
4680,h2oai/h2o4gpu,tests_sklearn/test_glm_sklearn.py,79f9bc346f9ba59c57c635bd277e9590983ba4c5,TODO: Should write this to file and avoid doing encoding if already exists,https://github.com/h2oai/h2o4gpu/commit/79f9bc346f9ba59c57c635bd277e9590983ba4c5,Yes
4681,cltk/cltk,cltk/corpus/classical_greek/beta_to_unicode.py,ef0626e75ac2624ede959cadee6ee40d3922334f,"\""\""\""Converts legacy encodings into Unicode || TODO for replacer.py: ||  - add perseus-style iota subscript and diaeresis || \""\""\""",https://github.com/cltk/cltk/commit/ef0626e75ac2624ede959cadee6ee40d3922334f,Yes
4682,cltk/cltk,cltk/corpus_api/greek/beta_to_unicode.py,69707bc656842fd90579936275d2de7b91050776,"\""\""\""Converts legacy encodings into Unicode || TODO for replacer.py: ||  - add perseus-style iota subscript and diaeresis || \""\""\""",https://github.com/cltk/cltk/commit/69707bc656842fd90579936275d2de7b91050776,Yes
4683,cltk/cltk,src/cltkv1/alphabet/grc/beta_to_unicode.py,3d602feb7ddbb9881ee362ff4898263e05647e70,"\""\""\""Converts legacy encodings into Unicode. ||  || TODO: Rm regex dependency || TODO: Add tests || \""\""\""",https://github.com/cltk/cltk/commit/3d602feb7ddbb9881ee362ff4898263e05647e70,Yes
4684,fastnlp/fastNLP,fastNLP/modules/decoder/seq2seq_decoder.py,15360e9724884e26ee76ae3933bd7e43f2a84fb9,get_sinusoid_encoding_table  # todo: \u5E94\u8BE5\u5C06position embedding\u79FB\u5230core,https://github.com/fastnlp/fastNLP/commit/15360e9724884e26ee76ae3933bd7e43f2a84fb9,Yes
4685,fastnlp/fastNLP,fastNLP/modules/decoder/seq2seq_decoder.py,b95aa56afb94a7c98ce362c0229441081351b4f2,get_sinusoid_encoding_table  # todo: \u5E94\u8BE5\u5C06position embedding\u79FB\u5230core,https://github.com/fastnlp/fastNLP/commit/b95aa56afb94a7c98ce362c0229441081351b4f2,Yes
4686,comic/grand-challenge.org,app/evaluation/models.py,bf582c839ec6ebe5e981816c4ade00c426a5c422,TODO: Check if the encoding method is included in the manifest,https://github.com/comic/grand-challenge.org/commit/bf582c839ec6ebe5e981816c4ade00c426a5c422,Yes
4687,comic/grand-challenge.org,app/evaluation/models.py,22111960b28aeb2f5670c564333e7871191da8e2,TODO: Check if the encoding method is included in the manifest,https://github.com/comic/grand-challenge.org/commit/22111960b28aeb2f5670c564333e7871191da8e2,Yes
4688,openml/automlbenchmark,automl/frameworks/RandomForest/exec.py,3032c7f807b8fb0736e4b9f9226fd2f81050f74c,TODO: If auto-sklearn & TPOT also require imputation & dummy encoding; let's move this to common_code,https://github.com/openml/automlbenchmark/commit/3032c7f807b8fb0736e4b9f9226fd2f81050f74c,Yes
4689,graknlabs/kglib,grakn_graphsage/src/encoders/encoders.py,ce799b5a59f5132d03ca8230c61a64a1a386e3df,TODO One-hot encoding of either type labels or of a tensor of type ids (some class renaming required in the,https://github.com/graknlabs/kglib/commit/ce799b5a59f5132d03ca8230c61a64a1a386e3df,Yes
4690,materialsvirtuallab/megnet,megnet/data/molecule.py,bc0f4c3483dd6d00693922c16335f56aa46e08dc,TODO (wardlt): One-hot encoding for the elements,https://github.com/materialsvirtuallab/megnet/commit/bc0f4c3483dd6d00693922c16335f56aa46e08dc,Yes
4691,mideind/GreynirPackage,src/reynir/bincompress.py,d3467d17ded3a21ef06d48adf87203ec790430f2,TODO: Encoding and decoding back and forth not terribly efficient,https://github.com/mideind/GreynirPackage/commit/d3467d17ded3a21ef06d48adf87203ec790430f2,Yes
4692,nltk/nltk,nltk_lite/contrib/shoebox/data.py,7b7ba329729043672f95d593a2eb88a92e695ea6,todo encoding; unicode fields; errors?,https://github.com/nltk/nltk/commit/7b7ba329729043672f95d593a2eb88a92e695ea6,Yes
4693,nltk/nltk,nltk_lite/corpora/toolbox.py,1e08917bfa1310ecd983427b4f5f8743cb9402b2,todo encoding; unicode fields; errors?,https://github.com/nltk/nltk/commit/1e08917bfa1310ecd983427b4f5f8743cb9402b2,Yes
4694,lucasrodes/whatstk,whatstk/core.py,6da6a8e58f5c7f9d81877c55bc8ab71392e27881,TODO: assert encoding and filename exist,https://github.com/lucasrodes/whatstk/commit/6da6a8e58f5c7f9d81877c55bc8ab71392e27881,Yes
4695,Mariewelt/OpenChem,modules/encoders/gcn_encoder.py,0a7e3c6a605f0e35c17edb31337ed1939c2b47ec,TODO: encoding of molecular graph into vector,https://github.com/Mariewelt/OpenChem/commit/0a7e3c6a605f0e35c17edb31337ed1939c2b47ec,Yes
4696,JohnVinyard/zounds,analyze/audiostream.py,30375cebfe4a7944d253dae78f30e5afbdfdd819,TODO: Handle encodings other than int16,https://github.com/JohnVinyard/zounds/commit/30375cebfe4a7944d253dae78f30e5afbdfdd819,Yes
4697,JohnVinyard/zounds,tests/analyze_test.py,30375cebfe4a7944d253dae78f30e5afbdfdd819,TODO: I've got the encoding (int16) hardcoded in a few places.,https://github.com/JohnVinyard/zounds/commit/30375cebfe4a7944d253dae78f30e5afbdfdd819,No
4698,JohnVinyard/zounds,analyze/audiostream.py,13ef571a03fe87926c30e7b3aa506d7a360ef244,TODO: Write test for encoding bug,https://github.com/JohnVinyard/zounds/commit/13ef571a03fe87926c30e7b3aa506d7a360ef244,Yes
4699,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_sgd.py,6a0e7f7f68eb02d6e724af4e359e915acc123c06,TODO: remove label encoding when class bug is fixed,https://github.com/scikit-learn/scikit-learn/commit/6a0e7f7f68eb02d6e724af4e359e915acc123c06,Yes
4700,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_sgd.py,0943c922e13b560e71ce828b4066286d4d812e7c,TODO: remove label encoding when class bug is fixed,https://github.com/scikit-learn/scikit-learn/commit/0943c922e13b560e71ce828b4066286d4d812e7c,Yes
4701,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_sgd.py,149d97f78e6919919d05879a0c841dfc5592a7b9,TODO: remove label encoding when class bug is fixed,https://github.com/scikit-learn/scikit-learn/commit/149d97f78e6919919d05879a0c841dfc5592a7b9,Yes
4702,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_sgd.py,39c87018b2812315e3e65482cb29a805dafb506f,TODO: remove label encoding when class bug is fixed,https://github.com/scikit-learn/scikit-learn/commit/39c87018b2812315e3e65482cb29a805dafb506f,Yes
4703,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_sgd.py,b0ab7146994fee2e2d05bfcc5123f6690ee2d913,TODO: remove label encoding when class bug is fixed,https://github.com/scikit-learn/scikit-learn/commit/b0ab7146994fee2e2d05bfcc5123f6690ee2d913,Yes
4704,delzac/cntkx,layers/models/language.py,dbd6190a96b8de78eaa7d973fa497b65889abe22,TODO: Check if weight is tied to encoding embedding,https://github.com/delzac/cntkx/commit/dbd6190a96b8de78eaa7d973fa497b65889abe22,Yes
4705,biolab/orange3,Orange/widgets/data/owpythonscript.py,b535452389d79b579dd499374bab66b4ecc87f1f,TODO: use `tokenize.detect_encoding`,https://github.com/biolab/orange3/commit/b535452389d79b579dd499374bab66b4ecc87f1f,Yes
4706,RasaHQ/rasa_core,rasa_core/training/generator.py,1689c64e6c8a6b0eb7f48329517dcf4321a41459,TODO don't like that: encoding after each event,https://github.com/RasaHQ/rasa_core/commit/1689c64e6c8a6b0eb7f48329517dcf4321a41459,Yes
4707,RasaHQ/rasa_core,rasa_core/training/generator.py,1689c64e6c8a6b0eb7f48329517dcf4321a41459,TODO might not work with label_featurizer encoding;,https://github.com/RasaHQ/rasa_core/commit/1689c64e6c8a6b0eb7f48329517dcf4321a41459,Yes
4708,lazybootsafe/AI2Match,match/chnlp/TextBrewer/examples/cmrc2018_example/processing.py,20e1096901f091737be28b5281496c824b2b2559,continue #TODO if doc_span_index>0: continue,https://github.com/lazybootsafe/AI2Match/commit/20e1096901f091737be28b5281496c824b2b2559,Yes
4709,arne-cl/discoursegraphs,src/discoursegraphs/readwrite/exportxml.py,5ea8774e2476a4967fae47e6b641eb9e71e9d5d0,''' || WARNING: academic ad-hoc code to export to CoNLL using igraph instead of || networkx. ||  || TODO: add markable span annotation to each antecedent\/anaphora during import || TODO: add edge_type to each edge ||  || needed functions \/ functionality || -------------------------------- ||  || get_pointing_chains(self.docgraph) || select_nodes_by_layer(self.docgraph; 'mmax:markable') || get_span(self.docgraph; markable_node_id) || dg.sentences || dg.node[sentence_id]['tokens'] || dg.get_token(tok_id) || ''',https://github.com/arne-cl/discoursegraphs/commit/5ea8774e2476a4967fae47e6b641eb9e71e9d5d0,No
4710,jkkummerfeld/slate,src/data.py,1ba863e477bd5b883434365e6babe991d35ca20d,TODO: Handle color given use of spans now instead of positions,https://github.com/jkkummerfeld/slate/commit/1ba863e477bd5b883434365e6babe991d35ca20d,Yes
4711,jkkummerfeld/slate,src/data.py,15436b8cbd5545199c0e82e1a3cf9a71a35ace97,TODO: Record the span too,https://github.com/jkkummerfeld/slate/commit/15436b8cbd5545199c0e82e1a3cf9a71a35ace97,Yes
4712,NLPatVCU/medaCy,medacy/pipeline_components/annotation/gold_annotator_component.py,c69888184b3d0ce3c639414d186b71e1bd7001fd,TODO REALLY clean this up asap - this method will find valid spans with annotation boundaries,https://github.com/NLPatVCU/medaCy/commit/c69888184b3d0ce3c639414d186b71e1bd7001fd,Yes
4713,NLPatVCU/medaCy,medacy/pipeline_components/metamap/metamap_component.py,c69888184b3d0ce3c639414d186b71e1bd7001fd,TODO spans are none when indices and token boundaries don't line up. This shouldn't happen here,https://github.com/NLPatVCU/medaCy/commit/c69888184b3d0ce3c639414d186b71e1bd7001fd,Yes
4714,NLPatVCU/medaCy,medacy/pipeline_components/feature_overlayers/metamap/metamap_all_types_component.py,5be19816b063ad9663f01fd1ecb240f1387a6082,TODO spans are none when indices and token boundaries don't line up.,https://github.com/NLPatVCU/medaCy/commit/5be19816b063ad9663f01fd1ecb240f1387a6082,Yes
4715,mozilla/bugbug,scripts/test_scheduling_history_retriever.py,2492ed58b4d6b14e03f0efbb46b71016ec716111,TODO: Increase timespan when https:\/\/github.com\/ahal\/ci-recipes\/issues\/6 is fixed.,https://github.com/mozilla/bugbug/commit/2492ed58b4d6b14e03f0efbb46b71016ec716111,Yes
4716,persephone-tools/persephone,persephone/datasets/na.py,db136b75d01eae6fbae7e07330ea9343e7720baf,TODO Address extrametrical span symbol \u25CA differently. For now;,https://github.com/persephone-tools/persephone/commit/db136b75d01eae6fbae7e07330ea9343e7720baf,Yes
4717,davidsbatista/BREDS,automatic-evaluation/evaluate.py,76643b3d06a6283f2dfe6ebe54a5ce41356b3232,"\""\""\"" || def query_a(queue; list_a; e1_type; e2_type; index): ||     #TODO: usar o metodo proximity_pmi ||     idx = open_dir(index) ||     while True: ||         r = queue.get_nowait() ||         entity1 = \""<\""+e1_type+\"">\""+r.e1+\""<\/\""+e1_type+\"">\"" ||         entity2 = \""<\""+e2_type+\"">\""+r.e2+\""<\/\""+e2_type+\"">\"" ||         t1 = query.Term(\""sentence\""; entity1) ||         t2 = query.Term(\""sentence\""; r.patterns) ||         t3 = query.Term(\""sentence\""; entity2) ||         q1 = spans.SpanNear2([t1; t2; t3]; slop=5; ordered=True) ||         q2 = spans.SpanNear2([t1; t3]; slop=5; ordered=True) ||  ||         with idx.searcher() as searcher: ||             entities_r = searcher.search(q1) ||             entities = searcher.search(q2) ||  ||             # ignore low occurrence even if PMI is 1 ||             if len(entities_r) == 1: ||                     continue ||  ||             # TODO: fazer stemming ou normaliza\u00E7\u00E3o da palavra a usar no query ||             if len(entities) > 0: ||                 pmi = float(len(entities_r)) \/ float(len(entities)) ||                 # TODO: qual o melhor valor de threshold ? ||                 if pmi >= 0.5: ||                     print entity1; '\\t'; r.patterns; '\\t'; entity2; pmi ||                     list_a.append(r) ||  ||         if queue.empty is True: ||             break || \""\""\""",https://github.com/davidsbatista/BREDS/commit/76643b3d06a6283f2dfe6ebe54a5ce41356b3232,No
4718,davidsbatista/BREDS,automatic-evaluation/evaluate.py,9a6146c55036dc1cb9e5611d2541034a084231c4,"\""\""\"" || def query_a(queue; list_a; e1_type; e2_type; index): ||     #TODO: usar o metodo proximity_pmi ||     idx = open_dir(index) ||     while True: ||         r = queue.get_nowait() ||         entity1 = \""<\""+e1_type+\"">\""+r.e1+\""<\/\""+e1_type+\"">\"" ||         entity2 = \""<\""+e2_type+\"">\""+r.e2+\""<\/\""+e2_type+\"">\"" ||         t1 = query.Term(\""sentence\""; entity1) ||         t2 = query.Term(\""sentence\""; r.patterns) ||         t3 = query.Term(\""sentence\""; entity2) ||         q1 = spans.SpanNear2([t1; t2; t3]; slop=5; ordered=True) ||         q2 = spans.SpanNear2([t1; t3]; slop=5; ordered=True) ||  ||         with idx.searcher() as searcher: ||             entities_r = searcher.search(q1) ||             entities = searcher.search(q2) ||  ||             # ignore low occurrence even if PMI is 1 ||             if len(entities_r) == 1: ||                     continue ||  ||             # TODO: fazer stemming ou normaliza\u00E7\u00E3o da palavra a usar no query ||             if len(entities) > 0: ||                 pmi = float(len(entities_r)) \/ float(len(entities)) ||                 # TODO: qual o melhor valor de threshold ? ||                 if pmi >= 0.5: ||                     print entity1; '\\t'; r.patterns; '\\t'; entity2; pmi ||                     list_a.append(r) ||  ||         if queue.empty is True: ||             break || \""\""\""",https://github.com/davidsbatista/BREDS/commit/9a6146c55036dc1cb9e5611d2541034a084231c4,No
4719,swabhs/open-sesame,src/conll09.py,c88c6f624c436bbffa490f790fb904a3d4bb9488,if felabel is None: # TODO: how is it inside a span and UNK?,https://github.com/swabhs/open-sesame/commit/c88c6f624c436bbffa490f790fb904a3d4bb9488,Yes
4720,vecto-ai/vecto,vecto/corpus/iterators.py,ae40d966a1a84f65915b8f3c8f53198d66c26f53,TODO: sentence may span over multiple lines; we should take this into account somehow,https://github.com/vecto-ai/vecto/commit/ae40d966a1a84f65915b8f3c8f53198d66c26f53,Yes
4721,pandas-profiling/pandas-profiling,pandas_profiling/report/structure/variables/render_path.py,906be41cecbc0464469fbfec4521f5a320170a76,TODO: colspan=2,https://github.com/pandas-profiling/pandas-profiling/commit/906be41cecbc0464469fbfec4521f5a320170a76,Yes
4722,proycon/foliatools,foliatools/foliaspec.py,91892fa52ef3c2e314f25c213843822452fda37a,TODO: find span roles,https://github.com/proycon/foliatools/commit/91892fa52ef3c2e314f25c213843822452fda37a,Yes
4723,HazyResearch/fonduer,tests/utils/data_model_utils/test_tabular.py,697b32e8c94d394f99881a7bba4f92ad4b8abb47,TODO: it'd be better to use the mention that spans multiple cols,https://github.com/HazyResearch/fonduer/commit/697b32e8c94d394f99881a7bba4f92ad4b8abb47,Yes
4724,chartbeat-labs/textacy,textacy/texts.py,aa92bc2801ad74cf7a0beea3b40634af6a1e9ff1,TODO: cache key terms; and return them as spacy spans,https://github.com/chartbeat-labs/textacy/commit/aa92bc2801ad74cf7a0beea3b40634af6a1e9ff1,Yes
4725,ecohealthalliance/EpiTator,annotator/geoname_annotator.py,5757857216415a34c94a744112b1a449a2860871,TODO: Add combined_span to geoname_spans being iterated,https://github.com/ecohealthalliance/EpiTator/commit/5757857216415a34c94a744112b1a449a2860871,Yes
4726,ecohealthalliance/EpiTator,epitator/count_annotator_2.py,091c287051fd801cd112246b990ed58e92d88aa8,TODO: This should really be an init() method for a CountSpan class.,https://github.com/ecohealthalliance/EpiTator/commit/091c287051fd801cd112246b990ed58e92d88aa8,Yes
4727,ecohealthalliance/EpiTator,epitator/infection_span.py,54bf30d4bb2ba5d2d2c3e1f3b2a810fa9e4da4ae,TODO: This should check that span is actually a noun chunk. It should also,https://github.com/ecohealthalliance/EpiTator/commit/54bf30d4bb2ba5d2d2c3e1f3b2a810fa9e4da4ae,Yes
4728,Rostlab/nalaf,nala/structures/data.py,eceebaf2e8dfc1cc61b4c8c6ef650fcb5f66db4c,todo check again with *span and unpacking,https://github.com/Rostlab/nalaf/commit/eceebaf2e8dfc1cc61b4c8c6ef650fcb5f66db4c,Yes
4729,DependableSystemsLab/TensorFI,TensorFI/fiLog.py,ed9ea0db1adb3ba5e8751139b42d3f11e68f32fa,FIXME: This won't work if the injection spans multiple days,https://github.com/DependableSystemsLab/TensorFI/commit/ed9ea0db1adb3ba5e8751139b42d3f11e68f32fa,Yes
4730,proycon/pynlpl,formats/fql.py,cff725015e8dd0671048298afdea866902bed2af,TODO: span annotation?,https://github.com/proycon/pynlpl/commit/cff725015e8dd0671048298afdea866902bed2af,Yes
4731,proycon/pynlpl,formats/folia.py,24b446e118635485ecf85fd6123d0a684ec84243,TODO: span roles don't take classes; derived off spanannotation allows too much,https://github.com/proycon/pynlpl/commit/24b446e118635485ecf85fd6123d0a684ec84243,Yes
4732,proycon/foliapy,formats/fql.py,5eb4d32bfddfc9b343540d90c65c35631a75080f,TODO: span annotation?,https://github.com/proycon/foliapy/commit/5eb4d32bfddfc9b343540d90c65c35631a75080f,Yes
4733,proycon/foliapy,formats/folia.py,13471e19e9a0da8a4a9414b65c84e5214ad96658,TODO: span roles don't take classes; derived off spanannotation allows too much,https://github.com/proycon/foliapy/commit/13471e19e9a0da8a4a9414b65c84e5214ad96658,Yes
4734,GilesStrong/lumin,lumin/nn/callbacks/data_callbacks.py,4ebe08737e4e3076fdb2ca6c5688466eace91168,TODO cont feat names no longer required only number; move to infer number of cont feats from model_builder and batch_yielder,https://github.com/GilesStrong/lumin/commit/4ebe08737e4e3076fdb2ca6c5688466eace91168,Yes
4735,GilesStrong/lumin,lumin/nn/ensemble/ensemble.py,d16d25768f0b677b548c7b0b3c9d36329991af25,TODO: check whether model_builder is necessary here,https://github.com/GilesStrong/lumin/commit/d16d25768f0b677b548c7b0b3c9d36329991af25,No
4736,yonkshi/text2imageNet,lenet/tensorcv/train/config.py,28cf797566877a80db00e33e4311f70ab7b241b2,TODO model.default_collection only in BaseModel class,https://github.com/yonkshi/text2imageNet/commit/28cf797566877a80db00e33e4311f70ab7b241b2,No
4737,Seanforfun/GMAN_Net_Haze_Removal,DehazeNet/dehazenet_multi_gpu_train.py,f3eb9864e9de8255464a1ffb1e95fab5091c389f,TODO Lida Xu please re-write the CNN model,https://github.com/Seanforfun/GMAN_Net_Haze_Removal/commit/f3eb9864e9de8255464a1ffb1e95fab5091c389f,Yes
4738,thuijskens/scikit-hyperband,hyperband/tests/test_hyperband.py,89398bae46fe4c5607ab63c87c1ccfd4c2b50fd4,"\""\""\"" || TODO: This test fails due to the random state not being properly fixed ||  || def test_hyperband(): ||     model; param_dist; X; y; rng = setup() ||     search = HyperbandSearchCV(model; param_dist; random_state=rng) ||     search.fit(X; y) ||  ||     # results = pd.DataFrame(search.cv_results_) ||     expected_params = { ||         'bootstrap': False; ||         'criterion': 'entropy'; ||         'max_depth': None; ||         'max_features': 7; ||         'min_samples_leaf': 2; ||         'min_samples_split': 2; ||         'n_estimators': 81 ||     } ||  ||     # assert(results.shape[0] == 186) TODO: sort out what the expected n_i and r_i values are ||     assert(search.best_params_ == expected_params) || \""\""\""",https://github.com/thuijskens/scikit-hyperband/commit/89398bae46fe4c5607ab63c87c1ccfd4c2b50fd4,Yes
4739,howl-anderson/seq2annotation,seq2annotation/server/tensorflow_inference.py,fc5fb0d63e49a8d51496a310c29ca3278c7243a0,TODO: feature translate should out of this main program for better compatible with keras and estimator model,https://github.com/howl-anderson/seq2annotation/commit/fc5fb0d63e49a8d51496a310c29ca3278c7243a0,Yes
4740,howl-anderson/seq2annotation,seq2annotation/server/tensorflow_inference.py,e2f03123d2c7e9271efd4e8ef88e390f89be26a9,TODO: feature translate should out of this main program for better compatible with keras and estimator model,https://github.com/howl-anderson/seq2annotation/commit/e2f03123d2c7e9271efd4e8ef88e390f89be26a9,Yes
4741,jinfagang/tfboys,tf2.0/image_classify/backbones/mobilenetv3.py,b2d9d9d49b8928e21042d75064e58399c4f8ee17,TODO: convert model to Functional or Sequential so that can be saved as h5 model,https://github.com/jinfagang/tfboys/commit/b2d9d9d49b8928e21042d75064e58399c4f8ee17,Yes
4742,sshleifer/object_detection_kitti,slim/models/model_deploy.py,a5c4fd06d21e85a231ec05cc5305478a6c2d6a73,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = slim.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = slim.deploy(config; model_fn; [inputs_queue]; optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/sshleifer/object_detection_kitti/commit/a5c4fd06d21e85a231ec05cc5305478a6c2d6a73,Yes
4743,sshleifer/object_detection_kitti,object_detection/evaluator.py,a4944a57ad2811e1f6a7a87589a9fc8a776e8d3c,TODO: This should be done in model's postprocess function ideally.,https://github.com/sshleifer/object_detection_kitti/commit/a4944a57ad2811e1f6a7a87589a9fc8a776e8d3c,Yes
4744,roscisz/TensorHive,tensorhive/api/api/users.py,e11498ae720b7c3e6f83c90b74adc75036ada766,TODO replace with database model,https://github.com/roscisz/TensorHive/commit/e11498ae720b7c3e6f83c90b74adc75036ada766,Yes
4745,roscisz/TensorHive,tests/unit/models/test_user_model.py,00981d3c2f4e8d123bdfc068872800b42e7f0a5a,TODO Move to test_role_model.py,https://github.com/roscisz/TensorHive/commit/00981d3c2f4e8d123bdfc068872800b42e7f0a5a,Yes
4746,rwightman/posenet-pytorch,posenet/utils.py,51cf48e3473362be1b5b07445b53433695c8d7b1,TODO explore impact of scaling on model performance,https://github.com/rwightman/posenet-pytorch/commit/51cf48e3473362be1b5b07445b53433695c8d7b1,Yes
4747,Koziev/chatbot,ruchatbot/layers/attention_decoder.py,2bf32c703fd354a42fced0cd34e8ed424044aea2,TODO: check that model is loading from .h5 correctly,https://github.com/Koziev/chatbot/commit/2bf32c703fd354a42fced0cd34e8ed424044aea2,Yes
4748,km1414/CNN-models,googlenet-lite/googlenet-lite.py,4f4eb185c4e3a4692c82f9ee2d18b89706524cf6,TODO: add auxiliary classifiers to model,https://github.com/km1414/CNN-models/commit/4f4eb185c4e3a4692c82f9ee2d18b89706524cf6,Yes
4749,limteng-rpi/mlmt,train_single.py,67df8445c5d409e4c07df3e1025972ce4b20ebc8,TODO: evaluate the model,https://github.com/limteng-rpi/mlmt/commit/67df8445c5d409e4c07df3e1025972ce4b20ebc8,Yes
4750,limteng-rpi/mlmt,train_single.py,67df8445c5d409e4c07df3e1025972ce4b20ebc8,TODO: save the best model,https://github.com/limteng-rpi/mlmt/commit/67df8445c5d409e4c07df3e1025972ce4b20ebc8,Yes
4751,limteng-rpi/mlmt,train_single.py,824b07c00dae1bcb3d29299f50dac48c912f9361,TODO: evaluate the model,https://github.com/limteng-rpi/mlmt/commit/824b07c00dae1bcb3d29299f50dac48c912f9361,Yes
4752,mycrazycracy/tf-kaldi-speaker,egs/voxceleb/v1/nnet/lib/train_mt.py,c5ccc12ea32927933eec7169a3b9fe5a51405b68,TODO: Change the model name to train different models,https://github.com/mycrazycracy/tf-kaldi-speaker/commit/c5ccc12ea32927933eec7169a3b9fe5a51405b68,Yes
4753,gregversteeg/LinearCorex,linear_corex.py,432a676cb581de61f689c0c7a7dc133f9d46dc97,TODO: We could estimate covariance of Y using a hierarchy of corex models!,https://github.com/gregversteeg/LinearCorex/commit/432a676cb581de61f689c0c7a7dc133f9d46dc97,No
4754,oroszgy/spacy-hungarian-models,src/model_builder/ner.py,82b6286d1e731bd5dd50ce6e00b9055f4b30af6b,FIXME: pre-train the model,https://github.com/oroszgy/spacy-hungarian-models/commit/82b6286d1e731bd5dd50ce6e00b9055f4b30af6b,Yes
4755,phohenecker/pytorch-transformer,examples/overfitting_test.py,e80dad100f507b1dcb056e719391408ca433ec32,"\""\""\""An implementation of the overfitting test for the Transformer model. ||  || A simple test; which often signifies bugs in the implementation of a model; is the overfitting test. To that end; the || considered model is trained and evaluated on the same tiny dataset; which it should be able to overfit easily. || Therefore; the final model should yield very high probabilities for the desired target values. If this is not the case; || however; then there is probably something wrong with the tested model and\/or its implementation. ||  || TODO: explain a bit more || \""\""\""",https://github.com/phohenecker/pytorch-transformer/commit/e80dad100f507b1dcb056e719391408ca433ec32,Yes
4756,Nasdin/VideoRecognition-realtime-autotrainer-alerts,data_munging/filterimages.py,ef0dcce7f169a887ce11f31d33d268efc7450ca1,TODO Keras models to detect whether an image is present,https://github.com/Nasdin/VideoRecognition-realtime-autotrainer-alerts/commit/ef0dcce7f169a887ce11f31d33d268efc7450ca1,Yes
4757,andreasvc/disco-dop,treebank.py,64206afe9c58aefd375b5858e89fc662298151be,FIXME: exclude accented characters for model 6?,https://github.com/andreasvc/disco-dop/commit/64206afe9c58aefd375b5858e89fc662298151be,Yes
4758,andreasvc/disco-dop,lexicon.py,77f22ff75b54fe0d850298e9e4479fbb2b5f20cf,FIXME: exclude accented characters for model 6?,https://github.com/andreasvc/disco-dop/commit/77f22ff75b54fe0d850298e9e4479fbb2b5f20cf,Yes
4759,andreasvc/disco-dop,discodop/parser.py,d9c10ee50894b858bda3b7489470d2fe0bc83be3,FIXME: do unknown word model stuff here.,https://github.com/andreasvc/disco-dop/commit/d9c10ee50894b858bda3b7489470d2fe0bc83be3,Yes
4760,andreasvc/disco-dop,discodop/parser.py,7fad03a6896ae53a40415fb757d15b54221eba28,FIXME: re-estimate unknown word model?,https://github.com/andreasvc/disco-dop/commit/7fad03a6896ae53a40415fb757d15b54221eba28,Yes
4761,Angzz/panoptic-fpn-gluon,tests/unittests/test_utils_segmentation.py,7caee8e1cb994730efa849cd10603c1294e2b759,TODO FIXME: change it to ADE20K dataset and pretrained model,https://github.com/Angzz/panoptic-fpn-gluon/commit/7caee8e1cb994730efa849cd10603c1294e2b759,Yes
4762,Erotemic/netharn,netharn/fit_harn.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,"\""\""\"" || CommandLine: ||     python ~\/code\/netharn\/netharn\/fit_harn.py __doc__ ||  || Notes: ||     when profiling ensure CUDA_LAUNCH_BLOCKING=1 ||  || Notes: ||     to use; your training session must have the concept of: ||         * epochs ||         * batch_size ||         * xpu ||         * train \/ validation datasets ||  ||     or better yet: ||         * a model ||         * a criterion ||         * an optimizer ||  || TODO: ||     [ ] - output \""glance\"" curves to disk ||     [x] - move logs to a logs folder. Keep a single master log in the root ||     [ ] - Why didnt the best_snapshot.pt get saved in the most recent yolo run? ||  || Example: ||     >>> import netharn as nh ||     >>> size = 3 ||     >>> max_epoch = 10 ||     >>> datasets = { ||     >>>     'train': nh.data.ToyData2d(size=size; border=1; n=256; rng=0); ||     >>>     'vali': nh.data.ToyData2d(size=size; border=1; n=128; rng=1); ||     >>> } ||     >>> hyper = { ||     >>>     # --- Data First ||     >>>     'datasets'    : datasets; ||     >>>     'nice'        : 'demo'; ||     >>>     'workdir'     : ub.ensure_app_cache_dir('netharn\/demo'); ||     >>>     'loaders'     : {'batch_size': 64}; ||     >>>     'xpu'         : nh.XPU.cast('auto'); ||     >>>     # --- Algorithm Second ||     >>>     'model'       : (nh.models.ToyNet2d; {}); ||     >>>     'optimizer'   : (nh.optimizers.SGD; { ||     >>>         'lr': 0.0001 ||     >>>     }); ||     >>>     'criterion'   : (nh.criterions.CrossEntropyLoss; {}); ||     >>>     #'criterion'   : (nh.criterions.FocalLoss; {}); ||     >>>     'initializer' : (nh.initializers.KaimingNormal; { ||     >>>         'param': 0; ||     >>>     }); ||     >>>     'scheduler'   : (nh.schedulers.ListedLR; { ||     >>>         'points': {0: .0001; 2: .01; 5: .015; 6: .005; 9: .001}; ||     >>>         'interpolate': True; ||     >>>     }); ||     >>>     'dynamics'   : {'batch_step': 4}; ||     >>>     'monitor'     : (nh.Monitor; { ||     >>>         'max_epoch': max_epoch; ||     >>>     }); ||     >>> } ||     >>> harn = FitHarn(hyper) ||     >>> harn.config['use_tqdm'] = 1 ||     >>> harn.initialize(reset='delete') ||     >>> harn.run() || \""\""\""",https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,No
4763,Erotemic/netharn,netharn/fit_harn.py,54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,"\""\""\"" || Notes: ||     when profiling ensure CUDA_LAUNCH_BLOCKING=1 ||  || Notes: ||     to use; your training session must have the concept of: ||         * epochs ||         * batch_size ||         * xpu ||         * train \/ validation datasets ||  ||     or better yet: ||         * a model ||         * a criterion ||         * an optimizer ||  || TODO: ||     [ ] - output \""glance\"" curves to disk ||     [x] - move logs to a logs folder. Keep a single master log in the root ||     [ ] - Why didnt the best_snapshot.pt get saved in the most recent yolo run? ||  || Notes: ||     In the following example we demonstrate how to use netharn to train a model ||     to solve a toy problem. ||  ||     In this toy problem; we do not extend the nh.FitHarn object; so we are ||     using the default behavior of ``run_batch``. The default ``on_batch``; and ||     ``on_epoch`` do nothing; so only loss will be the only measurement of ||     performance. ||  ||     For further examples please see the examples directory. These example show ||     how to extend nh.FitHarn to measure performance wrt a particular problem. ||     The MNIST and CIFAR examples are the most simple. The YOLO example is more ||     complex.  The IBEIS example depends on non-public data \/ software; but can ||     still be useful to look at.  Its complexity is more than CIFAR but less ||     than YOLO. ||  || CommandLine: ||     xdoctest netharn.fit_harn __doc__:0 ||     xdoctest netharn.fit_harn __doc__:0 --progiter ||  || Example: ||     >>> import netharn as nh ||     >>> hyper = nh.HyperParams(**{ ||     >>>     # ================ ||     >>>     # Environment Components ||     >>>     'workdir'     : ub.ensure_app_cache_dir('netharn\/tests\/demo'); ||     >>>     'nice'        : 'demo'; ||     >>>     'xpu'         : nh.XPU.cast('auto'); ||     >>>     # workdir is a directory where intermediate results can be saved ||     >>>     # nice symlinks <workdir>\/fit\/nice\/<nice> -> ..\/runs\/<hashid> ||     >>>     # XPU auto select a gpu if idle and VRAM>6GB else a cpu ||     >>>     # ================ ||     >>>     # Data Components ||     >>>     'datasets'    : {  # dict of plain ol torch.data.Dataset instances ||     >>>         'train': nh.data.ToyData2d(size=3; border=1; n=256; rng=0); ||     >>>         'vali': nh.data.ToyData2d(size=3; border=1; n=128; rng=1); ||     >>>         'test': nh.data.ToyData2d(size=3; border=1; n=128; rng=1); ||     >>>     }; ||     >>>     'loaders'     : {'batch_size': 64}; # DataLoader instances or kw ||     >>>     # ================ ||     >>>     # Algorithm Components ||     >>>     # Note the (cls; kw) tuple formatting ||     >>>     'model'       : (nh.models.ToyNet2d; {}); ||     >>>     'optimizer'   : (nh.optimizers.SGD; { ||     >>>         'lr': 0.0001 ||     >>>     }); ||     >>>     # focal loss is usually better than nh.criterions.CrossEntropyLoss ||     >>>     'criterion'   : (nh.criterions.FocalLoss; {}); ||     >>>     'initializer' : (nh.initializers.KaimingNormal; { ||     >>>         'param': 0; ||     >>>     }); ||     >>>     # these may receive an overhaul soon ||     >>>     'scheduler'   : (nh.schedulers.ListedLR; { ||     >>>         'points': {0: .0001; 2: .01; 5: .015; 6: .005; 9: .001}; ||     >>>         'interpolate': True; ||     >>>     }); ||     >>>     'monitor'     : (nh.Monitor; { ||     >>>         'max_epoch': 10; ||     >>>     }); ||     >>>     # dynamics are a config option that modify the behavior of the main ||     >>>     # training loop. These parameters effect the learned model. ||     >>>     'dynamics'   : {'batch_step': 4}; ||     >>> }) ||     >>> harn = nh.FitHarn(hyper) ||     >>> # non-algorithmic behavior configs (do not change learned models) ||     >>> harn.config['prog_backend'] = 'tqdm' ||     >>> if ub.argflag('--progiter'):  # I prefer progiter (I may be biased) ||     ...     harn.config['prog_backend'] = 'progiter' ||     >>> # start training. ||     >>> harn.initialize(reset='delete') ||     >>> harn.run()  # note: run calls initialize it hasn't already been called. ||     >>> # xdoc: +IGNORE_WANT ||     RESET HARNESS BY DELETING EVERYTHING IN TRAINING DIR ||     Symlink: ...tests\/demo\/fit\/runs\/demo\/keyeewlr -> ...tests\/demo\/fit\/nice\/demo ||     .... already exists ||     .... and points to the right place ||     Initializing tensorboard (dont forget to start the tensorboard server) ||     Model has 824 parameters ||     Mounting ToyNet2d model on CPU ||     Initializing model weights ||      * harn.train_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr' ||      * harn.nice_dpath  = '...tests\/demo\/fit\/nice\/demo' ||     Snapshots will save to harn.snapshot_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr\/torch_snapshots' ||     dont forget to start: ||         tensorboard --logdir ...tests\/demo\/fit\/nice ||     === begin training === ||     epoch lr:0.001 \u2502 vloss: 0.1409 (n_bad_epochs=00; best=0.1409): 100%|\u2588| 10\/10 [00:01<00:00;  9.95it\/s]  0:00<?; ?it\/s] ||     train x64 \u2502 loss:0.147 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8\/8 [00:00<00:00; 130.56it\/s] ||     vali x64 \u2502 loss:0.140 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4\/4 [00:00<00:00; 342.04it\/s] ||     test x64 \u2502 loss:0.140 \u2502: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4\/4 [00:00<00:00; 342.92it\/s] ||     <BLANKLINE> ||     Maximum harn.epoch reached; terminating ... ||     <BLANKLINE> ||     training completed ||     current lrs: [0.001] ||     harn.train_dpath = '...tests\/demo\/fit\/runs\/demo\/keyeewlr' ||     harn.nice_dpath  = '...tests\/demo\/fit\/nice\/demo' ||     view tensorboard results for this run via: ||         tensorboard --logdir ...tests\/demo\/fit\/nice ||     exiting fit harness. || \""\""\""",https://github.com/Erotemic/netharn/commit/54bb6c1f371c12649df97ed5b2c5f4710e97bd0a,No
4764,Erotemic/netharn,netharn/examples/cifar.py,5f79282c043a0d58be110ce7719c72f3402fbd58,"\""\""\"" || The examples\/cifar.py is probably the most clear example of what netharn is and || what it's trying to do \/ not do. ||  || The basic idea is make an object that inherits from nh.FitHarn. This is our || harness object. It will contain the hyperparameters as well as the learning || state. All the training loop boilerplate has already been written in the parent || class; so all our child class needs to do is: define `prepare_batch` (not || usually needed) and `run_batch`. Code to measure and record performance should || be placed in `on_batch` and `on_epoch`. ||  || The `train` function is our main entry point. It reads parameters from the || command line to override defaults. It then consructs the `HyperParams` object || and constructs an instance of `CIFAR_FitHarn` and calls `harn.run()`. ||  || This begins the training process. At a high level the harness will load the || data using torch DataLoaders; and call `run_batch` when it needs to compute the || model outputs and loss based on the input data. The returned loss is used to || update the model weights if `harn.tag === 'train'`; for validation; test; and || calibration (todo) datasets the loss is simply recorded. ||  || After `run_batch` finishes the `on_batch` function is called; where you can || optionally return a dict of scalars to log as measurements for this batch (note || loss is always recorded; so we need not return it here; but loss components may || be useful). A similar thing happens in `on_epoch`; where you should return || metrics about the entire dataset. ||  || The training harness manages the fit directory structure based on a hash of the || hyperparameters; the creation of algorithm component instance (e.g. model; || optimizer); initializing model weights; restarting from the most recent epoch; || updating the learning rates; various training loop boilerplate details; || checking divergence; reporting progress; handling differences between train; || validation; and test sets. In short; netharn handles the necessary parts and || let the developer focus on the important parts. || \""\""\""",https://github.com/Erotemic/netharn/commit/5f79282c043a0d58be110ce7719c72f3402fbd58,No
4765,Erotemic/netharn,netharn/fit_harn.py,26924a149f6bc51dc19a3f1b2ca04897cd5287a1,TODO: might be good to check for multiple model exports at this time,https://github.com/Erotemic/netharn/commit/26924a149f6bc51dc19a3f1b2ca04897cd5287a1,Yes
4766,Erotemic/netharn,netharn/hyperparams.py,3c00f88b302692511944e4778cc5b93e68c2bcde,TODO: if pretrained is another netharn model; then we should read that,https://github.com/Erotemic/netharn/commit/3c00f88b302692511944e4778cc5b93e68c2bcde,Yes
4767,Erotemic/netharn,netharn/fit_harn.py,11dd3d227cbb397304522932872aba936f0c216d,TODO: might be good to check for multiple model exports at this time,https://github.com/Erotemic/netharn/commit/11dd3d227cbb397304522932872aba936f0c216d,Yes
4768,Erotemic/netharn,netharn/models/classical.py,ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,"\""\""\"" || WIP ||  || This file should contain classes (that behave like torch models); but they || implement the learning of classical learning algorithms like SVM and || RandomForest. ||  || Deep networks are amazing at learning features. However; I don't think it's || very useful to use linear logicstic regression as a classifier. In many cases I || think an SVM or a RandomForest might produce a superior classification model; || but this has yet to be shown. ||  || TODO: ||     - [ ] Classical Abstract API ||     - [ ] Integration with the FitHarn ||         - [ ] How do we swap netharn's backprop+SGD with sklearn's SVM and RandomForest fit methods? ||         - [ ] Netharn needs a \""classical\"" implementation of \""run\"". ||             - [ ] Simply use the data loader to load the data ||             - [ ] Defaults should encourage use with deep features. ||     - [ ] RandomForest ||     - [ ] SVM || \""\""\""",https://github.com/Erotemic/netharn/commit/ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,No
4769,aldro61/kover,core/kover/learning/experiment_CART.py,6af8bdcc1ba8be83dd8a721a8f7ad1b1fe230309,TODO: missing model_equivalent_rules; rule importances,https://github.com/aldro61/kover/commit/6af8bdcc1ba8be83dd8a721a8f7ad1b1fe230309,Yes
4770,nardeas/SyntaxNet-Tensorflow,wrapper.py,29fe7b12c007c4761152769561af53d14b477054,TODO: Functionality to automatically download CoNLL models,https://github.com/nardeas/SyntaxNet-Tensorflow/commit/29fe7b12c007c4761152769561af53d14b477054,Yes
4771,felixchenfy/Realtime-Action-Recognition,src/s5_test.py,4e61c338786efb12c9593bfc9f7f0bc45b40481b,''' || Test action recognition on || (1) a video; (2) a folder of images; (3) or web camera. ||  || Input: ||     classes: data_proc\/classes.csv # TODO: change this to a config file ||     model: model\/trained_classifier.pickle ||  || Output: ||     result video:    output\/${video_name}\/video.avi ||     result skeleton: output\/${video_name}\/skeleton_res\/XXXXX.txt ||     visualization by cv2.imshow() in img_displayer || ''',https://github.com/felixchenfy/Realtime-Action-Recognition/commit/4e61c338786efb12c9593bfc9f7f0bc45b40481b,Yes
4772,yao23/Machine_Learning_Playground,TaoBaoClothesMatching/clothes_matcher.py,49d0748b8edd5fedafbb1a44e34db8b3ec5a7aa3,TODO finish codes of loading models from local file here.,https://github.com/yao23/Machine_Learning_Playground/commit/49d0748b8edd5fedafbb1a44e34db8b3ec5a7aa3,Yes
4773,fpaupier/tensorflow-serving_sidecar,slim/deployment/model_deploy.py,e78e2946749dd804dc3868eee8973161222c3d68,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/fpaupier/tensorflow-serving_sidecar/commit/e78e2946749dd804dc3868eee8973161222c3d68,Yes
4774,kainoj/colnet,src/loader.py,8f64b12575e5ab2992a5eecb37462aefdea06c12,TODO(Przemek): implement loading a model,https://github.com/kainoj/colnet/commit/8f64b12575e5ab2992a5eecb37462aefdea06c12,Yes
4775,jason718/game-feature-learning,eval-3rd-party/voc_cls/phikr_caffe/python/detect.py,c1844d400fe3d191c3e012f8c6eb83dfc46ff7e9,"\""\""\"" || detector.py is an out-of-the-box windowed detector || callable from the command line. ||  || By default it configures and runs the Caffe reference ImageNet model. || Note that this model was trained for image classification and not detection; || and finetuning for detection can be expected to improve results. ||  || The selective_search_ijcv_with_python code required for the selective search || proposal mode is available at ||     https:\/\/github.com\/sergeyk\/selective_search_ijcv_with_python ||  || TODO: || - batch up image filenames as well: don't want to load all of them into memory || - come up with a batching scheme that preserved order \/ keeps a unique ID || \""\""\""",https://github.com/jason718/game-feature-learning/commit/c1844d400fe3d191c3e012f8c6eb83dfc46ff7e9,Yes
4776,JingqingZ/KG4ZeroShotText,src_reject/train_seen.py,632602bfc1150c5dea2a2083f692c61b9dd99427,TODO: mistake: the model name should be selected_tfidf,https://github.com/JingqingZ/KG4ZeroShotText/commit/632602bfc1150c5dea2a2083f692c61b9dd99427,Yes
4777,sld/supervised-embedding-model,test.py,38aa88e52260f9a817a2d5583d938f1f91beb438,TODO: move to tensorflow model?,https://github.com/sld/supervised-embedding-model/commit/38aa88e52260f9a817a2d5583d938f1f91beb438,Yes
4778,CoderPat/OpenGNN,opengnn/utils/copying_wrapper.py,5a4dc2862dbc68e41e5168fca4c0beb34b7defbc,TODO AND WARNING: this is a hotfix so that copying works for trained models.,https://github.com/CoderPat/OpenGNN/commit/5a4dc2862dbc68e41e5168fca4c0beb34b7defbc,Yes
4779,diningphil/CGMM,models/graph_classifiers/CGMM.py,e9c8987e4c5474cadc0929571cac5496241fe003,TODO I could make NetWrapper more flexible allowing to preprocess the batch before giving it to the model!,https://github.com/diningphil/CGMM/commit/e9c8987e4c5474cadc0929571cac5496241fe003,Yes
4780,simonfqy/PADME,dcCustom/models/tensorgraph/graph_models.py,6cdc8b93c0836f154a0ca42e668ef7ef80d104ba,TODO: aside from the name; DTNNModel is unmodified. May need modification like GraphConvModel did.,https://github.com/simonfqy/PADME/commit/6cdc8b93c0836f154a0ca42e668ef7ef80d104ba,Yes
4781,simonfqy/PADME,dcCustom/models/tensorgraph/graph_models.py,6cdc8b93c0836f154a0ca42e668ef7ef80d104ba,TODO: aside from the name; DAGModel is unmodified. May need modification like GraphConvModel did.,https://github.com/simonfqy/PADME/commit/6cdc8b93c0836f154a0ca42e668ef7ef80d104ba,Yes
4782,simonfqy/PADME,dcCustom/models/tensorgraph/graph_models.py,6cdc8b93c0836f154a0ca42e668ef7ef80d104ba,TODO: aside from the name; MPNNModel is unmodified. May need modification like GraphConvModel did.,https://github.com/simonfqy/PADME/commit/6cdc8b93c0836f154a0ca42e668ef7ef80d104ba,Yes
4783,simonfqy/PADME,dcCustom/molnet/run_benchmark_models.py,c418999ae4cf8092da623fb69fcb597d6150741d,TODO: If you want to use this function; please update it according to model_regression(),https://github.com/simonfqy/PADME/commit/c418999ae4cf8092da623fb69fcb597d6150741d,Yes
4784,xingchensong/Speech-Transformer-tf2.0,train_transformer.py,c3b7bc84dee9ddf18463ce6018f42ee7586d9bca,TODO: build model or load pre-trained model,https://github.com/xingchensong/Speech-Transformer-tf2.0/commit/c3b7bc84dee9ddf18463ce6018f42ee7586d9bca,Yes
4785,ealcobaca/pymfe,python/pymfe/model_based.py,ca25f6c479545e5354128152f8ef3336596be08e,"\""\""\""Module dedicated to extraction of Model Based Metafeatures. ||  || Todo: ||     * Implement metafeatures. ||     * Improve documentation. || \""\""\""",https://github.com/ealcobaca/pymfe/commit/ca25f6c479545e5354128152f8ef3336596be08e,Yes
4786,arose13/rosey,tests/test_models.py,a9b0435b7b6e03acaae16ffd147617b7ed7a4bbf,TODO 11\/11\/2018 continue and compare with statsmodels result,https://github.com/arose13/rosey/commit/a9b0435b7b6e03acaae16ffd147617b7ed7a4bbf,Yes
4787,perslev/MultiPlanarUNet,image/auditor.py,7df421118cf033e0f95328fac5c6bd4c7009027a,TODO: Integrate this with the logic of the MultiPlanarUNet.models,https://github.com/perslev/MultiPlanarUNet/commit/7df421118cf033e0f95328fac5c6bd4c7009027a,Yes
4788,perslev/MultiPlanarUNet,image/auditor.py,7df421118cf033e0f95328fac5c6bd4c7009027a,TODO: model type.,https://github.com/perslev/MultiPlanarUNet/commit/7df421118cf033e0f95328fac5c6bd4c7009027a,No
4789,proceduralia/pytorch-neural-enhance,learn_transform.py,4fe391a84f231dba0e7ae4ada96460aec81ae3f5,TODO add evaluation; model checkpointing; tensorboard,https://github.com/proceduralia/pytorch-neural-enhance/commit/4fe391a84f231dba0e7ae4ada96460aec81ae3f5,Yes
4790,annomator/annomator_1.0,tf_slim_obj_det/deployment/model_deploy.py,042cc36cba515855a4cbc963df4328123d624c9f,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/annomator/annomator_1.0/commit/042cc36cba515855a4cbc963df4328123d624c9f,Yes
4791,adzialocha/tomomibot,tomomibot/session.py,96d8dba9d05492c3c9e012ff6aed6fe0aa4853b0,@TODO Just play them for now; RNN model prediction later,https://github.com/adzialocha/tomomibot/commit/96d8dba9d05492c3c9e012ff6aed6fe0aa4853b0,Yes
4792,dtrckd/pymake,pymake/expe/format.py,e41d7ca725cc1478d039460ede2e9936f80fd2e2,@todo integrate Frontend and Model to that workflow,https://github.com/dtrckd/pymake/commit/e41d7ca725cc1478d039460ede2e9936f80fd2e2,Yes
4793,dtrckd/pymake,pymake/core/gramexp.py,67da33f17177ee338786eedfe0e03f5fbc807ff3,_model_ext = @Todo: dense(numpy\/pk.gz) or sparse => gt...?,https://github.com/dtrckd/pymake/commit/67da33f17177ee338786eedfe0e03f5fbc807ff3,Yes
4794,dtrckd/pymake,pymake/frontend/manager.py,007725c9ac6104a91f1896e2a924609a205e7d6b,_model_ext = @Todo: dense(numpy\/pk.gz) or sparse => gt...?,https://github.com/dtrckd/pymake/commit/007725c9ac6104a91f1896e2a924609a205e7d6b,Yes
4795,KMouratidis/EDA_miner,apps/analyze/Model_Builder.py,0784e6c55904de277dcc1d65328c116177fb6120,TODO: add ensemble models as last-step models (voting; ensembles; etc),https://github.com/KMouratidis/EDA_miner/commit/0784e6c55904de277dcc1d65328c116177fb6120,Yes
4796,KMouratidis/EDA_miner,apps/analyze/Pipelines.py,0784e6c55904de277dcc1d65328c116177fb6120,TODO: Implement score function for all models; including clustering,https://github.com/KMouratidis/EDA_miner/commit/0784e6c55904de277dcc1d65328c116177fb6120,Yes
4797,KMouratidis/EDA_miner,utils.py,0784e6c55904de277dcc1d65328c116177fb6120,TODO: this needs to be replaced by the one in Model_Builder,https://github.com/KMouratidis/EDA_miner/commit/0784e6c55904de277dcc1d65328c116177fb6120,Yes
4798,KMouratidis/EDA_miner,apps/analyze/models/graph_structures.py,221755d9f132c240ad384d4d323e05939bd855b8,TODO: add ensemble models as last-step models (voting; ensembles; etc),https://github.com/KMouratidis/EDA_miner/commit/221755d9f132c240ad384d4d323e05939bd855b8,Yes
4799,KMouratidis/EDA_miner,apps/analyze/Clustering.py,7a7ba6c9c997550511ac2e54698f7a206c827b8e,TODO: Find a meaningful way (metric) to notify the user of model score.,https://github.com/KMouratidis/EDA_miner/commit/7a7ba6c9c997550511ac2e54698f7a206c827b8e,Yes
4800,KMouratidis/EDA_miner,EDA_miner/modeling/models/graph_structures.py,d0505ee0ec2d00f12832966f9b313f83aa4a9a58,TODO: add ensemble models as last-step models (voting; ensembles; etc),https://github.com/KMouratidis/EDA_miner/commit/d0505ee0ec2d00f12832966f9b313f83aa4a9a58,Yes
4801,KMouratidis/EDA_miner,EDA_miner/modeling/pipelines.py,d0505ee0ec2d00f12832966f9b313f83aa4a9a58,TODO: This is exactly the same as in single_model so you might,https://github.com/KMouratidis/EDA_miner/commit/d0505ee0ec2d00f12832966f9b313f83aa4a9a58,Yes
4802,KMouratidis/EDA_miner,EDA_miner/modeling/pipelines.py,d0505ee0ec2d00f12832966f9b313f83aa4a9a58,TODO: EVERYTHING below here is the same as in single_model.,https://github.com/KMouratidis/EDA_miner/commit/d0505ee0ec2d00f12832966f9b313f83aa4a9a58,No
4803,elitcloud/elit,elit/component.py,4b2f24ed90962d4766f2bf5a8045abb820926bd7,TODO: LSTMModel needs to be reimplemented,https://github.com/elitcloud/elit/commit/4b2f24ed90962d4766f2bf5a8045abb820926bd7,Yes
4804,IBM/MAX-Image-Caption-Generator,core/backend.py,e768521d7c360312cde9d78d4c58091e1ac0b058,TODO Replace this part with SavedModel,https://github.com/IBM/MAX-Image-Caption-Generator/commit/e768521d7c360312cde9d78d4c58091e1ac0b058,Yes
4805,IBM/MAX-Image-Caption-Generator,core/run_inference.py,e768521d7c360312cde9d78d4c58091e1ac0b058,TODO Replace this part with SavedModel,https://github.com/IBM/MAX-Image-Caption-Generator/commit/e768521d7c360312cde9d78d4c58091e1ac0b058,Yes
4806,Stable-Baselines-Team/stable-baselines,baselines/common/tests/test_atari.py,6bff8e607154b6568c4d229f2c03d23a04d1066e,FIXME: However the fix requires a refactoring of ALL the models to deal with their session internaly.,https://github.com/Stable-Baselines-Team/stable-baselines/commit/6bff8e607154b6568c4d229f2c03d23a04d1066e,Yes
4807,Stable-Baselines-Team/stable-baselines,tests/test_multiple_learn.py,4fada47f1b71b7548c935b1f01c6fb04199b3d54,TODO: Fix multiple-learn on commented-out models (Issue #619).,https://github.com/Stable-Baselines-Team/stable-baselines/commit/4fada47f1b71b7548c935b1f01c6fb04199b3d54,Yes
4808,PPPLDeepLearning/plasma-python,plasma/models/builder.py,2cde1a763083616f35bc8b77dfabbc3db1a1aa07,FIXME this is essentially the ModelBuilder.build_model,https://github.com/PPPLDeepLearning/plasma-python/commit/2cde1a763083616f35bc8b77dfabbc3db1a1aa07,Yes
4809,sbl-sdsc/mmtf-pyspark,mmtfPyspark/src/demos/ml/secondaryStructureWord2VecModelEncoder.py,fd92a3c9860a63d1f424afdacf7e80cf7b913cb0,TODO need model file,https://github.com/sbl-sdsc/mmtf-pyspark/commit/fd92a3c9860a63d1f424afdacf7e80cf7b913cb0,Yes
4810,sbl-sdsc/mmtf-pyspark,mmtfPyspark/utils/columnarStructure.py,e6e9af6a95544902fc3e621a289712f7f3cbc744,TODO use model number as input and return only one model at a time; e.g. default: model=1,https://github.com/sbl-sdsc/mmtf-pyspark/commit/e6e9af6a95544902fc3e621a289712f7f3cbc744,Yes
4811,sbl-sdsc/mmtf-pyspark,mmtfPyspark/utils/mmtfSubstructure.py,225ecd1a3db1e9586ab46d7f1b6fa0dc6d00bcf4,TODO consider multiple models,https://github.com/sbl-sdsc/mmtf-pyspark/commit/225ecd1a3db1e9586ab46d7f1b6fa0dc6d00bcf4,Yes
4812,MichiganCOG/M-PACT,models/template_model.py,0f3fe74336e559f2f0e831c8fdb90b883fa4027a,# TODO: Add default model weights to models\/weights\/ and import them here  #,https://github.com/MichiganCOG/M-PACT/commit/0f3fe74336e559f2f0e831c8fdb90b883fa4027a,Yes
4813,MichiganCOG/M-PACT,models/model_template.py,79972094626144e8767e566256388f773140f503,TODO Every model must return a layer named 'logits',https://github.com/MichiganCOG/M-PACT/commit/79972094626144e8767e566256388f773140f503,Yes
4814,MichiganCOG/M-PACT,models/model_template.py,722e4387520b394819f07277b4382a9f81e26f09,TODO Every model must return a layer named 'logits',https://github.com/MichiganCOG/M-PACT/commit/722e4387520b394819f07277b4382a9f81e26f09,Yes
4815,MichiganCOG/M-PACT,models/tsn/tsn_model.py,722e4387520b394819f07277b4382a9f81e26f09,TODO Every model must return a layer named 'logits',https://github.com/MichiganCOG/M-PACT/commit/722e4387520b394819f07277b4382a9f81e26f09,Yes
4816,MichiganCOG/M-PACT,models/tsn/tsn_model.py,722e4387520b394819f07277b4382a9f81e26f09,# TODO: Add default model weights to models\/weights\/ and import them here  #,https://github.com/MichiganCOG/M-PACT/commit/722e4387520b394819f07277b4382a9f81e26f09,Yes
4817,aws-samples/deep-learning-models,models/nlp/albert/run_squad.py,3f9e8df7c6c217e7627e9f40b779e33830dacc8c,TODO: Improve. If only tf.keras.clone_model(model) worked.,https://github.com/aws-samples/deep-learning-models/commit/3f9e8df7c6c217e7627e9f40b779e33830dacc8c,Yes
4818,NLPatVCU/medaCy,medacy/tools/model_from_file.py,41f0c32bb257f4f16025f11050def5b5ee53b673,TODO Finish implementing loading in and initializing a Model object from a stored description,https://github.com/NLPatVCU/medaCy/commit/41f0c32bb257f4f16025f11050def5b5ee53b673,Yes
4819,NLPatVCU/medaCy,medacy/pipeline_components/metamap/metamap_component.py,e85ee0eac5582eba1b248d9dd212b6c0351b7858,TODO REMOVE when implemnting live model prediction,https://github.com/NLPatVCU/medaCy/commit/e85ee0eac5582eba1b248d9dd212b6c0351b7858,Yes
4820,NLPatVCU/medaCy,medacy/pipeline_components/metamap/metamap_component.py,e85ee0eac5582eba1b248d9dd212b6c0351b7858,TODO refactor second part of if statement when implementing live model prediction,https://github.com/NLPatVCU/medaCy/commit/e85ee0eac5582eba1b248d9dd212b6c0351b7858,Yes
4821,NLPatVCU/medaCy,medacy/pipeline_components/metamap/metamap_component.py,2a1c0f788e8192ec9a95f194990b496ab6720e34,TODO REMOVE when implemnting live model prediction,https://github.com/NLPatVCU/medaCy/commit/2a1c0f788e8192ec9a95f194990b496ab6720e34,Yes
4822,NLPatVCU/medaCy,medacy/pipeline_components/metamap/metamap_component.py,2a1c0f788e8192ec9a95f194990b496ab6720e34,TODO refactor second part of if statement when implementing live model prediction,https://github.com/NLPatVCU/medaCy/commit/2a1c0f788e8192ec9a95f194990b496ab6720e34,Yes
4823,NLPatVCU/medaCy,medacy/pipeline_components/feature_overlayers/metamap/metamap_all_types_component.py,5be19816b063ad9663f01fd1ecb240f1387a6082,TODO refactor second part of if statement when implementing live model prediction,https://github.com/NLPatVCU/medaCy/commit/5be19816b063ad9663f01fd1ecb240f1387a6082,Yes
4824,Epistimio/orion,src/orion/analysis/partial_dependency_utils.py,794efebc69ff3cc9451d5789d16aee8dd908891f,TODO move model stuff from lpi to analysis.base,https://github.com/Epistimio/orion/commit/794efebc69ff3cc9451d5789d16aee8dd908891f,Yes
4825,castorini/castor,anserini_dependency/api.py,ad69d3abc24e22608c14c332ec81e07bbe99a124,FIXME: get the answer from the PyTorch model here,https://github.com/castorini/castor/commit/ad69d3abc24e22608c14c332ec81e07bbe99a124,Yes
4826,molecularsets/moses,mnist4molecules/junction-tree-vae/train.py,8228c7b6a759877791d3b6ad150f204c18cfa943,model.load_state_dict(torch.load(jt_config.model_load)) # TODO,https://github.com/molecularsets/moses/commit/8228c7b6a759877791d3b6ad150f204c18cfa943,No
4827,molecularsets/moses,mnist4molecules/junction_tree/scripts/train.py,50a03868dc3f71791d5fbd619f726aaf6a14c595,model.load_state_dict(torch.load(jt_config.model_load)) # TODO,https://github.com/molecularsets/moses/commit/50a03868dc3f71791d5fbd619f726aaf6a14c595,No
4828,uchicago-cs/deepdish,deepdish/tools/caffe/maker.py,1b1e36a4c89ef27bce2f56be839f8bc265b14c29,Test model (TODO: this is an ugly and brittle line),https://github.com/uchicago-cs/deepdish/commit/1b1e36a4c89ef27bce2f56be839f8bc265b14c29,Yes
4829,castorini/hedwig,anserini_dependency/api.py,ad69d3abc24e22608c14c332ec81e07bbe99a124,FIXME: get the answer from the PyTorch model here,https://github.com/castorini/hedwig/commit/ad69d3abc24e22608c14c332ec81e07bbe99a124,Yes
4830,analysiscenter/batchflow,batchflow/tests/model_save_load_test.py,557eb6ecf87da2c3c8f2947e40bfcaf0d367de0c,TODO Create Pipeline; init model; predict on fake dataset; save predictions; save model,https://github.com/analysiscenter/batchflow/commit/557eb6ecf87da2c3c8f2947e40bfcaf0d367de0c,Yes
4831,analysiscenter/batchflow,batchflow/tests/model_save_load_test.py,557eb6ecf87da2c3c8f2947e40bfcaf0d367de0c,TODO load saved model; predict on same dataset; compare predictions to saved ones,https://github.com/analysiscenter/batchflow/commit/557eb6ecf87da2c3c8f2947e40bfcaf0d367de0c,Yes
4832,textpipe/textpipe,textpipe/wrappers.py,799ea921f82f2614d26a4bca6fe713e8b87a7eaf,TODO: throw error if given model does not exist (instead of returning empty vectors),https://github.com/textpipe/textpipe/commit/799ea921f82f2614d26a4bca6fe713e8b87a7eaf,Yes
4833,mozilla/bugbug,scripts/check.py,f4b2b938be638533bcdab54ea0d35043d9ee3ff1,TODO: What is the standard file path of the models?,https://github.com/mozilla/bugbug/commit/f4b2b938be638533bcdab54ea0d35043d9ee3ff1,No
4834,mozilla/bugbug,http_service/models.py,1bae5834abd49f70c69da84d36b66f5f5cf503b0,TODO: Do not crash when the asked model is not one of the trained models,https://github.com/mozilla/bugbug/commit/1bae5834abd49f70c69da84d36b66f5f5cf503b0,Yes
4835,mozilla/bugbug,http_service/models.py,8fc95985191ad28b76d4329562133a1d7ea585de,TODO: Do not crash when the asked model is not one of the trained models,https://github.com/mozilla/bugbug/commit/8fc95985191ad28b76d4329562133a1d7ea585de,Yes
4836,mozilla/bugbug,http_service/models.py,a8faa48d8a14c3f100c0d7b90744790712cb25fe,TODO: Cache the model in the process memory; it's quite hard as the RQ,https://github.com/mozilla/bugbug/commit/a8faa48d8a14c3f100c0d7b90744790712cb25fe,Yes
4837,mozilla/bugbug,scripts/regressor_finder.py,77ec8b529dfc0b726b9696668a41e84000c0ca70,TODO: Switch to the pure Defect model; as it's better in this case.,https://github.com/mozilla/bugbug/commit/77ec8b529dfc0b726b9696668a41e84000c0ca70,Yes
4838,mozilla/bugbug,bugbug/models/fixtime.py,c63ddc8ea13d9de032648512c72314710a122d25,TODO: Support modeling fix time at filing time or at assignment time,https://github.com/mozilla/bugbug/commit/c63ddc8ea13d9de032648512c72314710a122d25,Yes
4839,mozilla/bugbug,bugbug/models/fixtime.py,c63ddc8ea13d9de032648512c72314710a122d25,TODO: Support modeling fix time for a subset of bugs (e.g. regressions only),https://github.com/mozilla/bugbug/commit/c63ddc8ea13d9de032648512c72314710a122d25,Yes
4840,mozilla/bugbug,bugbug/models/fixtime.py,c63ddc8ea13d9de032648512c72314710a122d25,TODO: Support modeling fix time with different number of quantiles,https://github.com/mozilla/bugbug/commit/c63ddc8ea13d9de032648512c72314710a122d25,Yes
4841,skggm/skggm,inverse_covariance/profiling/statistical_power.py,c9cdad1999b6e88bf23d64dee9a174b7b2575c9c,''' || TODO:   ||     This needs to become something that can measure performance of the  ||     randomized ModelAverage method since thresholding the output there is  ||     appropriate. || ''',https://github.com/skggm/skggm/commit/c9cdad1999b6e88bf23d64dee9a174b7b2575c9c,Yes
4842,msmbuilder/osprey,osprey/strategies.py,6e8bb5b43bbd38458a81855a3396c5e9b2ae68a4,TODO make type of model dependent on input param,https://github.com/msmbuilder/osprey/commit/6e8bb5b43bbd38458a81855a3396c5e9b2ae68a4,No
4843,persephone-tools/persephone,persephone/tests/test_tutorial.py,5ee2ad7f7f36ea36e89c530748999a5fd4c67a4d,TODO Assert the convergence of the model at the end by reading the,https://github.com/persephone-tools/persephone/commit/5ee2ad7f7f36ea36e89c530748999a5fd4c67a4d,Yes
4844,persephone-tools/persephone,persephone/tests/experiments/test_na.py,f4b7560bdac301b4536647331101905131d0cdb9,TODO Currently assumes we're on slug. Need to package up the model and,https://github.com/persephone-tools/persephone/commit/f4b7560bdac301b4536647331101905131d0cdb9,Yes
4845,SMTorg/smt,smt/sm.py,52c44e72bc7ea73cd59f0badfd61326dd8858de7,TODO: Complete the mixture of expert model: verify from if self.options['name'] == 'MixExp': (predict),https://github.com/SMTorg/smt/commit/52c44e72bc7ea73cd59f0badfd61326dd8858de7,Yes
4846,SMTorg/smt,smt/extensions/moe.py,fd62de4bacf57437a96e9d6c87fc9394fcb7ec60,TODO : choice of the surrogate model experts to be used,https://github.com/SMTorg/smt/commit/fd62de4bacf57437a96e9d6c87fc9394fcb7ec60,Yes
4847,SMTorg/smt,smt/extensions/moe.py,fd62de4bacf57437a96e9d6c87fc9394fcb7ec60,TODO : add factory to get proper surrogate model object,https://github.com/SMTorg/smt/commit/fd62de4bacf57437a96e9d6c87fc9394fcb7ec60,Yes
4848,SMTorg/smt,smt/extensions/moe.py,c1afd18a2eb7c8cce3b92b5aeaeeb880cefdce65,TODO : MOE should be a true 'surrogate model' object,https://github.com/SMTorg/smt/commit/c1afd18a2eb7c8cce3b92b5aeaeeb880cefdce65,Yes
4849,SMTorg/smt,smt/extensions/moe.py,c1afd18a2eb7c8cce3b92b5aeaeeb880cefdce65,TODO: should we add leaf surrogate models options?,https://github.com/SMTorg/smt/commit/c1afd18a2eb7c8cce3b92b5aeaeeb880cefdce65,No
4850,SMTorg/smt,smt/surrogate_models/surrogate_model.py,8cfc6467dd328c21681c3a559bf17bb628694cf9,TODO: Complete the mixture of expert model: verify from if self.options['name'] == 'MixExp': (predict),https://github.com/SMTorg/smt/commit/8cfc6467dd328c21681c3a559bf17bb628694cf9,Yes
4851,chainer/onnx-chainer,tests/test_external_converter.py,262347a0f18ecb5289268ebe2566defdc8889b52,TODO(take-cheeze) Investigate why onnx.checker.check_model succeed,https://github.com/chainer/onnx-chainer/commit/262347a0f18ecb5289268ebe2566defdc8889b52,Yes
4852,alan-turing-institute/sktime,sktime/forecasting/forecasters.py,5353a0e1fb8a5e4345d1827ca84e99320ccd7cec,TODO how to pass seasonal frequency for ARIMA model,https://github.com/alan-turing-institute/sktime/commit/5353a0e1fb8a5e4345d1827ca84e99320ccd7cec,No
4853,alan-turing-institute/sktime,sktime/forecasting/forecasters.py,e9a4d39e238bc8076f0f76c1286552cb45e23c90,TODO add more constructor\/fit options from statsmodels,https://github.com/alan-turing-institute/sktime/commit/e9a4d39e238bc8076f0f76c1286552cb45e23c90,Yes
4854,SaltieRL/Saltie,bot_code/models/base_model.py,43f667b02b6f6a3725e46c7a8a8f4c8d53b4e3cf,TODO: This seems sepcific to agents. Refactor this out of base_model.py,https://github.com/SaltieRL/Saltie/commit/43f667b02b6f6a3725e46c7a8a8f4c8d53b4e3cf,Yes
4855,IntelAI/models,benchmarks/common/base_benchmark_util.py,560b0cfdee615d08d07ca4a600620294c27009ee,TODO: change this to search for the model_init.py based on the,https://github.com/IntelAI/models/commit/560b0cfdee615d08d07ca4a600620294c27009ee,Yes
4856,IntelAI/models,models/image_recognition/tensorflow/squeezenet/fp32/deployment/model_deploy.py,5f0f82d5b1303d17522789a0132c7aa222348420,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/IntelAI/models/commit/5f0f82d5b1303d17522789a0132c7aa222348420,Yes
4857,mondejar/ecg-classification,my_dnn_mitdb.py,8ceb7d449a146e9b66db9e35b6975f612377a74d,TODO but this must be done with my own Model Classifier at the loss selection,https://github.com/mondejar/ecg-classification/commit/8ceb7d449a146e9b66db9e35b6975f612377a74d,Yes
4858,ThibaultGROUEIX/3D-CODED,training/train_sup.py,53a2b52338e87905ba3fcfd9ae33eb65ed39f6f7,TODO : #TODO : Regress should be in the model,https://github.com/ThibaultGROUEIX/3D-CODED/commit/53a2b52338e87905ba3fcfd9ae33eb65ed39f6f7,Yes
4859,brightmart/ai_law,HAN_train.py,30a11f154301c77706bb06e364a27759030b29dc,"save_path = FLAGS.ckpt_dir + \""model.ckpt\"" #TODO temp remove==>only save checkpoint for each epoch once.",https://github.com/brightmart/ai_law/commit/30a11f154301c77706bb06e364a27759030b29dc,Yes
4860,brightmart/ai_law,HAN_train.py,1ddc6853194f18da0b2ada4afbdaf08662abc786,assign_pretrained_word_embedding(sess; vocabulary_index2word; vocab_size; model;FLAGS.word2vec_model_path2;model.Embedding2) #TODO,https://github.com/brightmart/ai_law/commit/1ddc6853194f18da0b2ada4afbdaf08662abc786,No
4861,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: BELOW IS NOT THE CASE IF MODEL IS NN - SETTING THE GLOBAL RANDOM SEED DOES SOMETHING,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
4862,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Check :attr:`module_name`'s library_helper for :attr:`model_initializer` for a default `hyperparameter` list,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
4863,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: Create `Keras` key based on compiled model architecture; which is far more accurate,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
4864,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/models.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: Grab all '__hh' attrs from `model.layers` - `load_model` fucks with '<kernel\/bias>_initializer',https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
4865,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/models.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: After setting `self.model` to result of `load_model`; revert the '__hh' attrs to saved values,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
4866,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/recorders.py,a8eb742cc6dd21f666fe37e40449919d5f8bc413,TODO: Model's `get_config()` returns the final learning rate; rather than the initial one; so experiment description,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/a8eb742cc6dd21f666fe37e40449919d5f8bc413,Yes
4867,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,0313fdf033240d365569d175a373947c87347262,TODO: Add `model` here; with a `TranslateTrace` decorator; and document it below,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/0313fdf033240d365569d175a373947c87347262,Yes
4868,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/optimization_core.py,0313fdf033240d365569d175a373947c87347262,model=None;  # TODO: May need to pass `model` from `set_experiment_guidelines`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/0313fdf033240d365569d175a373947c87347262,Yes
4869,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,"@TranslateTrace(\""model\""; (\""model_initializer\""; \""model_init_params\""))  # TODO: Add when tested with `Mirror`",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,Yes
4870,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,TODO: When `TranslateTrace` added document `model` below with expectation that if `model`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,Yes
4871,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,TODO: ... given; (`model_initializer`; `model_init_params`) should not be; and vice versa,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,No
4872,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,TODO: `model` (Class instance; default=None);,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,No
4873,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,"TODO: `model_initializer`\/`model_init_params` docstring types += \""default=None\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,No
4874,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiments.py,5da3d6f22ea35adaec13a71f91091eb617b412d6,self._model_original = model  # TODO: Add for `TranslateTrace`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/5da3d6f22ea35adaec13a71f91091eb617b412d6,No
4875,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/key_handler.py,b2c44294f81ac4170ea89ada6c47193a8c9c741e,TODO: Check here if callable; and using a `Trace`d model\/model_initializer,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/b2c44294f81ac4170ea89ada6c47193a8c9c741e,Yes
4876,oscarknagg/few-shot,few_shot/maml.py,aecd896796ba2020b0b0ea48276c0b2aff203427,TODO: determine num input channels from model or input data,https://github.com/oscarknagg/few-shot/commit/aecd896796ba2020b0b0ea48276c0b2aff203427,Yes
4877,oscarknagg/few-shot,few_shot/maml.py,b0a974b16389f5412466013ff06fc9d16422c1f5,TODO: determine num input channels from model or input data,https://github.com/oscarknagg/few-shot/commit/b0a974b16389f5412466013ff06fc9d16422c1f5,Yes
4878,BenWhetton/keras-surgeon,src/kerasprune/prune.py,193eb26c01466c2e5e5d1bf6ac7d92feacd71264,TODO: This won't work with layers shared between models\/messy layers,https://github.com/BenWhetton/keras-surgeon/commit/193eb26c01466c2e5e5d1bf6ac7d92feacd71264,Yes
4879,f90/Wave-U-Net,Predict.py,86661874f4a302bdb1b4834641055e19832f9395,TODO add a pretrained model?,https://github.com/f90/Wave-U-Net/commit/86661874f4a302bdb1b4834641055e19832f9395,No
4880,graykode/gpt-2-Pytorch,generate_unconditional_samples.py,5110327ef3098259803e0180865124e59108fce5,@TODO replace tensorflow to pytorch model,https://github.com/graykode/gpt-2-Pytorch/commit/5110327ef3098259803e0180865124e59108fce5,Yes
4881,voxelmorph/voxelmorph,voxelmorph/layers.py,cedececa6ae35f3491d882b43c5ac8ebc387e5a9,"\""\""\"" || Layers for voxelmorph model ||  || TODO: clean up and join with neuron.layers || \""\""\""",https://github.com/voxelmorph/voxelmorph/commit/cedececa6ae35f3491d882b43c5ac8ebc387e5a9,Yes
4882,pavlin-policar/openTSNE,tsne/tsne.py,7aaa295c13987c7711fa3f15f58998c5cce13774,TODO: This is wrong. We'd want to use the PCA model fit on training data,https://github.com/pavlin-policar/openTSNE/commit/7aaa295c13987c7711fa3f15f58998c5cce13774,Yes
4883,youyuge34/Anime-InPainting,demo_patch.py,9bdd68b1f07e7a96afdc69f6b66b405a2f8a4539,TODO: edge model,https://github.com/youyuge34/Anime-InPainting/commit/9bdd68b1f07e7a96afdc69f6b66b405a2f8a4539,Yes
4884,ShuangLI59/person_search,caffe-fast-rcnn/python/detect.py,52350f294541ceee9cb5b3c04ab728a7babf0bed,"\""\""\"" || detector.py is an out-of-the-box windowed detector || callable from the command line. ||  || By default it configures and runs the Caffe reference ImageNet model. || Note that this model was trained for image classification and not detection; || and finetuning for detection can be expected to improve results. ||  || The selective_search_ijcv_with_python code required for the selective search || proposal mode is available at ||     https:\/\/github.com\/sergeyk\/selective_search_ijcv_with_python ||  || TODO: || - batch up image filenames as well: don't want to load all of them into memory || - come up with a batching scheme that preserved order \/ keeps a unique ID || \""\""\""",https://github.com/ShuangLI59/person_search/commit/52350f294541ceee9cb5b3c04ab728a7babf0bed,No
4885,astorfi/3D-convolutional-speaker-recognition,1-development/deployment/model_deploy.py,af8b21a2ac3a04cbe60a749846a63ce1584112b8,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/astorfi/3D-convolutional-speaker-recognition/commit/af8b21a2ac3a04cbe60a749846a63ce1584112b8,No
4886,astorfi/3D-convolutional-speaker-recognition,2-enrollment/deployment/model_deploy.py,af8b21a2ac3a04cbe60a749846a63ce1584112b8,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/astorfi/3D-convolutional-speaker-recognition/commit/af8b21a2ac3a04cbe60a749846a63ce1584112b8,No
4887,astorfi/3D-convolutional-speaker-recognition,3-evaluation/deployment/model_deploy.py,af8b21a2ac3a04cbe60a749846a63ce1584112b8,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/astorfi/3D-convolutional-speaker-recognition/commit/af8b21a2ac3a04cbe60a749846a63ce1584112b8,No
4888,brightmart/bert_language_understanding,data_util_hdf5.py,b3bb4a3acc10a28a2718a72442feaf804b37327d,TODO model.Embedding. assign this value to our embedding variables of our model.,https://github.com/brightmart/bert_language_understanding/commit/b3bb4a3acc10a28a2718a72442feaf804b37327d,Yes
4889,brightmart/bert_language_understanding,train_bert_fine_tuning.py,29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,from model.bert_model import BertModel # TODO TODO TODO test whether pretrain can boost perofrmance with other model,https://github.com/brightmart/bert_language_understanding/commit/29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,Yes
4890,brightmart/bert_language_understanding,train_bert_lm.py,29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,from model.bert_model import BertModel # TODO TODO TODO test whether pretrain can boost perofrmance with other model,https://github.com/brightmart/bert_language_understanding/commit/29dcb8611592f1ed65fa31fbec2bb6fff3b5c863,Yes
4891,maxpumperla/elephas,tests/test_ml_model.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test basic ml model\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,Yes
4892,maxpumperla/elephas,tests/test_spark_model.py,328b026cf5525b716bb41f26922183b35d3075b8,TODO test basic spark model\uFFFF,https://github.com/maxpumperla/elephas/commit/328b026cf5525b716bb41f26922183b35d3075b8,Yes
4893,benfred/implicit,tests/recommender_base_test.py,7f30220d6b0be36963b9d25cc7f56827bf7077d3,TODO: if we set regularization for the model to be sufficiently high; the,https://github.com/benfred/implicit/commit/7f30220d6b0be36963b9d25cc7f56827bf7077d3,Yes
4894,shenweichen/DeepCTR,tests/models/FGCNN_test.py,0c0a7189dbdee25aa538e2986d5cc24430c02077,TODO: add model_io check,https://github.com/shenweichen/DeepCTR/commit/0c0a7189dbdee25aa538e2986d5cc24430c02077,Yes
4895,shenweichen/DeepCTR,tests/models/FGCNN_test.py,b787cdf9d9d1c7f500f91ccdeb87aa6d7725ae14,TODO: add model_io check,https://github.com/shenweichen/DeepCTR/commit/b787cdf9d9d1c7f500f91ccdeb87aa6d7725ae14,Yes
4896,AKSHAYUBHAT/DeepVideoAnalytics,repos/object_detection/evaluator.py,e1068503267ee475d33e77523b049165a6548614,TODO: This should be done in model's postprocess function ideally.,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/e1068503267ee475d33e77523b049165a6548614,Yes
4897,AKSHAYUBHAT/DeepVideoAnalytics,repos/tfdetection/object_detection/evaluator.py,54370fcfcd1b6fe237fe004045103fd12da756f1,TODO: This should be done in model's postprocess function ideally.,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/54370fcfcd1b6fe237fe004045103fd12da756f1,Yes
4898,AKSHAYUBHAT/DeepVideoAnalytics,repos/object_detection/eval_util.py,19b2103de8a502a6569c6a1dce0d75c5465d66dd,TODO: This should be done in model's postprocess,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/19b2103de8a502a6569c6a1dce0d75c5465d66dd,Yes
4899,AKSHAYUBHAT/DeepVideoAnalytics,repos/object_detection/evaluator.py,19b2103de8a502a6569c6a1dce0d75c5465d66dd,TODO: This should be done in model's postprocess function ideally.,https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/19b2103de8a502a6569c6a1dce0d75c5465d66dd,Yes
4900,AKSHAYUBHAT/DeepVideoAnalytics,repos/slim/deployment/model_deploy.py,19b2103de8a502a6569c6a1dce0d75c5465d66dd,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/commit/19b2103de8a502a6569c6a1dce0d75c5465d66dd,Yes
4901,sloria/TextBlob,nltk-3.0a0/nltk/app/concordance_app.py,d539a164ed1cac8239b632a778946248a00c0c42,todo: refactor the model such that it is less state sensitive,https://github.com/sloria/TextBlob/commit/d539a164ed1cac8239b632a778946248a00c0c42,Yes
4902,deepfakes/faceswap,scripts/convert.py,bb489f4f517052fe4b15a6d249acde5bdc6beb60,TODO move the model load and the converter creation in a method called on init; but after the arg parsing,https://github.com/deepfakes/faceswap/commit/bb489f4f517052fe4b15a6d249acde5bdc6beb60,Yes
4903,deepfakes/faceswap,plugins/plugin_loader.py,b8598be2b68a6d11b81d871d6bf453809a7cdd5e,TODO Remove this hacky fix when we move them to the same models,https://github.com/deepfakes/faceswap/commit/b8598be2b68a6d11b81d871d6bf453809a7cdd5e,Yes
4904,deepfakes/faceswap,plugins/train/model/_base.py,a62fddf78698269b6b30fbc36b2a4e38208a9176,TODO Currently any models which have a list input will not contain the main model,https://github.com/deepfakes/faceswap/commit/a62fddf78698269b6b30fbc36b2a4e38208a9176,Yes
4905,dsgissin/DiscriminativeActiveLearning,models.py,1cc684947a890d5ee44a88e1546d3a805c2af70d,train the model:  TODO - fix the ugly if statements and put this in the arguments of the script,https://github.com/dsgissin/DiscriminativeActiveLearning/commit/1cc684947a890d5ee44a88e1546d3a805c2af70d,Yes
4906,lopuhin/transformer-lm,lm/inference.py,dded91f624b82ffbe0d8f27c0023e9118c643f77,TODO - check prev argument for the model,https://github.com/lopuhin/transformer-lm/commit/dded91f624b82ffbe0d8f27c0023e9118c643f77,Yes
4907,Shun14/TextBoxes_plusplus_Tensorflow,deployment/model_deploy.py,dc5d7b7c5bc9df657bf66122c54685af158fc3a6,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||     g = tf.Graph() ||  ||     # Set up DeploymentConfig ||     config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||     # Create the global step on the device storing the variables. ||     with tf.device(config.variables_device()): ||         global_step = slim.create_global_step() ||  ||     # Define the inputs ||     with tf.device(config.inputs_device()): ||         images; labels = LoadData(...) ||         inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||     # Define the optimizer. ||     with tf.device(config.optimizer_device()): ||         optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||     # Define the model including the loss. ||     def model_fn(inputs_queue): ||         images; labels = inputs_queue.dequeue() ||         predictions = CreateNetwork(images) ||         slim.losses.log_loss(predictions; labels) ||  ||     model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                    optimizer=optimizer) ||  ||     # Run training. ||     slim.learning.train(model_dp.train_op; my_log_dir; ||                                             summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||     * outputs: The return values of the calls to `model_fn()`. ||     * scope: The scope used to create the clone. ||     * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||     * train_op: An operation that run the optimizer training op and include ||         all the update ops created by `model_fn`. Present only if an optimizer ||         was specified. ||     * summary_op: An operation that run the summaries created by `model_fn` ||         and process_gradients. ||     * total_loss: A `Tensor` that contains the sum of all losses created by ||         `model_fn` plus the regularization losses. ||     * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||     * num_clones: Number of model clones to deploy in each replica. ||     * clone_on_cpu: True if clones should be placed on CPU. ||     * replica_id: Integer.  Index of the replica for which the model is ||             deployed.  Usually 0 for the chief replica. ||     * num_replicas: Number of replicas to use. ||     * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||     * worker_job_name: A name for the worker job. ||     * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||     - describe side effect to the graph. ||     - what happens to summaries and update_ops. ||     - which graph collections are altered. ||     - write a tutorial on how to use this. ||     - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/Shun14/TextBoxes_plusplus_Tensorflow/commit/dc5d7b7c5bc9df657bf66122c54685af158fc3a6,Yes
4908,wuhao5688/RNN-TrajModel,trajmodel.py,93533d4a764089c54ad3d5e83809c42e90f6beb3,"fwd_params = [v for v in tf.all_variables() if v.name.startswith(self.model_scope + \""\/\"" + var_scope)]  # TODO",https://github.com/wuhao5688/RNN-TrajModel/commit/93533d4a764089c54ad3d5e83809c42e90f6beb3,Yes
4909,wuhao5688/RNN-TrajModel,trajmodel.py,93533d4a764089c54ad3d5e83809c42e90f6beb3,"params = [v for v in tf.all_variables() if v.name.startswith(self.model_scope + \""\/\"" + var_scope)]  # TODO",https://github.com/wuhao5688/RNN-TrajModel/commit/93533d4a764089c54ad3d5e83809c42e90f6beb3,Yes
4910,mdangschat/ctc-asr,python/model.py,a2ea955a2690e465779a2cf12b1d2c396c5f3420,TODO Move to model.py,https://github.com/mdangschat/ctc-asr/commit/a2ea955a2690e465779a2cf12b1d2c396c5f3420,Yes
4911,tugstugi/mongolian-speech-recognition,train.py,ce775775ef949cac4345e7dfbf8dce1f9b0e254e,TODO: check whether the model is QuartzNet,https://github.com/tugstugi/mongolian-speech-recognition/commit/ce775775ef949cac4345e7dfbf8dce1f9b0e254e,Yes
4912,rwightman/posenet-python,posenet/utils.py,51cf48e3473362be1b5b07445b53433695c8d7b1,TODO explore impact of scaling on model performance,https://github.com/rwightman/posenet-python/commit/51cf48e3473362be1b5b07445b53433695c8d7b1,Yes
4913,tasoc/starclass,run_training.py,695cb85e29b5aa21b1045071bd42407eebf90ae7,TODO: Make sure this doesn't load a previous trained model!,https://github.com/tasoc/starclass/commit/695cb85e29b5aa21b1045071bd42407eebf90ae7,Yes
4914,tasoc/starclass,starclass/features/freqextr2.py,b32557030223a74db3a15b31f22d93f5288a4038,TODO: Replace with ps.ls.model?,https://github.com/tasoc/starclass/commit/b32557030223a74db3a15b31f22d93f5288a4038,Yes
4915,tasoc/starclass,starclass/features/freqextr.py,6276d95831804debc2ffac47575270e3d443f8f8,TODO: Replace with ps.ls.model?,https://github.com/tasoc/starclass/commit/6276d95831804debc2ffac47575270e3d443f8f8,Yes
4916,tasoc/starclass,starclass/features/freqextr.py,52818d57210c3dceb5bcc06f4a8a6eaa1b1ae732,TODO: Replace with ps.ls.model?,https://github.com/tasoc/starclass/commit/52818d57210c3dceb5bcc06f4a8a6eaa1b1ae732,Yes
4917,tasoc/starclass,starclass/features/powerspectrum.py,60c1a106af9727d65d49691c3a8751c8f3093f58,TODO: Replace with ps.ls.model?,https://github.com/tasoc/starclass/commit/60c1a106af9727d65d49691c3a8751c8f3093f58,Yes
4918,sintefneodroid/agent,agents/ppo_agent.py,9a5454a2f54703047c94934a615170e62aba6aaa,TODO: dont use _model as model,https://github.com/sintefneodroid/agent/commit/9a5454a2f54703047c94934a615170e62aba6aaa,Yes
4919,sintefneodroid/agent,agents/ac_agent.py,ef380ba210d60ae0436482509079962ac512202e,TODO: do not use _model as model,https://github.com/sintefneodroid/agent/commit/ef380ba210d60ae0436482509079962ac512202e,Yes
4920,AllenInstitute/mouse_connectivity_models,voxel_model/model_data.py,61d89222edb6c7eb03905ac1db2002fe45eb6f36,TODO : revise ModelData docstring :: Attributes,https://github.com/AllenInstitute/mouse_connectivity_models/commit/61d89222edb6c7eb03905ac1db2002fe45eb6f36,Yes
4921,AllenInstitute/mouse_connectivity_models,voxel_model/model_data.py,61d89222edb6c7eb03905ac1db2002fe45eb6f36,TODO : revise ModelData docstring :: Examples,https://github.com/AllenInstitute/mouse_connectivity_models/commit/61d89222edb6c7eb03905ac1db2002fe45eb6f36,Yes
4922,AllenInstitute/mouse_connectivity_models,mcmodels/core/base.py,f2e0658f91eea6fb80e3be86f3434b257d9a2e78,TODO: integrate into VoxelModelCache,https://github.com/AllenInstitute/mouse_connectivity_models/commit/f2e0658f91eea6fb80e3be86f3434b257d9a2e78,Yes
4923,ur-whitelab/hoomd-tf,htf/tensorflowcompute.py,9edc8c68c8309b8532817d7a55b01edfc9362957,TODO Check model dtype,https://github.com/ur-whitelab/hoomd-tf/commit/9edc8c68c8309b8532817d7a55b01edfc9362957,Yes
4924,dlt-rilmta/emtsv,personalities.py,fbe3e1898cfa1d20508e30140a04bd2ac9dce15c,TODO: fix model path later!,https://github.com/dlt-rilmta/emtsv/commit/fbe3e1898cfa1d20508e30140a04bd2ac9dce15c,Yes
4925,dlt-rilmta/emtsv,config.py,b2c2b8caaf1f27cbc22e602f826bbe69da9bd8fb,TODO: Chunk and NER model + NER config,https://github.com/dlt-rilmta/emtsv/commit/b2c2b8caaf1f27cbc22e602f826bbe69da9bd8fb,Yes
4926,dlt-rilmta/emtsv,config.py,a2504a712e3636f761852ce7b69635e225dee73b,TODO: Train new model!,https://github.com/dlt-rilmta/emtsv/commit/a2504a712e3636f761852ce7b69635e225dee73b,Yes
4927,nearthlab/image-segmentation,maskrcnn/layers/feature_pyramid_net.py,81154272814b6caa3a49d30d0018569ea40a71d0,TODO: replace this with segmentation_models' FPN,https://github.com/nearthlab/image-segmentation/commit/81154272814b6caa3a49d30d0018569ea40a71d0,Yes
4928,Merck/deepbgc,deepbgc/util.py,66a62a89bab08420d329cc4758402fe3513924ba,TODO: enable users to run model with older incompatible Pfam DB versions?,https://github.com/Merck/deepbgc/commit/66a62a89bab08420d329cc4758402fe3513924ba,Yes
4929,Neuraxio/Neuraxle,neuraxle/metaopt/random.py,2c69f0058b15dbd9df3f648fde5d7af52ecb01c2,TODO: use the splits and average the results?? instead of picking best model...,https://github.com/Neuraxio/Neuraxle/commit/2c69f0058b15dbd9df3f648fde5d7af52ecb01c2,Yes
4930,mikevoets/jama16-retina-replication,train.py,465d303e2cad7311b0c9c9be717520650abf493c,TODO: Load saved model.,https://github.com/mikevoets/jama16-retina-replication/commit/465d303e2cad7311b0c9c9be717520650abf493c,Yes
4931,ShreyAmbesh/Traffic-Rule-Violation-Detection-System,eval_util.py,c92e89118945f181e91391c12c51117e58def69e,TODO: This should be done in model's postprocess,https://github.com/ShreyAmbesh/Traffic-Rule-Violation-Detection-System/commit/c92e89118945f181e91391c12c51117e58def69e,Yes
4932,henrysky/astroNN,astroNN/shared/nn_tools.py,8c63439163afc1625612a8e45780784a1398ee53,TODO: Model Precision,https://github.com/henrysky/astroNN/commit/8c63439163afc1625612a8e45780784a1398ee53,No
4933,araffin/srl-zoo,evaluation/generateNNImages.py,701b04a5a70821b044e0e078b44c4300316ad513,TODO FIX AND ADD MODEL NAME TO SUPERVISED!,https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,Yes
4934,araffin/srl-zoo,evaluation/makeMovieComparingKNNAcrossModels.py,701b04a5a70821b044e0e078b44c4300316ad513,# TODO elif 'Priors' in model_folder: #                print 'Processi           print 'Processing Robotics priors model: '; path_to_neighbors,https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,Yes
4935,araffin/srl-zoo,evaluation/plotStates.py,701b04a5a70821b044e0e078b44c4300316ad513,ONLY FOR FAST TESTING !!:   model_name = MOBILE_ROBOT#STATIC_BUTTON_SIMPLEST#'pushingButton3DAugmented' #TODO REMOVE-testing  model_name = MOBILE_ROBOT,https://github.com/araffin/srl-zoo/commit/701b04a5a70821b044e0e078b44c4300316ad513,No
4936,araffin/srl-zoo,pipeline.py,1b48736e4499aa6960c5faed727effdfd6d7e18e,TODO create folder for experiment and models,https://github.com/araffin/srl-zoo/commit/1b48736e4499aa6960c5faed727effdfd6d7e18e,Yes
4937,araffin/srl-zoo,main.py,c42ccf51ac4e4b4d10f1248580c82924a0492178,A future enhancement TODO for running on multiple GPU: CUDA_VISIBLE_DEVICES=2;3 python main.py   and then also model = torch.nn.DataParallel(model; device_ids=[0;1]).cuda(),https://github.com/araffin/srl-zoo/commit/c42ccf51ac4e4b4d10f1248580c82924a0492178,Yes
4938,araffin/srl-zoo,main.py,b9801ed7b2541c0b86b1543ffffc7a12b20da858,A future enhancement TODO for running on multiple GPU: CUDA_VISIBLE_DEVICES=2;3 python main.py   and then also model = torch.nn.DataParallel(model; device_ids=[0;1]).cuda(),https://github.com/araffin/srl-zoo/commit/b9801ed7b2541c0b86b1543ffffc7a12b20da858,Yes
4939,araffin/srl-zoo,train.py,413b4121a079a9b103bc064cd0082cb67fe3929b,A future enhancement TODO for running on multiple GPU: CUDA_VISIBLE_DEVICES=2;3 python main.py   and then also model = torch.nn.DataParallel(model; device_ids=[0;1]).cuda(),https://github.com/araffin/srl-zoo/commit/413b4121a079a9b103bc064cd0082cb67fe3929b,Yes
4940,araffin/srl-zoo,baselines/autoencoder.py,58b5577b32dd65993913066060d2ce531fd56e68,TODO: load best model before predicting states,https://github.com/araffin/srl-zoo/commit/58b5577b32dd65993913066060d2ce531fd56e68,Yes
4941,araffin/srl-zoo,train.py,58b5577b32dd65993913066060d2ce531fd56e68,TODO: load best model before predicting states,https://github.com/araffin/srl-zoo/commit/58b5577b32dd65993913066060d2ce531fd56e68,Yes
4942,araffin/srl-zoo,train.py,fa1a5db226bd4c3c2759e6d2a70fbd1821467ab5,TODO: load best model before predicting states,https://github.com/araffin/srl-zoo/commit/fa1a5db226bd4c3c2759e6d2a70fbd1821467ab5,Yes
4943,araffin/srl-zoo,train.py,f80391f39df7ac1bb47a3bbba6c44ed0dbc0ef5a,TODO: load best model before predicting states,https://github.com/araffin/srl-zoo/commit/f80391f39df7ac1bb47a3bbba6c44ed0dbc0ef5a,Yes
4944,araffin/srl-zoo,train.py,f4961d6baf3fd809881f75484433904753aa9452,TODO: load best model before predicting states,https://github.com/araffin/srl-zoo/commit/f4961d6baf3fd809881f75484433904753aa9452,Yes
4945,YannDubs/disentangling-vae,run_experiment.py,6817e29ef47d956e94777a3bc6ab2d9321ef0bab,"TODO: use load_metadata in utils.modelIO + don't just use \""specs.json\""",https://github.com/YannDubs/disentangling-vae/commit/6817e29ef47d956e94777a3bc6ab2d9321ef0bab,Yes
4946,YannDubs/disentangling-vae,main_viz.py,b7ffd628e6821f86d8dce76244710d569eae3008,"TODO: use load_metadata in utils.modelIO + don't just use \""specs.json\""",https://github.com/YannDubs/disentangling-vae/commit/b7ffd628e6821f86d8dce76244710d569eae3008,Yes
4947,YannDubs/disentangling-vae,main_viz.py,d2640f048fb1183db660a60cf0656be918a9b5b3,"TODO: use load_metadata in utils.modelIO + don't just use \""specs.json\""",https://github.com/YannDubs/disentangling-vae/commit/d2640f048fb1183db660a60cf0656be918a9b5b3,Yes
4948,JingqingZ/BaiduTraffic,src/train.py,942814b5d5ec362e2e26d74cf0b62f5b202635cc,"model_name=\""all_comb_model_%d\"" % config.impact_k; # TODO update name",https://github.com/JingqingZ/BaiduTraffic/commit/942814b5d5ec362e2e26d74cf0b62f5b202635cc,Yes
4949,regel/loudml,loudml/loudml/timeseries.py,ac6994b8607595009f8df18dbf6af1b7a466d049,TODO have a Model.logger to prefix all logs with model name,https://github.com/regel/loudml/commit/ac6994b8607595009f8df18dbf6af1b7a466d049,Yes
4950,regel/loudml,loudml/loudml/fingerprints.py,07f192c18da57e63da30b151491560b576f77c6c,TODO have a Model.logger to prefix all logs with model name,https://github.com/regel/loudml/commit/07f192c18da57e63da30b151491560b576f77c6c,Yes
4951,regel/loudml,loudml/loudml/donut.py,be739a3a28157899fb339579dc2bcd134b587abd,TODO have a Model.logger to prefix all logs with model name,https://github.com/regel/loudml/commit/be739a3a28157899fb339579dc2bcd134b587abd,Yes
4952,maciejczyzewski/neural-chessboard,train.py,eb915e9ed37a252eb87b482e4c2c8a92776d5591,FIXME: it should be more general (MAIN model compt.),https://github.com/maciejczyzewski/neural-chessboard/commit/eb915e9ed37a252eb87b482e4c2c8a92776d5591,Yes
4953,ardamavi/Game-Bot,get_model.py,59ccaa260b6d50dc3f5e475fb84868023f60b96b,TODO: Model here,https://github.com/ardamavi/Game-Bot/commit/59ccaa260b6d50dc3f5e475fb84868023f60b96b,No
4954,drckf/paysage,paysage/paysage/models.py,b09b6c78761245e3662f3d3dcef5b8d1361d0aaf,"\""\""\""   || #TODO: || class HopfieldModel(LatentModel): ||      ||     def __init__(self; nvis; nhid): ||         self.layers = {} ||         self.layers['visible'] = layers.IsingLayer(nvis) ||         self.layers['hidden'] = layers.GaussianLayer(nhid) ||          ||         self.params = {} ||         self.params['weights'] = numpy.random.normal(loc=0.0; scale=1.0; size=(self.layers['visible'].len; self.layers['hidden'].len)).astype(dtype=numpy.float32) ||         self.params['bias'] = numpy.ones_like(self.layers['visible'].loc)   ||  ||  || class HookeMachine(LatentModel): ||      ||     def __init__(self; nvis; nhid; vis_type='gauss'; hid_type='expo'):    ||         assert vis_type.lower() in ['gauss'; 'ising'] ||         assert hid_type.lower() in ['expo'; 'bern'] ||          ||         self.layers = {} ||         self.layers['visible'] = layers.get(vis_type)(nvis) ||         self.layers['hidden'] = layers.get(hid_type)(nhid) ||          ||         self.params = {} ||         self.params['weights'] = numpy.random.normal(loc=0.0; scale=1.0; size=(self.layers['visible'].len; self.layers['hidden'].len)).astype(dtype=numpy.float32) ||         self.params['bias'] = numpy.ones_like(self.layers['hidden'].loc)   ||         self.params['T'] = numpy.ones(1; dtype=numpy.float32) ||                  ||         self.deriv = {} ||         self.deriv['weights'] = numpy.zeros_like(self.params['weights']) ||         self.deriv['bias'] = numpy.zeros_like(self.params['bias']) ||         self.params['T'] = numpy.zeros_like(self.params['T']) ||          ||         self.set_vis(numpy.zeros_like(self.layers['visible'].loc)) ||          ||     def set_vis(self; vis): ||         self.vis = vis ||         self.diff = (self.params['weights'].T - vis).T ||         self.squared_dist = numpy.sum(self.diff ** 2; axis=0) ||         self.layers['hidden'].update_params(self.params['bias'] + self.squared_dist \/ (2 * self.params['T'])) ||         self.energy = -numpy.sum(numpy.log(self.layers['hidden'].partition_function()))         ||          ||     def visible_conditional_params(self; hid): ||         total = numpy.sum(hid) ||         loc = numpy.dot(self.params['weights']; hid) \/ total ||         scale = self.params['T'] \/ total * numpy.ones_like(self.layers['visible'].loc) ||         return (loc; scale) ||          ||     def update_visible_params(self; hid): ||         self.layers['visible'].update_params(*self.visible_conditional_params(hid)) ||          ||     def derivatives(self; vis; key): ||         self.update_hidden_params(vis) ||         hidden_mean = self.layers['hidden'].mean() ||         if key == 'bias': ||             # del H(v; k) \/ del b ||             return hidden_mean ||         elif key == 'weights': ||             # del H(v; k) \/ del W ||             return (self.difference(vis) * hidden_mean.T) \/ self.params['T'] ||         elif key == 'T': ||             # del H(v;k) \/ del T ||             return numpy.dot(hidden_mean.T; self.squared_distance(vis)) ||         else: ||             raise ValueError('unknown key: {}'.format(key)) ||     \""\""\""",https://github.com/drckf/paysage/commit/b09b6c78761245e3662f3d3dcef5b8d1361d0aaf,Yes
4955,drckf/paysage,paysage/paysage/fit.py,5f5ff89d04f145d7807bf20d85557f5714096414,TODO: move resampling into the model class so that it can be alternated with gibbs,https://github.com/drckf/paysage/commit/5f5ff89d04f145d7807bf20d85557f5714096414,Yes
4956,drckf/paysage,paysage/fit.py,fb730773ed0061243fc6bff6ea66bb7519a7529e,TODO: should import the State class from model.py,https://github.com/drckf/paysage/commit/fb730773ed0061243fc6bff6ea66bb7519a7529e,Yes
4957,drckf/paysage,paysage/fit.py,2f17365b4e0ad411d5083e396fe254552ec22bac,TODO: should import the State class from model.py,https://github.com/drckf/paysage/commit/2f17365b4e0ad411d5083e396fe254552ec22bac,Yes
4958,maximecb/gym-miniworld,experiments/rand_agent.py,1d17bead39eb8b1643423ab5df01339b62a7b79a,TODO; start with 10 random models; evaluate them,https://github.com/maximecb/gym-miniworld/commit/1d17bead39eb8b1643423ab5df01339b62a7b79a,Yes
4959,cbg-ethz/pybda,koios/fit/glm_fit.py,4194310894b8fa8a5ce1a175b43fde09101f3b8e,TODO print p values for binomial model,https://github.com/cbg-ethz/pybda/commit/4194310894b8fa8a5ce1a175b43fde09101f3b8e,Yes
4960,DTUComputeStatisticsAndDataAnalysis/MBPLS,Unipls/unipls/mbpls.py,6f55e03ce86f5ab7fea55ec0fe1c0567352bbdd1,TODO: Write a bit about the models used in this text field,https://github.com/DTUComputeStatisticsAndDataAnalysis/MBPLS/commit/6f55e03ce86f5ab7fea55ec0fe1c0567352bbdd1,No
4961,ELS-RD/anonymisation,flair_train.py,bd82109b29d44ebd3a425ef9a1efb796feda7c36,TODO optimize LR https:\/\/github.com\/flairNLP\/flair\/blob\/master\/resources\/docs\/TUTORIAL_8_MODEL_OPTIMIZATION.md,https://github.com/ELS-RD/anonymisation/commit/bd82109b29d44ebd3a425ef9a1efb796feda7c36,Yes
4962,intel/dffml,tests/integration/test_dataflow.py,ba96ad764a1788491307ca93e619f6791bfbf1f0,TODO Figure out how nested model config options will work,https://github.com/intel/dffml/commit/ba96ad764a1788491307ca93e619f6791bfbf1f0,No
4963,drivendataorg/zamba,src/tests/tests.py,c8cb8befca500dd30c0de6a5d50fad8aa665fc1a,TODO load model graph into model class called by click,https://github.com/drivendataorg/zamba/commit/c8cb8befca500dd30c0de6a5d50fad8aa665fc1a,No
4964,drivendataorg/zamba,zamba/models/cnnensemble_model.py,8fcb6d64f83be76f9f61282721ff4f28271c68ad,TODO: for all models types; train a single model on the whole dataset,https://github.com/drivendataorg/zamba/commit/8fcb6d64f83be76f9f61282721ff4f28271c68ad,Yes
4965,drivendataorg/zamba,zamba/models/cnnensemble/src/single_frame_cnn.py,8f27ebfe85bbd6cb99a2ab047c9377a65054d023,TODO: it's worth to switch back to the correct preprocess_input when InceptionResNetV2 model is re-trained,https://github.com/drivendataorg/zamba/commit/8f27ebfe85bbd6cb99a2ab047c9377a65054d023,Yes
4966,holm-aune-bachelor2018/ctc,model.py,b5b90e3e8eb3db120ea9b821a32649b928522097,TODO: model.add? or x= ...,https://github.com/holm-aune-bachelor2018/ctc/commit/b5b90e3e8eb3db120ea9b821a32649b928522097,No
4967,undertheseanlp/automatic_speech_recognition,huanluyen2/sphinxtrain/python/cmusphinx/mllr.py,afa6d928441befcda9fc52034e6de3e15f050cdb,"\""\""\"" || Adapt acoustic models using maximum-likelihood linear regression. ||  || This module implements single-class mean and variance adaptation using || MLLR as described in M.J.F. Gales & P.C. Woodland; \\\""Mean and Variance || Adaptation within the MLLR Framework\\\""; Computer Speech and Language; || vol. 10; pp 249-264. ||  || TODO: Multiple regression classes. || \""\""\""",https://github.com/undertheseanlp/automatic_speech_recognition/commit/afa6d928441befcda9fc52034e6de3e15f050cdb,Yes
4968,WMD-group/SMACT,smact/structure_prediction/probability_models.py,f18278088a532c4edede4d6e9ccaa0a60bb7b712,"\""\""\""Probability models for species substitution. ||  || Implements base class :class:`SubstitutionModel`; || which can be extended to allow for development of new || lambda tables. An example of such an extension; || :class:`RadiusModel`; is also implemented. ||  || Todo: ||     * Allow for parallelism in lambda table calculations ||       by implementing a `sub_probs` abstractmethod ||       that :meth:`SubstitutionModel.gen_lambda` uses; ||       if available. ||  || \""\""\""",https://github.com/WMD-group/SMACT/commit/f18278088a532c4edede4d6e9ccaa0a60bb7b712,Yes
4969,hypnosapos/cartpole-rl-remote,src/qlearning_agent.py,caff687ce87569179ed5385976f859ef7762c47f,# TODO Change to call remote model,https://github.com/hypnosapos/cartpole-rl-remote/commit/caff687ce87569179ed5385976f859ef7762c47f,Yes
4970,hypnosapos/cartpole-rl-remote,cartpole/client/shell_cmd.py,6214677d4719f27c315cb6db747b7d79b37f34af,TODO: auto-modeling by custom config (get_model(**config)); defaults to {},https://github.com/hypnosapos/cartpole-rl-remote/commit/6214677d4719f27c315cb6db747b7d79b37f34af,Yes
4971,hypnosapos/cartpole-rl-remote,cartpole/model/__init__.py,6214677d4719f27c315cb6db747b7d79b37f34af,TODO: model tunning; pass layers and other config to get custom models,https://github.com/hypnosapos/cartpole-rl-remote/commit/6214677d4719f27c315cb6db747b7d79b37f34af,Yes
4972,ml-libs/mlserve,mlserve/handlers.py,be2863b5c11221e06631aec0e3c159f07d6cc9a2,TODO: introduce exception in case of model failure to predict,https://github.com/ml-libs/mlserve/commit/be2863b5c11221e06631aec0e3c159f07d6cc9a2,Yes
4973,betterlife/betterlifepsi,psi/app/models/supplier_sales.py,e789348cdb785f5b5529d8b1fca98ce79d35362b,TODO: This report should be constructed based on sales order line model; not supplier mode.,https://github.com/betterlife/betterlifepsi/commit/e789348cdb785f5b5529d8b1fca98ce79d35362b,No
4974,diffgram/diffgram,sdk/diffgram/brain/brain.py,3582038c6f2df7cda9d73e638f19bd75cc653ae9,TODO clarify difference between local path and url to download model,https://github.com/diffgram/diffgram/commit/3582038c6f2df7cda9d73e638f19bd75cc653ae9,Yes
4975,hackingmaterials/automatminer,matbench/pipeline.py,c4c970804c4f567ecf1bdbf2619493197ebf3ba9,todo: should save tpot model here,https://github.com/hackingmaterials/automatminer/commit/c4c970804c4f567ecf1bdbf2619493197ebf3ba9,Yes
4976,hackingmaterials/automatminer,matbench/automl/adaptors.py,6bb13f6c4bcf27cd9acf2a997110911d1d52faa2,todo: the top models (including one model type with mutliple,https://github.com/hackingmaterials/automatminer/commit/6bb13f6c4bcf27cd9acf2a997110911d1d52faa2,Yes
4977,hackingmaterials/automatminer,matbench/automl/adaptors.py,6bb13f6c4bcf27cd9acf2a997110911d1d52faa2,todo: combinations of model params).,https://github.com/hackingmaterials/automatminer/commit/6bb13f6c4bcf27cd9acf2a997110911d1d52faa2,No
4978,hackingmaterials/automatminer,automatminer/automl/adaptors.py,fefd6ba4eea465bf8a5bdab66a485599d5187324,todo: the top models (including one model type with mutliple,https://github.com/hackingmaterials/automatminer/commit/fefd6ba4eea465bf8a5bdab66a485599d5187324,Yes
4979,hackingmaterials/automatminer,automatminer/automl/adaptors.py,fefd6ba4eea465bf8a5bdab66a485599d5187324,todo: combinations of model params).,https://github.com/hackingmaterials/automatminer/commit/fefd6ba4eea465bf8a5bdab66a485599d5187324,Yes
4980,theislab/diffxpy,diffxpy/base.py,9348d34382ccc2bf118bd0bdfb7356e12e652c01,TODO extract MLE and std of model here.,https://github.com/theislab/diffxpy/commit/9348d34382ccc2bf118bd0bdfb7356e12e652c01,Yes
4981,ucloud/uai-sdk,examples/tensorflow/train/slim/code/deployment/model_deploy.py,33018523cc2b4efe7b9872aebb7d2211a9d68482,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/ucloud/uai-sdk/commit/33018523cc2b4efe7b9872aebb7d2211a9d68482,Yes
4982,bioidiap/bob,src/python/db/lib/multipie/dumplist.py,9be9d9dcf6e2548c21978cec6e9b828e76283ebc,TODO: model_ids,https://github.com/bioidiap/bob/commit/9be9d9dcf6e2548c21978cec6e9b828e76283ebc,No
4983,bioidiap/bob,src/python/db/lib/mobio/dumplist.py,2ff0a9f959dd129d4e8e9de33abc9e83bf0032e3,TODO: model_ids,https://github.com/bioidiap/bob/commit/2ff0a9f959dd129d4e8e9de33abc9e83bf0032e3,No
4984,bioidiap/bob,src/python/db/lib/scface/dumplist.py,33cbdf0b83fd547df310fe5f9702ca93620f7dfe,TODO: model_ids,https://github.com/bioidiap/bob/commit/33cbdf0b83fd547df310fe5f9702ca93620f7dfe,No
4985,bioidiap/bob,src/python/db/lib/xm2vts/dumplist.py,fa594a954eac82f65c968f9c88210c84b4c58c4c,TODO: model_ids,https://github.com/bioidiap/bob/commit/fa594a954eac82f65c968f9c88210c84b4c58c4c,No
4986,bioidiap/bob,src/python/db/lib/banca_small/dumplist.py,63ddc3e381aab01377e03bf2b15f3af438d88321,TODO: model_ids,https://github.com/bioidiap/bob/commit/63ddc3e381aab01377e03bf2b15f3af438d88321,No
4987,bioidiap/bob,src/python/db/lib/faceverif_fl/dumplist.py,a27ce8e56edcd70a67d301390e24c6b32c40ae4f,TODO: model_ids,https://github.com/bioidiap/bob/commit/a27ce8e56edcd70a67d301390e24c6b32c40ae4f,No
4988,bioidiap/bob,python/db/lib/banca/checkfiles.py,feff507e3cbd0402da749aebcd3b98eccee9531a,TODO: model_ids,https://github.com/bioidiap/bob/commit/feff507e3cbd0402da749aebcd3b98eccee9531a,No
4989,bioidiap/bob,python/db/lib/banca_small/checkfiles.py,a8c7340c6157fb7a672f72c143814ba73e724ee3,TODO: model_ids,https://github.com/bioidiap/bob/commit/a8c7340c6157fb7a672f72c143814ba73e724ee3,No
4990,bioidiap/bob,python/db/lib/biosecure/checkfiles.py,87ae15154ecc895bb56a995a5ea5df6363235f99,TODO: model_ids,https://github.com/bioidiap/bob/commit/87ae15154ecc895bb56a995a5ea5df6363235f99,No
4991,bioidiap/bob,python/db/lib/faceverif_fl/checkfiles.py,555ef1384f3ac71075f7ff42460e883f74134926,TODO: model_ids,https://github.com/bioidiap/bob/commit/555ef1384f3ac71075f7ff42460e883f74134926,No
4992,bioidiap/bob,python/db/lib/mobio/checkfiles.py,e884dea760936c1234cd2279e803e1a3dc4393aa,TODO: model_ids,https://github.com/bioidiap/bob/commit/e884dea760936c1234cd2279e803e1a3dc4393aa,No
4993,bioidiap/bob,python/db/lib/scface/checkfiles.py,a178ef23037c0ed55bc615528bec3169fa05286c,TODO: model_ids,https://github.com/bioidiap/bob/commit/a178ef23037c0ed55bc615528bec3169fa05286c,No
4994,bioidiap/bob,python/db/lib/xm2vts/checkfiles.py,264111f6d05781ca784498592c10fcaf4c9533d1,TODO: model_ids,https://github.com/bioidiap/bob/commit/264111f6d05781ca784498592c10fcaf4c9533d1,No
4995,bioidiap/bob,python/db/lib/multipie/checkfiles.py,2ee9804378261138cccb9fc86bb37ee23c614408,TODO: model_ids,https://github.com/bioidiap/bob/commit/2ee9804378261138cccb9fc86bb37ee23c614408,No
4996,bioidiap/bob,python/db/lib/fir/dumplist.py,831df3e04823077845f262989a25e5ef84f6efb8,TODO: model_ids,https://github.com/bioidiap/bob/commit/831df3e04823077845f262989a25e5ef84f6efb8,No
4997,nengo/nengo-dl,nengo_dl/simulator.py,84f49d5291cc4d3f416dad9e1e2f6578a7c93e80,TODO: error if object not in invariant_inputs or model.probes,https://github.com/nengo/nengo-dl/commit/84f49d5291cc4d3f416dad9e1e2f6578a7c93e80,No
4998,nengo/nengo-dl,nengo_dl/simulator.py,5da45781a5cea40efb89b79b2cddc61842f03478,TODO: document important attributes (e.g. keras_model),https://github.com/nengo/nengo-dl/commit/5da45781a5cea40efb89b79b2cddc61842f03478,No
4999,scvae/scvae,main.py,98aeeafd032edc4f8edd9ab661f97edb6aa245a5,TODO Add argument to skip modelling.,https://github.com/scvae/scvae/commit/98aeeafd032edc4f8edd9ab661f97edb6aa245a5,Yes
5000,scvae/scvae,auxiliary.py,6290e98a97611aea131f74893aa051828e8d8f6f,TODO Move auxiliary model functions to `models\/auxiliary.py`,https://github.com/scvae/scvae/commit/6290e98a97611aea131f74893aa051828e8d8f6f,Yes
5001,hachmannlab/chemml,chemml/search/active.py,b6d64bb41e8ad4bae3719028c8a3f7ede1e0b760,Todo: support for sklearn linear models,https://github.com/hachmannlab/chemml/commit/b6d64bb41e8ad4bae3719028c8a3f7ede1e0b760,No
5002,IBM/mi-prometheus,problems/image_text_to_class/clevr.py,b06f7620f2977700ef699b9a774376cd4008f489,TODO: Here; we should be able to add these 2 new lists to DataDict so that they can be used in model.plot().,https://github.com/IBM/mi-prometheus/commit/b06f7620f2977700ef699b9a774376cd4008f489,No
5003,IBM/mi-prometheus,problems/image_text_to_class/clevr.py,9af1ec6de010e66e2b9d3832f2683852de63adc4,TODO: Here; we should be able to add these 2 new lists to DataDict so that they can be used in model.plot().,https://github.com/IBM/mi-prometheus/commit/9af1ec6de010e66e2b9d3832f2683852de63adc4,No
5004,IBM/mi-prometheus,problems/image_text_to_class/clevr.py,80593f8153c1e020fbd1b378bbf8ae83528bf680,TODO: Here; we should be able to add these 2 new lists to DataDict so that they can be used in model.plot().,https://github.com/IBM/mi-prometheus/commit/80593f8153c1e020fbd1b378bbf8ae83528bf680,No
5005,IBM/mi-prometheus,trainer.py,1cdfe57ea74f885f241ef8298d744d5a146ce9d2,"\""\""\"" || trainer.py: Contains the code implementation of the main worker of mi-prometheus. || This worker in particular is called the `episodic trainer` and will take care of training || a specified model on a specified problem for a given number of episodes (among other adjustable || parameters). ||  || #TODO: Enhance this description and documentation. ||  || \""\""\""",https://github.com/IBM/mi-prometheus/commit/1cdfe57ea74f885f241ef8298d744d5a146ce9d2,No
5006,NRCan/geo-deep-learning,train_segmentation.py,ba634cde3e3332c761df7374a680edc27397c72f,nir_model = copy.deepcopy(model) # TODO: change to load only the part that we want,https://github.com/NRCan/geo-deep-learning/commit/ba634cde3e3332c761df7374a680edc27397c72f,Yes
5007,cesium-ml/cesium_web,flask_server.py,96ce02f11939d798cc36c24c7985b6bc3eec993a,TODO: add following info: associated featuresets; models,https://github.com/cesium-ml/cesium_web/commit/96ce02f11939d798cc36c24c7985b6bc3eec993a,No
5008,cesium-ml/cesium_web,flask_server.py,21b53816b4a75d55f61b35d5b781a35b2e0f9a5b,TODO: add following info: associated featuresets; models,https://github.com/cesium-ml/cesium_web/commit/21b53816b4a75d55f61b35d5b781a35b2e0f9a5b,No
5009,IBM/MAX-Object-Detector,training/training_code/object_detection/slim/deployment/model_deploy.py,e7aef82865713ee7bd109156a2eb2fadd57f2387,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/IBM/MAX-Object-Detector/commit/e7aef82865713ee7bd109156a2eb2fadd57f2387,Yes
5010,ucam-smt/sgnmt,cam/sgnmt/blocks/vanilla_decoder.py,eb61c8e69bd99717771ac205e9d47b23fc650565,Initialize model (TODO: do i really need this?),https://github.com/ucam-smt/sgnmt/commit/eb61c8e69bd99717771ac205e9d47b23fc650565,No
5011,ucam-smt/sgnmt,cam/sgnmt/predictors/blocks_neural.py,eb61c8e69bd99717771ac205e9d47b23fc650565,Initialize model (TODO: really necessary?),https://github.com/ucam-smt/sgnmt/commit/eb61c8e69bd99717771ac205e9d47b23fc650565,No
5012,ucam-smt/sgnmt,cam/sgnmt/predictors/chainer_lstm.py,d07e2b22f2412b86458f31d7d4c28ec6c8f209d1,TODO Marcin: Load model from path,https://github.com/ucam-smt/sgnmt/commit/d07e2b22f2412b86458f31d7d4c28ec6c8f209d1,Yes
5013,BaderLab/saber,kari/sequence_processing_model.py,ce3ede7fe4844cfc2d29365796e23f0adea2d97a,TODO (johngiorgi): make model checkpointing a config param,https://github.com/BaderLab/saber/commit/ce3ede7fe4844cfc2d29365796e23f0adea2d97a,Yes
5014,BaderLab/saber,kari/models/multi_task_lstm_crf.py,31ac9d9c8fa2f0de999a3dd15944090cfa568444,TODO (johngiorgi): consider introduction a new function; create_model(),https://github.com/BaderLab/saber/commit/31ac9d9c8fa2f0de999a3dd15944090cfa568444,Yes
5015,BaderLab/saber,kari/models/multi_task_lstm_crf.py,31ac9d9c8fa2f0de999a3dd15944090cfa568444,TODO (johngiorgi): need to clear the models after each fold,https://github.com/BaderLab/saber/commit/31ac9d9c8fa2f0de999a3dd15944090cfa568444,Yes
5016,BaderLab/saber,kari/sequence_processing_model.py,720540f250073f9ab002b432b20b5e822f8ab960,TODO (johngiorgi): consider introduction a new function; create_model(),https://github.com/BaderLab/saber/commit/720540f250073f9ab002b432b20b5e822f8ab960,Yes
5017,BaderLab/saber,kari/models/multi_task_lstm_crf.py,c9811274642b4da4438c9d059ad2eeb19fd920f1,TODO (johngiorgi): I need to name the models based on their dataset folder,https://github.com/BaderLab/saber/commit/c9811274642b4da4438c9d059ad2eeb19fd920f1,Yes
5018,BaderLab/saber,kari/models/multi_task_lstm_crf.py,c9811274642b4da4438c9d059ad2eeb19fd920f1,TODO (johngiorgi): https:\/\/machinelearningmastery.com\/dropout-regularization-deep-learning-models-keras\/,https://github.com/BaderLab/saber/commit/c9811274642b4da4438c9d059ad2eeb19fd920f1,No
5019,BaderLab/saber,kari/sequence_processor.py,c9811274642b4da4438c9d059ad2eeb19fd920f1,TODO (johngiorgi): make model checkpointing a config param,https://github.com/BaderLab/saber/commit/c9811274642b4da4438c9d059ad2eeb19fd920f1,Yes
5020,BaderLab/saber,saber/utils_models.py,025f6d72a8f9aabd4e065430e81a6fc19c74643a,TODO (johngiorgi) add verbosity parameter for printing model summary,https://github.com/BaderLab/saber/commit/025f6d72a8f9aabd4e065430e81a6fc19c74643a,Yes
5021,BaderLab/saber,saber/test/test_multi_task_lstm_crf.py,2f8b223cb9b531c8e71bb940a56573005aac64d9,TODO (johngiorgi): fix some of the test_model_attributes_after_creation_of_model tests,https://github.com/BaderLab/saber/commit/2f8b223cb9b531c8e71bb940a56573005aac64d9,Yes
5022,BaderLab/saber,saber/preprocessor.py,802fc0ba16c714703410d9a27a2dd3eea46f2768,TODO (johngiorgi): Read about spacys models; choose the most,https://github.com/BaderLab/saber/commit/802fc0ba16c714703410d9a27a2dd3eea46f2768,No
5023,BaderLab/saber,saber/sequence_processor.py,4fdd6c5e88e2ed766f9657bf4732bb2b4e634809,TODO: change to the following statement with new models,https://github.com/BaderLab/saber/commit/4fdd6c5e88e2ed766f9657bf4732bb2b4e634809,Yes
5024,BaderLab/saber,saber/tests/test_multi_task_lstm_crf.py,97623f68836314834a6869dc912eeb5842367722,TODO (johngiorgi): fix some of the test_model_attributes_after_creation_of_model tests,https://github.com/BaderLab/saber/commit/97623f68836314834a6869dc912eeb5842367722,Yes
5025,BaderLab/saber,saber/tests/test_trainer.py,97623f68836314834a6869dc912eeb5842367722,TODO (johngiorgi): add a dummy model fixture,https://github.com/BaderLab/saber/commit/97623f68836314834a6869dc912eeb5842367722,Yes
5026,EducationalTestingService/rsmtool,rsmtool/model.py,a77a3956b17524c85f8181c56c82db6f0fb3eb54,TODO: compute betas for linear SKLL models?,https://github.com/EducationalTestingService/rsmtool/commit/a77a3956b17524c85f8181c56c82db6f0fb3eb54,Yes
5027,EducationalTestingService/rsmtool,rsmtool/modeler.py,322166e39151a1fb5b8445f41e5a59313b21c9e6,TODO: compute betas for linear SKLL models?,https://github.com/EducationalTestingService/rsmtool/commit/322166e39151a1fb5b8445f41e5a59313b21c9e6,Yes
5028,jhu-lcsr/costar_plan,costar_models/python/costar_models/goal_sampler.py,e2b6bf22998452c65b4635d92f4149e02ad0882c,TODO: fix actor model,https://github.com/jhu-lcsr/costar_plan/commit/e2b6bf22998452c65b4635d92f4149e02ad0882c,Yes
5029,jhu-lcsr/costar_plan,costar_models/python/costar_models/goal_sampler.py,6e16485eaa909e25279bf2f026637a1937047d19,TODO: fix actor model,https://github.com/jhu-lcsr/costar_plan/commit/6e16485eaa909e25279bf2f026637a1937047d19,Yes
5030,jhu-lcsr/costar_plan,costar_models/python/costar_models/goal_sampler.py,0cae9299728dc55fd9635d4b41636f2c831d3f1c,TODO: fix actor model,https://github.com/jhu-lcsr/costar_plan/commit/0cae9299728dc55fd9635d4b41636f2c831d3f1c,Yes
5031,jhu-lcsr/costar_plan,costar_google_brainrobotdata/grasp_train.py,5f2a4295f027596de6b7b0303f1883d5635f20af,TODO(ahundt) consider making image_model_weights shared vs separate configurable,https://github.com/jhu-lcsr/costar_plan/commit/5f2a4295f027596de6b7b0303f1883d5635f20af,Yes
5032,jhu-lcsr/costar_plan,costar_google_brainrobotdata/cornell_grasp_train_classification.py,255eab1cd9236c5f0ba357cee5361897e0c7e7e5,Quite good kfold; best hyperparams from 2018-04 2000 model hyperopt run TODO(ahundt) add details from kfold run,https://github.com/jhu-lcsr/costar_plan/commit/255eab1cd9236c5f0ba357cee5361897e0c7e7e5,No
5033,jhu-lcsr/costar_plan,costar_google_brainrobotdata/costar_block_stacking_train_regression.py,e7239f2f2ee168cb389bfb95af4a8c9e0b2971ce,TODO(ahundt) it seems set_trainable_layers in grasp_model.py has a bug?,https://github.com/jhu-lcsr/costar_plan/commit/e7239f2f2ee168cb389bfb95af4a8c9e0b2971ce,Yes
5034,jhu-lcsr/costar_plan,costar_hyper/costar_block_stacking_train_regression.py,d98af11625416a37a1125306e8ecac7b94996dc2,TODO(ahundt) it seems set_trainable_layers in grasp_model.py has a bug?,https://github.com/jhu-lcsr/costar_plan/commit/d98af11625416a37a1125306e8ecac7b94996dc2,Yes
5035,google-research/google-research,cnn_quantization/tf_cnn_benchmarks/benchmark_cnn.py,7772843ee5eadeb625a3932882f1fb0e852e2e84,lead to NaNs in some models (resnet50).  TODO(tucker): fix it.,https://github.com/google-research/google-research/commit/7772843ee5eadeb625a3932882f1fb0e852e2e84,Yes
5036,google-research/google-research,meta_reward_learning/semantic_parsing/nsm/graph_factory.py,d0c80b240d279fbe2420adf30a837689f5530746,TODO(rishabhagarwal): Hack for loading a model trained on cloud machine.,https://github.com/google-research/google-research/commit/d0c80b240d279fbe2420adf30a837689f5530746,Yes
5037,google-research/google-research,uq_benchmark_2019/cifar/run_train.py,79da4e2626944a07622348e753ece2971f535247,TODO(yovadia): Figure out why save_model() wants to serialize ModelOptions.,https://github.com/google-research/google-research/commit/79da4e2626944a07622348e753ece2971f535247,Yes
5038,google-research/google-research,constrained_language_typology/sigtyp_reader_main.py,07c42b122d364de5f29b18b195e0d5bc779d9af2,"r\""\""\""Reader for the format provided by SIGTYP 2020 Shared Task. ||  || More information on the format is available here: ||   https:\/\/sigtyp.github.io\/st2020.html ||  || Example: || -------- ||  Clone the GitHub data to ST2020_DIR. Then run: ||  ||  > ST2020_DIR=... ||  > python3 sigtyp_reader_main.py --sigtyp_dir ${ST2020_DIR}\/data \\ ||     --output_dir ${OUTPUT_DIR} ||  ||  The above will create \""train.csv\""; \""dev.csv\"" and \""test_blinded.csv\"" files ||  converted from the format provided by SIGTYP. Our models should be able to ||  injest these csv files. Along each of the above files; an accompanying ||  \""data_train_*.json.gz\"" file is generated that contains metainformation on ||  various features and their values. ||  || TODO: || ----- || Following needs to be done: ||   - Latitude and longitude need to be on a point on a unit sphere? Keep as is ||     and add three further columns for (x;y;z)? ||   - Country codes are *several*. ||   - Other types of SOMs. ||   - Use BaseMap for visualizations? || \""\""\""",https://github.com/google-research/google-research/commit/07c42b122d364de5f29b18b195e0d5bc779d9af2,No
5039,google-research/google-research,gfsa/model/end_to_end_stack.py,9929c88b664800a25b8716c22068dd77d80bd5ee,TODO(ddjohnson) Move common layers out of `edge_supervision_models`.,https://github.com/google-research/google-research/commit/9929c88b664800a25b8716c22068dd77d80bd5ee,Yes
5040,OpenNMT/OpenNMT-py,onmt/modules/Ensemble.py,a473c777983b7f2e123f996c9b15607cb1979701,FIXME: anything to check or copy from other model_opt?,https://github.com/OpenNMT/OpenNMT-py/commit/a473c777983b7f2e123f996c9b15607cb1979701,Yes
5041,mlflow/mlflow,mlflow/pyfunc/__init__.py,7a69ba64ef269b2a5534e16fe888050a83224ef3,"\""\""\""Export \/ Import of generic python models. ||  || This module defines generic filesystem format for python models and provides utilities || for saving and loading to and from this format. The format is self contained in a sense || that it includes all necessary information for anyone to load it and use it. Dependencies || are either stored directly with the model or referenced via a conda environment. ||  || The convention for pyfunc models is to have a predict method or function with the following || signature ||  || predict(data: pandas.DataFrame) -> pandas.DataFrame ||  || This convention is relied upon by other mlflow components. ||  || Pyfunc model format is defined as a directory structure containing all required data; code and || configuration: ||  || .\/dst-path\/ ||     .\/MLmodel - config ||     <code> - any code packaged with the model (specified in the conf file; see below) ||     <data> - any data packaged with the model (specified in the conf file; see below) ||     <env>  - conda environment definition (specified in the conf file; see below) ||  || It must contain MLmodel file in its root with \""python_function\"" format with the following || parameters: ||  ||    - loader_module [required]: ||          Python module that can load the model. Expected as module identifier ||           e.g. ``mlflow.sklearn``; it will be imported via importlib.import_module. ||          The imported module must contain function with the following signature: ||  ||               load_pyfunc(path: string) -> <pyfunc model> ||  ||          The path argument is specified by the data parameter and may refer to a file or directory. ||  ||    - code [optional]: ||         relative path to a directory containing the code packaged with this model. ||         All files and directories inside this directory are added to the python path ||         prior to importing the model loader. ||  ||    - data [optional]: ||          relative path to a file or directory containing model data. ||          the path is passed to the model loader. ||  ||    - env [optional]: ||          relative path to an exported conda environment. If present this environment ||          should be activated prior to running the model. ||  || Example: ||  || ``` || >tree example\/sklearn_iris\/mlruns\/run1\/outputs\/linear-lr || \u251C\u2500\u2500 MLmodel || \u251C\u2500\u2500 code || \u2502\u00A0\u00A0 \u251C\u2500\u2500 sklearn_iris.py || \u2502\u00A0\u00A0 || \u251C\u2500\u2500 data || \u2502\u00A0\u00A0 \u2514\u2500\u2500 model.pkl || \u2514\u2500\u2500 mlflow_env.yml ||  || >cat example\/sklearn_iris\/mlruns\/run1\/outputs\/linear-lr\/MLmodel || python_function: ||   code: code ||   data: data\/model.pkl ||   env: mlflow_env.yml ||   main: sklearn_iris ||  || ``` || Todo: || * Get default conda_env of the project. || \""\""\""",https://github.com/mlflow/mlflow/commit/7a69ba64ef269b2a5534e16fe888050a83224ef3,No
5042,mlflow/mlflow,mlflow/pytorch/pickle_module.py,2a3764010a3ac7cda53f6df8290983c75633920b,"\""\""\"" || This module imports contents from CloudPickle in a way that is compatible with the || ``pickle_module`` parameter of PyTorch's model persistence function: ``torch.save`` || (see https:\/\/github.com\/pytorch\/pytorch\/blob\/692898fe379c9092f5e380797c32305145cd06e1\/torch\/ || serialization.py#L192). It is included as a distinct module from :mod:`mlflow.pytorch` to avoid || polluting the namespace with wildcard imports. ||  || Calling ``torch.save(...; pickle_module=mlflow.pytorch.pickle_module)`` will persist PyTorch model || definitions using CloudPickle; leveraging improved pickling functionality such as the ability || to capture class definitions in the \""__main__\"" scope. ||  || TODO: Remove this module or make it an alias of CloudPickle when CloudPickle and PyTorch have || compatible pickling APIs. || \""\""\""",https://github.com/mlflow/mlflow/commit/2a3764010a3ac7cda53f6df8290983c75633920b,Yes
5043,mlflow/mlflow,mlflow/utils/search_utils.py,ac68e6be4eb850b765e3a3ec1315655c8bb0543b,TODO: Tech debt. Refactor search code into common utils; tracking server; and model,https://github.com/mlflow/mlflow/commit/ac68e6be4eb850b765e3a3ec1315655c8bb0543b,Yes
5044,mlflow/mlflow,mlflow/statsmodels.py,6f32cdf9829945b55f1474625f5b9d0e44d027ee,TODO: move this to a specific mlflow.statsmodels.tsa flavor? Time series models,https://github.com/mlflow/mlflow/commit/6f32cdf9829945b55f1474625f5b9d0e44d027ee,Yes
5045,ludwig-ai/ludwig,ludwig/models/model.py,16cd40e486376434c6dff079e63f12e6f3872754,todo: tf2 change ludwig.Model not be subclass of tensorflow.keras Model class?,https://github.com/ludwig-ai/ludwig/commit/16cd40e486376434c6dff079e63f12e6f3872754,Yes
5046,ludwig-ai/ludwig,ludwig/models/model.py,87c962eb21167aa6ab80f7dd6654b1e89deff9e1,todo: tf2 change ludwig.Model not be subclass of tensorflow.keras Model class?,https://github.com/ludwig-ai/ludwig/commit/87c962eb21167aa6ab80f7dd6654b1e89deff9e1,Yes
5047,ludwig-ai/ludwig,ludwig/predict.py,6b100587fdca8963bb8f9baa1ed9c5a25d15d0cb,model.close_session()  # todo tf2 code clean -up,https://github.com/ludwig-ai/ludwig/commit/6b100587fdca8963bb8f9baa1ed9c5a25d15d0cb,Yes
5048,ludwig-ai/ludwig,ludwig/contribs/comet.py,f5f7de0d530781ae18f452dbbefb7fa109240507,TODO tf2: currently no clear way to set model graph,https://github.com/ludwig-ai/ludwig/commit/f5f7de0d530781ae18f452dbbefb7fa109240507,Yes
5049,ludwig-ai/ludwig,ludwig/api.py,b7a1ade7b7ac4c835f31877cd532d3fe0deb1bbc,todo refactoring: maybe replace the self.model_definition paramter,https://github.com/ludwig-ai/ludwig/commit/b7a1ade7b7ac4c835f31877cd532d3fe0deb1bbc,Yes
5050,ludwig-ai/ludwig,ludwig/api.py,b7a1ade7b7ac4c835f31877cd532d3fe0deb1bbc,TODO: support loading other model types based on definition,https://github.com/ludwig-ai/ludwig/commit/b7a1ade7b7ac4c835f31877cd532d3fe0deb1bbc,Yes
5051,ad12/DOSMA,dosma/cli.py,28443d6ef9d1da66b4078838da1f966120d220b2,TODO: Input shape should be determined by combination of model + scan.,https://github.com/ad12/DOSMA/commit/28443d6ef9d1da66b4078838da1f966120d220b2,Yes
5052,brendanhasz/probflow,bk/models.py,a8d7c90d1294e3aedc2aed482e93a23ceed7601b,TODO: build child models,https://github.com/brendanhasz/probflow/commit/a8d7c90d1294e3aedc2aed482e93a23ceed7601b,Yes
5053,brendanhasz/probflow,bk/models.py,a8d7c90d1294e3aedc2aed482e93a23ceed7601b,TODO: Construct this from built child models,https://github.com/brendanhasz/probflow/commit/a8d7c90d1294e3aedc2aed482e93a23ceed7601b,Yes
5054,brendanhasz/probflow,bk/models.py,a8d7c90d1294e3aedc2aed482e93a23ceed7601b,TODO: build the model,https://github.com/brendanhasz/probflow/commit/a8d7c90d1294e3aedc2aed482e93a23ceed7601b,Yes
5055,brendanhasz/probflow,bk/models.py,a8d7c90d1294e3aedc2aed482e93a23ceed7601b,TODO: fit the model,https://github.com/brendanhasz/probflow/commit/a8d7c90d1294e3aedc2aed482e93a23ceed7601b,Yes
5056,brendanhasz/probflow,bk/base_models.py,24e53c9b5af054934d31cdfaecee7448c2843d2d,"\""\""\""Abstract model classes. ||  || TODO: more info... ||  || \""\""\""",https://github.com/brendanhasz/probflow/commit/24e53c9b5af054934d31cdfaecee7448c2843d2d,No
5057,brendanhasz/probflow,bk/base_models.py,24e53c9b5af054934d31cdfaecee7448c2843d2d,TODO: build the model,https://github.com/brendanhasz/probflow/commit/24e53c9b5af054934d31cdfaecee7448c2843d2d,Yes
5058,brendanhasz/probflow,bk/base_models.py,24e53c9b5af054934d31cdfaecee7448c2843d2d,TODO: fit the model,https://github.com/brendanhasz/probflow/commit/24e53c9b5af054934d31cdfaecee7448c2843d2d,Yes
5059,brendanhasz/probflow,bk/models.py,24e53c9b5af054934d31cdfaecee7448c2843d2d,"\""\""\""Common already-made models. ||  || TODO: more info... ||  || \""\""\""",https://github.com/brendanhasz/probflow/commit/24e53c9b5af054934d31cdfaecee7448c2843d2d,No
5060,brendanhasz/probflow,bk/base_models.py,b6355a4d0103536c8a1c78bb0273ee6084a2d572,TODO: recursively build this model's args,https://github.com/brendanhasz/probflow/commit/b6355a4d0103536c8a1c78bb0273ee6084a2d572,Yes
5061,brendanhasz/probflow,bk/base_models.py,da12e9b38585dd7bf20936f140e5d78dddcc0103,TODO: DiscreteModel (for poisson etc)\uFFFF,https://github.com/brendanhasz/probflow/commit/da12e9b38585dd7bf20936f140e5d78dddcc0103,No
5062,brendanhasz/probflow,bk/base_models.py,676e0eb9cf54088ab320a3b9bfe205e0bee07156,TODO: but will have to SAMPLE from model and compute prob multiple times?,https://github.com/brendanhasz/probflow/commit/676e0eb9cf54088ab320a3b9bfe205e0bee07156,No
5063,brendanhasz/probflow,bk/base_models.py,5d41ae2d7fdb98d012e4e2bae708e946e0083440,TODO: and should use mean model; not sampling,https://github.com/brendanhasz/probflow/commit/5d41ae2d7fdb98d012e4e2bae708e946e0083440,Yes
5064,brendanhasz/probflow,bk/layers.py,e0189599ff3988d149c9468aaaba723c58b97791,TODO: should return built_model; mean_model,https://github.com/brendanhasz/probflow/commit/e0189599ff3988d149c9468aaaba723c58b97791,Yes
5065,brendanhasz/probflow,bk/layers.py,0263d5b7ce09d1790e7f441302141e016416b85c,TODO: i feel like BaseLayer should have everything BaseModel has;,https://github.com/brendanhasz/probflow/commit/0263d5b7ce09d1790e7f441302141e016416b85c,No
5066,brendanhasz/probflow,bk/distributions.py,e1944b89d285da1f268c88f53b88596005953f30,TODO: so the values are the sampled values? so self.built_model.sample()?,https://github.com/brendanhasz/probflow/commit/e1944b89d285da1f268c88f53b88596005953f30,Yes
5067,brendanhasz/probflow,bk/layers.py,e1944b89d285da1f268c88f53b88596005953f30,TODO: need to account for the jacobian if input is a BaseModel,https://github.com/brendanhasz/probflow/commit/e1944b89d285da1f268c88f53b88596005953f30,Yes
5068,brendanhasz/probflow,bk/variables.py,e1944b89d285da1f268c88f53b88596005953f30,TODO: should inherit layer? model?,https://github.com/brendanhasz/probflow/commit/e1944b89d285da1f268c88f53b88596005953f30,Yes
5069,brendanhasz/probflow,bk/variables.py,01a818f9ebdec2f116ca025d293666c13d9a297e,TODO: should inherit layer? model?  Layer; I think.,https://github.com/brendanhasz/probflow/commit/01a818f9ebdec2f116ca025d293666c13d9a297e,Yes
5070,brendanhasz/probflow,src/probflow/core.py,e80c95f1c6dca0447d57d87b54c37f124aee0263,TODO: recurse down the model; setting param._session = sess for each parameter,https://github.com/brendanhasz/probflow/commit/e80c95f1c6dca0447d57d87b54c37f124aee0263,Yes
5071,brendanhasz/probflow,src/probflow/core.py,952505c94c7c4422a8762f0e6f2b4cb41889eef6,TODO: recurse down the model; setting param._session = sess for each parameter,https://github.com/brendanhasz/probflow/commit/952505c94c7c4422a8762f0e6f2b4cb41889eef6,Yes
5072,brendanhasz/probflow,src/probflow/utils/data.py,00a01c40cfbaffc4d99cee3f49ced47e54bf1aaf,TODO: ensure x data shape matches model._ph['x'] shape (only if fit),https://github.com/brendanhasz/probflow/commit/00a01c40cfbaffc4d99cee3f49ced47e54bf1aaf,Yes
5073,brendanhasz/probflow,src/probflow/callbacks.py,733fe5906b0189b5457b71ea55b4283b71683322,TODO: restore_best_weights? using save_model and load_model?,https://github.com/brendanhasz/probflow/commit/733fe5906b0189b5457b71ea55b4283b71683322,Yes
5074,brendanhasz/probflow,src/probflow/models.py,733fe5906b0189b5457b71ea55b4283b71683322,"\""\""\""Models. ||  || Models are objects which take Tensor(s) as input; perform some computation on  || those Tensor(s); and output probability distributions. ||  || TODO: more... ||  || * :func:`.Model` || * :func:`.ContinuousModel` || * :func:`.DiscreteModel` || * :func:`.CategoricalModel` || * :func:`.save_model` || * :func:`.load_model` ||  || ---------- ||  || \""\""\""",https://github.com/brendanhasz/probflow/commit/733fe5906b0189b5457b71ea55b4283b71683322,No
5075,brendanhasz/probflow,src/probflow/callbacks/early_stopping.py,6b961353b37189581ef76086b6e103633e947b84,TODO: restore_best_weights? using save_model and load_model?,https://github.com/brendanhasz/probflow/commit/6b961353b37189581ef76086b6e103633e947b84,Yes
5076,brendanhasz/probflow,src/probflow/models/__init__.py,6b961353b37189581ef76086b6e103633e947b84,"\""\""\"" || Models are objects which take Tensor(s) as input; perform some computation || on those Tensor(s); and output probability distributions. ||  || TODO: more... ||  || * :class:`.Model` || * :class:`.ContinuousModel` || * :class:`.DiscreteModel` || * :class:`.CategoricalModel` ||  || ---------- ||  || \""\""\""",https://github.com/brendanhasz/probflow/commit/6b961353b37189581ef76086b6e103633e947b84,No
5077,cyschneck/Hydra,NN_gender_class.py,141390e8a63aedf9bdfe4b05a791cb81e72cb51d,TODO: update with better model for testing (currently ~85% on testing; ~99% on training),https://github.com/cyschneck/Hydra/commit/141390e8a63aedf9bdfe4b05a791cb81e72cb51d,Yes
5078,emlynjdavies/PySilCam,pysilcam/__main__.py,437e0a736ac025213d5389c870d344031817eed4,@todo the loading of the model and prediction functions should be within a class that is initialized by starting a,https://github.com/emlynjdavies/PySilCam/commit/437e0a736ac025213d5389c870d344031817eed4,Yes
5079,limteng-rpi/neural_name_tagging,model.py,28ccb6892394dbedd1a1604845298bce6697c6f3,TODO: init function for saved model,https://github.com/limteng-rpi/neural_name_tagging/commit/28ccb6892394dbedd1a1604845298bce6697c6f3,Yes
5080,M4gicT0/hybrid-dataset-factory,dataset_factory.py,34679e0b552e7e146c4277f139f4aa7d7b6829b0,'' ||     ----- TODO ----- ||  || [ ] Match the perspective via camera height estimation (with camera || calibration) || [ ] WHY IS IT SO UGLY???! || [ ] Thread it! || [x] Random positioning of the gate || [x] Boundaries definition for the gate (relative to the mesh's size) || [x] Compute the center of the gate || [ ] Compute the presence of the gate in the image frame || [?] Compute the distance to the gate || [ ] Camera calibration (use the correct parameters) || [x] Project on transparent background || [x] Overlay with background image || [ ] Model the camera distortion || [ ] Apply the distortion to the OpenGL projection || [ ] Histogram equalization of both images (hue; saturation; luminence ?...) || [ ] Motion blur (shader ?) || [ ] Anti alisasing (shader ?) || [ ] Ship it! ||  || ''',https://github.com/M4gicT0/hybrid-dataset-factory/commit/34679e0b552e7e146c4277f139f4aa7d7b6829b0,No
5081,apacha/Mensural-Detector,tensorflow_object_detection/eval_util.py,22404eb2931793c36fd2ecf361b81166eed6726b,TODO: This should be done in model's postprocess,https://github.com/apacha/Mensural-Detector/commit/22404eb2931793c36fd2ecf361b81166eed6726b,Yes
5082,apacha/Mensural-Detector,slim/deployment/model_deploy.py,f54532c1ddedb69c7531d13ad793c251a87957d3,"\""\""\""Deploy Slim models across multiple clones and replicas. ||  || # TODO(sguada) docstring paragraph by (a) motivating the need for the file and || # (b) defining clones. ||  || # TODO(sguada) describe the high-level components of model deployment. || # E.g. \""each model deployment is composed of several parts: a DeploymentConfig; || # which captures A; B and C; an input_fn which loads data.. etc ||  || To easily train a model on multiple GPUs or across multiple machines this || module provides a set of helper functions: `create_clones`; || `optimize_clones` and `deploy`. ||  || Usage: ||  ||   g = tf.Graph() ||  ||   # Set up DeploymentConfig ||   config = model_deploy.DeploymentConfig(num_clones=2; clone_on_cpu=True) ||  ||   # Create the global step on the device storing the variables. ||   with tf.device(config.variables_device()): ||     global_step = slim.create_global_step() ||  ||   # Define the inputs ||   with tf.device(config.inputs_device()): ||     images; labels = LoadData(...) ||     inputs_queue = slim.data.prefetch_queue((images; labels)) ||  ||   # Define the optimizer. ||   with tf.device(config.optimizer_device()): ||     optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate; FLAGS.momentum) ||  ||   # Define the model including the loss. ||   def model_fn(inputs_queue): ||     images; labels = inputs_queue.dequeue() ||     predictions = CreateNetwork(images) ||     slim.losses.log_loss(predictions; labels) ||  ||   model_dp = model_deploy.deploy(config; model_fn; [inputs_queue]; ||                                  optimizer=optimizer) ||  ||   # Run training. ||   slim.learning.train(model_dp.train_op; my_log_dir; ||                       summary_op=model_dp.summary_op) ||  || The Clone namedtuple holds together the values associated with each call to || model_fn: ||   * outputs: The return values of the calls to `model_fn()`. ||   * scope: The scope used to create the clone. ||   * device: The device used to create the clone. ||  || DeployedModel namedtuple; holds together the values needed to train multiple || clones: ||   * train_op: An operation that run the optimizer training op and include ||     all the update ops created by `model_fn`. Present only if an optimizer ||     was specified. ||   * summary_op: An operation that run the summaries created by `model_fn` ||     and process_gradients. ||   * total_loss: A `Tensor` that contains the sum of all losses created by ||     `model_fn` plus the regularization losses. ||   * clones: List of `Clone` tuples returned by `create_clones()`. ||  || DeploymentConfig parameters: ||   * num_clones: Number of model clones to deploy in each replica. ||   * clone_on_cpu: True if clones should be placed on CPU. ||   * replica_id: Integer.  Index of the replica for which the model is ||       deployed.  Usually 0 for the chief replica. ||   * num_replicas: Number of replicas to use. ||   * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas. ||   * worker_job_name: A name for the worker job. ||   * ps_job_name: A name for the parameter server job. ||  || TODO(sguada): ||   - describe side effect to the graph. ||   - what happens to summaries and update_ops. ||   - which graph collections are altered. ||   - write a tutorial on how to use this. ||   - analyze the possibility of calling deploy more than once. ||  ||  || \""\""\""",https://github.com/apacha/Mensural-Detector/commit/f54532c1ddedb69c7531d13ad793c251a87957d3,No
5083,flairNLP/flair,flair/embeddings.py,eb2d601c11dc19cb192f43e8695d11bd275bfbfa,by default; use_cache is false (for older pre-trained models TODO: remove in version 0.4),https://github.com/flairNLP/flair/commit/eb2d601c11dc19cb192f43e8695d11bd275bfbfa,Yes
5084,flairNLP/flair,flair/embeddings.py,647cb4c7757c67b9023dbffc688e31c3ba3e4efa,make compatible with serialized models (TODO: remove),https://github.com/flairNLP/flair/commit/647cb4c7757c67b9023dbffc688e31c3ba3e4efa,Yes
5085,flairNLP/flair,flair/embeddings.py,61880f7041462141d34409f67b7665142ef4c0f9,make compatible with serialized models (TODO: remove),https://github.com/flairNLP/flair/commit/61880f7041462141d34409f67b7665142ef4c0f9,Yes
5086,flairNLP/flair,flair/embeddings.py,b982463ff731c0e468144df0dfb095f421933c9e,make compatible with serialized models (TODO: remove),https://github.com/flairNLP/flair/commit/b982463ff731c0e468144df0dfb095f421933c9e,Yes
5087,flairNLP/flair,flair/embeddings/token.py,d565c536d328d9d380de7f4a2bd2cff6c855d71a,TODO check if this is necessary is this method is called before prepare_for_model,https://github.com/flairNLP/flair/commit/d565c536d328d9d380de7f4a2bd2cff6c855d71a,Yes
5088,HealthCatalyst/healthcareai-py,healthcareai/deploy_supervised_model.py,9c55d8551a062af0eb58cfd21f175c1aa15e9a7c,TODO: think about moving this to model_eval mtry function,https://github.com/HealthCatalyst/healthcareai-py/commit/9c55d8551a062af0eb58cfd21f175c1aa15e9a7c,Yes
5089,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,887770ccfba4dec9fec46ff81876ae0968267b03,TODO save models and stats,https://github.com/HealthCatalyst/healthcareai-py/commit/887770ccfba4dec9fec46ff81876ae0968267b03,Yes
5090,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,e1223a3352d9002b4614283f47176b5126e3dca1,TODO save models and stats,https://github.com/HealthCatalyst/healthcareai-py/commit/e1223a3352d9002b4614283f47176b5126e3dca1,Yes
5091,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,e1223a3352d9002b4614283f47176b5126e3dca1,# TODO loop over all the models?,https://github.com/HealthCatalyst/healthcareai-py/commit/e1223a3352d9002b4614283f47176b5126e3dca1,Yes
5092,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,fbabbe7356fecf68cd29032d2a71e53c51a6123b,TODO save models and stats,https://github.com/HealthCatalyst/healthcareai-py/commit/fbabbe7356fecf68cd29032d2a71e53c51a6123b,Yes
5093,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,fbabbe7356fecf68cd29032d2a71e53c51a6123b,# TODO loop over all the models?,https://github.com/HealthCatalyst/healthcareai-py/commit/fbabbe7356fecf68cd29032d2a71e53c51a6123b,Yes
5094,HealthCatalyst/healthcareai-py,healthcareai/common/model_eval.py,6b42020432e620b60f9b52a8c2dfda44fe533c6e,TODO is it possible to get to this return if you are in develop_model_mode?,https://github.com/HealthCatalyst/healthcareai-py/commit/6b42020432e620b60f9b52a8c2dfda44fe533c6e,Yes
5095,HealthCatalyst/healthcareai-py,healthcareai/deploy_supervised_model.py,9ad3830c409d28759b31f38a0cf452f8d3ba60d5,TODO This might change as deploy no longer trains a model,https://github.com/HealthCatalyst/healthcareai-py/commit/9ad3830c409d28759b31f38a0cf452f8d3ba60d5,Yes
5096,HealthCatalyst/healthcareai-py,healthcareai/deploy_supervised_model.py,89db9bc2c485326b41965b01f0071e80ba423553,TODO This might change as deploy no longer trains a model,https://github.com/HealthCatalyst/healthcareai-py/commit/89db9bc2c485326b41965b01f0071e80ba423553,Yes
5097,HealthCatalyst/healthcareai-py,healthcareai/trained_models/trained_supervised_model.py,5b29c18fa27b26822f38f13aa7dcb12b52ab1e38,TODO should this timestamp a model name automatically? (for example 2017-04-26_01.33.55_random_forest.pkl),https://github.com/HealthCatalyst/healthcareai-py/commit/5b29c18fa27b26822f38f13aa7dcb12b52ab1e38,Yes
5098,HealthCatalyst/healthcareai-py,healthcareai/simple_mode.py,ac2738a5e3c5613a9d5bb464a0b1d3ce3b640547,TODO keeping these column names as part of the saved model avoids all the hassle of dropping grain and other,https://github.com/HealthCatalyst/healthcareai-py/commit/ac2738a5e3c5613a9d5bb464a0b1d3ce3b640547,Yes
5099,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,a60ec935c02e3fb45c2e3193efbca85a799dc8ee,TODO factor model here?,https://github.com/HealthCatalyst/healthcareai-py/commit/a60ec935c02e3fb45c2e3193efbca85a799dc8ee,Yes
5100,HealthCatalyst/healthcareai-py,healthcareai/common/model_eval.py,4a25a4c0d506a4c0ea64cd35abdfe88c76f2a7aa,TODO refactor this to take an arbitrary number of models rather than just a linear and random forest,https://github.com/HealthCatalyst/healthcareai-py/commit/4a25a4c0d506a4c0ea64cd35abdfe88c76f2a7aa,Yes
5101,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,4a25a4c0d506a4c0ea64cd35abdfe88c76f2a7aa,TODO refactor this to take an arbitrary number of models rather than just a linear and random forest,https://github.com/HealthCatalyst/healthcareai-py/commit/4a25a4c0d506a4c0ea64cd35abdfe88c76f2a7aa,Yes
5102,HealthCatalyst/healthcareai-py,healthcareai/simple_mode.py,4a25a4c0d506a4c0ea64cd35abdfe88c76f2a7aa,TODO This will not work without a linear and random forest model for now until the base function is refactored,https://github.com/HealthCatalyst/healthcareai-py/commit/4a25a4c0d506a4c0ea64cd35abdfe88c76f2a7aa,Yes
5103,HealthCatalyst/healthcareai-py,example_simple_classification.py,fa122381d514c7f2c050c7331bc811bcb0d5a4b4,TODO this is broken - it might look like tools.plot_roc(models=[random_forest; linear; knn]),https://github.com/HealthCatalyst/healthcareai-py/commit/fa122381d514c7f2c050c7331bc811bcb0d5a4b4,Yes
5104,HealthCatalyst/healthcareai-py,healthcareai/simple_mode.py,fa122381d514c7f2c050c7331bc811bcb0d5a4b4,TODO put TrainedSupervisedModel into advanced class and compare how it feels with the linear_regression(),https://github.com/HealthCatalyst/healthcareai-py/commit/fa122381d514c7f2c050c7331bc811bcb0d5a4b4,Yes
5105,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,a36fc5f5aa194f0775860b87d2e06cfce8534026,TODO because these now all return TSMs it will be additionally slow by all the factor models.,https://github.com/HealthCatalyst/healthcareai-py/commit/a36fc5f5aa194f0775860b87d2e06cfce8534026,Yes
5106,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,a36fc5f5aa194f0775860b87d2e06cfce8534026,TODO Could these be trained separately then after the best is found; train the factor model and add to TSM?,https://github.com/HealthCatalyst/healthcareai-py/commit/a36fc5f5aa194f0775860b87d2e06cfce8534026,Yes
5107,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,a36fc5f5aa194f0775860b87d2e06cfce8534026,TODO should the factor model be either 1) optional or 2) separate?,https://github.com/HealthCatalyst/healthcareai-py/commit/a36fc5f5aa194f0775860b87d2e06cfce8534026,Yes
5108,HealthCatalyst/healthcareai-py,healthcareai/develop_supervised_model.py,7a22e6e88fd7599f4ed528610053d2b42d2a5cf6,TODO should the factor model be either 1) optional or 2) separate?,https://github.com/HealthCatalyst/healthcareai-py/commit/7a22e6e88fd7599f4ed528610053d2b42d2a5cf6,Yes
5109,HealthCatalyst/healthcareai-py,healthcareai/common/model_eval.py,ff2c1f95bbe17f7bfcf89b4f97a879527ce3defb,TODO is it possible to get to this return if you are in develop_model_mode?,https://github.com/HealthCatalyst/healthcareai-py/commit/ff2c1f95bbe17f7bfcf89b4f97a879527ce3defb,Yes
5110,X-DataInitiative/tick,tick/inference/hawkes_adm4.py,86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,TODO add approx to model,https://github.com/X-DataInitiative/tick/commit/86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,Yes
5111,X-DataInitiative/tick,tick/optim/__init__.py,86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,TODO: add notebooks for each models : PoissonReg; CoxPartial; Hawkes; that illustrates simulation and inference of the models; and compares solvers for each models,https://github.com/X-DataInitiative/tick/commit/86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,Yes
5112,X-DataInitiative/tick,tick/hawkes/inference/hawkes_adm4.py,aad38ed09d20ba8c0db98d36338e5ddd9a087b89,TODO add approx to model,https://github.com/X-DataInitiative/tick/commit/aad38ed09d20ba8c0db98d36338e5ddd9a087b89,Yes
5113,YerevaNN/mimic3-benchmarks,mimic3models/decompensation/keras_main.py,b5a4888a2c56aaf6a25043085e413bb420fba166,TODO: write callback for model save,https://github.com/YerevaNN/mimic3-benchmarks/commit/b5a4888a2c56aaf6a25043085e413bb420fba166,Yes
5114,YerevaNN/mimic3-benchmarks,mimic3models/in_hospital_mortality/keras_main.py,b5a4888a2c56aaf6a25043085e413bb420fba166,TODO: write callback for model save,https://github.com/YerevaNN/mimic3-benchmarks/commit/b5a4888a2c56aaf6a25043085e413bb420fba166,Yes
5115,vita-epfl/openpifpaf,openpifpaf/network/factory.py,886418ba0059c6408cddaea0cec7d8a51a325621,TODO: set check_hash to True on final model,https://github.com/vita-epfl/openpifpaf/commit/886418ba0059c6408cddaea0cec7d8a51a325621,Yes
5116,vita-epfl/openpifpaf,openpifpaf/network/factory.py,0e35ff22ad0b85b68f81affee1f3c38968f23231,TODO: set check_hash to True on final model,https://github.com/vita-epfl/openpifpaf/commit/0e35ff22ad0b85b68f81affee1f3c38968f23231,Yes
5117,onnx/onnxmltools,onnxmltools/convert/xgboost/operator_converters/XGBoost.py,eef63ee1920be8d336d6788c55d3dfc158cd81e1,TODO: check it is implemented. The model cannot be loaded when they are present.,https://github.com/onnx/onnxmltools/commit/eef63ee1920be8d336d6788c55d3dfc158cd81e1,Yes
5118,datmo/datmo,datmo/monitoring.py,edc98f1bec393e2e38b08701a35753688799bdaf,TODO change for model to deployment version,https://github.com/datmo/datmo/commit/edc98f1bec393e2e38b08701a35753688799bdaf,Yes
5119,mme/vergeml,vergeml/env.py,9a5804c872899243a50885dec1d66fbfcfc8ecae,TODO This exists so that models can set up default values,https://github.com/mme/vergeml/commit/9a5804c872899243a50885dec1d66fbfcfc8ecae,Yes
5120,mila-iqia/babyai,scripts/intelligent_expert.py,31ccec125bd8dfeec67b7df897e227c39b03f193,TODO: seems like we should store args.model to restore it after this loading,https://github.com/mila-iqia/babyai/commit/31ccec125bd8dfeec67b7df897e227c39b03f193,No
5121,mila-iqia/babyai,scripts/intelligent_expert.py,31ccec125bd8dfeec67b7df897e227c39b03f193,TODO: what's model_name?,https://github.com/mila-iqia/babyai/commit/31ccec125bd8dfeec67b7df897e227c39b03f193,Yes
5122,mila-iqia/babyai,scripts/train_il.py,6c7d6f244bc146d7c1cbd74d89ae37521ae42515,TODO: The logger is define a bit later (needs the model name) - change this to a log message ?,https://github.com/mila-iqia/babyai/commit/6c7d6f244bc146d7c1cbd74d89ae37521ae42515,Yes
5123,mila-iqia/babyai,scripts/train_rl.py,6c7d6f244bc146d7c1cbd74d89ae37521ae42515,TODO: The logger is define a bit later (needs the model name) - change this to a log message ?,https://github.com/mila-iqia/babyai/commit/6c7d6f244bc146d7c1cbd74d89ae37521ae42515,Yes
5124,mozilla/TTS,TTS/tts/utils/synthesis.py,639fa292616db7aa740088f5e06094cd35d5ba1a,TODO: test this for tacotron models,https://github.com/mozilla/TTS/commit/639fa292616db7aa740088f5e06094cd35d5ba1a,Yes
5125,mozilla/TTS,TTS/tts/utils/synthesis.py,13c6665c92668e54d6ad10cda3e87e61fb8def39,TODO: test this for tacotron models,https://github.com/mozilla/TTS/commit/13c6665c92668e54d6ad10cda3e87e61fb8def39,Yes
5126,mozilla/TTS,TTS/bin/train_speedy_speech.py,cf869e8922436be2bee704c24126ea3810a384a8,TODO: fix optimizer init; model.cuda() needs to be called before,https://github.com/mozilla/TTS/commit/cf869e8922436be2bee704c24126ea3810a384a8,Yes
5127,scikit-learn-contrib/lightning,lightning/dual_cd.py,819cabf63eb35092870b5a89856f903ae03cec99,FIXME: can trim the model,https://github.com/scikit-learn-contrib/lightning/commit/819cabf63eb35092870b5a89856f903ae03cec99,Yes
5128,deepchem/deepchem,deepchem/models/tensorflow_models/__init__.py,21831e01cda03c1b32db3c171903eecc505e5db7,TODO(rbharath): config and model_params overlap significantly. Maybe just,https://github.com/deepchem/deepchem/commit/21831e01cda03c1b32db3c171903eecc505e5db7,No
5129,deepchem/deepchem,deepchem/models/__init__.py,12db403c92b32ae90518aed7d38ce517ae9e130e,TODO(rbharath): This is a hack based on fact that multi-tasktype models,https://github.com/deepchem/deepchem/commit/12db403c92b32ae90518aed7d38ce517ae9e130e,No
5130,deepchem/deepchem,deepchem/utils/evaluate.py,5e4c08ad8dea9f6285bdadf298ab2ce86c39d535,TODO(rbharath): This is a hack based on fact that multi-tasktype models,https://github.com/deepchem/deepchem/commit/5e4c08ad8dea9f6285bdadf298ab2ce86c39d535,No
5131,deepchem/deepchem,deepchem/models/sklearn_models/sklean_model.py,8969c493d0bf25e4447b8d2cb812ebecb09ccf5f,"FIXME: Signature of \""fit\"" incompatible with supertype \""Model\""",https://github.com/deepchem/deepchem/commit/8969c493d0bf25e4447b8d2cb812ebecb09ccf5f,Yes
5132,deepchem/deepchem,deepchem/models/xgboost_models/xgboost_model.py,78c35b93d9c665bb5a42eb9d6c03ab202e84bcbe,"FIXME: Return type \""None\"" of \""fit\"" incompatible with return type \""float\"" in supertype \""Model\""",https://github.com/deepchem/deepchem/commit/78c35b93d9c665bb5a42eb9d6c03ab202e84bcbe,Yes
5133,deepchem/deepchem,deepchem/models/tests/test_reload.py,8a015062717a7eee6d6325e06ed9fc4d369fedc6,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.,https://github.com/deepchem/deepchem/commit/8a015062717a7eee6d6325e06ed9fc4d369fedc6,Yes
5134,dmlc/gluon-nlp,scripts/conversion_tools/convert_pytorch_transformers.py,acee22635101ad278a5ba5384c427da20af52c43,TODO: change this to your bert model and tokenizer used in pytorch-transformer,https://github.com/dmlc/gluon-nlp/commit/acee22635101ad278a5ba5384c427da20af52c43,Yes
5135,DistrictDataLabs/yellowbrick,yellowbrick/classifier.py,5eee25b8260ab56319d765b96d200b97c362e687,TODO: Do a better job of guessing defaults from the model,https://github.com/DistrictDataLabs/yellowbrick/commit/5eee25b8260ab56319d765b96d200b97c362e687,Yes
5136,DistrictDataLabs/yellowbrick,yellowbrick/classifier.py,4f050577adc939548a0b90622a83703efc7b29bc,TODO: ensure that the number of models is only 2,https://github.com/DistrictDataLabs/yellowbrick/commit/4f050577adc939548a0b90622a83703efc7b29bc,Yes
5137,DistrictDataLabs/yellowbrick,yellowbrick/utils.py,241edcaa81187027defaaf2e73123ae3abb0a9a1,TODO: once we make ScoreVisualizer and ModelVisualizer pass through,https://github.com/DistrictDataLabs/yellowbrick/commit/241edcaa81187027defaaf2e73123ae3abb0a9a1,No
5138,DistrictDataLabs/yellowbrick,yellowbrick/utils.py,640afa8419e838a42e5ad8317433577234645b38,TODO: once we make ScoreVisualizer and ModelVisualizer pass through,https://github.com/DistrictDataLabs/yellowbrick/commit/640afa8419e838a42e5ad8317433577234645b38,No
5139,DistrictDataLabs/yellowbrick,yellowbrick/utils/types.py,03724ed8b6ebddb7c6f914a5c3444462545fe03a,TODO: once we make ScoreVisualizer and ModelVisualizer pass through,https://github.com/DistrictDataLabs/yellowbrick/commit/03724ed8b6ebddb7c6f914a5c3444462545fe03a,No
5140,DistrictDataLabs/yellowbrick,tests/test_regressor/test_alphas.py,6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,TODO: parametrize with models when unittest dependency removed,https://github.com/DistrictDataLabs/yellowbrick/commit/6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,Yes
5141,DistrictDataLabs/yellowbrick,tests/test_regressor/test_alphas.py,6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,TODO: parametrize with models when unittest dependency removed (new test case),https://github.com/DistrictDataLabs/yellowbrick/commit/6aec0854756c7a91aee1f30da7a92ddb5e6fa4cb,Yes
5142,DistrictDataLabs/yellowbrick,yellowbrick/cluster/icdm.py,2f23976c2110a8d58beff4592f2ac8b705d593dc,TODO: is this how sklearn stores all centers in the model?,https://github.com/DistrictDataLabs/yellowbrick/commit/2f23976c2110a8d58beff4592f2ac8b705d593dc,Yes
5143,DistrictDataLabs/yellowbrick,yellowbrick/regressor/influence.py,fe14cfda91503292f0019cf91e8fede76902d52e,TODO: honestly this was done because it was only in the statsmodels,https://github.com/DistrictDataLabs/yellowbrick/commit/fe14cfda91503292f0019cf91e8fede76902d52e,No
5144,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/triplet_loss.py,e638a4760c0519cac825d9661a3b5f0a928a507c,TODO. learn feature normalization and store it as a layer in the model,https://github.com/pyannote/pyannote-audio/commit/e638a4760c0519cac825d9661a3b5f0a928a507c,Yes
5145,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/agg_triplet_loss.py,ec3d6c674f221d9a209bfecf691641b0cb2f250c,TODO. learn feature normalization and store it as a layer in the model,https://github.com/pyannote/pyannote-audio/commit/ec3d6c674f221d9a209bfecf691641b0cb2f250c,Yes
5146,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/wtf_triplet_loss.py,df76ab77d6f84dd1c9cd04e40e60b093ea0b42e2,TODO. learn feature normalization and store it as a layer in the model,https://github.com/pyannote/pyannote-audio/commit/df76ab77d6f84dd1c9cd04e40e60b093ea0b42e2,Yes
5147,pyannote/pyannote-audio,pyannote/audio/train/trainer.py,26a096c2e9cce537bd67f53955008aed7a570e15,TODO. check that model specs are coherent,https://github.com/pyannote/pyannote-audio/commit/26a096c2e9cce537bd67f53955008aed7a570e15,Yes
5148,pyannote/pyannote-audio,pyannote/audio/applications/base.py,1d88c5816e7a176d53faaf12d5edbc24f0ee562f,TODO. get rid of from_model_pt,https://github.com/pyannote/pyannote-audio/commit/1d88c5816e7a176d53faaf12d5edbc24f0ee562f,Yes
5149,pyannote/pyannote-audio,pyannote/audio/applications/base.py,b6cab97a8b9134bdf45becbf2c5c3e57d51e663c,TODO: add support for torch.hub models directly in docopt,https://github.com/pyannote/pyannote-audio/commit/b6cab97a8b9134bdf45becbf2c5c3e57d51e663c,Yes
5150,pyannote/pyannote-audio,pyannote/audio/labeling/tasks/base.py,98e7c298a4b9f35e6ac7e993e241a5f009e51e6b,TODO: in pyannote.audio.train.model,https://github.com/pyannote/pyannote-audio/commit/98e7c298a4b9f35e6ac7e993e241a5f009e51e6b,Yes
5151,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/center_loss.py,3eb7e89b9f149128c7eed1d8b95348da746213e5,FIXME add support for pretrained model with different specs,https://github.com/pyannote/pyannote-audio/commit/3eb7e89b9f149128c7eed1d8b95348da746213e5,Yes
5152,theislab/scanpy,scanpy/tools/sim.py,c22e48abe45a6ccca5918bbf689637caa4b31250,"\""\""\"" || Simulate Artificial Data || ======================== ||  || From package Scanpy (https:\/\/github.com\/theislab\/scanpy). || Written in Python 3 (compatible with 2). || Copyright 2016-2017 F. Alexander Wolf (http:\/\/falexwolf.de). ||      || Simulate stochastic dynamic systems to model gene expression dynamics and || cause-effect data. ||  || TODO || ---- || Beta Version. The code will be reorganized soon. || \""\""\""",https://github.com/theislab/scanpy/commit/c22e48abe45a6ccca5918bbf689637caa4b31250,No
5153,catalyst-team/catalyst,examples/mnist_gans/models.py,b22f99034d3fbad4606037aedd5c314b166e590b,TODO: add conv models,https://github.com/catalyst-team/catalyst/commit/b22f99034d3fbad4606037aedd5c314b166e590b,No
5154,catalyst-team/catalyst,catalyst/core/callbacks/optimizer.py,30bfb7e8c5dce5144ee8570b87db962d32cf030e,@TODO: add model grads support and visualization,https://github.com/catalyst-team/catalyst/commit/30bfb7e8c5dce5144ee8570b87db962d32cf030e,Yes
5155,Cloud-CV/EvalAI,apps/challenges/utils.py,c24cb0afb53d8462b8177c9129c0f6c29139a105,TODO - Add storage bucket name field in django models,https://github.com/Cloud-CV/EvalAI/commit/c24cb0afb53d8462b8177c9129c0f6c29139a105,Yes
5156,probcomp/bayeslite,src/metamodels/loom_metamodel.py,ca6953e4fca6fb756213655b49ecfca657a3dabd,"TODO should we use \""generator\"" or \""metamodel\"" in the name of",https://github.com/probcomp/bayeslite/commit/ca6953e4fca6fb756213655b49ecfca657a3dabd,Yes
5157,DragonComputer/Dragonfire,dragonfire/conversational/__init__.py,1b000c167a7d08dafa0673308ded2f494c5a12fa,TODO: For now; the model are trained for a specific dataset (because of the maxLength which define the,https://github.com/DragonComputer/Dragonfire/commit/1b000c167a7d08dafa0673308ded2f494c5a12fa,Yes
5158,DragonComputer/Dragonfire,dragonfire/conversational/__init__.py,1b000c167a7d08dafa0673308ded2f494c5a12fa,TODO: Put a limit size (ex: 3GB for the modelDir),https://github.com/DragonComputer/Dragonfire/commit/1b000c167a7d08dafa0673308ded2f494c5a12fa,Yes
5159,DragonComputer/Dragonfire,dragonfire/conversational/__init__.py,1b000c167a7d08dafa0673308ded2f494c5a12fa,We need to restore the model length because of the textData associated and the vocabulary size (TODO: Compatibility mode between different maxLength),https://github.com/DragonComputer/Dragonfire/commit/1b000c167a7d08dafa0673308ded2f494c5a12fa,Yes
5160,plasticityai/magnitude,pymagnitude/third_party/allennlp/predictors/biaffine_dependency_parser.py,96f0fc6a8839cf1c824febcb170ab75ce8a6f854,TODO(Mark) Make the language configurable and based on a model attribute.,https://github.com/plasticityai/magnitude/commit/96f0fc6a8839cf1c824febcb170ab75ce8a6f854,Yes
5161,mozilla/TTS,train.py,08162157eedf1bc2bf431af20d70f4c0c546fef1,TODO: fix optimizer init; model.cuda() needs to be called before,https://github.com/mozilla/TTS/commit/08162157eedf1bc2bf431af20d70f4c0c546fef1,Yes
5162,src-d/ml,sourced/ml/cmd/train_id_split.py,4bdc930b3eddb9ab71e9c98996be03f53b014979,TODO: Use modelforge to save the model,https://github.com/src-d/ml/commit/4bdc930b3eddb9ab71e9c98996be03f53b014979,Yes
5163,openml/openml-python,openml/flows/sklearn_converter.py,929fec14bec17bc47fb441efc1649ac031ab5e66,TODO: assert that only on first recursion lvl `parent_model` can be None,https://github.com/openml/openml-python/commit/929fec14bec17bc47fb441efc1649ac031ab5e66,Yes
5164,openml/openml-python,openml/runs/functions.py,7ec40452fb71ce9c08de36902f607391415a16ce,TODO: if possible; give a warning if model is already fitted (acceptable in case of custom experimentation;,https://github.com/openml/openml-python/commit/7ec40452fb71ce9c08de36902f607391415a16ce,Yes
5165,openml/openml-python,openml/runs/functions.py,fcfa7d9493ec65073bc7ce860d8ea0c9f4addbc2,TODO: if possible; give a warning if model is already fitted (acceptable in case of custom experimentation;,https://github.com/openml/openml-python/commit/fcfa7d9493ec65073bc7ce860d8ea0c9f4addbc2,Yes
5166,openml/openml-python,tests/test_runs/test_run_functions.py,cd7d74bd15d642bbd1ee6a0e0dedac49c24e5cf7,TODO add test about initializing a model from a run given a parameter distribution - also,https://github.com/openml/openml-python/commit/cd7d74bd15d642bbd1ee6a0e0dedac49c24e5cf7,Yes
5167,openml/openml-python,openml/extensions/sklearn/extension.py,0f8b7f0966a1ebb4e7c848268e904402818891ef,TODO: assert that only on first recursion lvl `parent_model` can be None,https://github.com/openml/openml-python/commit/0f8b7f0966a1ebb4e7c848268e904402818891ef,Yes
5168,openml/openml-python,openml/extensions/sklearn/extension.py,0f8b7f0966a1ebb4e7c848268e904402818891ef,fixme str(model) might contain (...),https://github.com/openml/openml-python/commit/0f8b7f0966a1ebb4e7c848268e904402818891ef,No
5169,openml/openml-python,openml/extensions/sklearn/extension.py,0f8b7f0966a1ebb4e7c848268e904402818891ef,TODO: if possible; give a warning if model is already fitted (acceptable,https://github.com/openml/openml-python/commit/0f8b7f0966a1ebb4e7c848268e904402818891ef,Yes
5170,onnx/onnx-caffe2,onnx_caffe2/backend.py,3cc1e2323b6c12c63793dc10b82801741d38f08d,TODO: Make this robust against an adversarial model namer,https://github.com/onnx/onnx-caffe2/commit/3cc1e2323b6c12c63793dc10b82801741d38f08d,Yes
5171,inferno-pytorch/inferno,inferno/trainers/basic.py,821b3ae247c010a6cf749c1af8a28d3b20de5a68,TODO some sanity checks on config_dict (e.g. whether the model is actually a model; etc),https://github.com/inferno-pytorch/inferno/commit/821b3ae247c010a6cf749c1af8a28d3b20de5a68,Yes
5172,microsoft/NimbusML,src/python/nimbusml/internal/utils/entrypoints.py,793739b8ab6b002bfccdbda2bedf6f915ce6125b,todo: load & return model blob,https://github.com/microsoft/NimbusML/commit/793739b8ab6b002bfccdbda2bedf6f915ce6125b,Yes
5173,menpo/menpo,pybug/model/linear.py,054a91e5e175c0f62aced2f43733cd357a00517a,TODO Implement project_out on PCAModel,https://github.com/menpo/menpo/commit/054a91e5e175c0f62aced2f43733cd357a00517a,Yes
5174,menpo/menpo,pybug/transform/statisticallydriven.py,a0a2850065cc6cdc8165c9603254c1c9524f7e7f,TODO the model needs to be able to generate it's jacobian.,https://github.com/menpo/menpo/commit/a0a2850065cc6cdc8165c9603254c1c9524f7e7f,Yes
5175,menpo/menpo,pybug/model/linear.py,5fc72af32b022c7926b35388d09c64e4d5631155,TODO Implement project_out on SimilarityModel,https://github.com/menpo/menpo/commit/5fc72af32b022c7926b35388d09c64e4d5631155,Yes
5176,menpo/menpo,pybug/model/linear.py,0bf02b7da9ac9889122e9644c698e6ffa41d4af1,TODO: better document what a linear model does.,https://github.com/menpo/menpo/commit/0bf02b7da9ac9889122e9644c698e6ffa41d4af1,No
5177,menpo/menpo,pybug/model/linear.py,0bf02b7da9ac9889122e9644c698e6ffa41d4af1,TODO: give a description of what it means to be a PCA model,https://github.com/menpo/menpo/commit/0bf02b7da9ac9889122e9644c698e6ffa41d4af1,No
5178,menpo/menpo,pybug/model/linear.py,1e4d7655b883d6d25c4432d4cc9a6b390ea459bf,TODO: better document what a linear model does.,https://github.com/menpo/menpo/commit/1e4d7655b883d6d25c4432d4cc9a6b390ea459bf,No
5179,menpo/menpo,pybug/model/linear.py,1e4d7655b883d6d25c4432d4cc9a6b390ea459bf,TODO Implement project_out on SimilarityModel,https://github.com/menpo/menpo/commit/1e4d7655b883d6d25c4432d4cc9a6b390ea459bf,Yes
5180,menpo/menpo,pybug/model/linear.py,1e4d7655b883d6d25c4432d4cc9a6b390ea459bf,TODO: give a description of what it means to be a PCA model,https://github.com/menpo/menpo/commit/1e4d7655b883d6d25c4432d4cc9a6b390ea459bf,No
5181,menpo/menpo,pybug/model/linear.py,86f34934e3101da59f8bd2f1da6c49abc9d97ae6,TODO: better document what a linear model does.,https://github.com/menpo/menpo/commit/86f34934e3101da59f8bd2f1da6c49abc9d97ae6,No
5182,menpo/menpo,pybug/model/linear.py,86f34934e3101da59f8bd2f1da6c49abc9d97ae6,TODO: give a description of what it means to be a PCA model,https://github.com/menpo/menpo/commit/86f34934e3101da59f8bd2f1da6c49abc9d97ae6,No
5183,menpo/menpo,pybug/model/linear.py,a91f9357c5e6613b4e3ce22a6c04017a7a61506b,TODO Implement project_out on SimilarityModel,https://github.com/menpo/menpo/commit/a91f9357c5e6613b4e3ce22a6c04017a7a61506b,Yes
5184,menpo/menpo,pybug/model/linear.py,a91f9357c5e6613b4e3ce22a6c04017a7a61506b,TODO Implement project_out on PCAModel,https://github.com/menpo/menpo/commit/a91f9357c5e6613b4e3ce22a6c04017a7a61506b,Yes
5185,menpo/menpo,pybug/model/linear.py,c61ceca129537217f6f25e6cba856d77b9694f51,TODO Implement project_out on PCAModel,https://github.com/menpo/menpo/commit/c61ceca129537217f6f25e6cba856d77b9694f51,Yes
5186,menpo/menpo,pybug/model/linear.py,4b171b01d565a52675cb026443612f89621d90ab,TODO: give a description of what it means to be a Similarity Model,https://github.com/menpo/menpo/commit/4b171b01d565a52675cb026443612f89621d90ab,No
5187,menpo/menpo,pybug/model/linear.py,91155da90de603b41f1f831e8f2042bbac7a6ce5,TODO: better document what a linear model does.,https://github.com/menpo/menpo/commit/91155da90de603b41f1f831e8f2042bbac7a6ce5,No
5188,menpo/menpo,pybug/model/linear.py,91155da90de603b41f1f831e8f2042bbac7a6ce5,TODO: give a description of what it means to be a PCA model,https://github.com/menpo/menpo/commit/91155da90de603b41f1f831e8f2042bbac7a6ce5,No
5189,menpo/menpo,menpo/fitmultilevel/clm/base.py,496ce4665da6a7863af4a986efd2f51e6a38cf12,TODO: this bit of logic should to be transferred down to PCAModel,https://github.com/menpo/menpo/commit/496ce4665da6a7863af4a986efd2f51e6a38cf12,Yes
5190,menpo/menpo,menpo/fitmultilevel/clm/base.py,4f8c4f94dbf4bfbeb408f62d8abdefeac0a6345c,TODO: this bit of logic should to be transferred down to PCAModel,https://github.com/menpo/menpo/commit/4f8c4f94dbf4bfbeb408f62d8abdefeac0a6345c,Yes
5191,menpo/menpo,menpo/fitmultilevel/aam/base.py,6a2aed969190927b6a4f7c0f4c1f7b6d572ed551,TODO: this bit of logic should to be transferred down to PCAModel,https://github.com/menpo/menpo/commit/6a2aed969190927b6a4f7c0f4c1f7b6d572ed551,Yes
5192,feedly/transfer-nlp,transfer_nlp/runners/runnersABC.py,5770f658a72c31c646854a10d69fed99c81e3c31,Register useful parameters and objects useful for model instantiation #TODO: do proper testing on this part,https://github.com/feedly/transfer-nlp/commit/5770f658a72c31c646854a10d69fed99c81e3c31,Yes
5193,feedly/transfer-nlp,transfer_nlp/predictors/predictor.py,8c483e1a28f37259b68a2f1a946ffaeb2e4fe0f3,Register useful parameters and objects useful for model instantiation #TODO: do proper testing on this part,https://github.com/feedly/transfer-nlp/commit/8c483e1a28f37259b68a2f1a946ffaeb2e4fe0f3,Yes
5194,online-ml/river,river/ensemble/streaming_random_patches.py,69ad941923233d40278caf5e5a8846b54f093bf1,TODO Find a way to verify if the model natively supports sample_weight,https://github.com/online-ml/river/commit/69ad941923233d40278caf5e5a8846b54f093bf1,Yes
5195,THUNLP-MT/THUMT,thumt/launcher/ensemble_translator.py,48aafdfe4ece5e5f9aea5b766bad40a9a49188a5,TODO: replace rnnsearch with model_cls.name,https://github.com/THUNLP-MT/THUMT/commit/48aafdfe4ece5e5f9aea5b766bad40a9a49188a5,Yes
5196,explosion/spacy-models,tests/lang/pl/test_tagger.py,124d903546447a8f5399f5f820a73dfc536b2ba9,TODO: switch back to nkjp when model config is updated,https://github.com/explosion/spacy-models/commit/124d903546447a8f5399f5f820a73dfc536b2ba9,Yes
5197,CPJKU/madmom,madmom/features/beats.py,9abd4b73fc9635486f3374400470c7744b4b16c1,# TODO: add multi_model stuff!,https://github.com/CPJKU/madmom/commit/9abd4b73fc9635486f3374400470c7744b4b16c1,Yes
5198,CPJKU/madmom,bin/OnsetDetector.py,7c547b70e51b9b150dd965b4fb74408e0f4661a8,TODO: make sure newer models are trained with mul=1,https://github.com/CPJKU/madmom/commit/7c547b70e51b9b150dd965b4fb74408e0f4661a8,Yes
5199,CPJKU/madmom,bin/OnsetDetectorLL.py,7c547b70e51b9b150dd965b4fb74408e0f4661a8,TODO: make sure newer models are trained with mul=1,https://github.com/CPJKU/madmom/commit/7c547b70e51b9b150dd965b4fb74408e0f4661a8,Yes
5200,CPJKU/madmom,bin/OnsetDetectorLL.py,7c547b70e51b9b150dd965b4fb74408e0f4661a8,TODO: make sure newer models are trained with diff_ratio=0.5,https://github.com/CPJKU/madmom/commit/7c547b70e51b9b150dd965b4fb74408e0f4661a8,Yes
5201,CPJKU/madmom,bin/OnsetDetector.py,e9b06b18f947f1aeec678e7ece01c65b797cc874,TODO: make sure newer models are trained with diff_ratio=0.5,https://github.com/CPJKU/madmom/commit/e9b06b18f947f1aeec678e7ece01c65b797cc874,Yes
5202,CPJKU/madmom,madmom/ml/nn/layers.py,a92614da244a84574f46f51205ebdfdfe3bb8a6b,TODO: old models do not have the init attribute; thus create it,https://github.com/CPJKU/madmom/commit/a92614da244a84574f46f51205ebdfdfe3bb8a6b,Yes
5203,CPJKU/madmom,tests/test_ml_nn.py,a92614da244a84574f46f51205ebdfdfe3bb8a6b,FIXME: these old models don't have the online attribute set; so we,https://github.com/CPJKU/madmom/commit/a92614da244a84574f46f51205ebdfdfe3bb8a6b,Yes
5204,CPJKU/madmom,madmom/ml/gmm.py,69fecb6d474685d1a04e006d1d27e8e1c8f896b7,TODO: old models have underscores at some variable names; thus rename,https://github.com/CPJKU/madmom/commit/69fecb6d474685d1a04e006d1d27e8e1c8f896b7,Yes
5205,CPJKU/madmom,madmom/ml/nn/layers.py,3de64c7d1c9d389aeb97b23ed94745d714a56da5,TODO: old models have a 'hid_init' instead of an 'init' attribute,https://github.com/CPJKU/madmom/commit/3de64c7d1c9d389aeb97b23ed94745d714a56da5,Yes
5206,nyu-mll/jiant,jiant/preprocess.py,3bf415c76290168750416686d159c6bb5aaead8d,"TODO: this is another place can be simplified by \""model-before-preprocess\"" reorganization",https://github.com/nyu-mll/jiant/commit/3bf415c76290168750416686d159c6bb5aaead8d,Yes
5207,oracle/Skater,skater/core/global_interpretation/partial_dependence.py,c9ac86e33f959592ae343691744ab0ca533c454d,Todo: add static version of model.predict_subset_classes; use here,https://github.com/oracle/Skater/commit/c9ac86e33f959592ae343691744ab0ca533c454d,Yes
5208,awslabs/sockeye,sockeye/encoder.py,a8e08d99ccc4cb2d024de1cd424808725ea7acbf,TODO break out EncoderConfig to allow use without populating options for full translation model,https://github.com/awslabs/sockeye/commit/a8e08d99ccc4cb2d024de1cd424808725ea7acbf,Yes
5209,awslabs/sockeye,sockeye/arguments.py,e934541132fb1c92cc9810d515450602d11e6662,TODO: At the moment LHUC is RNN specific. We should support other models as well.,https://github.com/awslabs/sockeye/commit/e934541132fb1c92cc9810d515450602d11e6662,Yes
5210,awslabs/sockeye,sockeye/constants.py,bf89a7eeabd433b603a1d50895ff18269c9eac04,TODO: make this configurable in the model; separately per target factor.,https://github.com/awslabs/sockeye/commit/bf89a7eeabd433b603a1d50895ff18269c9eac04,Yes
5211,optuna/optuna,optuna/storages/rdb/storage.py,2c72a8f6f946d079f825a5e303f6d585d84be114,TODO(ohta): Remove this workaround when `number` field is added to `TrialModel`.,https://github.com/optuna/optuna/commit/2c72a8f6f946d079f825a5e303f6d585d84be114,Yes
5212,optuna/optuna,optuna/storages/rdb/storage.py,30f9d8cb6d868d1fdb163bc6081b43cee99e7cd7,TODO(ohta): Remove this workaround when `number` field is added to `TrialModel`.,https://github.com/optuna/optuna/commit/30f9d8cb6d868d1fdb163bc6081b43cee99e7cd7,Yes
5213,optuna/optuna,optuna/storages/rdb/storage.py,1c3e89bdb3002335ad7612431b92723864b2b37d,TODO(ohta): Remove this workaround when `number` field is added to `TrialModel`.,https://github.com/optuna/optuna/commit/1c3e89bdb3002335ad7612431b92723864b2b37d,Yes
5214,google/uis-rnn,demo.py,f4b67e7d3de68580d857167d842d508bf237282a,TODO: support using pretrained model.,https://github.com/google/uis-rnn/commit/f4b67e7d3de68580d857167d842d508bf237282a,Yes
5215,explosion/thinc,thinc/api.py,3865fcf08e29f2af7d4ad84561ff5413b57568b6,TODO: How do we strip the arg checking from Model?,https://github.com/explosion/thinc/commit/3865fcf08e29f2af7d4ad84561ff5413b57568b6,No
5216,explosion/thinc,thinc/extra/wrappers.py,d5546c00d4b9c41e808949543e74d772e6e66f6e,TODO: We should return also h_n e.g. for seq2seq models,https://github.com/explosion/thinc/commit/d5546c00d4b9c41e808949543e74d772e6e66f6e,Yes
5217,explosion/thinc,thinc/model.py,bb871da0287cea1bea630d265a908bf182e2a46b,TODO: Which settings should we expose via Model.visualize?,https://github.com/explosion/thinc/commit/bb871da0287cea1bea630d265a908bf182e2a46b,No
5218,explosion/thinc,thinc/layers/clone.py,6f58050eaac70506f56f692214646cc1267f068a,TODO: input \/ output types for model?,https://github.com/explosion/thinc/commit/6f58050eaac70506f56f692214646cc1267f068a,No
5219,pytorch/translate,pytorch_translate/semi_supervised.py,408229ae54b91fcdc6760ac96e5bf177c40d01c7,TODO: Generalize this to be able to use other model classes like Transformer,https://github.com/pytorch/translate/commit/408229ae54b91fcdc6760ac96e5bf177c40d01c7,Yes
5220,pytorch/translate,pytorch_translate/tasks/semi_supervised_task.py,408229ae54b91fcdc6760ac96e5bf177c40d01c7,TODO: Generalize this to be able to use other model classes like Transformer,https://github.com/pytorch/translate/commit/408229ae54b91fcdc6760ac96e5bf177c40d01c7,Yes
5221,pytorch/translate,pytorch_translate/dual_learning/dual_learning_models.py,62500acec498d02179b8efacaf9a06d0a3082eae,TODO: pass to dual model too,https://github.com/pytorch/translate/commit/62500acec498d02179b8efacaf9a06d0a3082eae,No
5222,pytorch/translate,pytorch_translate/dual_learning/dual_learning_models.py,62500acec498d02179b8efacaf9a06d0a3082eae,TODO (T36875783): instantiate a langauge model,https://github.com/pytorch/translate/commit/62500acec498d02179b8efacaf9a06d0a3082eae,Yes
5223,pytorch/translate,pytorch_translate/rescoring.py,0f86b46bad588e5af8cf249255a8cc4da3e358ef,TODO (T40938917): Allow loading of multiple rescoring models,https://github.com/pytorch/translate/commit/0f86b46bad588e5af8cf249255a8cc4da3e358ef,Yes
5224,pytorch/translate,pytorch_translate/rescoring/model_scorers.py,13efc210d865f7300220708b6132fe428d478039,TODO (T40938917): Allow loading of multiple rescoring models,https://github.com/pytorch/translate/commit/13efc210d865f7300220708b6132fe428d478039,Yes
5225,pytorch/translate,pytorch_translate/rescoring/model_scorers.py,ea50d01062b4ee780a9a621c66802874e5694961,TODO: (T41818693) Map translation model vs LM model differences,https://github.com/pytorch/translate/commit/ea50d01062b4ee780a9a621c66802874e5694961,No
5226,pytorch/translate,pytorch_translate/ensemble_export.py,e4b2a5d84a82cf70e052a769747cff0c509b354e,TODO: model ensemble,https://github.com/pytorch/translate/commit/e4b2a5d84a82cf70e052a769747cff0c509b354e,Yes
5227,cltk/cltk,cltk/phonology/middle_english/transcription.py,23e9e37d61fbc2a62e6547dbe078dde8760fba5e,"\""\""\"" || The hyphenation\/syllabification algorithm is based on the typical syllable structure model of onset\/nucleus\/coda. || TODO: Add hypothesized IPA transcription || \""\""\""",https://github.com/cltk/cltk/commit/23e9e37d61fbc2a62e6547dbe078dde8760fba5e,Yes
5228,cltk/cltk,cltk/corpus/readers.py,320e810184204d9171e683241adea4c5c1c73a04,TODO and add:  ['latin_text_perseus'; 'latin_treebank_perseus'; 'latin_text_latin_library'; 'phi5'; 'phi7'; 'latin_proper_names_cltk'; 'latin_models_cltk'; 'latin_pos_lemmata_cltk'; 'latin_treebank_index_thomisticus'; 'latin_lexica_perseus'; 'latin_training_set_sentence_cltk'; 'latin_word2vec_cltk'; 'latin_text_antique_digiliblt'; 'latin_text_corpus_grammaticorum_latinorum'; 'latin_text_poeti_ditalia'],https://github.com/cltk/cltk/commit/320e810184204d9171e683241adea4c5c1c73a04,Yes
5229,cltk/cltk,src/cltkv1/wrappers/stanford.py,037662f4962856c444109c2c070fb3525647275d,TODO: This is a weak check for the models actually being downloaded and valid,https://github.com/cltk/cltk/commit/037662f4962856c444109c2c070fb3525647275d,Yes
5230,cltk/cltk,src/cltkv1/wrappers/stanford.py,037662f4962856c444109c2c070fb3525647275d,TODO: Use ``models_dir`` var from below and make self. or global to module,https://github.com/cltk/cltk/commit/037662f4962856c444109c2c070fb3525647275d,Yes
5231,cltk/cltk,src/cltkv1/embeddings/embeddings.py,c0d3b0a765a424a84ab37afad0b964a2652b5e3a,TODO: Check all 4 types of model reading on cpu w\/ enough memory,https://github.com/cltk/cltk/commit/c0d3b0a765a424a84ab37afad0b964a2652b5e3a,Yes
5232,cltk/cltk,src/cltkv1/core/data_types.py,98e41cc7ccf97267c178d2ab209f5da2b12ab063,TODO: Check; this probably loads model a second time,https://github.com/cltk/cltk/commit/98e41cc7ccf97267c178d2ab209f5da2b12ab063,Yes
5233,cltk/cltk,src/cltkv1/data/clone.py,61d6f6824c5f87ae5628061afa325001af3b9e52,TODO: Decide whether to drop repos w\/o models,https://github.com/cltk/cltk/commit/61d6f6824c5f87ae5628061afa325001af3b9e52,Yes
5234,cltk/cltk,src/cltkv1/dependency/stanza.py,13fff8524c52cc4fca2caa04836ecde7d024a9d8,TODO: This is a weak check for the models actually being downloaded and valid,https://github.com/cltk/cltk/commit/13fff8524c52cc4fca2caa04836ecde7d024a9d8,Yes
5235,cltk/cltk,src/cltkv1/dependency/stanza.py,13fff8524c52cc4fca2caa04836ecde7d024a9d8,TODO: Use ``models_dir`` var from below and make self. or global to module,https://github.com/cltk/cltk/commit/13fff8524c52cc4fca2caa04836ecde7d024a9d8,Yes
5236,IntelPython/sdc,examples/d4p_linreg.py,d15bdaca47e194fe9fa43e1a6ffc39958111c0e9,FIXME res.model.InterceptFlag,https://github.com/IntelPython/sdc/commit/d15bdaca47e194fe9fa43e1a6ffc39958111c0e9,Yes
5237,IntelPython/sdc,hpat/ml/d4p.py,d15bdaca47e194fe9fa43e1a6ffc39958111c0e9,- spec.model_base: [optional] base string for C lookup function; FIXME: should not be necessary,https://github.com/IntelPython/sdc/commit/d15bdaca47e194fe9fa43e1a6ffc39958111c0e9,Yes
5238,fastnlp/fastNLP,fastNLP/api/processor.py,dc7f8ef8d4fb301de394c10339495787dda3c4b4,TODO \u5F53\u524D\u7684\u5B9E\u73B0\u4F1A\u5BFC\u81F4\u4E4B\u540E\u7684processor\u9700\u8981\u77E5\u9053model\u8F93\u51FA\u7684output\u7684key\u662F\u4EC0\u4E48,https://github.com/fastnlp/fastNLP/commit/dc7f8ef8d4fb301de394c10339495787dda3c4b4,No
5239,fastnlp/fastNLP,fastNLP/core/trainer.py,ad0a8c177554ee1a5c4656ea2c8a06aa369f0ca5,TODO \u8FD9\u91CC\u53EF\u80FD\u4F1A\u9047\u5230\u95EE\u9898\uFF0C\u4E07\u4E00\u7528\u6237\u5728model\u5185\u90E8\u4FEE\u6539\u4E86prediction\u7684device\u5C31\u4F1A\u6709\u95EE\u9898,https://github.com/fastnlp/fastNLP/commit/ad0a8c177554ee1a5c4656ea2c8a06aa369f0ca5,No
5240,fastnlp/fastNLP,fastNLP/core/trainer.py,785c41ded5c56bf54614d475f57cc1895a820957,TODO \u8FD9\u91CC\u53EF\u80FD\u4F1A\u9047\u5230\u95EE\u9898\uFF0C\u4E07\u4E00\u7528\u6237\u5728model\u5185\u90E8\u4FEE\u6539\u4E86prediction\u7684device\u5C31\u4F1A\u6709\u95EE\u9898,https://github.com/fastnlp/fastNLP/commit/785c41ded5c56bf54614d475f57cc1895a820957,No
5241,fastnlp/fastNLP,fastNLP/models/seq2seq_model.py,15360e9724884e26ee76ae3933bd7e43f2a84fb9,todo \u53C2\u8003fairseq\u7684FairseqModel\u7684\u5199\u6CD5,https://github.com/fastnlp/fastNLP/commit/15360e9724884e26ee76ae3933bd7e43f2a84fb9,No
5242,DIVA-DIA/DeepDIVA,template/CIFAR_CNN_classifier.py,7875d579a8aeffd60b5ab70ded47cc9d74353145,TODO make way that the model and the criterion are also passed as parameter with introspection thingy as the optimizer,https://github.com/DIVA-DIA/DeepDIVA/commit/7875d579a8aeffd60b5ab70ded47cc9d74353145,No
5243,DIVA-DIA/DeepDIVA,template/standard.py,5e9ad24e9b51617f9de1c8cd17f36e7d53ed83b1,TODO Load model expected size from the actual model,https://github.com/DIVA-DIA/DeepDIVA/commit/5e9ad24e9b51617f9de1c8cd17f36e7d53ed83b1,Yes
5244,DIVA-DIA/DeepDIVA,template/standard.py,19ea2579ed34a284c73c3caa83a2639f1207f8d6,TODO Load model expected size from the actual model,https://github.com/DIVA-DIA/DeepDIVA/commit/19ea2579ed34a284c73c3caa83a2639f1207f8d6,Yes
5245,DIVA-DIA/DeepDIVA,template/standard.py,81906fc71337ac21282ed2a5afa1147264c767a4,TODO make way that the model and the criterion are also passed as parameter with introspection thingy as the optimizer,https://github.com/DIVA-DIA/DeepDIVA/commit/81906fc71337ac21282ed2a5afa1147264c767a4,No
5246,DIVA-DIA/DeepDIVA,template/standard.py,f92916ae2eeac5abe9ee29f807bcd2c5107626ca,TODO Load model expected size from the actual model,https://github.com/DIVA-DIA/DeepDIVA/commit/f92916ae2eeac5abe9ee29f807bcd2c5107626ca,Yes
5247,DIVA-DIA/DeepDIVA,template/standard.py,f92916ae2eeac5abe9ee29f807bcd2c5107626ca,TODO make way that the model and the criterion are also passed as parameter with introspection thingy as the optimizer,https://github.com/DIVA-DIA/DeepDIVA/commit/f92916ae2eeac5abe9ee29f807bcd2c5107626ca,No
5248,DIVA-DIA/DeepDIVA,template/runner/semantic_segmentation/semantic_segmentation.py,c7092400bceebc2c8bff5d5939dfed4e06fe312c,TODO best model is not saved if epoch = 1,https://github.com/DIVA-DIA/DeepDIVA/commit/c7092400bceebc2c8bff5d5939dfed4e06fe312c,Yes
5249,DIVA-DIA/DeepDIVA,models/semantic_segmentation/Deeplabv3.py,4f516fb463fad4291e30bd88ee1b19623eb66ecf,TODO: make different functions for different models,https://github.com/DIVA-DIA/DeepDIVA/commit/4f516fb463fad4291e30bd88ee1b19623eb66ecf,Yes
5250,DIVA-DIA/DeepDIVA,models/semantic_segmentation/SegNet.py,4f516fb463fad4291e30bd88ee1b19623eb66ecf,TODO: make different functions for different VGG models,https://github.com/DIVA-DIA/DeepDIVA/commit/4f516fb463fad4291e30bd88ee1b19623eb66ecf,Yes
5251,DIVA-DIA/DeepDIVA,template/runner/divahisdb_semantic_segmentation/divahisdb_semantic_segmentation.py,db503214d9c9154e496bcf8299822d6157db1665,TODO best model is not saved if epoch = 1,https://github.com/DIVA-DIA/DeepDIVA/commit/db503214d9c9154e496bcf8299822d6157db1665,Yes
5252,neuronets/nobrainer,train.py,90bb6e58f42b75b08669f9953e24fd184b0aa6f4,"\""\""\""Script to train highres3dnet model. ||  || The input CSV must have two columns: ||     1. filepaths of features ||     2. filepaths of corresponding labels ||  || TODO || ---- || - Make this script more general. Ideally; one could drop in their model and ||     loss function. || - Move some common methods (eg; i\/o) to dedicated modules. || - Dice coefficient for class 1 (brainmask) is sometimes NaN. || - Input of 1 * 128**3 is too large for 1080ti. This seems to be related to the ||     `input_fn` used. || - Remove pandas as a dependency. Make pure python reader that accepts CSV or ||     TSV as input. || \""\""\""",https://github.com/neuronets/nobrainer/commit/90bb6e58f42b75b08669f9953e24fd184b0aa6f4,Yes
5253,neuronets/nobrainer,train.py,54200bc13c0b6e63cea50d143a2f83037ad04024,"\""\""\""Example script to train model. ||  || The input CSV must have two columns: ||     1. filepaths of features ||     2. filepaths of corresponding labels ||  || TODO || ---- || - Dice coefficient for class 1 (brainmask) is sometimes NaN. This occurs when ||     Dice should be zero. || - Input of 1 * 128**3 is too large for 1080ti to train HighRes3DNet. It is OK ||     for MeshNet. This issue seems to be related to the `input_fn` used. || \""\""\""",https://github.com/neuronets/nobrainer/commit/54200bc13c0b6e63cea50d143a2f83037ad04024,Yes
5254,neuronets/nobrainer,nobrainer/losses.py,393e9d269ed9364173effdc2738340b588c8b3f1,TODO: add priors from existing Keras model.,https://github.com/neuronets/nobrainer/commit/393e9d269ed9364173effdc2738340b588c8b3f1,Yes
5255,neuronets/nobrainer,nobrainer/training.py,393e9d269ed9364173effdc2738340b588c8b3f1,TODO: can we test if the model is compiled? We lose the optimizer,https://github.com/neuronets/nobrainer/commit/393e9d269ed9364173effdc2738340b588c8b3f1,Yes
5256,neuronets/nobrainer,nobrainer/training.py,393e9d269ed9364173effdc2738340b588c8b3f1,TODO: if we can load weights after compiling model; load the most recent,https://github.com/neuronets/nobrainer/commit/393e9d269ed9364173effdc2738340b588c8b3f1,Yes
5257,comic/grand-challenge.org,django/comicmodels/migrations/0025_add_RegistrationRequest_permissions.py,ed6b72c5d5a946009519a06ee87a9377b41b136e,TODO add permissions for all comicmodels and registrationRequest,https://github.com/comic/grand-challenge.org/commit/ed6b72c5d5a946009519a06ee87a9377b41b136e,Yes
5258,comic/grand-challenge.org,django/comicmodels/admin.py,0e1e82b305f744e4cc1873f1ae141d817fbf8cd6,TODO: This class should derive from ComicModelAdmin and not from GuardedModelAdmin,https://github.com/comic/grand-challenge.org/commit/0e1e82b305f744e4cc1873f1ae141d817fbf8cd6,Yes
5259,comic/grand-challenge.org,app/evaluation/tests/test_models.py,4945d709e7f145c42bcbfa7eb4191b53cd3bd2ad,TODO: Add some model tests,https://github.com/comic/grand-challenge.org/commit/4945d709e7f145c42bcbfa7eb4191b53cd3bd2ad,Yes
5260,comic/grand-challenge.org,app/grandchallenge/algorithms/models.py,b5963ca0e0b9c31c769fe6ad4f8c0f556b833bc4,TODO: This class is mostly duplicate from evaluation\/models.py.,https://github.com/comic/grand-challenge.org/commit/b5963ca0e0b9c31c769fe6ad4f8c0f556b833bc4,No
5261,comic/grand-challenge.org,app/grandchallenge/retina_api/views.py,3a5e2199b7dc6986712d6eade21ebd3061c97337,TODO patient only images? Propose model change,https://github.com/comic/grand-challenge.org/commit/3a5e2199b7dc6986712d6eade21ebd3061c97337,No
5262,larq/larq,larq/quantized_variable.py,4e3a66b5bdca183584b8d6b8fa7ae33c6e47b06e,TODO: Find a better way to support SavedModel. Exposing private attributes is,https://github.com/larq/larq/commit/4e3a66b5bdca183584b8d6b8fa7ae33c6e47b06e,Yes
5263,Rostlab/nalaf,demo_predict.py,e37491ab7081be584dbc73be2ddc0bb360a2424a,TODO include default_model & example.txt under resources\/,https://github.com/Rostlab/nalaf/commit/e37491ab7081be584dbc73be2ddc0bb360a2424a,Yes
5264,Rostlab/nalaf,nala/bootstrapping/iteration.py,e303968169edbb5e077b01196cd6ad1a8cab4b5c,todo save model to iteration_0 folder as bin_model,https://github.com/Rostlab/nalaf/commit/e303968169edbb5e077b01196cd6ad1a8cab4b5c,Yes
5265,Rostlab/nalaf,nala/bootstrapping/iteration.py,bbde717927003b364fe17d745719656a5cadd501,todo major sophisticated automatic execution (check what is missing e.g. bin_model),https://github.com/Rostlab/nalaf/commit/bbde717927003b364fe17d745719656a5cadd501,Yes
5266,Rostlab/nalaf,nalaf/download_data.py,13bf86d5973aa24fc75f93b73571308700bc94c3,TODO download non-packaged [example_entity_model](https:\/\/github.com\/Rostlab\/nalaf\/blob\/develop\/nalaf\/data\/example_entity_model),https://github.com/Rostlab/nalaf/commit/13bf86d5973aa24fc75f93b73571308700bc94c3,No
5267,graknlabs/kglib,kglib/kgcn/examples/animal_trade/main.py,40e2a2234ad26dc8189f32a7f232dc9ffc1b7e94,"raise ValueError(\""Model is not persisted; so training must be performed\"") # TODO is this true?",https://github.com/graknlabs/kglib/commit/40e2a2234ad26dc8189f32a7f232dc9ffc1b7e94,No
5268,graknlabs/kglib,kglib/kgcn/pipeline/utils.py,88690ec7bb66691e4d0b1e8b7c97e7e3f0277472,TODO This is the desired implementation; but the graphs are altered by the model to have duplicated reversed,https://github.com/graknlabs/kglib/commit/88690ec7bb66691e4d0b1e8b7c97e7e3f0277472,No
5269,materialsvirtuallab/megnet,megnet/data/graph.py,585654be2c2a0881477b213dbafd3714d92a92b5,"TODO (wardlt): Consider making \""num_*_features\"" funcs to simplify making a MEGNet model",https://github.com/materialsvirtuallab/megnet/commit/585654be2c2a0881477b213dbafd3714d92a92b5,Yes
5270,DigitalSlideArchive/HistomicsTK,server/__init__.py,85900ca9f75014890ea65c53962d7fd1b2104133,TODO: check if the xml adheres to slicer execution model,https://github.com/DigitalSlideArchive/HistomicsTK/commit/85900ca9f75014890ea65c53962d7fd1b2104133,Yes
5271,DigitalSlideArchive/HistomicsTK,server/__init__.py,2b2d34db3d10f6c4273a336c65989e9fae59cae8,TODO: check if the xml adheres to slicer execution model xml schema,https://github.com/DigitalSlideArchive/HistomicsTK/commit/2b2d34db3d10f6c4273a336c65989e9fae59cae8,Yes
5272,DigitalSlideArchive/HistomicsTK,server/rest_slicer_cli.py,3e35e894f4c044a4bbddcdf9ec7e062065315917,TODO: check if xml adheres to slicer execution model xml schema,https://github.com/DigitalSlideArchive/HistomicsTK/commit/3e35e894f4c044a4bbddcdf9ec7e062065315917,Yes
5273,DependableSystemsLab/TensorFI,Tests/NotWorking/mnist_deep.py,ed9ea0db1adb3ba5e8751139b42d3f11e68f32fa,FIXME: Code to save and restore the model if Training is skipped,https://github.com/DependableSystemsLab/TensorFI/commit/ed9ea0db1adb3ba5e8751139b42d3f11e68f32fa,Yes
5274,a2i2/surround,surround/visualise.py,7eba40075de5cf111568f2b4cf48b6a44fa2ab53,"\""\""\"" visualise.py ||  || Visualises the output from training a classifier. ||  || Supports both binary and multi class classifiers. ||  || Use cases: ||  - Visualising the output from training a model ||  - Viewing the output from running batch predictions on a dataset ||  || TODO: Order confusion matrix by most popular class to least popular class || TODO: Output file format in HTML. Always print to the screen. || TODO: Visualisation function should be different from function the output metrics || TODO: Wrap in a Visualiser interface for use in Surround || TODO: Support multiple ground truth and prediction columns || TODO: Add flag to output file with incorrect records. True by default. || TODO: Rename module to visualise_classifier.py ||  || TODO: Add a flag to set probability thresholds || TODO: Add a flag that describes each aspect of the generated report in human readable terminology ||  || \""\""\""",https://github.com/a2i2/surround/commit/7eba40075de5cf111568f2b4cf48b6a44fa2ab53,Yes
5275,Aifred-Health/Vulcan,vulcanai2/models/AbstractNetwork.py,56bfa5aebecedaaed14d4d858b4adfdfafbed214,TODO: this should be the same for every model; given that you pass a config??,https://github.com/Aifred-Health/Vulcan/commit/56bfa5aebecedaaed14d4d858b4adfdfafbed214,No
5276,Aifred-Health/Vulcan,vulcanai2/models/BaseNetwork.py,343f3f3196ca62a1237c48fa6521d005dc6ec49f,TODO: this should be the same for every model; given that you pass a config??,https://github.com/Aifred-Health/Vulcan/commit/343f3f3196ca62a1237c48fa6521d005dc6ec49f,No
5277,Aifred-Health/Vulcan,vulcanai2/models/snapshot_ensemble.py,4f1a5bc867e5d2adb3ef62dc8feda102b9b58879,TODO: Fix bc it writes in the same folder several models,https://github.com/Aifred-Health/Vulcan/commit/4f1a5bc867e5d2adb3ef62dc8feda102b9b58879,Yes
5278,Aifred-Health/Vulcan,vulcanai2/models/snapshot_ensemble.py,4f1a5bc867e5d2adb3ef62dc8feda102b9b58879,TODO: Fix to load the correct models,https://github.com/Aifred-Health/Vulcan/commit/4f1a5bc867e5d2adb3ef62dc8feda102b9b58879,Yes
5279,Aifred-Health/Vulcan,vulcanai2/models/snapshot_ensemble.py,90629ecf104f4efaf92a7304eeec573219966fcb,TODO: Fix bc it writes in the same folder several models,https://github.com/Aifred-Health/Vulcan/commit/90629ecf104f4efaf92a7304eeec573219966fcb,Yes
5280,Aifred-Health/Vulcan,vulcanai2/models/snapshot_ensemble.py,90629ecf104f4efaf92a7304eeec573219966fcb,TODO: Fix to load the correct models,https://github.com/Aifred-Health/Vulcan/commit/90629ecf104f4efaf92a7304eeec573219966fcb,Yes
5281,Aifred-Health/Vulcan,examples/fashion_conv_dense_test.py,fa1524e56970b4451958edc5cef79216c92b0ac2,TODO: to train parts of the model in different device,https://github.com/Aifred-Health/Vulcan/commit/fa1524e56970b4451958edc5cef79216c92b0ac2,Yes
5282,Aifred-Health/Vulcan,examples/fashion_conv_dense_test.py,b7c890a59c21f87475c019153f17a6e1cb26f06a,# TODO: to train parts of the model in different device,https://github.com/Aifred-Health/Vulcan/commit/b7c890a59c21f87475c019153f17a6e1cb26f06a,Yes
5283,nltk/nltk,lite/nltk_lite/probability.py,f9fc8bcdafd590084a7cf6e77c57981a224d07ea,"\""\""\"" || Classes for representing and processing probabilistic information. ||  || The L{FreqDist} class is used to encode X{frequency distributions}; || which count the number of times that each outcome of an experiment || occurs. ||  || The L{ProbDistI} class defines a standard interface for X{probability || distributions}; which encode the probability of each outcome for an || experiment.  There are two types of probability distribution: ||  ||   - X{derived probability distributions} are created from frequency ||     distributions.  They attempt to model the probability distribution ||     that generated the frequency distribution. ||   - X{analytic probability distributions} are created directly from ||     parameters (such as variance). ||  || The L{ConditionalFreqDist} class and L{ConditionalProbDistI} interface || are used to encode conditional distributions.  Conditional probability || distributions can be derived or analytic; but currently the only || implementation of the C{ConditionalProbDistI} interface is || L{ConditionalProbDist}; a derived distribution. ||  || The L{ProbabilisticMixIn} class is a mix-in class that can be used to || associate probabilities with data classes (such as C{Token} or || C{Tree}). ||  || @group Frequency Distributions: FreqDist || @group Derived Probability Distributions: ProbDistI; MLEProbDist; ||     LidstoneProbDist; LaplaceProbDist; ELEProbDist; HeldoutProbDist; ||     CrossValidationProbDist || @group Analyitic Probability Distributions: UniformProbDist || @group Conditional Distributions: ConditionalFreqDist; ||     ConditionalProbDistI; ConditionalProbDist || @group Probabilistic Mix-In: ProbabilisticMixIn || @sort: FreqDist; ProbDistI; MLEProbDist; LidstoneProbDist; LaplaceProbDist;  ||     ELEProbDist; HeldoutProbDist; CrossValidationProbDist; UniformProbDist; ||     ConditionalFreqDist; ConditionalProbDistI; ConditionalProbDist ||  || @todo: Better handling of log probabilities. || \""\""\""",https://github.com/nltk/nltk/commit/f9fc8bcdafd590084a7cf6e77c57981a224d07ea,Yes
5284,nltk/nltk,nltk/draw/concordance.py,44c62ba631fd38cebd4dbca18b28e304ef65425a,todo: refactor the model such that it is less state sensitive,https://github.com/nltk/nltk/commit/44c62ba631fd38cebd4dbca18b28e304ef65425a,Yes
5285,nltk/nltk,nltk/align/ibm5.py,28171d338147abda83474b7c27003dbc57a3a339,TODO use Model 4 scoring instead of Model 5,https://github.com/nltk/nltk/commit/28171d338147abda83474b7c27003dbc57a3a339,Yes
5286,tensorflow/tensor2tensor,tensor2tensor/bin/rl_with_imagination_loop.py,b50894a1100c8f7e769524f3d1b353bad4494b5f,TODO - last_model :=,https://github.com/tensorflow/tensor2tensor/commit/b50894a1100c8f7e769524f3d1b353bad4494b5f,No
5287,tensorflow/tensor2tensor,tensor2tensor/data_generators/gym.py,f09c4860683ddf54f7bb9f42e6ef751b0dacdced,TODO: adjust regexp for different models,https://github.com/tensorflow/tensor2tensor/commit/f09c4860683ddf54f7bb9f42e6ef751b0dacdced,Yes
5288,tensorflow/tensor2tensor,tensor2tensor/data_generators/gym.py,fb39be70df95a7607aa893ba0419bdc210320ec8,TODO: adjust regexp for different models,https://github.com/tensorflow/tensor2tensor/commit/fb39be70df95a7607aa893ba0419bdc210320ec8,Yes
5289,tensorflow/tensor2tensor,tensor2tensor/mesh_tensorflow/mtf_transformer_compat.py,1d5814b74856f6f783bcdcb46eaa97cd36d51d16,"\""\""\""Temporary hack for decoding mtf_transformer models. ||  || This is a transformer implementation in regular TensorFlow which is || checkpoint-compatible with MtfTransformer for eval\/inference. ||  || The purpose of this model is to run inference on MtfTransformer models. || We are working on native decoding in MtfTransformer which will be faster and || cleaner. ||  || TODO(noam): Remove once we can decode in mtf. || \""\""\""",https://github.com/tensorflow/tensor2tensor/commit/1d5814b74856f6f783bcdcb46eaa97cd36d51d16,Yes
5290,RaRe-Technologies/gensim,acme/lee-wiki-streamParsing2008-cosines.py,d77450c7df7e5877947727d0c8264bf7928d5e65,TODO serialize the entire tfidf model. We'll need it later  to create a sparseMatrixSimilarity object,https://github.com/RaRe-Technologies/gensim/commit/d77450c7df7e5877947727d0c8264bf7928d5e65,Yes
5291,RaRe-Technologies/gensim,gensim/models/ldamodelmulticore.py,fc0d6811a61c34b852a46b68523df7aa81d84081,"\""\""\"" || Latent Dirichlet Allocation (LDA) in Python; using all cores to parallelize and || speed up model training. ||  || The parallelization uses multiprocessing; in case this doesn't work for you for || some reason; try `LdaModel` which is an equivalent; but more straightforward and || single-core implementation. ||  || FIXME wiki timings ||  || This module allows both LDA model estimation from a training corpus and inference of topic || distribution on new; unseen documents. The model can also be updated with new documents || for online training. ||  || The core estimation code is based on the `onlineldavb.py` script by M. Hoffman [1]_; see || **Hoffman; Blei; Bach: Online Learning for Latent Dirichlet Allocation; NIPS 2010.** ||  || The algorithm: ||  || * is **streamed**: training documents may come in sequentially; no random access required; || * runs in **constant memory** w.r.t. the number of documents: size of the ||   training corpus does not affect memory footprint; can process corpora larger than RAM; and || * is **distributed**: makes use of a cluster of machines; if available; to ||   speed up model estimation. ||  || .. [1] http:\/\/www.cs.princeton.edu\/~mdhoffma || \""\""\""",https://github.com/RaRe-Technologies/gensim/commit/fc0d6811a61c34b852a46b68523df7aa81d84081,No
5292,RaRe-Technologies/gensim,gensim/models/coherencemodel.py,6151747e65d8fa0b3e15554b31b627f1f4879298,FIXME : Meant to work for LDAModel; LdaVowpalWabbit right now. Make it work for others.,https://github.com/RaRe-Technologies/gensim/commit/6151747e65d8fa0b3e15554b31b627f1f4879298,Yes
5293,RaRe-Technologies/gensim,gensim/models/ldaseqmodel.py,1ae13385995ec4a16ed5bbe149b89fb2137b6a8b,TODO: replace fit_lda_post with appropriate ldamodel functions; if possible.,https://github.com/RaRe-Technologies/gensim/commit/1ae13385995ec4a16ed5bbe149b89fb2137b6a8b,Yes
5294,RaRe-Technologies/gensim,gensim/test/test_hdpmodel.py,098be5fb22e5e6dbdb30354a3814938fa8061c69,TODO create show_topic in HdpModel and then test,https://github.com/RaRe-Technologies/gensim/commit/098be5fb22e5e6dbdb30354a3814938fa8061c69,Yes
5295,RaRe-Technologies/gensim,gensim/models/atmodel.py,739f34ee29381ebb4b005934b60b7f264d08e01c,TODO: this is duplication of code in LdaModel. Refactor.,https://github.com/RaRe-Technologies/gensim/commit/739f34ee29381ebb4b005934b60b7f264d08e01c,Yes
5296,RaRe-Technologies/gensim,gensim/models/atmodel.py,739f34ee29381ebb4b005934b60b7f264d08e01c,TODO: this method is somewhat similar to the one in LdaModel. Refactor if possible.,https://github.com/RaRe-Technologies/gensim/commit/739f34ee29381ebb4b005934b60b7f264d08e01c,Yes
5297,RaRe-Technologies/gensim,gensim/models/atmodel.py,739f34ee29381ebb4b005934b60b7f264d08e01c,TODO: This method is very similar to the one in LdaModel. Refactor.,https://github.com/RaRe-Technologies/gensim/commit/739f34ee29381ebb4b005934b60b7f264d08e01c,Yes
5298,explosion/spaCy,spacy/util.py,97647c46cdbd623e34da4c162ceecd4b97b0946e,TODO: Allow passing in full model path and only require one argument,https://github.com/explosion/spaCy/commit/97647c46cdbd623e34da4c162ceecd4b97b0946e,Yes
5299,explosion/spaCy,spacy/_ml.py,5cbefcba1743701a4d895123178a885454cf6c45,TODO: Unset this once we don't want to support models previous models.,https://github.com/explosion/spaCy/commit/5cbefcba1743701a4d895123178a885454cf6c45,Yes
5300,explosion/spaCy,examples/training/train_textcat.py,f1b86dff8cef8802f5493bad4331d23e016dca7c,TODO: Remove this once we're not supporting models trained with thinc <6.9.0,https://github.com/explosion/spaCy/commit/f1b86dff8cef8802f5493bad4331d23e016dca7c,Yes
5301,explosion/spaCy,spacy/_ml.py,b22e42af7f2b9593fa47d8b4920c43e64f98f737,TODO: Unset this once we don't want to support models previous models.,https://github.com/explosion/spaCy/commit/b22e42af7f2b9593fa47d8b4920c43e64f98f737,Yes
5302,BrikerMan/Kashgari,tests/test_labeling/test_bi_lstm_model.py,c0f300d8927e92d1cf3db1e8926fa1d7b462f01b,TODO: fix load model,https://github.com/BrikerMan/Kashgari/commit/c0f300d8927e92d1cf3db1e8926fa1d7b462f01b,Yes
5303,PetrochukM/PyTorch-NLP,lib/hyperparameter_optimization.py,0cdbcfbe4a5508e902d60152001b29fb0f331fe4,TODO: Multiply by the number of dimensions so it scales the number of models,https://github.com/PetrochukM/PyTorch-NLP/commit/0cdbcfbe4a5508e902d60152001b29fb0f331fe4,Yes
5304,yzhao062/pyod,pyod/models/glosh.py,f0a8bb4bd93b87945f9e48dad7363f34ef86e27e,TODO: fix broken model here,https://github.com/yzhao062/pyod/commit/f0a8bb4bd93b87945f9e48dad7363f34ef86e27e,Yes
5305,yzhao062/pyod,pyod/test/test_feat_bagging.py,e2487c560433686e337be99e6389a7f728d2fdce,TODO: finish the tests once the main model is ready,https://github.com/yzhao062/pyod/commit/e2487c560433686e337be99e6389a7f728d2fdce,Yes
5306,yzhao062/pyod,pyod/models/feature_bagging.py,851da67b32b52b4c7ebb8d60e906ad8466600ab2,TODO: should support parallelization at the model level,https://github.com/yzhao062/pyod/commit/851da67b32b52b4c7ebb8d60e906ad8466600ab2,Yes
5307,yzhao062/pyod,examples/temp_do_not_use_lscp.py,f4f77a51e0ec472bcd2986444a96f62fba5e5b99,TODO: discuss how to standardize data for different model types,https://github.com/yzhao062/pyod/commit/f4f77a51e0ec472bcd2986444a96f62fba5e5b99,Yes
5308,prihoda/golem,golem/models.py,5cf64fc15673b64da1cdb8a501d06946c4be17e5,class Attachment(models.Model):  TODO,https://github.com/prihoda/golem/commit/5cf64fc15673b64da1cdb8a501d06946c4be17e5,No
5309,raamana/neuropredict,neuropredict/base.py,8b30b844b79ff9ec61456a77f8b7de57d6def01f,TODO for API use; pred_model and dim_reducer must be validated here again,https://github.com/raamana/neuropredict/commit/8b30b844b79ff9ec61456a77f8b7de57d6def01f,Yes
5310,raviqqe/tensorflow-qnd,qnd/experiment.py,37025d95d9898381bfabfedfdc89ae45a4c90c4a,TODO: Where to set this? Or; is this the same as Estimator.model_dir?,https://github.com/raviqqe/tensorflow-qnd/commit/37025d95d9898381bfabfedfdc89ae45a4c90c4a,Yes
5311,adalca/neurite,neuron/generators.py,ab2a7a7e8f870bdd83955778623a13163ef2e40e,todo: recompute model,https://github.com/adalca/neurite/commit/ab2a7a7e8f870bdd83955778623a13163ef2e40e,No
5312,adalca/neurite,neuron/sandbox.py,4f8dd6a90bc22eace46b1babdba0d59800363112,TODO: we can do this automatically when setting up the models by having,https://github.com/adalca/neurite/commit/4f8dd6a90bc22eace46b1babdba0d59800363112,Yes
5313,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/trainer.py,2a04be038624bd8867b01b4b7bfe967b010fb941,TODO: remove self.testing condition because model.summarize() is wiping out the weights,https://github.com/PyTorchLightning/pytorch-lightning/commit/2a04be038624bd8867b01b4b7bfe967b010fb941,Yes
5314,PyTorchLightning/pytorch-lightning,pytorch_lightning/trainer/model_hooks.py,1caf8beb2cf341ff033286b1b924a1c1ff273a64,TODO - refector this function to accept model_name; instance; parent so it makes more sense,https://github.com/PyTorchLightning/pytorch-lightning/commit/1caf8beb2cf341ff033286b1b924a1c1ff273a64,Yes
5315,PyTorchLightning/pytorch-lightning,pytorch_lightning/utilities/model_utils.py,b0f77a74a1a5cfa8d67fab5730ecd37d1915a184,TODO - refector this function to accept model_name; instance; parent so it makes more sense,https://github.com/PyTorchLightning/pytorch-lightning/commit/b0f77a74a1a5cfa8d67fab5730ecd37d1915a184,Yes
5316,PyTorchLightning/pytorch-lightning,pl_examples/basic_examples/simple_image_classifier.py,22bd742214d28c54a983c1d9699479ad0363d88a,todo: without passing model it fails for missing best weights,https://github.com/PyTorchLightning/pytorch-lightning/commit/22bd742214d28c54a983c1d9699479ad0363d88a,Yes
5317,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/ddp_sequential_plugin.py,ef8ef12fd0b1fa9318aa7a9930389bab8c8ef5d5,TODO currently no support for vertical model parallel,https://github.com/PyTorchLightning/pytorch-lightning/commit/ef8ef12fd0b1fa9318aa7a9930389bab8c8ef5d5,Yes
5318,PyTorchLightning/pytorch-lightning,pytorch_lightning/accelerators/ddp_cpu_spawn_accelerator.py,2d54116baa1284057d68bde52daf49ee19c5f899,Todo: required argument `model` is not used,https://github.com/PyTorchLightning/pytorch-lightning/commit/2d54116baa1284057d68bde52daf49ee19c5f899,Yes
5319,PyTorchLightning/pytorch-lightning,pytorch_lightning/accelerators/tpu_accelerator.py,2d54116baa1284057d68bde52daf49ee19c5f899,Todo: required argument `model` is not used,https://github.com/PyTorchLightning/pytorch-lightning/commit/2d54116baa1284057d68bde52daf49ee19c5f899,Yes
5320,PyTorchLightning/pytorch-lightning,pytorch_lightning/utilities/model_helpers.py,a884866ff0b72f54412c2ffd38f86b5c928c1395,TODO - refector this function to accept model_name; instance; parent so it makes more sense,https://github.com/PyTorchLightning/pytorch-lightning/commit/a884866ff0b72f54412c2ffd38f86b5c928c1395,Yes
5321,PyTorchLightning/pytorch-lightning,pytorch_lightning/accelerators/tpu_accelerator.py,5ae6926a520ecaa21fd96f3ebd15b9069dbd880a,Todo: required argument `model` is not used,https://github.com/PyTorchLightning/pytorch-lightning/commit/5ae6926a520ecaa21fd96f3ebd15b9069dbd880a,Yes
5322,PyTorchLightning/pytorch-lightning,tests/callbacks/test_callbacks.py,48718d7ce74f712da881e9585d66a8e0d54e7004,todo: enabled since internally we wrap the model for optimizer step; this should be fixed,https://github.com/PyTorchLightning/pytorch-lightning/commit/48718d7ce74f712da881e9585d66a8e0d54e7004,Yes
5323,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/training_type/rpc_sequential.py,9064b83caf817c624ad5241080e5a68d678a5ea6,TODO currently no support for vertical model parallel,https://github.com/PyTorchLightning/pytorch-lightning/commit/9064b83caf817c624ad5241080e5a68d678a5ea6,Yes
5324,PyTorchLightning/pytorch-lightning,pytorch_lightning/core/lightning.py,b434c479e7be787c49be2df381011bed3dc8f070,todo: think about better way without need to dump model to drive,https://github.com/PyTorchLightning/pytorch-lightning/commit/b434c479e7be787c49be2df381011bed3dc8f070,Yes
5325,PyTorchLightning/pytorch-lightning,pytorch_lightning/core/memory.py,b434c479e7be787c49be2df381011bed3dc8f070,todo: seems it does not work with quantized models - it returns 0.0,https://github.com/PyTorchLightning/pytorch-lightning/commit/b434c479e7be787c49be2df381011bed3dc8f070,Yes
5326,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/training_type/ddp_spawn.py,c81b2a81891cc39e454b841b1ed221b734e39cee,TODO: is there a better way than accessing trainer through model -> trainer?,https://github.com/PyTorchLightning/pytorch-lightning/commit/c81b2a81891cc39e454b841b1ed221b734e39cee,Yes
5327,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/training_type/tpu_spawn.py,bb92754119934c79f97de38ce59470f0059f3a0d,TODO: is there a better way than accessing trainer through model -> trainer?,https://github.com/PyTorchLightning/pytorch-lightning/commit/bb92754119934c79f97de38ce59470f0059f3a0d,Yes
5328,flennerhag/mlens,mlens/parallel/handles.py,caab535236c6be38e9ce42cfe90a28beb5ec8abe,TODO: disallow None pipeline (modify model selection case handling),https://github.com/flennerhag/mlens/commit/caab535236c6be38e9ce42cfe90a28beb5ec8abe,Yes
5329,kengz/SLM-Lab,slm_lab/agent/__init__.py,f7c257ebab143ec83f1e1ffbe8dc49e3b43a628e,TODO save model,https://github.com/kengz/SLM-Lab/commit/f7c257ebab143ec83f1e1ffbe8dc49e3b43a628e,No
5330,kengz/SLM-Lab,slm_lab/agent/net/feedforward.py,d670e94d3f8e938858e938b1d3efeac5389a26d3,TODO see openai baselines; model.py;  _mlp is so clean,https://github.com/kengz/SLM-Lab/commit/d670e94d3f8e938858e938b1d3efeac5389a26d3,No
5331,kengz/SLM-Lab,run_lab.py,d96b2eb3157add4c4cb2faa7d853b28e0a1effc0,TODO turn on save\/load model mode,https://github.com/kengz/SLM-Lab/commit/d96b2eb3157add4c4cb2faa7d853b28e0a1effc0,Yes
5332,kengz/SLM-Lab,run_lab.py,2ce74a407886737b8e8d717ad518c6c126f6b06b,TODO hack and set the eval model location inside info_space,https://github.com/kengz/SLM-Lab/commit/2ce74a407886737b8e8d717ad518c6c126f6b06b,Yes
5333,kengz/SLM-Lab,run_lab.py,68c85f8c91d2b5024065e79e5180443499b85fd3,TODO hack and set the eval model location inside info_space,https://github.com/kengz/SLM-Lab/commit/68c85f8c91d2b5024065e79e5180443499b85fd3,Yes
5334,kengz/SLM-Lab,slm_lab/experiment/control.py,6f1ede65f6ddc0fd513e4aa121022ff3aaf134ed,TODO eval call using eval_model_prepath,https://github.com/kengz/SLM-Lab/commit/6f1ede65f6ddc0fd513e4aa121022ff3aaf134ed,Yes
5335,jmwoloso/pychattr,pychattr/channel_attribution/_mixins.py,477c1c08ed347b291e1a1d9d8505f652dd10f006,TODO: incorporate into heuristic models,https://github.com/jmwoloso/pychattr/commit/477c1c08ed347b291e1a1d9d8505f652dd10f006,Yes
5336,JohnVinyard/zounds,config.py,59369117f84fe0fcd4048e36be1a5baf39e810b5,TODO: I don't like the redundancy here; i.e.; the model class is a key in,https://github.com/JohnVinyard/zounds/commit/59369117f84fe0fcd4048e36be1a5baf39e810b5,Yes
5337,JohnVinyard/zounds,config.py,bea8b76d66e94099c7b8e0ea999f062fb82eb3be,TODO: FrameModel needs to have a metaclass that knows how to map extractors,https://github.com/JohnVinyard/zounds/commit/bea8b76d66e94099c7b8e0ea999f062fb82eb3be,Yes
5338,JohnVinyard/zounds,model/pattern.py,bea8b76d66e94099c7b8e0ea999f062fb82eb3be,TODO: Move this into Model,https://github.com/JohnVinyard/zounds/commit/bea8b76d66e94099c7b8e0ea999f062fb82eb3be,Yes
5339,JohnVinyard/zounds,util.py,1641d6034e1aa30ca8790344f3bb58d8f57052d8,TODO: This is used in analyze.extractor and model.frame. Can it be,https://github.com/JohnVinyard/zounds/commit/1641d6034e1aa30ca8790344f3bb58d8f57052d8,No
5340,JohnVinyard/zounds,model/framesearch.py,db0ad8902f5e009e598dea387ece8b29d6556d94,TODO: I'm not sure the model module package is the appropriate place for this;,https://github.com/JohnVinyard/zounds/commit/db0ad8902f5e009e598dea387ece8b29d6556d94,Yes
5341,ryfeus/gcf-packs,pandas_numpy/sources/pandas/core/api.py,255a05a5980efb8b096c283d79872d0695886161,TODO: Remove import when statsmodels updates #18264,https://github.com/ryfeus/gcf-packs/commit/255a05a5980efb8b096c283d79872d0695886161,Yes
5342,stevewyl/nlp_toolkit,nlp_toolkit/model_zoo.py,7d5a7999b4b3427f77a305f0d8fd573758f32880,"\""\""\"" || Model Zoos. || TODO add shape size for each layer || \""\""\""",https://github.com/stevewyl/nlp_toolkit/commit/7d5a7999b4b3427f77a305f0d8fd573758f32880,Yes
5343,explosion/spaCy,examples/pipeline/wiki_entity_linking/train_el.py,c6ca8649d7ab67af88af1682fa93a63fc635481c,TODO ? link_vectors_to_models(self.vocab),https://github.com/explosion/spaCy/commit/c6ca8649d7ab67af88af1682fa93a63fc635481c,No
5344,explosion/spaCy,examples/pipeline/wiki_entity_linking/train_el.py,b5470f3d753dd3bac3423121a44c0862a67b607c,TODO ? link_vectors_to_models(self.vocab) depr?,https://github.com/explosion/spaCy/commit/b5470f3d753dd3bac3423121a44c0862a67b607c,No
5345,explosion/spaCy,spacy/cli/debug_data.py,43b960c01b0c64e56859ad5eb304a5422af46516,TODO: handle base model,https://github.com/explosion/spaCy/commit/43b960c01b0c64e56859ad5eb304a5422af46516,Yes
5346,scikit-learn/scikit-learn,scikits/learn/feature_select/tests/test_feature_select.py,60ddea6aed7b3e9abbac32a9a7a8eddc8c57645e,"\""\""\"" || Todo: cross-check the F-value with stats model || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/60ddea6aed7b3e9abbac32a9a7a8eddc8c57645e,No
5347,scikit-learn/scikit-learn,scikits/learn/glm.py,28c57f78494045191719c158343fa8615cf53705,## TODO: intercept for all models,https://github.com/scikit-learn/scikit-learn/commit/28c57f78494045191719c158343fa8615cf53705,Yes
5348,scikit-learn/scikit-learn,examples/plot_gmm.py,7ada25ae8c847192a5c91a317de9fa8aaaed2fad,"\""\""\"" || Simple Gaussian Mixture model plotting example ||  || TODO: use the faithful dataset || \""\""\""",https://github.com/scikit-learn/scikit-learn/commit/7ada25ae8c847192a5c91a317de9fa8aaaed2fad,Yes
5349,scikit-learn/scikit-learn,scikits/learn/glm/base.py,df8c593ce694869e11fd459fc310d0ce0a55788e,## TODO: intercept for all models,https://github.com/scikit-learn/scikit-learn/commit/df8c593ce694869e11fd459fc310d0ce0a55788e,Yes
5350,scikit-learn/scikit-learn,skeletons/exercise_04_face_recognition.py,40beda3eb74bb5a4a89b2304d160c2785c4595dc,TODO store the best model in a variable named 'clf',https://github.com/scikit-learn/scikit-learn/commit/40beda3eb74bb5a4a89b2304d160c2785c4595dc,Yes
5351,scikit-learn/scikit-learn,sklearn/linear_model/_base.py,306826f7b6bf5fd61af74062c0ba8f0f21aa3dae,FIXME in 1.2: parameter 'normalize' should be removed from linear models,https://github.com/scikit-learn/scikit-learn/commit/306826f7b6bf5fd61af74062c0ba8f0f21aa3dae,Yes
5352,eliorc/denzel,denzel/app/logic.py,dce1ef3f1a094066304fbf7753621070f8003264,TODO - DSG - transform data into model ready data. Must return list; where each element is an example.,https://github.com/eliorc/denzel/commit/dce1ef3f1a094066304fbf7753621070f8003264,Yes
5353,eliorc/denzel,denzel/app/logic.py,18a5b259ff524265d0a2df35ceb6e9aeece27594,TODO - DSG - load model from disk to memory,https://github.com/eliorc/denzel/commit/18a5b259ff524265d0a2df35ceb6e9aeece27594,Yes
5354,eliorc/denzel,denzel/app/logic.py,5520ca2ebb037649043d7f536d94fb8e7211e007,TODO - DSG - transform data into model ready data. Must return list; where each element is an example.,https://github.com/eliorc/denzel/commit/5520ca2ebb037649043d7f536d94fb8e7211e007,Yes
5355,sraashis/deepdyn,nbee/torchbee.py,6f7a844c0ec1cf1e8ed08fdb653fa4821c5c84bc,Todo score accumulation to check if this model is better than the saved one,https://github.com/sraashis/deepdyn/commit/6f7a844c0ec1cf1e8ed08fdb653fa4821c5c84bc,Yes
5356,DreamingRaven/nemesyst,Misc/movieLenseDataCombiner.py,524d06a1d4521c578c9b4ee679492080f3701b58,combine column headers into one list #TODO: this is probably not neccessary as lists are combined at modelling,https://github.com/DreamingRaven/nemesyst/commit/524d06a1d4521c578c9b4ee679492080f3701b58,Yes
5357,manuwhs/Trapyng,Other/libs (copy)/ML/GaussianProcess.py,df8c673a0b2963d29f4402c83912b5c8320475db,TODO: Estimate likelihood of jghj datasetjgj; given the model,https://github.com/manuwhs/Trapyng/commit/df8c673a0b2963d29f4402c83912b5c8320475db,Yes
5358,manuwhs/Trapyng,libs/ML/GaussianProcess.py,df8c673a0b2963d29f4402c83912b5c8320475db,TODO: Estimate likelihood of jghj datasetjgj; given the model,https://github.com/manuwhs/Trapyng/commit/df8c673a0b2963d29f4402c83912b5c8320475db,Yes
5359,manuwhs/Trapyng,libs/BBBLSTM/BBB_LSTM_Model.py,020f88a525e1718a5cfbbccde8410c321ff08a4b,TODO: maybe due to the initial embedding that has to be done; all inputs are given when defining the model;,https://github.com/manuwhs/Trapyng/commit/020f88a525e1718a5cfbbccde8410c321ff08a4b,Yes
5360,manuwhs/Trapyng,libs/BBBLSTM/BBB_LSTM_Model.py,020f88a525e1718a5cfbbccde8410c321ff08a4b,TODO: maybe do not execute this line in the training model to save computation ? Maybe it wouldnt be executed anyway ?,https://github.com/manuwhs/Trapyng/commit/020f88a525e1718a5cfbbccde8410c321ff08a4b,Yes
5361,manuwhs/Trapyng,Examples/5.2 HBM/pymc3-master/pymc3/sampling.py,6b25ff82f383b3dac092795657538789ae7160c8,TODO sample_posterior_predictive_w is currently only work for model with,https://github.com/manuwhs/Trapyng/commit/6b25ff82f383b3dac092795657538789ae7160c8,No
5362,thu-ml/zhusuan,zhusuan/variational/base.py,f86abdce252e773079767cdbdb388deffdd857c0,TODO: cache bn for the same `meta_model` and `variational`.,https://github.com/thu-ml/zhusuan/commit/f86abdce252e773079767cdbdb388deffdd857c0,Yes
5363,biolab/orange3,Orange/widgets/data/owpreprocess.py,2533f3b5aee7b84166f61fa93e6601b0dad8f8bf,TODO: Model based FS (random forest variable importance; ...); RFE,https://github.com/biolab/orange3/commit/2533f3b5aee7b84166f61fa93e6601b0dad8f8bf,No
5364,biolab/orange3,Orange/widgets/gui.py,fb0fd3fde882361c83337fcf09e418949774218b,FIXME: irrespective of PyListModel check; this might\/should always,https://github.com/biolab/orange3/commit/fb0fd3fde882361c83337fcf09e418949774218b,No
5365,biolab/orange3,Orange/widgets/data/owfeaturestatistics.py,77fde44aa229d9b7c16092b5b82bd43b22b085e2,TODO: Implement filtering on the model,https://github.com/biolab/orange3/commit/77fde44aa229d9b7c16092b5b82bd43b22b085e2,Yes
5366,dmlc/dgl,tutorials/models/gcnTutorialNew.py,68ec624782bb4e4fb9f1adf1088cf39c1826533e,This is a simple implementation of Kipf & Welling's Semi-Supervised Classificaton with Graph Convolutional Networks in ICLR 2017; which propose a simple yet efficient model that extends convolutional neual network from the grid structured data we all familiar and like to graphs; like social network and knowledge graph. It starts from the framework of spectral graph convolutions and makes reasonable simplifications to achieve both faster training and higher prediction accuracy. It also achieves start-of-the-art classification results on a number of graph datasets like CORA; etc. \/TODO: elaborate.,https://github.com/dmlc/dgl/commit/68ec624782bb4e4fb9f1adf1088cf39c1826533e,No
5367,dmlc/dgl,apps/kg/train_mxnet.py,15b951d4c4d4a75cc30442d5dd72fb72ed110e09,TODO: loading model emb only work for genernal Embedding; not for ExternalEmbedding,https://github.com/dmlc/dgl/commit/15b951d4c4d4a75cc30442d5dd72fb72ed110e09,No
5368,dmlc/dgl,apps/kg/train_pytorch.py,15b951d4c4d4a75cc30442d5dd72fb72ed110e09,TODO: loading model emb only work for genernal Embedding; not for ExternalEmbedding,https://github.com/dmlc/dgl/commit/15b951d4c4d4a75cc30442d5dd72fb72ed110e09,No
5369,dmlc/gluon-cv,tests/unittests/test_segmentation.py,dfac51fcd0fbceb3f394a1d8f1ab7c8652e9cf78,TODO FIXME: change it to ADE20K dataset and pretrained model,https://github.com/dmlc/gluon-cv/commit/dfac51fcd0fbceb3f394a1d8f1ab7c8652e9cf78,Yes
5370,dmlc/gluon-cv,docs/tutorials/depth/demo_monodepth2.py,1c41c89c414cc9e447bbc9d2f96f3b6ac519c07c,"\""\""\""01. Predict depth from a single image with pre-trained Monodepth2 models || =========================================================================== ||  || TODO || \""\""\""",https://github.com/dmlc/gluon-cv/commit/1c41c89c414cc9e447bbc9d2f96f3b6ac519c07c,No
5371,microsoft/nni,src/sdk/pynni/nni/nas/pytorch/darts/trainer.py,2116189f0e144a605b1f688c8b6bba0a3ff92625,TODO: should use model instead of self.model,https://github.com/microsoft/nni/commit/2116189f0e144a605b1f688c8b6bba0a3ff92625,Yes
5372,microsoft/nni,src/sdk/pynni/nni/compression/torch/pruning/amc/channel_pruning_env.py,e9f3cddf95e58ca17641db793c420a1b6e5424c0,TODO replace this speedup implementation with nni.compression.torch.ModelSpeedup,https://github.com/microsoft/nni/commit/e9f3cddf95e58ca17641db793c420a1b6e5424c0,Yes
5373,OpenMined/PySyft,syft/federated/federated_client.py,0a062b1e942edb4e509e0f3902e2f3ae42c1d627,TODO: how to get the actual model?,https://github.com/OpenMined/PySyft/commit/0a062b1e942edb4e509e0f3902e2f3ae42c1d627,No
5374,OpenMined/PySyft,syft/federated/federated_client.py,85d4fd776e2debe74e73da8bb35231abad3b5b61,TODO: how to get the actual model?,https://github.com/OpenMined/PySyft/commit/85d4fd776e2debe74e73da8bb35231abad3b5b61,No
5375,allenai/allennlp,allennlp/service/server.py,88d3803e9d1933b9be0303775b410bf026978361,TODO: replace with actual models,https://github.com/allenai/allennlp/commit/88d3803e9d1933b9be0303775b410bf026978361,Yes
5376,allenai/allennlp,allennlp/predictors/biaffine_dependency_parser.py,09c2cc5616a8aec897f8fe06fda13179955bec69,TODO(Mark) Make the language configurable and based on a model attribute.,https://github.com/allenai/allennlp/commit/09c2cc5616a8aec897f8fe06fda13179955bec69,Yes
5377,ray-project/ray,python/ray/rllib/policy_gradient/test/test.py,420013774ca09875243a6e8b84d9dc3f269cc8df,TODO(ekl): move to rllib\/models dir,https://github.com/ray-project/ray/commit/420013774ca09875243a6e8b84d9dc3f269cc8df,Yes
5378,ray-project/ray,rllib/contrib/bandits/agents/policy.py,6ddf84b019be0b6dee52c9699f7c6ebe416dc3d3,TODO: Have a separate model catalogue for bandits,https://github.com/ray-project/ray/commit/6ddf84b019be0b6dee52c9699f7c6ebe416dc3d3,Yes
5379,ray-project/ray,rllib/models/tf/__init__.py,66df8b8c3522dcb47ce163f1d8f9503c094459ce,TODO(sven): Add once ModelV1 is deprecated and we no longer cause circular,https://github.com/ray-project/ray/commit/66df8b8c3522dcb47ce163f1d8f9503c094459ce,Yes
5380,ray-project/ray,rllib/models/torch/__init__.py,66df8b8c3522dcb47ce163f1d8f9503c094459ce,TODO(sven): Add once ModelV1 is deprecated and we no longer cause circular,https://github.com/ray-project/ray/commit/66df8b8c3522dcb47ce163f1d8f9503c094459ce,Yes
5381,ray-project/ray,rllib/agents/dqn/distributional_q_model.py,5537fe13b097097668f9c08a00051e8b7a2d1980,TODO(sven): Move `add_layer_norm` into ModelCatalog as,https://github.com/ray-project/ray/commit/5537fe13b097097668f9c08a00051e8b7a2d1980,Yes
5382,ray-project/ray,rllib/agents/dqn/dqn_torch_model.py,22ccc43670dac93eb7fe81520a84cf3979d05693,TODO(sven): Move `add_layer_norm` into ModelCatalog as,https://github.com/ray-project/ray/commit/22ccc43670dac93eb7fe81520a84cf3979d05693,Yes
5383,ray-project/ray,rllib/agents/dyna/dyna.py,14405b90d5457863d71168c613b4961d34f19cc5,TODO: (sven) allow for having a default model config over many,https://github.com/ray-project/ray/commit/14405b90d5457863d71168c613b4961d34f19cc5,Yes
5384,ray-project/ray,rllib/utils/framework.py,d3bc20b727b1e4c58644a5be303b8f9be0510549,TODO: (sven) move to models\/utils.py,https://github.com/ray-project/ray/commit/d3bc20b727b1e4c58644a5be303b8f9be0510549,Yes
5385,ray-project/ray,rllib/policy/dynamic_tf_policy.py,732197e23a937b7b6d196936519c16ec6317ea9f,TODO: (sven) hack; but works for `target_[q_]?model`.,https://github.com/ray-project/ray/commit/732197e23a937b7b6d196936519c16ec6317ea9f,Yes
5386,NervanaSystems/neon,neon/tests/sanity_check.py,3f90773a6694aaf472b1ad84e02dece87cf1df48,TODO: modelpar currently broken on synthetic-sanity_check.yaml,https://github.com/NervanaSystems/neon/commit/3f90773a6694aaf472b1ad84e02dece87cf1df48,No
5387,NervanaSystems/neon,neon/tests/speed_check.py,3f90773a6694aaf472b1ad84e02dece87cf1df48,TODO: modelpar currently broken on synthetic-sanity_check.yaml,https://github.com/NervanaSystems/neon/commit/3f90773a6694aaf472b1ad84e02dece87cf1df48,No
5388,microsoft/MMdnn,mmdnn/conversion/keras/keras2_emitter.py,bd7ee442d7b15842d1619480a46d77c54c55a567,TODO: arguments won't be saved in keras export model,https://github.com/microsoft/MMdnn/commit/bd7ee442d7b15842d1619480a46d77c54c55a567,Yes
5389,chainer/chainer,chainer/graph_optimimzations/static_graph.py,4b9cbf779b6918b8f5710aeb7965ede6e9eb1864,todo: add test that use the same random seed with two models: a static chain,https://github.com/chainer/chainer/commit/4b9cbf779b6918b8f5710aeb7965ede6e9eb1864,Yes
5390,ealcobaca/pymfe,pymfe/complexity.py,70e58ce4494a521fabb30b96657629b9542b63f1,TODO: This feature seems to be a normalized version of the,https://github.com/ealcobaca/pymfe/commit/70e58ce4494a521fabb30b96657629b9542b63f1,No
5391,ealcobaca/pymfe,pymfe/complexity.py,70e58ce4494a521fabb30b96657629b9542b63f1,TODO: If the categorical attributes are considered; this feature is,https://github.com/ealcobaca/pymfe/commit/70e58ce4494a521fabb30b96657629b9542b63f1,No
5392,ealcobaca/pymfe,pymfe/complexity.py,2d382ec699f34b2b76c1b4be235f4fab426ba594,TODO: If the categorical attributes are considered; this feature is,https://github.com/ealcobaca/pymfe/commit/2d382ec699f34b2b76c1b4be235f4fab426ba594,No
5393,ealcobaca/pymfe,pymfe/complexity.py,8354becab0e8b59bb7f39fe792b9fe5a08399b7b,TODO: This feature seems to be a normalized version of the,https://github.com/ealcobaca/pymfe/commit/8354becab0e8b59bb7f39fe792b9fe5a08399b7b,No
5394,ealcobaca/pymfe,pymfe/complexity.py,8354becab0e8b59bb7f39fe792b9fe5a08399b7b,TODO: If the categorical attributes are considered; this feature is,https://github.com/ealcobaca/pymfe/commit/8354becab0e8b59bb7f39fe792b9fe5a08399b7b,No
5395,GilesStrong/lumin,lumin/nn/data/fold_yielder.py,03bd07f1f69f5117b6fa13f5a85b53e938fe709d,''' || Todo: || - Add categorical features || - Add method to FoldYielder to import other data into correct format; e.g. csv; root || - Make HEPAugFoldYielder able to augment targets as well || ''',https://github.com/GilesStrong/lumin/commit/03bd07f1f69f5117b6fa13f5a85b53e938fe709d,Yes
5396,GilesStrong/lumin,lumin/nn/data/fold_yielder.py,c40840a1ddf0a1f81ec0b227f9123660692941bb,TODO Decide how to handle missing features when deprocessing matrix data,https://github.com/GilesStrong/lumin/commit/c40840a1ddf0a1f81ec0b227f9123660692941bb,Yes
5397,omegaml/omegaml,omegaml/tests/features/environment.py,bacd5dff3831d0feaa3f49b15328a416e7888adf,FIXME we do this because context.feature is set dynamically in EE testing,https://github.com/omegaml/omegaml/commit/bacd5dff3831d0feaa3f49b15328a416e7888adf,Yes
5398,noxouille/rt-mrcnn,model.py,2368d206f079adf1bfe7ccb23824d5d18c22e031,TODO: check if stride of 2 causes alignment issues if the featuremap,https://github.com/noxouille/rt-mrcnn/commit/2368d206f079adf1bfe7ccb23824d5d18c22e031,Yes
5399,noxouille/rt-mrcnn,model.py,2368d206f079adf1bfe7ccb23824d5d18c22e031,TODO: add assert to varify feature map sizes match what's in config,https://github.com/noxouille/rt-mrcnn/commit/2368d206f079adf1bfe7ccb23824d5d18c22e031,Yes
5400,thuijskens/scikit-hyperband,hyperband/tests/test_hyperband.py,89398bae46fe4c5607ab63c87c1ccfd4c2b50fd4,"\""\""\"" || TODO: This test fails due to the random state not being properly fixed ||  || def test_hyperband(): ||     model; param_dist; X; y; rng = setup() ||     search = HyperbandSearchCV(model; param_dist; random_state=rng) ||     search.fit(X; y) ||  ||     # results = pd.DataFrame(search.cv_results_) ||     expected_params = { ||         'bootstrap': False; ||         'criterion': 'entropy'; ||         'max_depth': None; ||         'max_features': 7; ||         'min_samples_leaf': 2; ||         'min_samples_split': 2; ||         'n_estimators': 81 ||     } ||  ||     # assert(results.shape[0] == 186) TODO: sort out what the expected n_i and r_i values are ||     assert(search.best_params_ == expected_params) || \""\""\""",https://github.com/thuijskens/scikit-hyperband/commit/89398bae46fe4c5607ab63c87c1ccfd4c2b50fd4,Yes
5401,howl-anderson/seq2annotation,seq2annotation/server/tensorflow_inference.py,fc5fb0d63e49a8d51496a310c29ca3278c7243a0,TODO: feature translate should out of this main program for better compatible with keras and estimator model,https://github.com/howl-anderson/seq2annotation/commit/fc5fb0d63e49a8d51496a310c29ca3278c7243a0,Yes
5402,howl-anderson/seq2annotation,seq2annotation/server/tensorflow_inference.py,e2f03123d2c7e9271efd4e8ef88e390f89be26a9,TODO: feature translate should out of this main program for better compatible with keras and estimator model,https://github.com/howl-anderson/seq2annotation/commit/e2f03123d2c7e9271efd4e8ef88e390f89be26a9,Yes
5403,mycrazycracy/tf-kaldi-speaker,model/multitask_v1/base_v1.py,c5ccc12ea32927933eec7169a3b9fe5a51405b68,TODO: If this is not true; the feature slicing should be differnet in the network building.,https://github.com/mycrazycracy/tf-kaldi-speaker/commit/c5ccc12ea32927933eec7169a3b9fe5a51405b68,Yes
5404,mycrazycracy/tf-kaldi-speaker,model/pooling.py,c5ccc12ea32927933eec7169a3b9fe5a51405b68,TODO: How to trim the auxiliary features? Align left or center?,https://github.com/mycrazycracy/tf-kaldi-speaker/commit/c5ccc12ea32927933eec7169a3b9fe5a51405b68,Yes
5405,PINTO0309/MobileNetV2-PoseEstimation,tf_pose/network_mobilenet_v2.py,5c406756e2850e6b4a626888aa8d0650e0109fd1,TODO : add more feature with downsample?,https://github.com/PINTO0309/MobileNetV2-PoseEstimation/commit/5c406756e2850e6b4a626888aa8d0650e0109fd1,Yes
5406,andreasvc/disco-dop,treebank.py,64206afe9c58aefd375b5858e89fc662298151be,FIXME: proper representation for arbitrary features,https://github.com/andreasvc/disco-dop/commit/64206afe9c58aefd375b5858e89fc662298151be,Yes
5407,andreasvc/disco-dop,discodop/treebank.py,65b7d449e12b8a57f6581ad251b3deeca7bed5ff,FIXME: proper representation for arbitrary features,https://github.com/andreasvc/disco-dop/commit/65b7d449e12b8a57f6581ad251b3deeca7bed5ff,Yes
5408,andreasvc/disco-dop,discodop/treebank.py,9cfbafec501259c712334e4bb630e6f7b5314328,TODO: decompose postag into individual morphological features,https://github.com/andreasvc/disco-dop/commit/9cfbafec501259c712334e4bb630e6f7b5314328,Yes
5409,andreasvc/disco-dop,discodop/treebank.py,132b31be7cdee22a56ca9853ce78ffdd10b61283,FIXME: split features in multiple attributes,https://github.com/andreasvc/disco-dop/commit/132b31be7cdee22a56ca9853ce78ffdd10b61283,Yes
5410,Erotemic/netharn,netharn/models/classical.py,ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,"\""\""\"" || WIP ||  || This file should contain classes (that behave like torch models); but they || implement the learning of classical learning algorithms like SVM and || RandomForest. ||  || Deep networks are amazing at learning features. However; I don't think it's || very useful to use linear logicstic regression as a classifier. In many cases I || think an SVM or a RandomForest might produce a superior classification model; || but this has yet to be shown. ||  || TODO: ||     - [ ] Classical Abstract API ||     - [ ] Integration with the FitHarn ||         - [ ] How do we swap netharn's backprop+SGD with sklearn's SVM and RandomForest fit methods? ||         - [ ] Netharn needs a \""classical\"" implementation of \""run\"". ||             - [ ] Simply use the data loader to load the data ||             - [ ] Defaults should encourage use with deep features. ||     - [ ] RandomForest ||     - [ ] SVM || \""\""\""",https://github.com/Erotemic/netharn/commit/ef932c663f51f5cb804a56ea4c63ff70c7d6bc31,Yes
5411,Erotemic/netharn,netharn/data/data_containers.py,b157c56653f4fbaf7a84e851d56cb1a4d5f29c08,"\""\""\"" || Proof-of-concept for porting mmcv DataContainer concept to netharn. Depending || on how well this works these features might be useful as a standalone module or || to contribute to torch proper. ||  || References: ||     https:\/\/github.com\/open-mmlab\/mmcv\/blob\/master\/mmcv\/parallel\/data_container.py ||     https:\/\/github.com\/open-mmlab\/mmcv\/blob\/master\/mmcv\/parallel\/collate.py ||     https:\/\/github.com\/open-mmlab\/mmcv\/blob\/master\/mmcv\/parallel\/scatter_gather.py ||  || FIXME 0 dimension tensors || \""\""\""",https://github.com/Erotemic/netharn/commit/b157c56653f4fbaf7a84e851d56cb1a4d5f29c08,Yes
5412,roberthangu/snn_object_recognition,dump-c2-spikes.py,06dbbe1a40c03c78886072a5a1246a6a66742d73,TODO: Add information about the number of prototype cells and the feature size,https://github.com/roberthangu/snn_object_recognition/commit/06dbbe1a40c03c78886072a5a1246a6a66742d73,Yes
5413,stephenjfox/Morph.py,morph/layers/widen.py,7d6e307dd2b5f44369398f6f7c235c8f848f30e0,FIXME: switch to layer.out_features?,https://github.com/stephenjfox/Morph.py/commit/7d6e307dd2b5f44369398f6f7c235c8f848f30e0,Yes
5414,stephenjfox/Morph.py,morph/layers/widen.py,7d6e307dd2b5f44369398f6f7c235c8f848f30e0,FIXME: switch to layer.in_features?,https://github.com/stephenjfox/Morph.py/commit/7d6e307dd2b5f44369398f6f7c235c8f848f30e0,Yes
5415,cuixuage/Machine_Learning,/5_DecisionTree/decision_tree.py,674636a6ddb0a28845443c24b6fb188dc2c9351b,FIXME  dict_key\u662Ffeature value;dict_value\u662Fsubspace-tree-root\u8282\u70B9,https://github.com/cuixuage/Machine_Learning/commit/674636a6ddb0a28845443c24b6fb188dc2c9351b,No
5416,cuixuage/Machine_Learning,/5_DecisionTree/decision_tree.py,08e546299f8b533ce0588ec164448aa3e683530c,FIXME  dict_key\u662Ffeature value;dict_value\u662Fsubspace-tree-root\u8282\u70B9,https://github.com/cuixuage/Machine_Learning/commit/08e546299f8b533ce0588ec164448aa3e683530c,No
5417,deepdiy/deepdiy,deepworm/model_zoo/mrcnn/mrcnn/model.py,82fdf733a72ac474469ee77e2247886b676004b9,TODO: check if stride of 2 causes alignment issues if the feature map,https://github.com/deepdiy/deepdiy/commit/82fdf733a72ac474469ee77e2247886b676004b9,Yes
5418,deepdiy/deepdiy,deepworm/model_zoo/mrcnn/mrcnn/model.py,82fdf733a72ac474469ee77e2247886b676004b9,TODO: add assert to varify feature map sizes match what's in config,https://github.com/deepdiy/deepdiy/commit/82fdf733a72ac474469ee77e2247886b676004b9,Yes
5419,priyankshah7/hypers,skhyper/process/_preprocessing.py,08793a62691f042293827e122229cbb3853ba1f7,TODO This is scaling samples. Perhaphs amend to give option to scale features as well.,https://github.com/priyankshah7/hypers/commit/08793a62691f042293827e122229cbb3853ba1f7,Yes
5420,ziqizhang/chase,python/src/exp/classifier_main.py,3c60baaf6b510847d6fe30af24e144bf7deb9cff,todo: this is not completed. feature dimension must be the same as training data,https://github.com/ziqizhang/chase/commit/3c60baaf6b510847d6fe30af24e144bf7deb9cff,Yes
5421,ziqizhang/chase,python/src/exp/classifier_traintest_main.py,5a8672b10bd37ca18458abe62f04f667be3b0297,todo: save features,https://github.com/ziqizhang/chase/commit/5a8672b10bd37ca18458abe62f04f667be3b0297,Yes
5422,diningphil/CGMM,models/graph_classifiers/CGMM.py,e9c8987e4c5474cadc0929571cac5496241fe003,TODO HANDLE DIFFERENT DATASETS FEATURES AND FIX OTHER THINGS (RUN TO SEE),https://github.com/diningphil/CGMM/commit/e9c8987e4c5474cadc0929571cac5496241fe003,Yes
5423,simonfqy/PADME,dcCustom/trans/test_transformers.py,0c8d29b9d67f09e8e220eca6a9533a134cd10d75,TODO(rbharath): Untransform doesn't work properly for binary feature,https://github.com/simonfqy/PADME/commit/0c8d29b9d67f09e8e220eca6a9533a134cd10d75,Yes
5424,yonghankim/Mask-RCNN,mrcnn/model.py,5c00fe59229cfe636013c1e172c1b392d2bbb337,TODO: check if stride of 2 causes alignment issues if the featuremap,https://github.com/yonghankim/Mask-RCNN/commit/5c00fe59229cfe636013c1e172c1b392d2bbb337,Yes
5425,yonghankim/Mask-RCNN,mrcnn/model.py,5c00fe59229cfe636013c1e172c1b392d2bbb337,TODO: add assert to varify feature map sizes match what's in config,https://github.com/yonghankim/Mask-RCNN/commit/5c00fe59229cfe636013c1e172c1b392d2bbb337,Yes
5426,ealcobaca/pymfe,pymfe/mfe/mfer.py,cd6ab44ce08bf55972c31137b5d9db572088b8fa,"\""\""\""Meta-feature extractor wrapper for MFE R package. ||  || This module is a wrapper to MFE package. MFE is a meta-feature extractor || package building in R. ||  || Example: ||     TODO ||         $ python example_google.py ||  || Todo: ||     * Create an simple example ||     * Create a method to extract meta-features from csv files ||     * You have to also use ``sphinx.ext.todo`` extension ||  || \""\""\""",https://github.com/ealcobaca/pymfe/commit/cd6ab44ce08bf55972c31137b5d9db572088b8fa,No
5427,ealcobaca/pymfe,mfe/__init__.py,e9b8a33c6a076e7fec929444911e84c92d79c59e,"\""\""\""EXtracts metafeatures from structured datasets. ||  || Todo: ||     More information here. || \""\""\""",https://github.com/ealcobaca/pymfe/commit/e9b8a33c6a076e7fec929444911e84c92d79c59e,Yes
5428,ealcobaca/pymfe,mfe/general.py,e9b8a33c6a076e7fec929444911e84c92d79c59e,"\""\""\""Module dedicated to extraction of General Metafeatures. ||  || Todo: ||     - Implement all metafeatures. ||     - Improve documentation. ||  || References: ||     1. \""Towards Reproducible Empirical Research in Meta-Learning\""; ||         Rivolli et al. URL: https:\/\/arxiv.org\/abs\/1808.10406 || \""\""\""",https://github.com/ealcobaca/pymfe/commit/e9b8a33c6a076e7fec929444911e84c92d79c59e,No
5429,ealcobaca/pymfe,pymfe/mfe.py,6b5c98c757d8df77106f3e977fe31d8f4176f292,"\""\""\""Main module for extracting metafeatures from datasets. ||  || Todo: ||     - Improve documentation. ||     - Implement MFE class. || \""\""\""",https://github.com/ealcobaca/pymfe/commit/6b5c98c757d8df77106f3e977fe31d8f4176f292,Yes
5430,ealcobaca/pymfe,python/pymfe/mfe.py,7b549d8da92adf9af3d83f6468ed3fbf4dfb79d8,"\""\""\""Main module for extracting metafeatures from datasets. ||  || Todo: ||     * Improve documentation. ||     * Implement MFE class. || \""\""\""",https://github.com/ealcobaca/pymfe/commit/7b549d8da92adf9af3d83f6468ed3fbf4dfb79d8,Yes
5431,ealcobaca/pymfe,python/pymfe/info_theory.py,ca25f6c479545e5354128152f8ef3336596be08e,"\""\""\""Module dedicated to extraction of Information Theory Metafeatures. ||  || Todo: ||     * Implement metafeatures. ||     * Improve documentation. || \""\""\""",https://github.com/ealcobaca/pymfe/commit/ca25f6c479545e5354128152f8ef3336596be08e,Yes
5432,ealcobaca/pymfe,python/pymfe/landmarking.py,ca25f6c479545e5354128152f8ef3336596be08e,"\""\""\""Module dedicated to extraction of Landmarking Metafeatures. ||  || Todo: ||     * Implement metafeatures. ||     * Improve documentation. || \""\""\""",https://github.com/ealcobaca/pymfe/commit/ca25f6c479545e5354128152f8ef3336596be08e,Yes
5433,ealcobaca/pymfe,python/pymfe/model_based.py,ca25f6c479545e5354128152f8ef3336596be08e,"\""\""\""Module dedicated to extraction of Model Based Metafeatures. ||  || Todo: ||     * Implement metafeatures. ||     * Improve documentation. || \""\""\""",https://github.com/ealcobaca/pymfe/commit/ca25f6c479545e5354128152f8ef3336596be08e,Yes
5434,ealcobaca/pymfe,python/pymfe/statistical.py,ca25f6c479545e5354128152f8ef3336596be08e,"\""\""\""Module dedicated to extraction of Statistical Metafeatures. ||  || Todo: ||     * Implement metafeatures. ||     * Improve documentation. || \""\""\""",https://github.com/ealcobaca/pymfe/commit/ca25f6c479545e5354128152f8ef3336596be08e,Yes
5435,VSainteuf/izitorch,izitorch/trainRack.py,0876ea7d96f6d1b626e2217a1a27fef37b9f8aa2,TODO Add a stop at convergence feature,https://github.com/VSainteuf/izitorch/commit/0876ea7d96f6d1b626e2217a1a27fef37b9f8aa2,Yes
5436,KMouratidis/EDA_miner,apps/analyze/models/pipeline_creator.py,7a7ba6c9c997550511ac2e54698f7a206c827b8e,TODO: Maybe skip the FeatureUnion if `len(parents)==1` ?,https://github.com/KMouratidis/EDA_miner/commit/7a7ba6c9c997550511ac2e54698f7a206c827b8e,Yes
5437,KMouratidis/EDA_miner,EDA_miner/modeling/models/pipeline_creator.py,d0505ee0ec2d00f12832966f9b313f83aa4a9a58,TODO: Maybe skip the FeatureUnion if `len(parents)==1` ?,https://github.com/KMouratidis/EDA_miner/commit/d0505ee0ec2d00f12832966f9b313f83aa4a9a58,Yes
5438,permfl/dictlearn,scripts/surfit.py,fa800d2a083afc2413b491ffe308c16f8ea8e0a7,"\""\""\"" || Surfit || ====== ||  ||  || Create a surface image from CT volume ||  || This script consists of three main steps: ||  ||     1. Denoise ||     2. Feature Enhancement ||     3. Surface Generation ||  ||  || 1. Denoise || ********** || TODO ||  || 2. Feature Enahncement || ********************** || TODO ||  ||  || 3. Surface Generation || ********************* || TODO ||  ||  ||  ||  ||  || \""\""\""",https://github.com/permfl/dictlearn/commit/fa800d2a083afc2413b491ffe308c16f8ea8e0a7,No
5439,aws-samples/deep-learning-models,models/resnet/tensorflow/train_imagenet_resnet_hvd.py,8f37d810df7fa035e98084b6fe948c0ca6a99ae7,TODO: Should be using feature columns?,https://github.com/aws-samples/deep-learning-models/commit/8f37d810df7fa035e98084b6fe948c0ca6a99ae7,No
5440,aws-samples/deep-learning-models,models/nlp/electra/run_pretraining.py,bda976272e0db701032048f99a378bb897603dc8,"\""\""\"" || Batch sizes: 32 = 5GB memory; 128 = 17GB ||  || The \""read -1 expected ...\"" errors are harmless and come from Docker. See https:\/\/github.com\/horovod\/horovod\/issues\/503 || Running Docker in privileged mode (docker run --privileged) solves the issue. ||  || Dataset handling: Lots of empty lines; use dataset.filter() to eliminate those. || For now; just grab one sentence. || TODO: Combine two segments into a single example. https:\/\/github.com\/google-research\/electra\/blob\/master\/build_pretraining_dataset.py || TODO: Add zero-padding for shorter sequences ||  || nlp feature request: Select from dataset with arbitrary slices || `nlp` package tutorial: https:\/\/colab.research.google.com\/github\/huggingface\/nlp\/blob\/master\/notebooks\/Overview.ipynb || \""\""\""",https://github.com/aws-samples/deep-learning-models/commit/bda976272e0db701032048f99a378bb897603dc8,Yes
5441,NLPatVCU/medaCy,medacy/tests/learn/test_feature_extractor.py,c69888184b3d0ce3c639414d186b71e1bd7001fd,TODO write tests for feature extractor once class is written,https://github.com/NLPatVCU/medaCy/commit/c69888184b3d0ce3c639414d186b71e1bd7001fd,Yes
5442,NLPatVCU/medaCy,medacy/learn/feature_extractor.py,5691a6f765fa91b2e0d248585d5ef968db49f6e5,TODO this should have options for window_wize; features to exclude; and anything else.,https://github.com/NLPatVCU/medaCy/commit/5691a6f765fa91b2e0d248585d5ef968db49f6e5,Yes
5443,sorgerlab/indra,belpy/biopax/processor.py,88a39b7c70ce4b5a1080c4573220b45de3908ff4,TODO: we can't handle modification features in,https://github.com/sorgerlab/indra/commit/88a39b7c70ce4b5a1080c4573220b45de3908ff4,Yes
5444,sorgerlab/indra,indra/sources/indra_db_rest/client_api.py,194484f4b443cdbb671155e214765f2193de71cc,TODO because we use the API Gateway; this feature is not longer needed.,https://github.com/sorgerlab/indra/commit/194484f4b443cdbb671155e214765f2193de71cc,Yes
5445,sorgerlab/indra,indra/sources/indra_db_rest/util.py,b4992845c37b5a9ecdd53edb7c8c038b9c75d78a,TODO because we use the API Gateway; this feature is not longer needed.,https://github.com/sorgerlab/indra/commit/b4992845c37b5a9ecdd53edb7c8c038b9c75d78a,Yes
5446,sorgerlab/indra,indra/sources/biopax/processor.py,21111fc850096345a06e20a53ec170a0e18884f0,TODO: handle BindingFeatures and FragmentFeatures here,https://github.com/sorgerlab/indra/commit/21111fc850096345a06e20a53ec170a0e18884f0,Yes
5447,Pinafore/qb,extractors/lm.py,507a03a2f01c33ef93828598dfc8fe26e1b99e68,TODO: make it so that question counts are removed in generating features,https://github.com/Pinafore/qb/commit/507a03a2f01c33ef93828598dfc8fe26e1b99e68,Yes
5448,Pinafore/qb,generate_makefile.py,507a03a2f01c33ef93828598dfc8fe26e1b99e68,(TODO): Perhaps create versions with different subsets of the features?,https://github.com/Pinafore/qb/commit/507a03a2f01c33ef93828598dfc8fe26e1b99e68,Yes
5449,Pinafore/qb,util/classifier.py,adbd8e2878523722639ab053522157f473b8b697,TODO: don't use most frequent bigrams; look by class via feature selection,https://github.com/Pinafore/qb/commit/adbd8e2878523722639ab053522157f473b8b697,Yes
5450,Pinafore/qb,extractors/lm.py,f0323d19f5fd0bd3a4f78e394d6587b72bcc1d8e,TODO: make it so that question counts are removed in generating features,https://github.com/Pinafore/qb/commit/f0323d19f5fd0bd3a4f78e394d6587b72bcc1d8e,Yes
5451,castorini/castor,sm_model/external_features.py,a67e2d12c4d79287de28b41e436fe6c4d9a108ef,TODO: add more external features like:,https://github.com/castorini/castor/commit/a67e2d12c4d79287de28b41e436fe6c4d9a108ef,Yes
5452,IntelAI/models,models/image_segmentation/tensorflow/maskrcnn/model.py,370048aa73e610e5f81d082f4651e91e74dceb80,TODO: check if stride of 2 causes alignment issues if the featuremap,https://github.com/IntelAI/models/commit/370048aa73e610e5f81d082f4651e91e74dceb80,Yes
5453,IntelAI/models,models/image_segmentation/tensorflow/maskrcnn/model.py,370048aa73e610e5f81d082f4651e91e74dceb80,TODO: add assert to varify feature map sizes match what's in config,https://github.com/IntelAI/models/commit/370048aa73e610e5f81d082f4651e91e74dceb80,Yes
5454,castorini/hedwig,sm_model/external_features.py,a67e2d12c4d79287de28b41e436fe6c4d9a108ef,TODO: add more external features like:,https://github.com/castorini/hedwig/commit/a67e2d12c4d79287de28b41e436fe6c4d9a108ef,Yes
5455,mozilla/bugbug,run.py,a2533876feb2dc5065959b6917af93465d5eb3cf,TODO: Alternative features; to integreate in bug_features.py,https://github.com/mozilla/bugbug/commit/a2533876feb2dc5065959b6917af93465d5eb3cf,Yes
5456,mozilla/bugbug,run.py,a2533876feb2dc5065959b6917af93465d5eb3cf,TODO: Try simply using all possible fields instead of extracting features manually.,https://github.com/mozilla/bugbug/commit/a2533876feb2dc5065959b6917af93465d5eb3cf,Yes
5457,mozilla/bugbug,bugbug/bug_features.py,30ee98533bbb137d9fa15bf2f492010a90146e73,TODO: Alternative features; to integreate in bug_features.py,https://github.com/mozilla/bugbug/commit/30ee98533bbb137d9fa15bf2f492010a90146e73,Yes
5458,mozilla/bugbug,bugbug/bug_features.py,30ee98533bbb137d9fa15bf2f492010a90146e73,TODO: Try simply using all possible fields instead of extracting features manually.,https://github.com/mozilla/bugbug/commit/30ee98533bbb137d9fa15bf2f492010a90146e73,Yes
5459,mozilla/bugbug,bugbug/model.py,51e3a3e9d4b5ccb78b134d5074b80f60c33c54b4,TODO: Actually implement feature importance visualization for multiclass problems.,https://github.com/mozilla/bugbug/commit/51e3a3e9d4b5ccb78b134d5074b80f60c33c54b4,Yes
5460,mozilla/bugbug,bugbug/model.py,7e5ec2b24a69ec6777c7a0f3c98f3d5b39446d7a,TODO: Actually implement feature importance visualization for multiclass problems.,https://github.com/mozilla/bugbug/commit/7e5ec2b24a69ec6777c7a0f3c98f3d5b39446d7a,Yes
5461,mozilla/bugbug,bugbug/commit_features.py,9995b8c236d055d13be291d997c0b7d2a2197d1d,TODO: Try simply using all possible fields instead of extracting features manually.,https://github.com/mozilla/bugbug/commit/9995b8c236d055d13be291d997c0b7d2a2197d1d,Yes
5462,mozilla/bugbug,bugbug/model.py,d2cecd4f68b4241cef18eff8d8b95d11813348c7,TODO: Actually implement feature importance visualization for multiclass problems.,https://github.com/mozilla/bugbug/commit/d2cecd4f68b4241cef18eff8d8b95d11813348c7,Yes
5463,mozilla/bugbug,bugbug/model.py,f4636ed7fed0dd2032418e94fa17572f616d3127,TODO: Actually implement feature importance visualization for multiclass problems.,https://github.com/mozilla/bugbug/commit/f4636ed7fed0dd2032418e94fa17572f616d3127,Yes
5464,skggm/skggm,inverse_covariance/metrics.py,66b411bd0cff6eabe13e6e4a99ebf17ab8d634f2,precision_nnz = 1. * np.count_nonzero(precision) \/ n_features # TODO: does this scaling make sense?,https://github.com/skggm/skggm/commit/66b411bd0cff6eabe13e6e4a99ebf17ab8d634f2,No
5465,persephone-tools/persephone,persephone/datasets/na.py,65edeea699140d8f66b2a3a67c8fc16200fdce9d,TODO Remove assumption of fbank features,https://github.com/persephone-tools/persephone/commit/65edeea699140d8f66b2a3a67c8fc16200fdce9d,Yes
5466,persephone-tools/persephone,persephone/model.py,091776bf5f9758a344ac624898e0e18acf89abe1,TODO Confirm that the feature files exist. Create them if they don't.,https://github.com/persephone-tools/persephone/commit/091776bf5f9758a344ac624898e0e18acf89abe1,Yes
5467,persephone-tools/persephone,persephone/model.py,13113c41a5e91e5c5d50d9084a135a6d90c7767e,TODO Make this None and infer feature_type from dimension of NN input layer.,https://github.com/persephone-tools/persephone/commit/13113c41a5e91e5c5d50d9084a135a6d90c7767e,Yes
5468,alan-turing-institute/sktime,sktime/highlevel.py,f4741210e116ed687a8418e494140071c3c75f2e,TODO input checks on target and feature args,https://github.com/alan-turing-institute/sktime/commit/f4741210e116ed687a8418e494140071c3c75f2e,Yes
5469,alan-turing-institute/sktime,sktime/highlevel.py,5c49291f012a466cfa288b91c7951d483cc7c375,TODO input checks on target and feature args,https://github.com/alan-turing-institute/sktime/commit/5c49291f012a466cfa288b91c7951d483cc7c375,Yes
5470,alan-turing-institute/sktime,sktime/highlevel.py,98e15a200043e7a9399fff9812e97de7828f08e3,TODO implement compatibility checks between metadata and task: e.g. for tsc; if features are present,https://github.com/alan-turing-institute/sktime/commit/98e15a200043e7a9399fff9812e97de7828f08e3,Yes
5471,alan-turing-institute/sktime,sktime/highlevel.py,d34fe90d7416878f606fce6bc2daab84ad627675,TODO implement compatibility checks between metadata and task: e.g. for tsc; if features are present,https://github.com/alan-turing-institute/sktime/commit/d34fe90d7416878f606fce6bc2daab84ad627675,Yes
5472,alan-turing-institute/sktime,sktime/highlevel.py,d34fe90d7416878f606fce6bc2daab84ad627675,TODO input checks on target and feature args,https://github.com/alan-turing-institute/sktime/commit/d34fe90d7416878f606fce6bc2daab84ad627675,Yes
5473,alan-turing-institute/sktime,sktime/highlevel.py,26a4c1ce5f1c3f1b2f1271ba04d18dcb0054f24c,TODO input checks on target and feature args,https://github.com/alan-turing-institute/sktime/commit/26a4c1ce5f1c3f1b2f1271ba04d18dcb0054f24c,Yes
5474,alan-turing-institute/sktime,sktime/highlevel.py,af2c7ee70476d57c17aa70ef65c56e61482f139a,TODO implement compatibility checks between metadata and task: e.g. for tsc; if features are present,https://github.com/alan-turing-institute/sktime/commit/af2c7ee70476d57c17aa70ef65c56e61482f139a,Yes
5475,alan-turing-institute/sktime,sktime/highlevel.py,3fe27a014d73d6e3ca0a28bc23d1c716f519c6dd,TODO input checks on target and feature args,https://github.com/alan-turing-institute/sktime/commit/3fe27a014d73d6e3ca0a28bc23d1c716f519c6dd,Yes
5476,alan-turing-institute/sktime,sktime/highlevel/tasks.py,7bba5e701ad58bbc3c62d6c703155a8afa2e507b,TODO input checks on target and feature args,https://github.com/alan-turing-institute/sktime/commit/7bba5e701ad58bbc3c62d6c703155a8afa2e507b,Yes
5477,alan-turing-institute/sktime,sktime/series_as_features/tests/test_all_series_as_features_estimators.py,a6f17bd586df6bbc8e6c783f08eda4c30d2353f9,TODO include series-as-features transformers,https://github.com/alan-turing-institute/sktime/commit/a6f17bd586df6bbc8e6c783f08eda4c30d2353f9,Yes
5478,alan-turing-institute/sktime,sktime/utils/_testing/__init__.py,a6f17bd586df6bbc8e6c783f08eda4c30d2353f9,TODO refactor Tabularizer as series-as-features composition meta-estimator;,https://github.com/alan-turing-institute/sktime/commit/a6f17bd586df6bbc8e6c783f08eda4c30d2353f9,Yes
5479,alan-turing-institute/sktime,sktime/series_as_features/tests/test_all_panel_estimators.py,848f30ff4d777a90a143263d37edd402e031e308,TODO include series-as-features transformers,https://github.com/alan-turing-institute/sktime/commit/848f30ff4d777a90a143263d37edd402e031e308,Yes
5480,spotify/spotify-tensorflow,scripts/read_tfrecord.py,aff92285336bc222c13457b6fee7b0cd58817379,TODO support sparse features,https://github.com/spotify/spotify-tensorflow/commit/aff92285336bc222c13457b6fee7b0cd58817379,Yes
5481,mondejar/ecg-classification,nn_mitdb.py,d693caf833e6bdfa27214861235bb9153b6289ad,1 TODO Preprocess data? norm? if RR interval; last 4 features are pre; post; local and global RR,https://github.com/mondejar/ecg-classification/commit/d693caf833e6bdfa27214861235bb9153b6289ad,Yes
5482,mondejar/ecg-classification,dnn_mitdb.py,d3d1c177251e749eedb449ac21a2681c67dd6778,1 TODO Preprocess data? norm? if RR interval; last 4 features are pre; post; local and global RR,https://github.com/mondejar/ecg-classification/commit/d3d1c177251e749eedb449ac21a2681c67dd6778,Yes
5483,mondejar/ecg-classification,my_dnn_mitdb.py,d3d1c177251e749eedb449ac21a2681c67dd6778,1 TODO Preprocess data? norm? if RR interval; last 4 features are pre; post; local and global RR,https://github.com/mondejar/ecg-classification/commit/d3d1c177251e749eedb449ac21a2681c67dd6778,Yes
5484,mondejar/ecg-classification,python/load_MITBIH.py,1a537902db885fd9782170c8561bcbb03e6a8982,TODO export features;labels .csv?,https://github.com/mondejar/ecg-classification/commit/1a537902db885fd9782170c8561bcbb03e6a8982,No
5485,mondejar/ecg-classification,python/train_SVM.py,938aff733db5165d8d1f88379b88449cf0537cbe,0) TODO if feature_Selection:,https://github.com/mondejar/ecg-classification/commit/938aff733db5165d8d1f88379b88449cf0537cbe,No
5486,loli/medpy,bin/others/analyzeforest.py,7c1897fa6aec3a044c1e90faee93a2ade5e6f8d8,feature usage # !TODO: Dangerous if max_depth is set to None; what happens then?,https://github.com/loli/medpy/commit/7c1897fa6aec3a044c1e90faee93a2ade5e6f8d8,Yes
5487,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/recorders.py,f20c0f0542911575e6eac34dc1106638c21a6a84,TODO: Record the column features in train df,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/f20c0f0542911575e6eac34dc1106638c21a6a84,Yes
5488,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/data/data_chunks/prediction_chunks.py,aaea5bb6c0452f694e9da3206836d0152b490bb9,TODO: Drop `suppress` - Was for `feature_engineer={}`,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/aaea5bb6c0452f694e9da3206836d0152b490bb9,Yes
5489,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/data/data_chunks/prediction_chunks.py,aaea5bb6c0452f694e9da3206836d0152b490bb9,TODO: Add `FeatureEngineer` method called after `inverse_transform` to format as DataFrame,https://github.com/HunterMcGushion/hyperparameter_hunter/commit/aaea5bb6c0452f694e9da3206836d0152b490bb9,Yes
5490,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/data/data_core.py,aaea5bb6c0452f694e9da3206836d0152b490bb9,"\""\""\""This module defines mechanisms for managing an experiment's various datasets; and each datasets's || inputs; targets; and predictions. ||  || **Important Contents** ||  || In order to maintain the states of different datasets across all divisions of an experiment and || amid transformations that may be applied to the data via || :mod:`~hyperparameter_hunter.feature_engineering`; two main classes are defined herein: ||  || 1. :class:`BaseDataChunk`: ||  ||     * Logical separations between \""columns\"" of data for a given :class:`BaseDataset` ||     * Held and maintained by :class:`BaseDataset` and its descendants ||     * Three primary descendants of :class:`BaseDataChunk`: ||  ||         1. :class:`InputChunk`: Maintains a dataset's input data (and transformations) ||         2. :class:`TargetChunk`: Maintains a dataset's target data (and transformations) ||         3. :class:`PredictionChunk`: Maintains a dataset's predictions (and transformations) ||  ||     * Descendants of :class:`BaseDataChunk` should implement the eight \""on_<division>_<point>\"" ||       callback methods defined by :class:`~hyperparameter_hunter.callbacks.bases.BaseCallback` ||  ||         * Because :class:`BaseDataChunk` subclasses are isolated from the experiment; these methods ||           need not invoke their `super` methods; although they are allowed to if necessary ||  ||     * :class:`NullDataChunk` does nothing but mimic the normal :class:`BaseDataChunk` child structure ||  ||         * Used for :class:`BaseDataset` subclasses lacking a particular data chunk; such as: ||  ||             1) `TestDataset`'s `TargetChunk`; because the targets for a test dataset are unknown; or ||             2) `TrainDataset`'s `PredictionChunk`; because predictions are not made on training data ||  || 2. :class:`BaseDataset`: ||  ||     # TODO: ... ||  || **Dataset Attribute Syntax** ||  || The intricate subclass network bolstering the module's predominant :class:`BaseDataset` subclasses || may be intimidating at first; but don't worry; there's a shortcut. Follow these steps to ensure || proper syntax and a valid result when accessing data from a || :class:`~hyperparameter_hunter.experiments.CVExperiment`: ||  || 1. {`data_train`; `data_oof`; `data_holdout`; `data_test`} - Dataset attribute || 2. {`input`; `target`; `prediction`} - Data chunk || 3. [`T`] - Optional transformation || 4. {`d`; `run`; `fold`; `rep`; `final`} - Division; initial (`d`) or `final` data ||  || By stacking three values (four if following optional step \""3\"") from the above formula; you can || access all of the interesting stuff stored in the datasets from the comfort of your experiment or || :func:`~hyperparameter_hunter.callbacks.bases.lambda_callback`. ||  || Related || ------- || :mod:`hyperparameter_hunter.callbacks.bases` ||     This module defines the core callback method structure mirrored by :class:`BaseDataCore`. ||     Despite the strong logical connection to this module; it is important to remember that the only ||     actual connection between the two modules is in :mod:`hyperparameter_hunter.callbacks.wranglers` || :mod:`hyperparameter_hunter.callbacks.wranglers` ||     # TODO: ... Handlers for the `Dataset`s to invoke callback methods with required parameters ||     This module defines the callback classes that act as handlers for the descendants of ||     :class:`BaseDataset` || :mod:`hyperparameter_hunter.experiments` ||     # TODO: ... || \""\""\""",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/aaea5bb6c0452f694e9da3206836d0152b490bb9,No
5491,HunterMcGushion/hyperparameter_hunter,hyperparameter_hunter/experiment_core.py,92326407b79fee1d96e1d4b19822291ee5a12f48,"TODO: Should target wranglers addition be contingent on `kwargs[\""feature_engineer\""].steps",https://github.com/HunterMcGushion/hyperparameter_hunter/commit/92326407b79fee1d96e1d4b19822291ee5a12f48,Yes
5492,rafellerc/Pytorch-SiamFC,training/summary_utils.py,34b7652607804ea2854a6beb147769c7cec562fd,TODO Add numbers in the final image to the feature channels.,https://github.com/rafellerc/Pytorch-SiamFC/commit/34b7652607804ea2854a6beb147769c7cec562fd,Yes
5493,lovit/soynlp,soynlp/lemmatizer/_predicate.py,1b4dafb20ba1d69f443d1e33cf8e2c981560e3c5,TODO: min_num_of_features -> init argument,https://github.com/lovit/soynlp/commit/1b4dafb20ba1d69f443d1e33cf8e2c981560e3c5,Yes
5494,yaserkl/RLSeq2Seq,model.py,7d8ac8c8fe2b5733ba27e06044afadfaa8bdeb7a,TODO: if wanna use time as a categorical feature,https://github.com/yaserkl/RLSeq2Seq/commit/7d8ac8c8fe2b5733ba27e06044afadfaa8bdeb7a,Yes
5495,yaserkl/RLSeq2Seq,run_summarization.py,7d8ac8c8fe2b5733ba27e06044afadfaa8bdeb7a,hps_dict.update({'dqn_input_feature_len':(FLAGS.dec_hidden_dim+FLAGS.max_dec_steps)}) # TODO: more test on this; if wanna use time as a categorical feature,https://github.com/yaserkl/RLSeq2Seq/commit/7d8ac8c8fe2b5733ba27e06044afadfaa8bdeb7a,Yes
5496,xvjiarui/GCNet,mmdet/models/detectors/mask_scoring_rcnn.py,466926eb499f4b5c93ce03bd7670ce766bb28e18,TODO: a more flexible way to decide which feature maps to use,https://github.com/xvjiarui/GCNet/commit/466926eb499f4b5c93ce03bd7670ce766bb28e18,Yes
5497,xvjiarui/GCNet,mmdet/models/detectors/grid_rcnn.py,b5418c2f74f447db2f86f365aa71793b29bdeeb7,TODO: a more flexible way to decide which feature maps to use,https://github.com/xvjiarui/GCNet/commit/b5418c2f74f447db2f86f365aa71793b29bdeeb7,Yes
5498,shenweichen/DeepCTR,tf_model/deepfm.py,15096ea01f1f1e584b574a6dd36058745c595cb6,TODO:  self.init_std\/ math.sqrt(float(dim))  #self.params['feature_dim'],https://github.com/shenweichen/DeepCTR/commit/15096ea01f1f1e584b574a6dd36058745c595cb6,No
5499,jisungk/deepjazz,preprocess_oldd.py,0ab55a81889c546286bcde4a207b2c38abe8cd97,TODO: maybe replace numDownSlope instead with a different feature:,https://github.com/jisungk/deepjazz/commit/0ab55a81889c546286bcde4a207b2c38abe8cd97,Yes
5500,SketchyScene/SketchyScene,Instance_Segmentation/libs/model.py,5728ae33732e425cfde01d358748964ffeb21061,TODO: check if stride of 2 causes alignment issues if the featuremap,https://github.com/SketchyScene/SketchyScene/commit/5728ae33732e425cfde01d358748964ffeb21061,Yes
5501,SketchyScene/SketchyScene,Instance_Segmentation/libs/model.py,5728ae33732e425cfde01d358748964ffeb21061,TODO: add assert to varify feature map sizes match what's in config,https://github.com/SketchyScene/SketchyScene/commit/5728ae33732e425cfde01d358748964ffeb21061,Yes
5502,mdangschat/ctc-asr,python/s_input.py,f4fb1ee074cce7f8df3ceb3428d10d14479b141a,mfcc_delta = librosa.feature.delta(mfcc; width=5; order=1)    TODO,https://github.com/mdangschat/ctc-asr/commit/f4fb1ee074cce7f8df3ceb3428d10d14479b141a,Yes
5503,mdangschat/ctc-asr,python/train.py,73ed7bb582d73cc32aa6c77173e82dc04fc3dbae,TODO Feature transform function.,https://github.com/mdangschat/ctc-asr/commit/73ed7bb582d73cc32aa6c77173e82dc04fc3dbae,Yes
5504,dsindex/etagger,input.py,cd4e9c22b4a89b1a76ecfe49c9303a20780e87a1,TODO gazetteer features,https://github.com/dsindex/etagger/commit/cd4e9c22b4a89b1a76ecfe49c9303a20780e87a1,Yes
5505,tasoc/starclass,run_starclass.py,bd0c70db1aa1bc7233ac00933669ce829d9a3edd,Path to TODO file and feature cache:,https://github.com/tasoc/starclass/commit/bd0c70db1aa1bc7233ac00933669ce829d9a3edd,No
5506,tasoc/starclass,starclass/MetaClassifier/Meta.py,4aa265bc2c782f11219748066d0b76c4cbaad311,Check for features TODO: THIS NEEDS TO BE CHANGED!,https://github.com/tasoc/starclass/commit/4aa265bc2c782f11219748066d0b76c4cbaad311,Yes
5507,astrazeneca-cgr-publications/mantis-ml-release,mantis_ml/modules/post_processing/process_classifier_results.py,390413713874913659d0796a8d36251bf916f9b3,TODO: Avg feature importance foo all classifiers,https://github.com/astrazeneca-cgr-publications/mantis-ml-release/commit/390413713874913659d0796a8d36251bf916f9b3,Yes
5508,astrazeneca-cgr-publications/mantis-ml-release,mantis_ml/modules/pre_processing/eda_wrapper.py,390413713874913659d0796a8d36251bf916f9b3,[BETA] - TODO: select features 'manually',https://github.com/astrazeneca-cgr-publications/mantis-ml-release/commit/390413713874913659d0796a8d36251bf916f9b3,Yes
5509,astrazeneca-cgr-publications/mantis-ml-release,mantis_ml/modules/pre_processing/feature_table_compiler.py,390413713874913659d0796a8d36251bf916f9b3,TODO: Look in to missing data ratios of disease-specific features,https://github.com/astrazeneca-cgr-publications/mantis-ml-release/commit/390413713874913659d0796a8d36251bf916f9b3,Yes
5510,BS-TrainingCenter/Gesture-Recognition-Mask-RCNN,mrcnn/model.py,d39ce8a79fbfb4f0b54b378a233ddfd2e3d00cfd,TODO: check if stride of 2 causes alignment issues if the featuremap,https://github.com/BS-TrainingCenter/Gesture-Recognition-Mask-RCNN/commit/d39ce8a79fbfb4f0b54b378a233ddfd2e3d00cfd,Yes
5511,BS-TrainingCenter/Gesture-Recognition-Mask-RCNN,mrcnn/model.py,d39ce8a79fbfb4f0b54b378a233ddfd2e3d00cfd,TODO: add assert to varify feature map sizes match what's in config,https://github.com/BS-TrainingCenter/Gesture-Recognition-Mask-RCNN/commit/d39ce8a79fbfb4f0b54b378a233ddfd2e3d00cfd,Yes
5512,IXarea/LittleGAN,model.py,5cb04c899036f3106ef51e07d3e72fef811efff6,Todo: try to discriminate the feature map,https://github.com/IXarea/LittleGAN/commit/5cb04c899036f3106ef51e07d3e72fef811efff6,Yes
5513,pathwayforte/pathway-forte,src/pathway_forte/prediction/random_removal.py,b912674a55f3b87f954f0c846ee27e7e39171d7e,TODO: Remove X% of the x_features,https://github.com/pathwayforte/pathway-forte/commit/b912674a55f3b87f954f0c846ee27e7e39171d7e,Yes
5514,pathwayforte/pathway-forte,src/pathway_forte/prediction/random_removal.py,b912674a55f3b87f954f0c846ee27e7e39171d7e,TODO: Return the features that were removed,https://github.com/pathwayforte/pathway-forte/commit/b912674a55f3b87f954f0c846ee27e7e39171d7e,Yes
5515,nearthlab/image-segmentation,maskrcnn/layers/region_proposal_net.py,81154272814b6caa3a49d30d0018569ea40a71d0,TODO: check if stride of 2 causes alignment issues if the feature map,https://github.com/nearthlab/image-segmentation/commit/81154272814b6caa3a49d30d0018569ea40a71d0,Yes
5516,Neuraxio/Neuraxle,neuraxle/union.py,88cf1d133d11ed8fec0acbe0202d3fefa5ed8003,TODO: FeatureUnion?,https://github.com/Neuraxio/Neuraxle/commit/88cf1d133d11ed8fec0acbe0202d3fefa5ed8003,No
5517,ShreyAmbesh/Traffic-Rule-Violation-Detection-System,anchor_generators/multiscale_grid_anchor_generator.py,c92e89118945f181e91391c12c51117e58def69e,TODO check the feature_map_shape_list is consistent with,https://github.com/ShreyAmbesh/Traffic-Rule-Violation-Detection-System/commit/c92e89118945f181e91391c12c51117e58def69e,Yes
5518,iver56/cross-adaptive-audio,sonic_annotator_analyzer.py,e7af87456ff287b094d31bff8de1f215b10cbbfd,TODO: add more features,https://github.com/iver56/cross-adaptive-audio/commit/e7af87456ff287b094d31bff8de1f215b10cbbfd,Yes
5519,iver56/cross-adaptive-audio,sonic_annotator_analyzer.py,e7af87456ff287b094d31bff8de1f215b10cbbfd,TODO: This logic needs to be checked. See if it works with multiple features.,https://github.com/iver56/cross-adaptive-audio/commit/e7af87456ff287b094d31bff8de1f215b10cbbfd,Yes
5520,santi-pdp/pase,spk_id/compose_prosofeats_interface.py,609444a6930ea295281a75d6ca52659c274293e5,TODO: extract prosodic features,https://github.com/santi-pdp/pase/commit/609444a6930ea295281a75d6ca52659c274293e5,Yes
5521,mnicnc404/CartoonGan-tensorflow,tf2/cartoonizer.py,32b3c6863637fba2ba1e72e62d39c26e5dfe7b0d,TODO: test the feature,https://github.com/mnicnc404/CartoonGan-tensorflow/commit/32b3c6863637fba2ba1e72e62d39c26e5dfe7b0d,Yes
5522,regel/loudml,loudml/loudml/timeseries.py,f56e191cf5fbc23c89cf68fa04f318d3f39f082e,FIXME: change according to feature preprocessing. tip: for standardization; use,https://github.com/regel/loudml/commit/f56e191cf5fbc23c89cf68fa04f318d3f39f082e,Yes
5523,mirzaevinom/data_science_bowl_2018,codes/model.py,3e5e9a7b0e9d6a3a6444f7ab4baefc1e9f6073a4,TODO: check if stride of 2 causes alignment issues if the featuremap,https://github.com/mirzaevinom/data_science_bowl_2018/commit/3e5e9a7b0e9d6a3a6444f7ab4baefc1e9f6073a4,Yes
5524,mirzaevinom/data_science_bowl_2018,codes/model.py,3e5e9a7b0e9d6a3a6444f7ab4baefc1e9f6073a4,TODO: add assert to varify feature map sizes match what's in config,https://github.com/mirzaevinom/data_science_bowl_2018/commit/3e5e9a7b0e9d6a3a6444f7ab4baefc1e9f6073a4,Yes
5525,swabhs/open-sesame,src/frameid.py,c88c6f624c436bbffa490f790fb904a3d4bb9488,TODO add more Baidu-style features here,https://github.com/swabhs/open-sesame/commit/c88c6f624c436bbffa490f790fb904a3d4bb9488,No
5526,ELS-RD/anonymisation,test.py,c0d7fa8ac820061f227d1206aceb5ac04f2af89f,feature_np[index; length:] = 0  # TODO replace 0 by dict,https://github.com/ELS-RD/anonymisation/commit/c0d7fa8ac820061f227d1206aceb5ac04f2af89f,Yes
5527,ELS-RD/anonymisation,test.py,c0d7fa8ac820061f227d1206aceb5ac04f2af89f,feature_copy[index; length:] = 0  # TODO replace 0 by dict,https://github.com/ELS-RD/anonymisation/commit/c0d7fa8ac820061f227d1206aceb5ac04f2af89f,Yes
5528,intel/dffml,model/transformers/dffml_model_transformers/ner/utils.py,3152df7bd3731c33602ea1c21efed20f180b9017,TODO clean up all this to leverage built-in features of tokenizers,https://github.com/intel/dffml/commit/3152df7bd3731c33602ea1c21efed20f180b9017,Yes
5529,hackingmaterials/automatminer,matbench/tests/test_featurize.py,3e1c3544891c74660d793b100225a12cce1f083e,TODO: add tests for the following once they return features not matrixes:,https://github.com/hackingmaterials/automatminer/commit/3e1c3544891c74660d793b100225a12cce1f083e,Yes
5530,hackingmaterials/automatminer,matbench/preprocess.py,b0a991362f96e62b0f1966e6d8193fb95a8636ce,TODO: Why are correlated features only pruned if the target is present?,https://github.com/hackingmaterials/automatminer/commit/b0a991362f96e62b0f1966e6d8193fb95a8636ce,Yes
5531,hackingmaterials/automatminer,matbench/preprocess.py,06660a5cb7d99334587ec83897e16f1745c8b61c,Todo: At the moment; scaling and feature reduction converts ints to floats,https://github.com/hackingmaterials/automatminer/commit/06660a5cb7d99334587ec83897e16f1745c8b61c,No
5532,hackingmaterials/automatminer,matbench/featurization/tests/test_featurize.py,e27e8d76438b95bbd4212487cb7ea415f8a840ad,TODO: add tests for the following once they return features not matrixes:,https://github.com/hackingmaterials/automatminer/commit/e27e8d76438b95bbd4212487cb7ea415f8a840ad,Yes
5533,hackingmaterials/automatminer,matbench/pipeline.py,98a0135bdac67dcf0eaa6370a0e3c498b64c56a3,todo: tests should include using custom (user speficied) features as well,https://github.com/hackingmaterials/automatminer/commit/98a0135bdac67dcf0eaa6370a0e3c498b64c56a3,Yes
5534,hackingmaterials/automatminer,mslearn/pipeline.py,99a1fc05dffe986df963ae8d92e02d50993520b5,todo: tests should include using custom (user speficied) features as well,https://github.com/hackingmaterials/automatminer/commit/99a1fc05dffe986df963ae8d92e02d50993520b5,Yes
5535,hackingmaterials/automatminer,mslearn/pipeline.py,5d9d4e54690f7ee5db18ce2fa2bbad6811288873,todo: tests should include using custom (user speficied) features as well,https://github.com/hackingmaterials/automatminer/commit/5d9d4e54690f7ee5db18ce2fa2bbad6811288873,Yes
5536,hackingmaterials/automatminer,benchdev/workflows.py,e521225424c9de99805e77e5a7c0519ef9eaf056,todo: change the feature_na_method!,https://github.com/hackingmaterials/automatminer/commit/e521225424c9de99805e77e5a7c0519ef9eaf056,Yes
5537,hackingmaterials/automatminer,automatminer_dev/submit.py,f2dfac70b45b1dea33a94644da3a8f5997fbbd11,todo: change the feature_na_method!,https://github.com/hackingmaterials/automatminer/commit/f2dfac70b45b1dea33a94644da3a8f5997fbbd11,Yes
5538,nyumaya/nyumaya_audio_recognition,python/multi_detector.py,b068737df335ede3af92b4fb15dc80966d8b347f,TODO: Optimize: When running multiple detectors feature extraction has to be done only once,https://github.com/nyumaya/nyumaya_audio_recognition/commit/b068737df335ede3af92b4fb15dc80966d8b347f,Yes
5539,hachmannlab/chemml,cheml/chem/MagpieFeatures.py,753a6e2b16ba8e4616ca96225fca2035288ab61b,TODO: Implement feature extraction parts of Magpie in python.,https://github.com/hachmannlab/chemml/commit/753a6e2b16ba8e4616ca96225fca2035288ab61b,Yes
5540,NRCan/geo-deep-learning,images_to_samples.py,d728da5afb04d213a3da93e334ca9093d54bd5a1,from rasterio.features import is_valid_geom #FIXME: wait for https:\/\/github.com\/mapbox\/rasterio\/issues\/1815 to be solved,https://github.com/NRCan/geo-deep-learning/commit/d728da5afb04d213a3da93e334ca9093d54bd5a1,Yes
5541,NRCan/geo-deep-learning,utils/verifications.py,84abd76bc84054664290c2fe8debe0010d40e547,TODO: test this with invalid features.,https://github.com/NRCan/geo-deep-learning/commit/84abd76bc84054664290c2fe8debe0010d40e547,Yes
5542,cesium-ml/cesium_web,flask_server.py,96ce02f11939d798cc36c24c7985b6bc3eec993a,TODO: add following info: associated featuresets; models,https://github.com/cesium-ml/cesium_web/commit/96ce02f11939d798cc36c24c7985b6bc3eec993a,Yes
5543,cesium-ml/cesium_web,flask_server.py,21b53816b4a75d55f61b35d5b781a35b2e0f9a5b,TODO: add following info: associated featuresets; models,https://github.com/cesium-ml/cesium_web/commit/21b53816b4a75d55f61b35d5b781a35b2e0f9a5b,Yes
5544,cesium-ml/cesium_web,cesium_app/flask_server.py,8a7d96aa9705f41e31e2b96d9338770c4a705dc4,TODO: Custom features script handling,https://github.com/cesium-ml/cesium_web/commit/8a7d96aa9705f41e31e2b96d9338770c4a705dc4,Yes
5545,cesium-ml/cesium_web,cesium_app/flask_server.py,fd1765d01ceae42d56b8b151f3003ee88d8d7062,TODO: Extract list of custom features from code,https://github.com/cesium-ml/cesium_web/commit/fd1765d01ceae42d56b8b151f3003ee88d8d7062,Yes
5546,cesium-ml/cesium_web,cesium_app/flask_server.py,da16b17f57cfc0202c7fb4b26db7c28d83e0cc5d,TODO: Extract list of custom features from code,https://github.com/cesium-ml/cesium_web/commit/da16b17f57cfc0202c7fb4b26db7c28d83e0cc5d,Yes
5547,jhu-lcsr/costar_plan,costar_task_plan/python/costar_task_plan/agent/keras_ddpg.py,00a050906c7927031a0da921fa19d466a0d8be05,TODO: terminology? feature or observation?,https://github.com/jhu-lcsr/costar_plan/commit/00a050906c7927031a0da921fa19d466a0d8be05,Yes
5548,jhu-lcsr/costar_plan,costar_task_plan/python/costar_task_plan/agent/keras_ddpg.py,88b598507fbe8859ec1ffed38238616c2d55a42d,TODO: terminology? feature or observation?,https://github.com/jhu-lcsr/costar_plan/commit/88b598507fbe8859ec1ffed38238616c2d55a42d,Yes
5549,google-research/google-research,constrained_language_typology/sigtyp_reader_main.py,07c42b122d364de5f29b18b195e0d5bc779d9af2,"r\""\""\""Reader for the format provided by SIGTYP 2020 Shared Task. ||  || More information on the format is available here: ||   https:\/\/sigtyp.github.io\/st2020.html ||  || Example: || -------- ||  Clone the GitHub data to ST2020_DIR. Then run: ||  ||  > ST2020_DIR=... ||  > python3 sigtyp_reader_main.py --sigtyp_dir ${ST2020_DIR}\/data \\ ||     --output_dir ${OUTPUT_DIR} ||  ||  The above will create \""train.csv\""; \""dev.csv\"" and \""test_blinded.csv\"" files ||  converted from the format provided by SIGTYP. Our models should be able to ||  injest these csv files. Along each of the above files; an accompanying ||  \""data_train_*.json.gz\"" file is generated that contains metainformation on ||  various features and their values. ||  || TODO: || ----- || Following needs to be done: ||   - Latitude and longitude need to be on a point on a unit sphere? Keep as is ||     and add three further columns for (x;y;z)? ||   - Country codes are *several*. ||   - Other types of SOMs. ||   - Use BaseMap for visualizations? || \""\""\""",https://github.com/google-research/google-research/commit/07c42b122d364de5f29b18b195e0d5bc779d9af2,No
5550,OpenNMT/OpenNMT-py,OpenNMT/translate.py,d88d3777c2b5c8c6471720252bca4c1987e6366e,FIXME: features? need to use extract from preprocess.py,https://github.com/OpenNMT/OpenNMT-py/commit/d88d3777c2b5c8c6471720252bca4c1987e6366e,Yes
5551,OpenNMT/OpenNMT-py,translate.py,c751a1dd8a724388437916e9d29653c45fc4f14f,FIXME: features? need to use extract from preprocess.py,https://github.com/OpenNMT/OpenNMT-py/commit/c751a1dd8a724388437916e9d29653c45fc4f14f,Yes
5552,OpenNMT/OpenNMT-py,onmt/Models.py,f2c2a5eecc9aff12a81ec95a30dc68bedac9cf0f,TODO: prepare for a future where tgt features are possible,https://github.com/OpenNMT/OpenNMT-py/commit/f2c2a5eecc9aff12a81ec95a30dc68bedac9cf0f,Yes
5553,OpenNMT/OpenNMT-py,onmt/Models.py,818ffcb4a4cc10465cb5c7a7354ae7c955f077d3,TODO: prepare for a future where tgt features are possible,https://github.com/OpenNMT/OpenNMT-py/commit/818ffcb4a4cc10465cb5c7a7354ae7c955f077d3,Yes
5554,OpenNMT/OpenNMT-py,onmt/ModelConstructor.py,1a8ff8269a6fdaa26f2c09a1e496d1ec8b71cdc7,TODO: prepare for a future where tgt features are possible,https://github.com/OpenNMT/OpenNMT-py/commit/1a8ff8269a6fdaa26f2c09a1e496d1ec8b71cdc7,Yes
5555,OpenNMT/OpenNMT-py,onmt/ModelConstructor.py,42ac0826455bacd670c431cd0c02ff5a4b144a6a,TODO: prepare for a future where tgt features are possible.,https://github.com/OpenNMT/OpenNMT-py/commit/42ac0826455bacd670c431cd0c02ff5a4b144a6a,Yes
5556,OpenNMT/OpenNMT-py,onmt/translate/translator.py,79ac152ded91c9c24cbd9af0b1b80fec6c8a44fb,TODO: support these blacklisted features.,https://github.com/OpenNMT/OpenNMT-py/commit/79ac152ded91c9c24cbd9af0b1b80fec6c8a44fb,Yes
5557,OpenNMT/OpenNMT-py,onmt/translate/translator.py,9ed1b04fa33c1a46c3b586c5bddd1232d4f8b123,TODO: support these blacklisted features.,https://github.com/OpenNMT/OpenNMT-py/commit/9ed1b04fa33c1a46c3b586c5bddd1232d4f8b123,Yes
5558,OpenNMT/OpenNMT-py,onmt/translate/translator.py,7685a96e027fbec01ec11847e1a0e8e4a6040ee6,TODO: support these blacklisted features,https://github.com/OpenNMT/OpenNMT-py/commit/7685a96e027fbec01ec11847e1a0e8e4a6040ee6,Yes
5559,OpenNMT/OpenNMT-py,onmt/translate/translator.py,7cbfbb38190baa0a7b98260435c47fa6beb13461,TODO: support these blacklisted features,https://github.com/OpenNMT/OpenNMT-py/commit/7cbfbb38190baa0a7b98260435c47fa6beb13461,Yes
5560,apacha/Mensural-Detector,object_detection/anchor_generators/multiscale_grid_anchor_generator.py,b11b29132897160a24c47f801dee8bf7b47fa46d,TODO check the feature_map_shape_list is consistent with,https://github.com/apacha/Mensural-Detector/commit/b11b29132897160a24c47f801dee8bf7b47fa46d,Yes
5561,apayne19/DoubleAuctionMarket,spot_environment_gui.py,419a2b15d6f61375dccd0aa012045983679b455e,TODO maybe add scrollbar feature for one supply\/demand list,https://github.com/apayne19/DoubleAuctionMarket/commit/419a2b15d6f61375dccd0aa012045983679b455e,Yes
5562,apayne19/DoubleAuctionMarket,GUI/spot_environment_gui.py,ec345d4df613293e06e433e2debaa82addb6c99e,TODO maybe add scrollbar feature for one supply\/demand list,https://github.com/apayne19/DoubleAuctionMarket/commit/ec345d4df613293e06e433e2debaa82addb6c99e,Yes
5563,cenkcorapci/fashion-parser,mrcnn/model.py,da8f367c33f8303311e389d3f7c66abb0be93ce4,TODO: check if stride of 2 causes alignment issues if the feature map,https://github.com/cenkcorapci/fashion-parser/commit/da8f367c33f8303311e389d3f7c66abb0be93ce4,Yes
5564,cenkcorapci/fashion-parser,mrcnn/model.py,da8f367c33f8303311e389d3f7c66abb0be93ce4,TODO: add assert to varify feature map sizes match what's in config,https://github.com/cenkcorapci/fashion-parser/commit/da8f367c33f8303311e389d3f7c66abb0be93ce4,Yes
5565,mlflow/mlflow,mlflow/sklearn/utils.py,77ebf43f8ff523ceb5b511a6e182c103be2d989f,TODO: Remove when FeatureHasher is implemented in PYPY,https://github.com/mlflow/mlflow/commit/77ebf43f8ff523ceb5b511a6e182c103be2d989f,Yes
5566,ludwig-ai/ludwig,ludwig/features/text_feature.py,a486158770e90f0df56adfad365a5a03f2408e46,todo: refactor to reuse SeuuqnceOutputFeature.postprocess_results,https://github.com/ludwig-ai/ludwig/commit/a486158770e90f0df56adfad365a5a03f2408e46,Yes
5567,ludwig-ai/ludwig,ludwig/models/model.py,360f6e8aee7989b7e649c21883026612964b9cf7,todo tf2: hardcoding for a single output feature - need to generalize,https://github.com/ludwig-ai/ludwig/commit/360f6e8aee7989b7e649c21883026612964b9cf7,Yes
5568,ludwig-ai/ludwig,ludwig/models/model.py,ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,todo tf2: hardcoding for a single output feature - need to generalize,https://github.com/ludwig-ai/ludwig/commit/ea5390a01f7ba2fdf60ab17126b7f74fdf9311d3,Yes
5569,ludwig-ai/ludwig,ludwig/models/ecd.py,d3f677b7357e4e08614f91981e4f8f1ed40fc291,output_last_hidden[output_feature_name] = decoder_last_hidden  #todo tf2 do we need this long-term give the above,https://github.com/ludwig-ai/ludwig/commit/d3f677b7357e4e08614f91981e4f8f1ed40fc291,Yes
5570,ludwig-ai/ludwig,tests/integration_tests/test_neuropod.py,80147372110ec63ff8fb71c0d9b59f0964d9a21a,set_feature(vocab_size=3);  # TODO fix,https://github.com/ludwig-ai/ludwig/commit/80147372110ec63ff8fb71c0d9b59f0964d9a21a,Yes
5571,ludwig-ai/ludwig,tests/integration_tests/test_neuropod.py,80147372110ec63ff8fb71c0d9b59f0964d9a21a,bag_feature(vocab_size=3);  # TODO fix,https://github.com/ludwig-ai/ludwig/commit/80147372110ec63ff8fb71c0d9b59f0964d9a21a,Yes
5572,ludwig-ai/ludwig,ludwig/features/audio_feature.py,7208bad7329902ba5293de65c43ebd2a9520f8b9,assert len(inputs.shape) == 2  # TODO: check correct inputs.shape as per sequence feature,https://github.com/ludwig-ai/ludwig/commit/7208bad7329902ba5293de65c43ebd2a9520f8b9,Yes
5573,ludwig-ai/ludwig,ludwig/experiment.py,2e7ed016e06095a947303d6a33aad9a3a047c1ed,todo revisit to see if can use output feature.postprocess_results(),https://github.com/ludwig-ai/ludwig/commit/2e7ed016e06095a947303d6a33aad9a3a047c1ed,Yes
5574,ludwig-ai/ludwig,ludwig/features/audio_feature.py,21ce706115fbfb108c124679f4515b248971a3e4,todo tf2: encoder_obj should be passed to the sequenceinputfeature,https://github.com/ludwig-ai/ludwig/commit/21ce706115fbfb108c124679f4515b248971a3e4,Yes
5575,ludwig-ai/ludwig,ludwig/features/text_feature.py,21ce706115fbfb108c124679f4515b248971a3e4,todo tf2: encoder_obj should be passed to the sequenceinputfeature,https://github.com/ludwig-ai/ludwig/commit/21ce706115fbfb108c124679f4515b248971a3e4,Yes
5576,ludwig-ai/ludwig,ludwig/features/timeseries_feature.py,21ce706115fbfb108c124679f4515b248971a3e4,todo tf2: encoder_obj should be passed to the sequenceinputfeature,https://github.com/ludwig-ai/ludwig/commit/21ce706115fbfb108c124679f4515b248971a3e4,Yes
5577,ludwig-ai/ludwig,ludwig/features/binary_feature.py,bb7bd0505d9b2dd5ad428b1929ed77c24ccf4917,todo maybe move code from add_feature_data here,https://github.com/ludwig-ai/ludwig/commit/bb7bd0505d9b2dd5ad428b1929ed77c24ccf4917,Yes
5578,astier/model-free-episodic-control,main.py,5054ff48c6a71a2948bd211a2cf08b7a1699ad29,TODO remove feature?,https://github.com/astier/model-free-episodic-control/commit/5054ff48c6a71a2948bd211a2cf08b7a1699ad29,Yes
5579,Cartus/AMR-Parser,data/align/scripts/feat2tree_v9.py,13085d32760add0a9e3fdd2c3db42f9bfa88914b,TODO: provenence feature based on Rule.STATETRANS? but seems that it is the only option for the AMR!,https://github.com/Cartus/AMR-Parser/commit/13085d32760add0a9e3fdd2c3db42f9bfa88914b,Yes
5580,Ekim-Yurtsever/DeepTL-Lane-Change-Classification,Mask_RCNN/mask_rcnn/model.py,91d4e598ca4bfba984ee5058f040aeac2f6071cb,TODO: check if stride of 2 causes alignment issues if the featuremap,https://github.com/Ekim-Yurtsever/DeepTL-Lane-Change-Classification/commit/91d4e598ca4bfba984ee5058f040aeac2f6071cb,Yes
5581,Ekim-Yurtsever/DeepTL-Lane-Change-Classification,Mask_RCNN/mask_rcnn/model.py,91d4e598ca4bfba984ee5058f040aeac2f6071cb,TODO: add assert to varify feature map sizes match what's in config,https://github.com/Ekim-Yurtsever/DeepTL-Lane-Change-Classification/commit/91d4e598ca4bfba984ee5058f040aeac2f6071cb,Yes
5582,akihiro-inui/MusicGenreClassifiaction,backend/src/feature_extraction/audio_feature_extraction.py,7b8ffea2639de461c8ed7a21186297bbe4640cb8,:TODO Add 6 low-level feature extraction,https://github.com/akihiro-inui/MusicGenreClassifiaction/commit/7b8ffea2639de461c8ed7a21186297bbe4640cb8,Yes
5583,akihiro-inui/MusicGenreClassifiaction,backend/src/data_process/data_process.py,954c4c6534a33a9f63797de204978d12562770b8,TODO: save feature as np,https://github.com/akihiro-inui/MusicGenreClassifiaction/commit/954c4c6534a33a9f63797de204978d12562770b8,Yes
5584,akihiro-inui/MusicGenreClassifiaction,frontend/app.py,dd5e5427730210dbfdbb4a4f0838cab67c1a61db,TODO: Subtract feature stats; run prediction,https://github.com/akihiro-inui/MusicGenreClassifiaction/commit/dd5e5427730210dbfdbb4a4f0838cab67c1a61db,Yes
5585,HealthCatalyst/healthcareai-py,hcpytools/develop_supervised_model.py,39002d735f4e8d32729496eefff353014e5cc68c,TODO: Get RandomizedLogistic (auto feature selection) working in place of linear,https://github.com/HealthCatalyst/healthcareai-py/commit/39002d735f4e8d32729496eefff353014e5cc68c,Yes
5586,HealthCatalyst/healthcareai-py,healthcareai/tests/test_top_factors.py,2c4039f072556f1a631057d5cf483b3e26d457d8,TODO Modify mu and sigma once feature scaling is built into the logistic regression,https://github.com/HealthCatalyst/healthcareai-py/commit/2c4039f072556f1a631057d5cf483b3e26d457d8,Yes
5587,HealthCatalyst/healthcareai-py,healthcareai/tests/test_top_factors.py,1bd06851879a5bbb58f674df12be25ca69f81aa6,TODO Modify mu and sigma once feature scaling is built into the logistic regression,https://github.com/HealthCatalyst/healthcareai-py/commit/1bd06851879a5bbb58f674df12be25ca69f81aa6,Yes
5588,HealthCatalyst/healthcareai-py,healthcareai/tests/test_top_factors.py,69c83b70124664f8a22337fc03bd442ee82b6652,TODO Modify mu and sigma once feature scaling is built into the logistic regression,https://github.com/HealthCatalyst/healthcareai-py/commit/69c83b70124664f8a22337fc03bd442ee82b6652,Yes
5589,X-DataInitiative/tick,tick/simulation/base/simu_with_features.py,86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,TODO: features simulation isn't launch each time we call simulate,https://github.com/X-DataInitiative/tick/commit/86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,Yes
5590,X-DataInitiative/tick,tick/simulation/base/simu_with_features.py,86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,TODO: check and correct also n_samples; n_features and cov_corr and features_scaling,https://github.com/X-DataInitiative/tick/commit/86ee34f98b1ac6f44e9e2a60e6216b9b0502a52c,Yes
5591,X-DataInitiative/tick,tick/base/simulation/simu_with_features.py,aad38ed09d20ba8c0db98d36338e5ddd9a087b89,TODO: features simulation isn't launch each time we call simulate,https://github.com/X-DataInitiative/tick/commit/aad38ed09d20ba8c0db98d36338e5ddd9a087b89,No
5592,X-DataInitiative/tick,tick/base/simulation/simu_with_features.py,aad38ed09d20ba8c0db98d36338e5ddd9a087b89,TODO: check and correct also n_samples; n_features and cov_corr and features_scaling,https://github.com/X-DataInitiative/tick/commit/aad38ed09d20ba8c0db98d36338e5ddd9a087b89,Yes
5593,mme/vergeml,vergeml/sources/features.py,b5cffa8a8e532d64bc38e58baa51ec38bf30285e,TODO rename image-features,https://github.com/mme/vergeml/commit/b5cffa8a8e532d64bc38e58baa51ec38bf30285e,Yes
5594,automagica/automagica,automagica/gui/frames.py,ea759131ef4a146d2f1b736299a4eac408b28f2d,TODO: add scrolling feature (currently disabled),https://github.com/automagica/automagica/commit/ea759131ef4a146d2f1b736299a4eac408b28f2d,Yes
5595,mapbox/robosat,robosat/features/parking.py,52299b855b77225b795b015944e31cbddb9ce44b,Todo: generalize and move to features.core,https://github.com/mapbox/robosat/commit/52299b855b77225b795b015944e31cbddb9ce44b,No
5596,deepchem/deepchem,deep_chem/utils/load.py,b86ddab0a071206ac45b922f655a309dd0bebb6f,"'' || def load_pdbbind_molecules(paths; dir_name=\""fingerprints\""): ||   \""\""\""Load dataset fingerprints and return fingerprints. ||   \""\""\"" ||   # TODO(rbharath): This is a total kludge. Clean up later. ||   dir_name = \""targets\"" ||   molecules = {} ||   for dataset_path in paths: ||     pickle_dir = os.path.join(dataset_path; dir_name) ||     pickle_files = os.listdir(pickle_dir) ||     if len(pickle_files) == 0: ||       raise ValueError(\""No Pickle Files found to load molecules\"") ||     for pickle_file in pickle_files: ||       with gzip.open(os.path.join(pickle_dir; pickle_file); \""rb\"") as f: ||         contents = pickle.load(f) ||         smiles; fingerprints; scaffolds; mol_ids = ( ||             contents[\""smiles\""]; contents[\""features\""]; ||             None; None) ||         for mol in range(len(contents[\""smiles\""])): ||           molecules[smiles[mol]] = {\""fingerprint\"": fingerprints[mol]; ||                                     \""scaffold\"": None; ||                                     \""mol_id\"": None} ||   return molecules  || '''",https://github.com/deepchem/deepchem/commit/b86ddab0a071206ac45b922f655a309dd0bebb6f,No
5597,deepchem/deepchem,deepchem/transformers/tests/test_transformers.py,44beedf33e5d37edde20301238131b8fba15636a,TODO(rbharath): Untransform doesn't work properly for binary feature,https://github.com/deepchem/deepchem/commit/44beedf33e5d37edde20301238131b8fba15636a,Yes
5598,deepchem/deepchem,deepchem/dock/pose_scoring.py,1ae9bc955995bb4a253be5474da0eaaedc4a8aa5,TODO: add pi_stack and cation_pi to feature_types (it's not trivial,https://github.com/deepchem/deepchem/commit/1ae9bc955995bb4a253be5474da0eaaedc4a8aa5,No
5599,deepchem/deepchem,examples/pdbbind/pdbbind_datasets.py,1ae9bc955995bb4a253be5474da0eaaedc4a8aa5,TODO: add pi_stack and cation_pi to feature_types (it's not trivial,https://github.com/deepchem/deepchem/commit/1ae9bc955995bb4a253be5474da0eaaedc4a8aa5,Yes
5600,DistrictDataLabs/yellowbrick,yellowbrick/features/pcoords.py,d16da6dab80eea9b93c95f2484d659b34ce7ebe4,TODO: Allow the user to specify this feature,https://github.com/DistrictDataLabs/yellowbrick/commit/d16da6dab80eea9b93c95f2484d659b34ce7ebe4,Yes
5601,DistrictDataLabs/yellowbrick,yellowbrick/contrib/missing/base.py,b10fc179a7eda5b6e32c101399ecab50e567b2b6,TODO: Refactor MissingDataVisualizer to make use of new features.,https://github.com/DistrictDataLabs/yellowbrick/commit/b10fc179a7eda5b6e32c101399ecab50e567b2b6,Yes
5602,DistrictDataLabs/yellowbrick,yellowbrick/features/base.py,b10fc179a7eda5b6e32c101399ecab50e567b2b6,TODO: allow the user specified features to filter the dataset,https://github.com/DistrictDataLabs/yellowbrick/commit/b10fc179a7eda5b6e32c101399ecab50e567b2b6,Yes
5603,pyannote/pyannote-audio,pyannote/audio/features/yaafe.py,c78253e0044549b84ae75ff842a997b5805a907f,FIXME use feature.crop(mode='center'; fixed=duration) instead,https://github.com/pyannote/pyannote-audio/commit/c78253e0044549b84ae75ff842a997b5805a907f,Yes
5604,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/triplet_loss.py,e638a4760c0519cac825d9661a3b5f0a928a507c,TODO. learn feature normalization and store it as a layer in the model,https://github.com/pyannote/pyannote-audio/commit/e638a4760c0519cac825d9661a3b5f0a928a507c,Yes
5605,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/agg_triplet_loss.py,ec3d6c674f221d9a209bfecf691641b0cb2f250c,TODO. learn feature normalization and store it as a layer in the model,https://github.com/pyannote/pyannote-audio/commit/ec3d6c674f221d9a209bfecf691641b0cb2f250c,Yes
5606,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/wtf_triplet_loss.py,df76ab77d6f84dd1c9cd04e40e60b093ea0b42e2,TODO. learn feature normalization and store it as a layer in the model,https://github.com/pyannote/pyannote-audio/commit/df76ab77d6f84dd1c9cd04e40e60b093ea0b42e2,Yes
5607,pyannote/pyannote-audio,pyannote/audio/embedding/approaches/base.py,5c1eda942eeaef4c0278dd8aa2e931e6d77ea888,TODO: use new pytorch feature that handle sorting automatically,https://github.com/pyannote/pyannote-audio/commit/5c1eda942eeaef4c0278dd8aa2e931e6d77ea888,Yes
5608,catalyst-team/catalyst,catalyst/dl/experiment/config.py,fa6dffdaf7a65afc85577153a2e151096fbc1634,@TODO: return the feature,https://github.com/catalyst-team/catalyst/commit/fa6dffdaf7a65afc85577153a2e151096fbc1634,No
5609,openml/openml-python,tests/test_runs/test_run_functions.py,9568cf0543267c27383efef2c3e2d4ef6312d6da,TODO: programmatically check wether these are indeed features (predict; correct),https://github.com/openml/openml-python/commit/9568cf0543267c27383efef2c3e2d4ef6312d6da,No
5610,openml/openml-python,tests/test_runs/test_run_functions.py,98aede1337aff856696d0ad5215d69af8cb71603,TODO: programmatically check wether these are indeed features (predict; correct),https://github.com/openml/openml-python/commit/98aede1337aff856696d0ad5215d69af8cb71603,No
5611,scikit-multiflow/scikit-multiflow,src/skmultiflow/evaluation/base_evaluator.py,259a336c8f0d8c3de29c76c2e3c7a34d8a05c361,TODO let the user choose the feature indices of interest,https://github.com/scikit-multiflow/scikit-multiflow/commit/259a336c8f0d8c3de29c76c2e3c7a34d8a05c361,Yes
5612,scikit-multiflow/scikit-multiflow,src/skmultiflow/evaluation/base_evaluator.py,3e9cdf545d08e5c2f9f2b8767ba032b19b5d5afd,TODO let the user choose the feature indices of interest,https://github.com/scikit-multiflow/scikit-multiflow/commit/3e9cdf545d08e5c2f9f2b8767ba032b19b5d5afd,Yes
5613,menpo/menpo,menpo/fitmultilevel/sdm/trainer.py,015064c81e26f72cfe50804927454b66ffb0679d,TODO: this should use check_feature_type,https://github.com/menpo/menpo/commit/015064c81e26f72cfe50804927454b66ffb0679d,No
5614,menpo/menpo,menpo/fit/regression/trainer.py,dba432cca8a2cae1a0ab4aed7624d477b2d3f9ba,TODO: Is this correct or should I return np.hstack((features; 1)) as,https://github.com/menpo/menpo/commit/dba432cca8a2cae1a0ab4aed7624d477b2d3f9ba,Yes
5615,chainer/chainer-chemistry,chainer_chemistry/dataset/splitters/stratified_splitter.py,11243eb4cdccfb66bece3147ba224fd643a5f29d,TODO: feature?,https://github.com/chainer/chainer-chemistry/commit/11243eb4cdccfb66bece3147ba224fd643a5f29d,No
5616,chainer/chainer-chemistry,tests/dataset_tests/preprocessors_tests/test_weavenet_preprocessor.py,4bd35912e53d875330d2792911f6eedee62389d2,TODO (nakago): test feature extraction behavior...,https://github.com/chainer/chainer-chemistry/commit/4bd35912e53d875330d2792911f6eedee62389d2,No
5617,online-ml/river,src/skmultiflow/evaluation/base_evaluator.py,259a336c8f0d8c3de29c76c2e3c7a34d8a05c361,TODO let the user choose the feature indices of interest,https://github.com/online-ml/river/commit/259a336c8f0d8c3de29c76c2e3c7a34d8a05c361,Yes
5618,online-ml/river,src/skmultiflow/evaluation/base_evaluator.py,3e9cdf545d08e5c2f9f2b8767ba032b19b5d5afd,TODO let the user choose the feature indices of interest,https://github.com/online-ml/river/commit/3e9cdf545d08e5c2f9f2b8767ba032b19b5d5afd,Yes
5619,online-ml/river,scikit-multiflow/src/skmultiflow/evaluation/base_evaluator.py,9aadf6260dd867c26f2b5d2b4f09fb9dacf5cfa2,TODO let the user choose the feature indices of interest,https://github.com/online-ml/river/commit/9aadf6260dd867c26f2b5d2b4f09fb9dacf5cfa2,Yes
5620,openeventdata/mordecai,mordecai/geoparse.py,fa5d364e3307365974ca33557b8ddb8981010b85,TODO check if most_common feature really isn't that useful,https://github.com/openeventdata/mordecai/commit/fa5d364e3307365974ca33557b8ddb8981010b85,No
5621,CPJKU/madmom,madmom/features/notes.py,e887d2c6df4b0d82b881e19c56350f77b8b3865b,TODO: this code is very similar to features.onsets.peak_picking();,https://github.com/CPJKU/madmom/commit/e887d2c6df4b0d82b881e19c56350f77b8b3865b,No
5622,CPJKU/madmom,madmom/ml/rnnlib.py,18cd7e3bd8c93029e50628611e325865ee2a3292,TODO: inherit from features.Activations,https://github.com/CPJKU/madmom/commit/18cd7e3bd8c93029e50628611e325865ee2a3292,Yes
5623,CPJKU/madmom,madmom/ml/rnnlib.py,918269db25314a51950a9f7a1d5c87d8361f9137,TODO: inherit from features.Activations,https://github.com/CPJKU/madmom/commit/918269db25314a51950a9f7a1d5c87d8361f9137,Yes
5624,CPJKU/madmom,madmom/features/beats.py,e8f81ebfce6c2405a3fa9c9a0f15aac58e524867,TODO: refactor this to use new feature.tempo functionality,https://github.com/CPJKU/madmom/commit/e8f81ebfce6c2405a3fa9c9a0f15aac58e524867,Yes
5625,CPJKU/madmom,madmom/features/beats.py,9abd4b73fc9635486f3374400470c7744b4b16c1,TODO: split the classes similar to madmom.features.onsets?,https://github.com/CPJKU/madmom/commit/9abd4b73fc9635486f3374400470c7744b4b16c1,Yes
5626,CPJKU/madmom,madmom/features/onsets.py,9abd4b73fc9635486f3374400470c7744b4b16c1,TODO: split the classes similar to madmom.features.beats?,https://github.com/CPJKU/madmom/commit/9abd4b73fc9635486f3374400470c7744b4b16c1,Yes
5627,CPJKU/madmom,madmom/features/beats.py,f3c08d12f064f1c9a47ea9dd9cf848e1299c0aa8,TODO: split the classes similar to madmom.features.beats?,https://github.com/CPJKU/madmom/commit/f3c08d12f064f1c9a47ea9dd9cf848e1299c0aa8,Yes
5628,EducationalTestingService/skll,skll/data.py,2a5215da0798bc53d9bd44d405269f5a28b42a55,TODO: Add some sort of check for duplicate feature names,https://github.com/EducationalTestingService/skll/commit/2a5215da0798bc53d9bd44d405269f5a28b42a55,Yes
5629,oracle/Skater,pyinterpret/core/global_interpretation/partial_dependence.py,a4c121d61ea8d58d4d6d5668a3dc50f98e26dbaf,TODO: evaluate cases when len(unique(feature))==2,https://github.com/oracle/Skater/commit/a4c121d61ea8d58d4d6d5668a3dc50f98e26dbaf,Yes
5630,oracle/Skater,pyinterpret/core/global_interpretation/partial_dependence.py,e1a1d01a493050c146b4e7bb467d47cdaaccad33,TODO: evaluate cases when len(unique(feature))==2,https://github.com/oracle/Skater/commit/e1a1d01a493050c146b4e7bb467d47cdaaccad33,Yes
5631,oracle/Skater,pyinterpret/tests/test_partial_dependence.py,e1a1d01a493050c146b4e7bb467d47cdaaccad33,TODO: check on the feature space approximation (V2),https://github.com/oracle/Skater/commit/e1a1d01a493050c146b4e7bb467d47cdaaccad33,Yes
5632,oracle/Skater,pyinterpret/core/global_interpretation/partial_dependence.py,45949842d6311afd2c28452273393db07e6228d5,TODO: evaluate cases when len(unique(feature))==2,https://github.com/oracle/Skater/commit/45949842d6311afd2c28452273393db07e6228d5,Yes
5633,oracle/Skater,skater/core/local_interpretation/text_interpreter.py,9cabb26008ca466ff5cc9f1651162757917a8e9a,TODO: extend support to other forms of Vectorization schemes - Feature Hashing,https://github.com/oracle/Skater/commit/9cabb26008ca466ff5cc9f1651162757917a8e9a,Yes
5634,awslabs/sockeye,sockeye/decoder.py,36bda4d2ad25daa896e5030eab70738599dd0b71,TODO: weight tying? lexicon and all other features the RNN supports?!,https://github.com/awslabs/sockeye/commit/36bda4d2ad25daa896e5030eab70738599dd0b71,No
5635,awslabs/sockeye,sockeye/image_captioning/extract_features.py,09a90021453e8d8254201d92a830cd8da0c6e2c6,TODO: enable caching to reuse features and resume computation,https://github.com/awslabs/sockeye/commit/09a90021453e8d8254201d92a830cd8da0c6e2c6,Yes
5636,explosion/thinc,thinc/tests/unit/test_ops.py,a21c152ecc8ca5c01d915d85a2503048078ad422,TODO: Not sure how this feature should work still...,https://github.com/explosion/thinc/commit/a21c152ecc8ca5c01d915d85a2503048078ad422,Yes
5637,explosion/thinc,thinc/api.py,a0897d077b3f59ffa1747470fa1a7871c421f1a0,TODO: Is this made obsolete by the FeatureExtractor?,https://github.com/explosion/thinc/commit/a0897d077b3f59ffa1747470fa1a7871c421f1a0,Yes
5638,explosion/thinc,thinc/wire.py,b2d383f29d4f9081215722837bcc03ffc2dce618,TODO: Is this made obsolete by the FeatureExtractor?,https://github.com/explosion/thinc/commit/b2d383f29d4f9081215722837bcc03ffc2dce618,Yes
5639,tensorflow/ranking,tensorflow_ranking/python/feature.py,30864426207a6fcd72b771730af4ccaf526866ea,TODO: Ensure only v2 Feature Columns are used.,https://github.com/tensorflow/ranking/commit/30864426207a6fcd72b771730af4ccaf526866ea,Yes
5640,tensorflow/ranking,tensorflow_ranking/python/keras/canned/dnn_test.py,02b64e17207390d4233297c0856ca3fa6784b3d6,TODO: Deserialized embedding feature column behavior is the,https://github.com/tensorflow/ranking/commit/02b64e17207390d4233297c0856ca3fa6784b3d6,No
5641,tensorflow/ranking,tensorflow_ranking/python/keras/canned/gam_test.py,609400fedf40bfcb588665143dc9e790734d076c,TODO: Deserialized embedding feature column behavior is the,https://github.com/tensorflow/ranking/commit/609400fedf40bfcb588665143dc9e790734d076c,No
5642,neuronets/nobrainer,train.py,90bb6e58f42b75b08669f9953e24fd184b0aa6f4,"\""\""\""Script to train highres3dnet model. ||  || The input CSV must have two columns: ||     1. filepaths of features ||     2. filepaths of corresponding labels ||  || TODO || ---- || - Make this script more general. Ideally; one could drop in their model and ||     loss function. || - Move some common methods (eg; i\/o) to dedicated modules. || - Dice coefficient for class 1 (brainmask) is sometimes NaN. || - Input of 1 * 128**3 is too large for 1080ti. This seems to be related to the ||     `input_fn` used. || - Remove pandas as a dependency. Make pure python reader that accepts CSV or ||     TSV as input. || \""\""\""",https://github.com/neuronets/nobrainer/commit/90bb6e58f42b75b08669f9953e24fd184b0aa6f4,Yes
5643,neuronets/nobrainer,train.py,54200bc13c0b6e63cea50d143a2f83037ad04024,"\""\""\""Example script to train model. ||  || The input CSV must have two columns: ||     1. filepaths of features ||     2. filepaths of corresponding labels ||  || TODO || ---- || - Dice coefficient for class 1 (brainmask) is sometimes NaN. This occurs when ||     Dice should be zero. || - Input of 1 * 128**3 is too large for 1080ti to train HighRes3DNet. It is OK ||     for MeshNet. This issue seems to be related to the `input_fn` used. || \""\""\""",https://github.com/neuronets/nobrainer/commit/54200bc13c0b6e63cea50d143a2f83037ad04024,No
5644,neuronets/nobrainer,nobrainer/dataset.py,b35fdfdff65e88b6d7ccb9b07eab3c76119845ce,TODO: in the future; multi-channel features should be supported.,https://github.com/neuronets/nobrainer/commit/b35fdfdff65e88b6d7ccb9b07eab3c76119845ce,Yes
5645,Rostlab/nalaf,tests/test_features.py,5fde23fe506f88f8b1a5513dc60bd6167eea5325,TODO implement separate test functions for each feature that is already implemented in test_generate,https://github.com/Rostlab/nalaf/commit/5fde23fe506f88f8b1a5513dc60bd6167eea5325,Yes
5646,Rostlab/nalaf,tests/test_features.py,3a38b34cb332007524ebf47fbcdcd8fc65973e43,TODO implement separate test functions for each feature that is already implemented in test_generate,https://github.com/Rostlab/nalaf/commit/3a38b34cb332007524ebf47fbcdcd8fc65973e43,Yes
5647,Rostlab/nalaf,nala/features/__init__.py,5b6580deca1bef80dc9107be0eaad18f836e5e60,TODO decorator that checks features,https://github.com/Rostlab/nalaf/commit/5b6580deca1bef80dc9107be0eaad18f836e5e60,No
5648,Rostlab/nalaf,nalaf/structures/data.py,b1b6df0068ea12938fb2a5fe71b7cf658f91e3a2,TODO move to edge features,https://github.com/Rostlab/nalaf/commit/b1b6df0068ea12938fb2a5fe71b7cf658f91e3a2,Yes
5649,Rostlab/nalaf,nalaf/structures/data.py,38f2d4d6a6c5d0e3018745fc851c20166cd1bb57,TODO move to edge features,https://github.com/Rostlab/nalaf/commit/38f2d4d6a6c5d0e3018745fc851c20166cd1bb57,Yes
5650,Rostlab/nalaf,nalaf/structures/data.py,28151e29fb42609cef1b3eb47ff96f0891634fca,TODO move to edge features,https://github.com/Rostlab/nalaf/commit/28151e29fb42609cef1b3eb47ff96f0891634fca,Yes
5651,Rostlab/nalaf,nalaf/structures/data.py,3cbf2ade8587cabdb6c9d13938ee7307e79cf434,TODO move to edge features,https://github.com/Rostlab/nalaf/commit/3cbf2ade8587cabdb6c9d13938ee7307e79cf434,Yes
5652,Rostlab/nalaf,nalaf/features/relations/new/dependency.py,93bf84f9e7bb969b9db7b902d9f6a5cb8e45261c,TODO investigate features,https://github.com/Rostlab/nalaf/commit/93bf84f9e7bb969b9db7b902d9f6a5cb8e45261c,Yes
5653,Rostlab/nalaf,nalaf/structures/data.py,2f5d0594bdc03032d9b43286b1147f55faf7dd83,TODO this would be better written in the (entities) FeatureGenerator,https://github.com/Rostlab/nalaf/commit/2f5d0594bdc03032d9b43286b1147f55faf7dd83,Yes
5654,Rostlab/nalaf,nalaf/features/__init__.py,88259f896cc944249b7fa6fa0f4acd28e7d89a5f,TODO the following is better written here; instead of the FeatureDictionary as originally written,https://github.com/Rostlab/nalaf/commit/88259f896cc944249b7fa6fa0f4acd28e7d89a5f,Yes
5655,Rostlab/nalaf,nalaf/structures/data.py,b7c6acca9e5ae6c097a324bc32ecedd5928d772b,TODO this would be better written in the (entities) FeatureGenerator,https://github.com/Rostlab/nalaf/commit/b7c6acca9e5ae6c097a324bc32ecedd5928d772b,Yes
5656,graknlabs/kglib,kglib/kgcn/core/ingest/preprocess/preprocess.py,095cc36d72473e26f6e445c786c948f1ebe620e9,TODO Drop support for ignoring features,https://github.com/graknlabs/kglib/commit/095cc36d72473e26f6e445c786c948f1ebe620e9,Yes
5657,materialsvirtuallab/megnet,megnet/data/graph.py,585654be2c2a0881477b213dbafd3714d92a92b5,"TODO (wardlt): Consider making \""num_*_features\"" funcs to simplify making a MEGNet model",https://github.com/materialsvirtuallab/megnet/commit/585654be2c2a0881477b213dbafd3714d92a92b5,Yes
5658,Aifred-Health/Vulcan,vulcanai2/models/cnn.py,c2ef463701d5e8d232502a230d10aa34dd6abe00,TODO: For dense; cast to Conv1D size e.g. (1; out_features).,https://github.com/Aifred-Health/Vulcan/commit/c2ef463701d5e8d232502a230d10aa34dd6abe00,Yes
5659,Aifred-Health/Vulcan,vulcanai2/models/cnn.py,8929ca605b5b2147cf1ece34e7078260e449d17c,TODO: For dense; cast to Conv1D size e.g. (1; out_features).,https://github.com/Aifred-Health/Vulcan/commit/8929ca605b5b2147cf1ece34e7078260e449d17c,Yes
5660,dkpro/dkpro-cassis,cassis/xmi.py,b44ff9154c057119e28e11327e3a78d9863c40fd,TODO: Parse feature values to their real type here; e.g. parse ints or floats,https://github.com/dkpro/dkpro-cassis/commit/b44ff9154c057119e28e11327e3a78d9863c40fd,Yes
5661,nltk/nltk,nltk/featstruct.py,1c976cdd388634b4c3cfb24110083d35a21837ff,"\""\""\"" || Basic data classes for representing feature structures.  A X{feature || structure} is a mapping from feature names to feature values; where: ||  ||   - Each X{feature name} is a case sensitive string. ||   - Each X{feature value} can be a base value (such as a string); a ||     variable; or a nested feature structure. ||  || Feature structures are typically used to represent partial information || about objects.  A feature name that is not mapped to a value stands || for a feature whose value is unknown (I{not} a feature without a || value).  Two feature structures that represent (potentially || overlapping) information about the same object can be combined by || X{unification}.  When two inconsistent feature structures are unified; || the unification fails and returns C{None}. ||  || Features can be specified using X{feature paths}; or tuples of feature || names that specify path through the nested feature structures to a || value.  Feature structures may contain reentrant feature values.  A || X{reentrant feature value} is a single feature value that can be || accessed via multiple feature paths.  Unification preserves the || reentrance relations imposed by both of the unified feature || structures.  In the feature structure resulting from unification; any || modifications to a reentrant feature value will be visible using any || of its feature paths.  Feature structures may also contain X{cyclic || feature values}; i.e.; values that recursively contain themself. ||  || Feature structure variables are encoded using the L{nltk.sem.Variable} || class.  The variables' values are tracked using a X{bindings} || dictionary; which maps variables to their values.  When two feature || structures are unified; a fresh bindings dictionary is created to || track their values; and before unification completes; all bound || variables are replaced by their values.  Thus; the bindings || dictionaries are usually strictly internal to the unification process. || However; it is possible to track the bindings of variables if you || choose to; by supplying your own initial bindings dictionary to the || L{unify() <FeatStruct.unify>} method. ||  || When unbound variables are unified with one another; they become || X{aliased}.  This is encoded by binding one variable to the other. ||  || @todo: add a fail parameter to unify?  This would be a function that ||    would be called if unificaiton fails; it could either raise a ||    UnificationFailure error; or return a value.  How would this be ||    useful?  Well; one example is that it could be used to find a ||    \""diff\"" between two feature structures -- i.e.; a list of all ||    feature paths with different values.  Anyway; the old version had ||    it.  Ask steven why it was introduced? ||  || @todo: Figure out yaml support.  Do we need any? ||  || @todo: support for mutable feature structures? ||  || @todo: define __div__ for feature structures? ||  || relative to category; we don't define... ||   - .symbol (we're not a Nonterminal) ||   - .head() ||   - .feature_names(); .has_features() -- eh ||   - .to_yaml() and .from_yaml() ||   - parsing of cfgs.. ||  || \""\""\""",https://github.com/nltk/nltk/commit/1c976cdd388634b4c3cfb24110083d35a21837ff,No
5662,nltk/nltk,nltk/grammar.py,3a1d0bf9a689762bdeb08b7de3433453ef238cdb,TODO: Add FeatureProduction; with a better def. of __str__:,https://github.com/nltk/nltk/commit/3a1d0bf9a689762bdeb08b7de3433453ef238cdb,Yes
5663,nltk/nltk,nltk/parse/transitionparser.py,8f9b23024f7caf136a03407c21d8edb0b48e7888,Todo : can come up with more complicated features set for better performance.,https://github.com/nltk/nltk/commit/8f9b23024f7caf136a03407c21d8edb0b48e7888,Yes
5664,nltk/nltk,nltk/parse/transitionparser.py,ccd46eef72aec101f90c48eafbd57880057289ce,Todo : can come up with more complicated features set for better,https://github.com/nltk/nltk/commit/ccd46eef72aec101f90c48eafbd57880057289ce,No
5665,Cysu/open-reid,reid/features/extract.py,4ef0c32bf724c52a08bcbd6c9e5fa14a1aab4445,TODO: Support extract features from multiple layers,https://github.com/Cysu/open-reid/commit/4ef0c32bf724c52a08bcbd6c9e5fa14a1aab4445,Yes
5666,yzhao062/pyod,pyod/models/feat_bagging.py,fec5e567f52fec1ec897431c3108c290746dcd10,TODO add a check for min_features; e.g. d<=3 & max_features as well,https://github.com/yzhao062/pyod/commit/fec5e567f52fec1ec897431c3108c290746dcd10,Yes
5667,yzhao062/pyod,pyod/test/test_feature_bagging.py,5073a2833f973c92485a0492b111e002bc82779f,TODO: failed due to sklearn uses 2 feature examples.,https://github.com/yzhao062/pyod/commit/5073a2833f973c92485a0492b111e002bc82779f,Yes
5668,yzhao062/pyod,pyod/test/test_lscp.py,9f784a04364e67eaf87a499557945b0f021f7521,TODO: failed due to sklearn uses 2 feature examples.,https://github.com/yzhao062/pyod/commit/9f784a04364e67eaf87a499557945b0f021f7521,Yes
5669,chrislit/abydos,abydos/phones/_phones.py,16315cb9a30f298ff9980533da62ba822a47b3e4,TODO: formulate this on the basis of _FEATURE_MASK,https://github.com/chrislit/abydos/commit/16315cb9a30f298ff9980533da62ba822a47b3e4,Yes
5670,prihoda/golem,golem/core/dialog_manager.py,d1411ec4634cb76964f54da38e1ad8d0fd9c4edd,TODO document this feature or remove it,https://github.com/prihoda/golem/commit/d1411ec4634cb76964f54da38e1ad8d0fd9c4edd,Yes
5671,raamana/neuropredict,psy.py,aa49d1b44948e726ddc76be51d9dff4bcb3384df,TODO perhaps I can have two arguments: one to specify feature type (which determines the reader); and another,https://github.com/raamana/neuropredict/commit/aa49d1b44948e726ddc76be51d9dff4bcb3384df,Yes
5672,raamana/neuropredict,rhst.py,cbb7ecb70c7cb5ad2abfa3eb8130ad021734fa8c,TODO generate visualizations for each feature set as well as a comparative summary!,https://github.com/raamana/neuropredict/commit/cbb7ecb70c7cb5ad2abfa3eb8130ad021734fa8c,Yes
5673,raamana/neuropredict,neuropredict/neuropredict.py,ea8604b6967062a2d60bb8aa2f3d633bb0265890,TODO perhaps I can have two arguments: one to specify feature type (which determines the reader); and another,https://github.com/raamana/neuropredict/commit/ea8604b6967062a2d60bb8aa2f3d633bb0265890,Yes
5674,raamana/neuropredict,neuropredict/rhst.py,ea8604b6967062a2d60bb8aa2f3d633bb0265890,TODO generate visualizations for each feature set as well as a comparative summary!,https://github.com/raamana/neuropredict/commit/ea8604b6967062a2d60bb8aa2f3d633bb0265890,Yes
5675,raamana/neuropredict,neuropredict/neuropredict.py,8d914209eed9ccd5dc16ed462e217e3a6403054a,TODO need to devise a way to avoid re-reading all the features from scratch every time.,https://github.com/raamana/neuropredict/commit/8d914209eed9ccd5dc16ed462e217e3a6403054a,Yes
5676,raamana/neuropredict,neuropredict/rhst.py,394630b87280b02710f03a5f01eadc360b58bee0,TODO if feature names are implemented; save them too,https://github.com/raamana/neuropredict/commit/394630b87280b02710f03a5f01eadc360b58bee0,Yes
5677,raamana/neuropredict,neuropredict/rhst.py,faa4f6868f3e19acc82c1d44eb7e07e1c385f4a5,TODO to achieve feature- or method-level parallization;,https://github.com/raamana/neuropredict/commit/faa4f6868f3e19acc82c1d44eb7e07e1c385f4a5,No
5678,raamana/neuropredict,neuropredict/rhst.py,57e805dee4693987b90bb3fc3ef1923dff73de01,TODO new feature: add additional metrics such as PPV,https://github.com/raamana/neuropredict/commit/57e805dee4693987b90bb3fc3ef1923dff73de01,Yes
5679,raamana/neuropredict,neuropredict/rhst.py,7f3025cb06c10160f84807fa9c7df7748e80ae9a,TODO optimize the num features to select as part of grid search,https://github.com/raamana/neuropredict/commit/7f3025cb06c10160f84807fa9c7df7748e80ae9a,Yes
5680,raamana/neuropredict,neuropredict/algorithms.py,d2a9d7cd2c7f0bc4789e3923ebf7a296060688fd,TODO optimize the num features to select as part of grid search,https://github.com/raamana/neuropredict/commit/d2a9d7cd2c7f0bc4789e3923ebf7a296060688fd,Yes
5681,omimo/PyMO,pymo/preprocessing.py,e1a5b1e261525394fe073af6462023aba5e0fac9,TODO: DynamicFeaturesAdder,https://github.com/omimo/PyMO/commit/e1a5b1e261525394fe073af6462023aba5e0fac9,Yes
5682,omimo/PyMO,pymo/preprocessing.py,e1a5b1e261525394fe073af6462023aba5e0fac9,TODO: ShapeFeaturesAdder,https://github.com/omimo/PyMO/commit/e1a5b1e261525394fe073af6462023aba5e0fac9,Yes
5683,PyTorchLightning/pytorch-lightning,pytorch_lightning/plugins/training_type/deepspeed.py,1302766f83a5fc861ae77ef6f8798cd0b1d606c7,todo (tchaton) awaiting this feature to move upstream to DeepSpeed,https://github.com/PyTorchLightning/pytorch-lightning/commit/1302766f83a5fc861ae77ef6f8798cd0b1d606c7,Yes
5684,ylongqi/openrec,openrec/recommenders/drr.py,1b1a390d5a9394aee42fd6622089aebeb71082e9,#TODO: add user features and concat here; use usergraph.extend,https://github.com/ylongqi/openrec/commit/1b1a390d5a9394aee42fd6622089aebeb71082e9,Yes
5685,JohnVinyard/zounds,model/frame.py,3e141751900cbf9ac8d058fe221b7bec8f3a7489,TODO: MultiFeature class (like for minhash features),https://github.com/JohnVinyard/zounds/commit/3e141751900cbf9ac8d058fe221b7bec8f3a7489,Yes
5686,JohnVinyard/zounds,model/frame.py,3e141751900cbf9ac8d058fe221b7bec8f3a7489,TODO: Grabs features; builds an Extractor chain; can check db for,https://github.com/JohnVinyard/zounds/commit/3e141751900cbf9ac8d058fe221b7bec8f3a7489,Yes
5687,JohnVinyard/zounds,model/frame.py,158313f920fd36272f59e6b8b6ee3ace7836d1f8,TODO: MultiFeature class (like for minhash features),https://github.com/JohnVinyard/zounds/commit/158313f920fd36272f59e6b8b6ee3ace7836d1f8,No
5688,JohnVinyard/zounds,model/frame.py,b26ca47e6b3b212dde66668b35419bef5588341d,TODO: Lazily compute unstored features when requested,https://github.com/JohnVinyard/zounds/commit/b26ca47e6b3b212dde66668b35419bef5588341d,Yes
5689,JohnVinyard/zounds,model/framesearch.py,db0ad8902f5e009e598dea387ece8b29d6556d94,TODO: ExtractorChain.prune() that can take multiple features and tests,https://github.com/JohnVinyard/zounds/commit/db0ad8902f5e009e598dea387ece8b29d6556d94,Yes
5690,JohnVinyard/zounds,learn/fetch.py,d659c56595eb0844950f7be447c4e2eadbbe708c,TODO: return every frame of this feature shuffled,https://github.com/JohnVinyard/zounds/commit/d659c56595eb0844950f7be447c4e2eadbbe708c,Yes
5691,JohnVinyard/zounds,zounds/model/framesearch.py,e73815c14a690c44a235ed4981a83cc83648671e,TODO: tf\/idf on features,https://github.com/JohnVinyard/zounds/commit/e73815c14a690c44a235ed4981a83cc83648671e,Yes
5692,JohnVinyard/zounds,zounds/model/framesearch.py,65e09c540712f00df9af5300b270376a6887777b,TODO: nbits could be inferred from the feature,https://github.com/JohnVinyard/zounds/commit/65e09c540712f00df9af5300b270376a6887777b,Yes
5693,JohnVinyard/zounds,zounds/analyze/feature/pitch.py,f99980309d0d0243d29532d88c0b29d3407e290b,TODO: Should I just expect input from the AutoCorrelation feature?,https://github.com/JohnVinyard/zounds/commit/f99980309d0d0243d29532d88c0b29d3407e290b,No
5694,JohnVinyard/zounds,zounds/model/framesearch.py,c00c069bffe5c1c8f1e9542d302321e46b75ad06,TODO: nbits could be inferred from the feature,https://github.com/JohnVinyard/zounds/commit/c00c069bffe5c1c8f1e9542d302321e46b75ad06,Yes
5695,JohnVinyard/zounds,zounds/node/data.py,2951fa5e3300ab0680e6e5f05122491cdceaa6db,TODO: Should I do a sub-database per feature; or organize the keys as,https://github.com/JohnVinyard/zounds/commit/2951fa5e3300ab0680e6e5f05122491cdceaa6db,No
5696,ryfeus/gcf-packs,tensorflow2.0/source/tensorflow/python/keras/engine/training_utils.py,44d36f9e5045fd859ee188d74dc31f3921ad4796,TODO(rohanj): This is a hack to get around not depending on feature_column and,https://github.com/ryfeus/gcf-packs/commit/44d36f9e5045fd859ee188d74dc31f3921ad4796,Yes
5697,jfilter/text-classification-keras,examples/fasttext_imdb.py,6f02a924513dc4277660302228b41adb2d0cf791,FIXME: NOT WORKING. PLEASE FIX ME. There seem to be something wrong with the n-gram features.,https://github.com/jfilter/text-classification-keras/commit/6f02a924513dc4277660302228b41adb2d0cf791,Yes
5698,scikit-learn/scikit-learn,examples/linear_model/logistic_l1_l2_sparsity.py,298259070ecc08f30c695bb5c6bb32e2c8d398a1,FIXME: the iris dataset has only 4 features!,https://github.com/scikit-learn/scikit-learn/commit/298259070ecc08f30c695bb5c6bb32e2c8d398a1,Yes
5699,scikit-learn/scikit-learn,sklearn/linear_model/tests/test_ridge.py,cd2ee7e454f9bedceb04ba8613a4e52d1d9a76bf,TODO: test also n_samples > n_features,https://github.com/scikit-learn/scikit-learn/commit/cd2ee7e454f9bedceb04ba8613a4e52d1d9a76bf,Yes
5700,scikit-learn/scikit-learn,sklearn/datasets/openml.py,ab82f5739f98a7ea18e8f8220506638a240ebce4,TODO: feature request OpenML.,https://github.com/scikit-learn/scikit-learn/commit/ab82f5739f98a7ea18e8f8220506638a240ebce4,Yes
5701,scikit-learn/scikit-learn,sklearn/datasets/tests/test_openml.py,ab82f5739f98a7ea18e8f8220506638a240ebce4,TODO: pass in a list of expected nominal features,https://github.com/scikit-learn/scikit-learn/commit/ab82f5739f98a7ea18e8f8220506638a240ebce4,Yes
5702,scikit-learn/scikit-learn,sklearn/compose/_column_transformer.py,da6614fe4211c0bb85200b65b01a2953cf1154b0,TODO: this should be `feature_names_in_` when we start having it,https://github.com/scikit-learn/scikit-learn/commit/da6614fe4211c0bb85200b65b01a2953cf1154b0,Yes
5703,scikit-learn/scikit-learn,sklearn/tests/test_pipeline.py,16f4ac90f0732988e3b7efe0c937eaff70e99692,TODO: Remove parametrization in 0.24 when None is removed for FeatureUnion,https://github.com/scikit-learn/scikit-learn/commit/16f4ac90f0732988e3b7efe0c937eaff70e99692,Yes
5704,scikit-learn/scikit-learn,sklearn/utils/__init__.py,4ce39dbc699cd9cd33ab2b67ca029698713aec3e,TODO: Remove when FeatureHasher is implemented in PYPY,https://github.com/scikit-learn/scikit-learn/commit/4ce39dbc699cd9cd33ab2b67ca029698713aec3e,Yes
5705,scikit-learn/scikit-learn,sklearn/compose/_column_transformer.py,d205638475ca542dc46862652e3bb0be663a8eac,TODO: also call _check_n_features(reset=False) in 0.24,https://github.com/scikit-learn/scikit-learn/commit/d205638475ca542dc46862652e3bb0be663a8eac,Yes
5706,scikit-learn/scikit-learn,sklearn/ensemble/_hist_gradient_boosting/binning.py,b4453f126f34447967f52996039d11b0d2fa0090,TODO: complexity is O(n_categorical_features * 255). Maybe this is,https://github.com/scikit-learn/scikit-learn/commit/b4453f126f34447967f52996039d11b0d2fa0090,No
5707,scikit-learn/scikit-learn,sklearn/preprocessing/tests/test_polynomial.py,27f1c737f2cfc6e589c292776df6f78c08a4da91,TODO: add PolynomialFeatures if it moves to _polynomial.py,https://github.com/scikit-learn/scikit-learn/commit/27f1c737f2cfc6e589c292776df6f78c08a4da91,Yes
5708,biolab/orange3,Orange/__init__.py,5cf6b70c5115524f48b8f3eb9a2ce2632ebe4933,"\""\""\"" || _import(\""data.sample\"") || _import(\""data.outliers\"") || _import(\""data.preprocess\"") || _import(\""data.preprocess.scaling\"") || _import(\""data.utils\"") || _import(\""data.discretization\"") || _import(\""data.continuization\"") || _import(\""data.filter\"") || _import(\""data.imputation\"") ||  || _import(\""feature\"") || _import(\""feature.construction\"") || _import(\""feature.construction.functionDecomposition\"") || _import(\""feature.construction.univariate\"") || _import(\""feature.discretization\"") || _import(\""feature.imputation\"") || _import(\""feature.scoring\"") || _import(\""feature.selection\"") ||  || _import(\""network\"") ||  || _import(\""stat\"") ||  || _import(\""statistics\"") || _import(\""statistics.estimate\"") || _import(\""statistics.contingency\"") || _import(\""statistics.distribution\"") || _import(\""statistics.basic\"") || _import(\""statistics.evd\"") ||  || _import(\""classification\"") || _import(\""classification.tree\"") ||  || _import(\""classification.rules\"") ||  || _import(\""classification.lookup\"") || _import(\""classification.bayes\"") || _import(\""classification.svm\"") || _import(\""classification.logreg\"") || _import(\""classification.knn\"") || _import(\""classification.majority\"") ||  || _import(\""tuning\"") ||  || _import(\""projection\"") || _import(\""projection.linear\"") || _import(\""projection.mds\"") || _import(\""projection.som\"") ||  || _import(\""ensemble\"") || _import(\""ensemble.bagging\"") || _import(\""ensemble.boosting\"") || _import(\""ensemble.forest\"") || _import(\""ensemble.stacking\"") ||  || _import(\""regression\"") || _import(\""regression.base\"") || _import(\""regression.earth\"") || _import(\""regression.lasso\"") || _import(\""regression.linear\"") || _import(\""regression.mean\"") || _import(\""regression.pls\"") || _import(\""regression.tree\"") ||  || _import(\""multitarget\"") || _import(\""multitarget.tree\"") ||  || _import(\""multilabel\"") || _import(\""multilabel.multibase\"") || _import(\""multilabel.br\"") || _import(\""multilabel.lp\"") || _import(\""multilabel.mlknn\"") || _import(\""multilabel.brknn\"") || _import(\""multilabel.mulan\"") ||  || _import(\""associate\"") ||  || _import(\""distance\"") ||  || _import(\""wrappers\"") ||  || _import(\""featureConstruction\"") || _import(\""featureConstruction.univariate\"") || _import(\""featureConstruction.functionDecomposition\"") ||  || _import(\""evaluation\"") || _import(\""evaluation.scoring\"") || _import(\""evaluation.testing\"") ||  || _import(\""clustering\"") || _import(\""clustering.kmeans\"") || _import(\""clustering.hierarchical\"") || _import(\""clustering.consensus\"") ||  || _import(\""misc\"") ||  || _import(\""utils\"") #TODO hide utils from the user || _import(\""utils.environ\"") || _import(\""utils.counters\"") || _import(\""utils.addons\"") || _import(\""utils.render\"") || _import(\""utils.serverfiles\"") ||  || _import_addons() || \""\""\""",https://github.com/biolab/orange3/commit/5cf6b70c5115524f48b8f3eb9a2ce2632ebe4933,No
5709,dmlc/dgl,tutorials/graph.py,797b20792f67d4817e34bf259f008ca127e77b89,"\""\""\"" || Use DGLGraph || ============ ||  || In this tutorial; we introduce how to use our graph class -- ``DGLGraph``. || The ``DGLGraph`` is the very core data structure in our library. It provides the basic || interfaces to manipulate graph structure; set\/get node\/edge features and convert || from\/to many other graph formats. You can also perform computation on the graph || using our message passing APIs. (TODO: give a link here to the message passing doc) || \""\""\""",https://github.com/dmlc/dgl/commit/797b20792f67d4817e34bf259f008ca127e77b89,No
5710,dmlc/dgl,tools/partition.py,0e153c4baf3394ca4043896b0195737c81ed8623,TODO I duplicate some node features.,https://github.com/dmlc/dgl/commit/0e153c4baf3394ca4043896b0195737c81ed8623,Yes
5711,OpenMined/PySyft,syft/frameworks/torch/optim/sgd.py,165a90d5faf06426f8ea266fcdba45bdba2eb523,TODO: all all the SGD features from PyTorch's SGD,https://github.com/OpenMined/PySyft/commit/165a90d5faf06426f8ea266fcdba45bdba2eb523,No
5712,OpenMined/PySyft,syft/generic/pointers/pointer_plan.py,8847d8176fa1b26222664d2ce9d57db247700caf,TODO implement this feature using MultiPointerTensor,https://github.com/OpenMined/PySyft/commit/8847d8176fa1b26222664d2ce9d57db247700caf,Yes
5713,chainer/chainer,chainer/datasets/pickle_dataset.py,09252fb964a7159156b3975033c46835717841ae,TODO: Avoid using undocumented feature,https://github.com/chainer/chainer/commit/09252fb964a7159156b3975033c46835717841ae,Yes
5714,chainer/chainer,chainer/datasets/pickle_dataset.py,cd8291bd7fe6bcac16167a4fe0d128932d46175d,TODO: Avoid using undocumented feature,https://github.com/chainer/chainer/commit/cd8291bd7fe6bcac16167a4fe0d128932d46175d,Yes
5715,chainer/chainer,examples/static_graph_optimizations/test_static_graph.py,4b9cbf779b6918b8f5710aeb7965ede6e9eb1864,todo: Optimizers do not yet support static graph feature.,https://github.com/chainer/chainer/commit/4b9cbf779b6918b8f5710aeb7965ede6e9eb1864,Yes
